It's kind of amusing knowing how almost universally terrible Printers are, to think that one got so annoying it sent a guy off on starting an entire movement.
A lot of computing history in general started at xerox
That‚Äôs profound
So, now we know why programmers inherently hate to help troubleshoot printer problems of family members and friends.
Thanks for sharing, a great read!
    lp0 on fire
as ex desktop IT, printers starting the FSM makes perfect natural sense. of course a broken printer induced such a long-lasting and passionate movement dedicated in part to making printers less awful or necessary.
Ballmer rolling in his bed thinking what could've been if printers weren't so awful (until surprisingly recently).
It really speaks to Stallman's genius to have this sort brush encounter with the printer in the AI Lab, and to then reason through the implications it has on software development generally, and finally to see it through to a concise articulation (the GPL) which immediately empowers anyone who not only wishes to participate in free software and free information, but also safeguards their own contributions to the world.
Not only are printers devices from hell, but they served as a warning as to what the technology landscape would eventually turn in to when late-stage capitalism did it's thing.
well now i don't know how to feel
Great read üëçüëç
Good book in general
Another fun technology origin story is that the first webcam was to see how full an office coffee pot was.
Printers still don‚Äôt work on my GNU/Linux system.
So  not the usual suspects (Unix,windows,Apple).
I wonder how much xerox makes use of open source software in todays markets.
Is it just me or this reads as a wattpad fanfic?
This article is based on the false premise that stallman started the movement.   It existed long before stallman. he actually derailed it with his copy left bullshit.
They still are!

Ink is so ridiculously expensive ...
https://www.youtube.com/watch?v=N9wsjroVlu8

Still relevant
Haha, so true.
I've never had a problem with printers on Linux.
This article does not claim that Stallman came up with the idea of sharing source code. In fact it is very explicit about sharing source code being something that was widespread well before Stallman first came up with his ideology.
Would you mind sharing some alternative articles that go through the start of the movement? I wasn't born until a decade later, it's hard for me to judge what started it.
What article?
Just get a cheap laser printer. Costs more up front, but toner doesn't dry out, lasts for a long time and doesn't cost more than the damn printer itself.
There *was* a period when a class of cheap ["winprinters"](https://en.wikipedia.org/wiki/Graphics_Device_Interface#GDI_printers) (that did most of processing on the computer and generally entirely assumed the host was microsoft windows) wouldn't work well at all, basically due to lack of non-windows drivers as manufacturers sure didn't give a fuck about anything other than windows (and at that sometimes only 9x/me windos not nt/modern-windows).  But they generally sucked (still kinda do in an abstract technical sense but processor power and driver support is such it doesn't matter as much nowadays).   

If you were always buying higher-end relatively *good* printers (i.e. postscript laser printers) you'd pretty much never have a problem even way back - as firstly they usually spoke either postscript or pcl or both anyway, which tended to always work pretty fine with native unix/linux printer stacks anyway, even before CUPS with lpd etc., and secondly driver authors would tend to be buying and working on drivers for printers they themselves actually wanted i.e. not terrible inkjet winprinters.
I‚Äôm my limited research, the cheap lasers have very expensive toner. They have to fleece you at one end.
Does anyone here have a rundown of the amortised cost of ownership of a 150-200$ laser printer ?
can confirm. i got the cheapest bw laser printer i could find for 40eur new and after 5 years i still have yet to exchange the initial demo toner that was supposed to last 500 pages... yeah, it lasts much more than that.
All laser printers have expensive toner.

The difference is that a cartridge is good for 5,000 sheets as opposed to 250.  And it doesn't dry out and get gummed up.
I got mine probably three years ago now. Have only bought toner once (it came with a small capacity toner thing to start). $75 or so every 2-3 years isn‚Äôt the end of the world. And the whole not having to deal with inkjet thing makes it way worth it when I actually want to use it.
Brother has fairly affordable toners for their low-end lasers (99pln for mine, so $22 roughly) that last way longer than the same price worth of ink, and there are plenty of cheap "counterfeit" ones since Brother doesn't seem to pull any of the nasty printer DRM shit. Their Linux drivers are unfortunately proprietary blobs, but there are open-source ones that work okay-ish.
i buy compatible toners. Half the price or less. The quality varies depending on the manufacturer.
The toner cartridges are expensive per purchase, but they tend to also print a fuckton of pages compared to inkjet cartridges.  And especially considering they also don't dry out, so you can actually fully use them.

Are you comparing the cost per-page?
I bought a $100 brother 12 years ago and I've only bought toner once for about $65 after the starter toner wore out. I rarely print but it's nice having it as an option.
Bought a Brother 2340DW 6 years ago, toner once (aftermarket, $50 maybe) so‚Ä¶ less than $10/year?
I bought a laser printer 3 years ago. Color even. It's still on the initial toner cartridge that came with the printer. 

Laser printers are worth it.
To save money on toner, i've used the same cartridge 3X past it's rated lifetime, by resetting the page counter in the printer. it just kept on printing with minimal quality differences.
Do the proprietary drivers at least work well? It‚Äôs ironic I‚Äôm asking that in a post about proprietary drivers starting the free software movement.
My little hp LaserJet 1018 takes toners you can get for < $25 USD / each on Amazon.
Are you refilling your toner cartridges? I can't look at them without making a mess.
I have no idea, the drivers for mine only come in 32-bit variety and only as deb or rpm, i haven't bothered with them. [Brlaser](https://github.com/pdewacht/brlaser) works okay though.
Yeah Brother has great drivers, no issues in Linux. They‚Äôre very simple.

I‚Äôve had a 2340DW for like 6 years going strong. Only had to buy toner once so far. Love the thing. Wireless works great so it just sits there and anyone can print to it. Solved all my printing woes.
My luck with the proprietary drivers seems to be worse than most others. The open source drivers have come a long way lately though.
I don't have much time for gaming anymore but played veloren a bit and absolutely loved it. Great job guys.
Reminds me superficially of Cubeworld.
Gonna check this out.
W8 a fucking moment I know this, it‚Äôs the game Mental Outlaw played for 44:00 minutes! Damn it looks even better now
That was the initial inspiration
Yup, a lot has changed since their video!
Following a popular GRUB Theme post on this sub, I noticed a lot of people in the comments found it difficult to search for good GRUB Themes. Pling is the only site with a decent amount of them, but the majority are massively outdated or extremely basic (Just a background without any theming on the actual GRUB components). So I dug through the entire GRUB section of Pling, r/UnixPorn, and even some french tech forum, to put together this collection of decent themes and make it easier for users to browse them.

Any contributions, be it themes you've made yourself or just found elsewhere, are welcome!
This is excellent, thanks for putting this together.
Cool thanks, will have to theme my grub today
Thanks, I've been looking for something like this for months!
> but the majority are massively outdated ... Just a background  
  
My biggest annoyance with Pling. Sooo many themes that are just someone else's stuff with a crappy background on it  
Thank you for taking your time to filter through all that
Have you tried https://store.kde.org/browse?cat=109&ord=latest ?
No problem! While I am accepting additional themes to be added by other users, I do also want to try and keep things mostly unique on the page too: So it doesn't run into the same problem as Pling once again.

Currently that's just upheld by "Change most of the defaults" and a vague "If you fork a theme, make it sufficiently different" rules but hopefully I'll be able to set something more concrete in future.
I did and ran into the same problems OP did. store.kde.org is just an alternate link to Pling. Same with store.gnome.org.
GitHub: [https://github.com/yaxollum/diagonator](https://github.com/yaxollum/diagonator)

Details: diagonator is an X11 compositor that draws diagonal lines across your screen to make you take a break from your computer. It is a fork of [xcompmgr](https://gitlab.freedesktop.org/xorg/app/xcompmgr), so in addition to diagonal lines, it supports the same special effects as xcompmgr.

I designed diagonator to be controlled by another application/shell script. A basic example of this would be a shell script that:
1. Sleeps for 25 minutes (for a 25 minute work period)
2. Runs `timeout 5m ./diagonator` (for a 5 minute break). This `timeout` command starts `diagonator` and then kills it after 5 minutes.
3. Goes back to step 1
App name by Doofenshmirtz.
\[ me reading between the lines anyway \]
Everything is fine but I have to ask something. Why?
For some reason this makes it look like the screen behind it is tilted.
Is there a way to make it only last for a minute instead of 5 minutes?
\>killall diagonator

&#x200B;

&#x200B;

and continue working....
Does it work? I mean do you take your breaks with it?
You won't. You'll try even harder to work with it.

If you want to take a break, [code yourself a talking clippy](https://youtu.be/3G_uCbKoG5A)
Neat project. Will it work in xwayland or only a native session?
could you add flags to change the thickness and amount of lines if I find that they aren't horribly distracting?
my laps display does that for me. 
It forces me to take a break everytime. 
Those lines won't go away unless I replace the display. XD
d'fuck .. i need a break from the physical world not from Linux
that's pretty damn cool
Of all the force-you-to-disengage apps this seems like the only one I've seen that would actually work. Everything else I can just turn off immediately without care but this is properly jarring and stops me visually processing the background.
Not sure where you work or what you do for work. I work for hours on end in a tech job that‚Äôs highly demanding. There are days I have back to back meetings where I literally work 8-9hours straight. The days I‚Äôm coding I get lost in my thoughts and then it‚Äôs midnight. I think I‚Äôd be upset with a forced break. Anyone else? Not a bad idea just not sure it would work for everyone.
TIHI. I don't want some silly computer program to tell me I need to take a break. I work at my own pace and I take a break whenever the current flow stops.    
     
Edit: Hmm, quite some negative vibe here. It's not that I am working for hours and hours. But when I'm writing code and good ideas keep flowing to my keyboard I want to go on. And I definitely don't want some tool to block my keyboard or screen because I should have stopped one minute before. I take plenty of breaks. Coming up with creative solutions is done by getting some coffee or even while taking a shower.
I can't tell if it's because of the lines, but that browser window looks crooked
Is there an emergency shortcut to remove the lines if you are doing something extremely important?
Looks interesting. I've been using https://apps.kde.org/en-gb/rsibreak/
That would annoy the hell out of me.
So what do you do for the 5 minutes? What happens? I have genuinely no idea what I'd spend it on.
It's cool to have something like this! I liek XFCE extensions that has a very similar functionality (Time Out plugin I believe it's called). Do you have plans to port it to Windows? I've had to use Windows for a while and didn't find anything that worked this way.
üé∂Doofenshmirtz Evil Software Incorporated üé∂
diagonator-inator
Hello! I wanted something that could force me to take a break from my computer but still allow me to quickly save my work/clean up what I'm doing.

Right now, I'm also working on a time management application that could control `diagonator` and schedule when to draw/remove the diagonal lines.
Its a feature, not a bug.
Sitting like a prawn for 5h makes my spine unhappy. This is a handy prompt to stand up occasionally haha
Hi! I think I could have been a bit more clear about this in my comment about the shell script thing. `diagonator` by itself doesn't have a timer to control how long it lasts (it stays on forever unless you kill it). If you wanted to make it last for one minute, you could use the `timeout` command:
```
timeout 1m ./diagonator
```
(I'll update my comment with this info)
Wow really? Is it really that simple? How did you even figure that out, you must be a really special boy.
Well it irritates you enough where you walk away. This is also a good troll install it on your Freinds PC and watch...
I haven't tried running it on XWayland, but I don't think it would work since Wayland doesn't support modular compositors the same way that X11 does (see [https://www.reddit.com/r/linux/comments/lx7rgw/x\_vs\_wayland\_compositors/](https://www.reddit.com/r/linux/comments/lx7rgw/x_vs_wayland_compositors/)). In order for diagonator to work on Wayland, it would probably have to be built as a plugin for another compositor.
Yeah, I'm planning to do that. For now, those options can be configured by modifying [https://github.com/yaxollum/diagonator/blob/main/options.h](https://github.com/yaxollum/diagonator/blob/main/options.h) and rebuilding the program.
Breaks should be mandatory. You need to refocus your retina, move your neck, move your back, stretch legs, drink water, ... There is physical health and mental health and if you and your body are not balanced, bad things happen in the end.  
I worked for couple of corporates and haven't seen one where IT employees wouldn't be able to sneek out and have a smoke once in a while. Smoking is obviously not good but at least they took break, relaxed and did the excercises even without noticing.
>  I think I‚Äôd be upset with a forced break.

You probably need a break, and don't realize it.  Do yourself a favor, and take breaks. Your mental health will thank you in the long run.
Great news - nobody is forcing you to install this!
>not sure it would work for everyone

I agree. Different people work in different ways and have different requirements. I'm currently a high school student on summer break, so thankfully there's pretty much nothing that I have to do right now!
And that's really bad for your health
ever heard of Pomodoro?
Apps like these do have an audience though. Personally I very much agree, it doesn't fit me at all, but it's still a neat thing that someone did so no need to shit on it.
Open a terminal and `killall diagonator`.
Stand up, walk around the house, stretch your legs, take a pee, take an armload of trash out of your office, fill up your water bottle, make yourself a coffee, step outside and try to see if you can spot a bird. 5 minutes go past really quickly.
Just use another laptop
Refill coffee.
Scroll on my phone
I'd probably keep working. This underestimates my capacity to absorb punishment.
Try this app... I don't remind well... I think it's called trik trok, something like that
I don't know much about graphics on Windows, so I'm not sure how this would be done on Windows (or whether it's even possible). I also use Windows sometimes, so it would be amazing if someone could port this to Windows!
he will become mayor of the fsf
Somehow EA is more evil...
Now you've got me singing the "Perry the Platypus" theme.

üé∂He's a semi aquatic  
Egg laying mammal of action... üé∂

Phineas and Ferb is one of my all-time favourites, I'm just re-watching every episode, about half way through now.
Just kidding mate. I meant as an average r/unixporn lover, I dont have a life so I wont take a break. Btw good work. Keep doing free and open sourced projects
Lol exactly. I humped because I rice my desktops 9 hours per day and I glad I can still see without glasses
Thank you so much!
What if i want the diagonals to appear in the other direction? Do you just call `rotanogaid` instead?
For friends: you make desktop screenshot, rotate screen 180, rotate screenshot 180 and make screenshot as desktop background. At first sight everything looks normal, but the moment you start moving mouse, all hell breaks loose.
This sounds crazy but breaks were easier when I worked from an office. Remote work has kind of made things more of a pain. There is no separation from work and home. On a positive note, I have a treadmill desk so I work standing or walking most of the day. Used to be on Fitbit and people asked me if I ran all day üòÇ. When I feel anxiety creeping up or that I‚Äôm clenching my jaw a lot I usually try to walk off. The jaw tension usually leads to a brutal headache. Thanks for your comment and sharing your thoughts.
Yeah I think you have a point. I‚Äôve been operating at this level for about 20 years. I do realize that it‚Äôs not healthy and some habits are really hard to break. Thanks for the tip. The reason I mentioned I‚Äôd be annoyed is because when I‚Äôm deep in though or working something I get frustrated when interrupted. Curious if that‚Äôs common in dev or problem solving roles?
Yeah totally understand that. No room for discussion here? Everyone uses gang mentality when someone points out the reality of what is modern day corporate work life.
Is that a citrus or a hat?
Hmmm. Could I request a shortcut function? üòÅ
I got myself a stretching rubber (or whatever it is called) and have set a timer for every 30 minutes. I'll just walk around and stretch. Never had any issues with a sore neck or stuff like that since then. Stretch, people! It improves your life.
I'd rather sniff limestone than think about that glowie app.
I don't have a windows machine handy anmyore, but it should be possible with e.g. AutoHotkey. You can create an always-on-top window with WS_EX_LAYERED (bitmap background with transparency) and WS_EX_TRANSPARENT (click-through) attributes and the  SetLayeredWindowAttributes function.
Thanks!
I'm planning to add command line flags for customizing the diagonals, but for now you can modify `DIAGONATOR_LINE_DIRECTION` in https://github.com/yaxollum/diagonator/blob/main/options.h to be an angle greater than 90 degrees.
Mix this in and it's pure hell my goal was to make it so you Think it's a bug
What worked for me: separate your work time and free time, even at home. Limit your work to 8h a day (if possible), more only if really really needed. Set hard limit when you finish your work day (17:00 or something). Separate your work and chill machine. If you are limited to 1 machine, have separate OSes. If you can't have separate OSes, have separate users with different themes and backgrounds.  
For break, move everything out of your reach - water, toilet, ... so you are forced to move of the PC.  
My current favourite life hack: I needed to fix the time when I move from office to home. So I made repeating meeting in calendar, every day, where it says something like: book beforehand if you want this slot. 50% don't even notice the message and just move the call to next day, the other 49% reads and are lazy to book the slot in advance so they move it to next day. Last 1 books there but I don't care, it is so rare...
It can be frustrating to having people constantly interrupting you when you're working on a problem.  Taking the actual break, no, not at all.  Breaks usually allow myself to step away and think about problem another way. 

From my experience it's only really common when your young, and a company can exploit you into that behavior. It's why google always wants fresh college kids. They'll put in the crazy "focus" to their detriment, because they don't know better.  When your older and have a family, you won't work all night trying to solve a problem and will want higher pay. So they always do the "too old" to hire thing.

I've also been doing this stuff for 20 years, and I've learned any extra effort I put in, I'll never be compensated for.   Working and never taking a lunch?  Over 20 years, that's around 216 days of unpaid work.   No all nighters, no after hours work. If a company demands it, I just find a better job. I don't live to work, work is just the thing I do to pay for the fun stuff.
Maybe take your anger at being exploited out on your boss, not a random subreddit.
it is the time-management technique that helps you work more productive and don't feel like a squeezed lemon at the end of a day. it makes you take a timed break in a loops after certain working time. for me it works great, I do a lot of brainwork and it really helps. you can read more about it on [Wikipedia](https://en.m.wikipedia.org/wiki/Pomodoro_Technique) or in more detail [here](https://m.youtube.com/watch?v=dQw4w9WgXcQ).

edit: there are multiple great apps to help you measure time. my fav for linux is Solanium (I use flatpak version). also there are alternatives for mobile.
It's actually "tomato", in italian.
"Stretching rubber" probably doesn't mean what I think it does, but it would never be able to pull it off once every 30 minutes
Yeah definitely not appreciated üòÇ.
There was nothing angry about my post. The only one getting upset over a comment is you. I‚Äôm asking if others share that element in their careers and perhaps how they manage their breaks.
Just for reference, [this](https://ae01.alicdn.com/kf/HTB1Na9bRFXXXXajXVXXq6xXFXXX4/1-2m-Natural-Elastic-Yoga-Resistance-Bands-Rubber-Body-Stretching-Belt-Pull-Strap-Gym-Exercise-Fitness.jpg) is what I mean.
You're a pretty shit communicator, you know that?
I‚Äôm crying now in the corner of my room. My life is over because a troll thinks I‚Äôm a bad communicator. I‚Äôve realized what a failure I am and my life has no meaning. Goodbye. Tell everyone that I love them.
Toodles!
Pretty sure you don't have a 60 foot turntable :D
Whoa I like how you used the amp, never occurred to me you can do that since I dont play guitars, which wire did you use?
Track ID?!  Thats a groove.
Is there someone moaning at the end of the video, or is my mind just too dirty?
nah im sure it is, that looks like a 75' tv
It's way above average
Hey, thanks, they are all 6.5 plug to a 3.5 for the rear mic in the pc
i think it's 'Love to love you baby'
It's the track but only a dirty mind would notice
I am glad to see improvements in displaying better feedback in discover. I feel one of the reasons people prefer using CLI package managers is because you have realtime feedback about what the thing is doing right now. Might not be a big deal for experienced users, but for people starting with linux having a good responsive app store is a huge upside
K e b a b
i feel like we are going to reach a point where we will see hello world programs licensed under gpl v3 lol
Why not acpi -t?
Try using it with an Arduino. My laptop (which is very old) gets very hot. So I programmed an Arduino to change the light of an LED depending on how hot the laptop is so that I can raise it. I forgot how I implemented that but I remember that there is an easier tool somewhere that came with the OS (Arch).
Was going to ask for screenshot, then realized its only command-line based.
Not bad. Good job
Thanks mate.
As someone who has absolutely no clue what RISC-V is, what is the use case for a laptop like this?
>With built-in PoS, NFT, and MetaMask-style wallet

Yeah, no thanks.
This is probably not going to happen.

Source: the cryptocurrency bullshit that consumes half of the announcement.
The pre order site says it's "web3 friendly" and the first 100 preorders come with an NFT. Sounds like bullshit.
> A Web3-friendly platform with NFT creation and publication plus integrated MetaMask-style wallet

No thanks
RISC-V? I wonder how well the machine is going to perform, considering that so far all the RISC-V machines I've seen were lagging behind even a Raspberry Pi 4 too much to be usable
pretty sure MNT's already announced work on a risc-v supporting mini-laptop sort of thing a few weeks ago, and with their track record with things like the reform i'd rather trust them on that regard, over anything cryptobros have anything to do with.
‚ÄúThinking Machine laptops? I‚Äôm talking about the 686 prototypes, with the artificial intelligence RISC chip.‚Äù
It is no HardwareShop here

PLS | NO ads.
Seems like a PR stunt, and a dumb one at that.
If you sign up, it just says they will contact you in the future. Didn't ask for money or anything....at least not yet.

It did ask to pick one of three packages. I don't remember the names but was $1k-$2k,$2k-$3k and $3k-$5k.

Also would not allow personal email...so no Gmail.
RISC-V is an ISA (Instruction Set Architecture), alternative to x86 and ARM.

RISC-V is currently primarily having market share in embedded, low cost and low power solutions. As far as I know, it's heavily being used in SSD controllers.

The laptop in question seems to be advertised as a "development platform", i.e. if you're targeting RISC-V hardware, you might as well use RISC-V for development, too. Ideally, this removes the need for an emulator or separate test platform, as you can just run binary code locally.

Now, the advertisement gets a bit weird when they mention Web3, NFT's, Metaverse etc, I don't know what's up with that. Maybe just marketing speak?
Cool points, mostly. RISC-V is the new hotness in architecture because it's an open ~~source~~ architecture.

Edit: wrong word
this one in particular it seems like they are focusing on it as a development laptop for riscv apps including deving embedded riscv.

With major features being:
 - Native RISC-V compile (very useful for testing on your local machine before pushing to another device)
 - "Supports most Linux variant operating systems"
 - Metamask style cyrpto wallet for testing crypto type applications

With plans to create an AI speaker setup (to showcase AI acceleration I assume), and AR style glasses for "metaverse" showcases.

So basically it marketed as a dev platform for getting into some of less mature tech stuff.

But from a practical standpoint on why RISCV, its modular nature makes it easier to work in a variety of use cases. With custom hardware being the real niche that it should excel into, but as with this case into general computing and potentially server systems as well. Bringing the possibility of bridging the gap from embedded, mobile, desktop, and servers, at least in this particular hurdle (the ISA) of portability.
Simplest way is, it is a new open source tech that will break the duopoly of ARM and X86. I'm especially happy that it's going to be a competitor to ARM, because x86 is dying anyway.
In case you would like an ELI5 video: https://www.youtube.com/watch?v=irH5eKzezsE
This is especially strange because most people trying to be an early-adopter of RISC-V will just install linux manually, and will never even see the bloatware. What is the target market here? The fact that they would even mention this in a press release is a sign of seriously misplaced priorities
Yeah this is a bad sign.   Instantly turned me off it.
NFT roughly translates to No Fucking Thanks.
I completely agree.

I've been using RISC-V boards since January 2017, and running Linux on one with slightly less than Pi 3 performance since early 2018 but this announcement has my bullshit alert going off at near maximum volume.

RISC-V has been gathering a \*lot\* of submerged momentum and is going to really break out in the next couple of years, but with all the attention it's only a matter of time until charlatans start to prey on the na√Øve and hopeful. I don't know whether this is like that or not, but it sure smells like it with their buzzword salad marketing.
I'm 99% confident there's no physical hardware with these, this will probably just be software using a standard secret store (like TPM ~~or ARM enclaves~~).

So the laptop itself will be free of crypto wank. I hope, anyway.

Edit: on the other hand I sort of understand agreeing to crypto product placement in exchange for some funding. The crypto crap is handled by LatticeX who are probably footing the bill to some extent.
I hate that a product with potential is killed by NFTs.
Yes, the title was so good and then this.

EDIT: I should say that I still can dream about things which _would_ be called something like "MetaVerse" or "cryptocurrency" somewhere in 1999. Which is exactly why this is so unpleasant.
lol, that's where I stopped looking at it!
Me before reading this very line : "Shiny new toy for me!"

Then, let's say they helped me hold on my rightly earned money.
Seems very underpowered laptop - more like a RivcV Chromebook.
They messed up the design of the chip so they want to make sure no one orders it before they can relese version 1.01.
As a software developer that works with blockchain, I can say that this whole web3/nft/metaverse shebang has absolutely fucking no place on my hardware or OS.
The early 2018 HiFive Unleashed was a little slower than a Pi 3.

Last year's HiFive Unmatched was kind of half way between Pi 3 and Pi 4 as long as you didn't need SIMD or crypto (SHA/AES).

Board that will be announced (and maybe even shipped) in the next six months look like they will leapfrog the Pi 4 and be around the new RK3588 boards that have just been coming out recently.

However that will still be without SIMD or crypto. Those instructions just got ratified in November last year so it will probably be well into 2023 before chips and boards are available with them.

M1-class chips are under development and will probably be available around 2024-2025. Will non-Apple ARM boards get there about the same time? Maybe.  Apple, of course isn't standing still.
You mean [this](https://mntre.com/media/reform_md/2022-06-20-introducing-mnt-pocket-reform.html)?

You can add an optional FPGA which could be loaded with a RISC-V core, not really what i would call a RISC-V laptop.
I used to administer thinking machines back in the day of super computers.
Some VC threw money at them to include crypto bloatware.
Interesting comment about the low power solution. Would this mean that the laptop being developed is going to be low power as well? PC power consumption has been on and off my mind lately.

It seems to me as the Web3 stuff is marketing speak. I think that if youre launching new architecture you need your niche, and targeting the tech savy NFT/Metaverse crowd is probably a good way to get a consumer base off the ground, as they are much more enthusiastic about new tech compared to other types of users who might as well get a laptop from best buy.
You forgot about IBM.  I was running hundreds of servers based on RISC-V for many years.  There is still a large market for it in certain areas.  But I hadn‚Äôt seen it in a laptop.
a vault  to safety storage, like you only use it to buy and sells stuff, not to work or gaming and internet, also as supgg0 said, dev on the hardware coded for
Ah, thats interesting! Thank you for the simple explanation.
It's open architecture, it's not necessarily open source. Companies can make silicon that implements the ISA without paying for licensing but they're not obligated to release their designs.
Since it's open source, could this be a potential future work-around for things like Pluton in case it ends up causing some sort of Linux-breaking fuckery? :0
I wouldnt say its dying but yeah with the M1 chip from Apple x86 has definitely taken a punch
Indeed.  The odd link to NFT and crypto moves this from "interesting technology" to a "likely scam".
Maybe they secured funding back when there was more hype around this stuff.
I thought that this is some kind of hardware wallet, just, eh, not external via USB. Still the conclusion is the same.
What if you do it softly and gently though?
Question: how is the power consumption on risc-v compared to ARM (ie, PI)? My issue isn't performance but my main problem is to do with power consumption, even the slightest Wh counts for me.
Still, companies do all sort of things to try to sell their stuff, some of it is pretty cringey (like intel "mac vs pc" spoof , still if a intel CPU will be better i will buy intel). 

I think you can just wait for the first products, read some reviews and then decide, you might say that this really makes you not want to pre order but honestly i think preorders are kind of a gamble anyway.
>ARM enclaves

enclaves != secret stores.

Enclaves run code in a secure context, secret stores store secrets
If you don't mind me asking, what was it like? Thinking Machines were well before my time, and were rare enough that there aren't nearly as many war stories floating around as old Unix.
Probably low power in that it compares with ARM chips on power consumption, hopefully not nearly as low power as an SSD controller. Looking at other RISC-V chips out there, I'd bet power consumption and performance look like Raspberry Pis, maybe a bit better on performance.

Then it's double-edged marketing speak, in that way. I know there are people who are excited about Web3 and NFTs, but most of the tech enthusiasts I know personally are somewhere between skeptical and actively put off by it. Concerning more people than you bring in seems like bad marketing, to me.

It gives me serious reservations about them, for one.
Woah, what? Did IBM develop server-class RISC-V chips when I wasn't looking?

You don't mean PowerPC by chance, do you?
Personally I'd stay far, far away from anyone unknown marketing custom hardware for anything crypto. Whether that's NFT BS or even your SSH keys. Sounds like a very easy way to collect a lot of keys at once.
Ah, sorry. I always forget that there's that distinction there.
That's the good thing about RISC-V, it has nothing to do with pluton. Pluton is something Microsoft is trying to do on x86 or ARM. RISC-V is a whole different ISA, it's open source and can't be fucked with by Microsoft. If they ever were able to, people (or companies) can just fork it and start their own path.
It will die eventually (I hope) after ARM and RISC-V become more mainstream.
If you're counting Joules then a Pi is not a good choice anyway, they aren't exactly economic with their design. Linear regulators, LEDs, low value voltage dividers on some lines... bleeding a few dozen mA just because. There are far better choices for an ARM based system if low power operation is the goal. I.MX8 based ones if your budget is unlimited, and headless allwinner ones if it isn't.
It doesn't really make sense to ask what the power consumption of an ISA is. You need to at least specify what core you are talking about, if not which SoC or board.

Bret's Tech recently reviewed the "MangoPi MQ Pro". That's got an Allwinner D1 Soc which in turn has 1 GB DRAM in the package with a T-Head C906 core running at 1 GHz. The board also has a BT/WIFI chip, so it's basically like a Pi Zero W (and by the looks has the same form-factor too).

Bret saw 0.62 W at idle, and 0.87 W when running `stress-ng --cpu`.

https://bret.dk/mangopi-mq-pro-benchmarks-review

The same test on a Pi Zero W gave 0.73 W at idle and 1.18 W with `stress-ng --cpu`.

https://bret.dk/raspberry-pi-zero-2-w-benchmarks-comparison

You can compare the other benchmarks in those articles to see they are roughly comparable CPU-wise, with the RISC-V looking consistently a little better than the ARM (the Pi Zero 2 with its four dual-issue A53 cores of course blows both away):

_Benchmark_ | _Pi Zero W_ | _MangoPi MQ Pro_
--------|------|--------
Dhrystone | 202 | 254
Double-Precision | 100 | 189
Execl Throughput | 50 | 54
File copy 1024 bufsize | 86 | 124
File copy 256 | 63 | 87
File copy 4096 | 167 | 233
Pipe throughput | 76 | 85
Pipe-based context | 37 | 38
Process Creation | 43 | 46
Shell scripts (1 concurrent) | 112 | 121
Shell Scripts (8 concurrent) | 102 | 110
System Call Overhead | 264 | 223

So the RISC-V uses roughly 75% of the power while giving generally 10% to 20% better performance. That's something close to 50% better energy efficiency.

In my experience, the difference gets bigger as you go down to microcontrollers, and smaller as you go up to more sophisticated applications processors. RISC-V core vendor SiFive had some customer press releases saying that their Cortex M0-M3 class cores come in at 1/3 the silicon area and 1/3 the power consumption of the comparable ARM cores.
Pi machines are cheap. But that low cost has a hidden cost. The parts used to make it are also cheap, and not very power efficient at all.
TIL. I rarely use these so I got my longitudes mixed with my latitudes.
The most recognized one is probably the ones you saw in the original Jurassic Park.  It the huge mass cube thing with 8 sub cubes.  They weren‚Äôt exactly a computer with an operating system like today.  They we more of a massive co-processor.  The main system that interfaces with ours was a Sun computer.  You would write the programs and run them on the Sun, but use the libraries that would then do what ever calculations on the cube.  I was using the CM-2.  It was something like 65k processors.  If I recall it was really good with SIMD problems.  If used for graphics, it could be considered something of a eGPU, but without a lot of instructions.  I recall us running an program called something like randomeblinklights, or something to blink the LEDs of the CPUs.  In general the leds could be used for dignostics, etc.  The cube looked pretty neat when opened as well.  I forget the networking component, but I think it was a like a 12 dimensional hypercube.  It had a huge curved like data-vault.  Both physically and at the time, large storage.  It was preRAID RAID.  I think ours was like 10GB.  This was around 1993-5 timeframe.  Basically before Ethernet took off.  We were using ATM and also DEC Giganet.  There was Ethernet, HP any net, VG, token ring.  We had used Wellfleat routers and a brand new router from a company from San Francisco.  Fun times.  We also administered a ton of IBM RISC6000 based computers.  Several SGI boxes, a Cray YMP, DEC Alphas, HPs, and some other exotic architectures.  VAX was in the mix, but I didn‚Äôt tough those.  We had some apple computers running apple net, many terminals, etc.  It was a lot of fun and you basically had a ton to learn.  In those days, you would get a base OS and a couple of compilers.  You had to then compile anything else you needed.  I spent a lot of time cross compiling and making patches to code to get it to compile on XYZ OS.  The amount of package libraries now is amazing.  I think as far as Linux goes, Debian had one of the first package management systems.  I stared with lots of floppies and Slackware.  I also still have a pressed CD of Red Hat mother day release.  I think it was version 0.2?  I had worked for SCRI, Super Computer Research Institute.  Their logo was actually the CM-2 cube.
I think the marketing speak is for the first category you're speaking of. Tech enthusiasts who know and work in tech probably know if they need a RISC-V laptop or not, whereas the broader audience who are excited about new shiny things will be interested in the Web3 NFT speak.
I think they are targeting a specific group of tech enthusiasts. They are targeting new stuff enthusiasts who buy into whatever is new. These are the kinds of people who are automatically sold into NFTs/Metaverse/web3/whatever. 

The more technical kinds of tech enthusiasts tend to have a deeper understanding of things, so the buzzwords don't really work for that crowd.
Check out the RISC-6000 series.  Probably the most popular.  I used to run close to a thousand of them in the 90s.
yea, if its shady its a scam, the thing has he said is that its low specs, so low cost, like a raspberry, for storage, ofc not safe until a specialized group label it
Excellent, that's what I was hoping for lol.
X86 doesn't seem to be dying anytime in the near features even if you take Apple's M series into account. Apple never was a part of the x86 ecosystem. Them getting similar performance finally is impressive, but there are a ton more advantages to x86 over just raw performance. And that's ignoring that x86 performance is a moving target itself.
The rule of the dumb (like me, too dumb to remember the words exactly, so KISS) : Enclaves give a hard-on to DRM sellers, Anti-virus and Virus people.

Secret Stores : anybody with an interest in properly storing secrets (and secrets as in Docker thingy or whatever else use that word)
That's who it brings in, but I think a lot of tech enthusiasts who are more skeptical of Web3 will see it and think "well I'd like a RISC-V laptop, but maybe I should wait until it's released, or someone more reputable announces one".

It's essentially a gamble that the first population is larger than the second. Not a bet I'd take, based on my experience.
>RISC-6000 series

Not the same thing. RISC-V only came out in 2010. There have been many RISC microarchitectures over the years.
Some artists offer straight digital downloads for purchase. Otherwise, youtube-dl or PirateBay.
If you buy vinyl records you often get a download code for free. 

Rip cd (borrowed from friend and family)
Soulseek!
Some people rip disks. Sometimes these people also share the resulting files via filesharing networks, like ed2k, bittorrent, soulseek, fopnu etc, so that those who don't rip disks could obtain those.

=)
One interesting option for you is to discover the netlabel music scene and cc music. It can be challenging and a worm-hole experience. I never thought I was much of an electronic music fan until I started digging in (electronica of all sorts is primarily what you will find ..but many other types too like metal). It's all free to share and download. 

&#x200B;

Good place to start:

[https://archive.org/details/netlabels](https://archive.org/details/netlabels)

[https://www.netwaves.org/](https://www.netwaves.org/)

[https://www.soundshiva.net/](https://www.soundshiva.net/)
YouTube-dl hasn't worked for me for ages, I'm not much better than uninstall reinstall try a couple of - varieties from the man. Lol.
You can try to use yt-dlp which is a fork that is being actively developed, the original youtube-dl hasn't seen development since last year.  

But to answer your question, yeah i get it from youtube with yt-dlp, although i guess the quality might not be great but i have low standards.
I'll take a look, thanks for helping
I'm doing my part. Haven't touched Shell since Shell.
Any chance the VRR patch makes it to this release?
[https://gitlab.gnome.org/GNOME/mutter/-/merge\_requests/1154](https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/1154)

The merge request is still open, and there's still a lot of testing to be done so it's tough to say.
Here is [a small screencast](https://github.com/madmurphy/nautilus-annotations/releases/download/0.8.4/nautilus-annotations-demo.webm) of the extension.
Very interesting application. How does gio store the annotations? Are they portable?
I have the feeling this was already possible back in Gnome 2.
Now we need Nautilus Thumbnails.
Kool !

It is searchable? I mean, the annotations?
>Very interesting application

Thank you!

>How does gio store the annotations?

They are stored in the `metadata::annotation` field of the file (which is stored in `~/.local/share/gvfs-metadata`).

>Are they portable?

What do you mean with ‚Äúportable‚Äù in this context?
> Long time ago GNOME Files (Nautilus) had the ability to handle custom annotations attached to files and directories.
from what i heard the current maintainer refuse anything he doesn't like or thinks a hassle even if there are people who are willing to support the feature or even take over.  
there have been a few heated arguments over the years when features where dropped
>Now we need Nautilus Thumbnails.

Nautilus > Preferences > Show thumbnails?
> It is searchable? I mean, the annotations?

That's a very good question. Let's imagine you have 10000 files in your hard disk, and only five of them are annotated. This means that there will be only five annotations saved in `~/.local/share/gvfs-metadata`. In theory it should be very simple to search for one single annotation among five. In practice, however, I have not found in GIO's documentation any way to do that. And without that shortcut, I would need to scroll among all the 10000 files and search for your annotation (possible but absurd, the annotation is not saved in these files after all). So at the moment the annotations are not searchable because of this reason.

You can do that however via shell. The way to access the annotations of a file is by launching

    gio info -a metadata::annotation /path/to/file

If you combine that with `find` you can create your search engine.

As soon as I find a shortcut for searching directly within `~/.local/share/gvfs-metadata` I will add a search engine to my extension. But as long as the only way I know remains that of using the brute force above I prefer to wait.
Probably exporting importing capabilities or when you move a file to another external storage
>They are stored in the metadata::annotation field of the file (which is stored in ~/.local/share/gvfs-metadata).

Why is this not done via xattrs?
Nautilus developers [were quite happy](https://discourse.gnome.org/t/new-extension-nautilus-annotations-need-help-with-internationalization/6324/2) about my extension. That doesn't mean it has to be included in Nautilus by default. Modularity is exactly the point of having extensions. From my experience they did remove features that were unmantained, but they also encouraged people to write extensions to fill the gap.

In the end it wasn't too bad. After all my extension for writing annotations is much better than the old and unmaintained Nautilus code.
I meant file picker thumbnails
> external storage

The information is not lost, but it is still saved under `~/.local/share/gvfs-metadata`. So it will be visible only when the file is displayed on the same computer.
> Why is this not done via xattrs?

Because doing it via GIO is simpler and allows to annotate everything (even the Network virtual folder). But experimenting with xattrs is definitely intriguing too. Maybe in the future versions I will find a way to integrate both approaches.
[The bug](https://wiki.installgentoo.com/wiki/File_Picker_meme) is only 18 years old
I thought those weren't part of Nautilus, technically speaking?
Banning symlinks is the worst possible solution to this problem.
>which simply forbids following symlinks within a filesystem, is his preferred solution: "It‚Äôs perfect. It does exactly what we need."

What a ridiculous view. "It's perfect, if the feature is completely unusable, it can't be used maliciously!"

At this point, may as well keep your computer turned off to be absolutely safe.
I fail to see the problem? By the time you have an attacker waiting for you that is watching for the exact nanosecond you run an important task so as to launch a TOCTTOU attack, you are already f*ed up. Doesn't make sense to over-restrict the entire rest of normal operations because of that - folder symlinks are very much a useful thing in desktop Linux, and restricting their use to only root is only going to exacerbate ``sudo curl run_from_internet.sh | bash`` issues.
Interesting read, although it seems to me that an option like O_NOFOLLOW which checks all parents would "solve" this, no?
You'll pry my symlinks from my cold, dead hands.

This article has distilled the issue down to: "Some programs don't adequately check whether they're dealing with a file or a symlink, so we should throw the baby out with the bath water"

Just fix your damn programs.
Symlinks considered harmful
This article is rubbish. The actual issue is not symlinks, it's root modifying files owned by a user. 

I believe there is a kernel parameter to disable this that is mostly used by default nowadays but generally speaking in POSIX the same thing applies with hardlinks, except even worse since you cannot actually determine whether a file is a hardlink without a TOCTOU issue.
If you will imagine for a moment a filesystem with only one underlying Ext4 system mounted -- wave your magick wand and now even /dev/ and /proc/ belong to the same ext4 partition as /home. In this magick world, do symlinks offer any advantages over hard links? As I understand, the difference is that symlinks "symbolically" link because they link by a path string, whereas hard links link by descriptors. Is this difference ever productively exploited in the wild or theory?
Never been a fan of symlinks. They always seemed like a hacky solution for problems that didn't have enough thought put into them.
The issue is not that they are suspect to TOCTOU - everything that can manipulate your application in thus way can also do numerous other things.

The solution is to not allow arbitrary programs to manipulate arbitrary paths. Use LSMs!
Flair checks out (sorry for low effort comment)
>By the time you have an attacker waiting for you that is watching for the exact nanosecond you run an important task so as to launch a TOCTTOU attack, you are already f*ed up.

The point is that symlinks allow *less privileged* programs to control what *more privileged* programs see, unless those more privileged programs are very carefully written. If you're already fucked if a less privileged program bad, you might has well not have privilege in the first place!
Not to mention that every Java instance I've ever seen uses them like it's getting paid by the link.  Good luck untangling that.
>I fail to see the problem? By the time you have an attacker waiting for you that is watching for the exact nanosecond you run an important task so as to launch a TOCTTOU attack, you are already f*ed up. 

It's a privilege escalation attack. Same as any other. 

If you don't think it's a big problem that every user and every application on your system can potentially be root, then, hey, Good for you. 

But most people have been fooled into thinking that it is possible for Linux to be a multiuser operating system.
I am guessing because it would break things.  So you'd have to subtly change the behavior of every application and library that uses it. 

But I could be wrong. 

If we are willing to break things then just making it so only root can make symbolic links would be a far more effective solution.
>Just fix your damn programs.

This problem has been around since 1965 or so.  Your advice has been shared by many people since then. Yet this advice has never lead to the problem ceasing to exist.

I don't think anybody should find this surprising.

The purpose of a OS is to make programs easier to write and easier to run.  That's it. That is the sole purpose a OS has in life.  Otherwise a OS is pointless. People can, and do, write applications to run on "bare hardware" with no OS at all.

IF your OS makes it exceptionally easy to the wrong thing, but exceptionally difficult to the the right thing... your OS design is kinda fucked.

That is literally the opposite of the way it should work.

But, don't worry, symbolic links are not going anywhere. We are stuck with them for life.
>This article is rubbish.

lol. No.

>The actual issue is not symlinks, it's root modifying files owned by a user.

Are you trying to imply that root should only be able to modify files owned by root?  If so, that is a interesting take.

Regardless..

The problem is that through the use of symbolic links a lower privilege process can control the view a high privilege process has of the file system.

That's the fundamental problem here. It allows for various number of fairly trivial privilege escalation exploits for unsuspecting programmers (which is the vast majority of them).

>I believe there is a kernel parameter to disable this that is mostly used by default nowadays

I think you are probably thinking of two sysctl settings:

* fs.protected\_symlinks
* fs.protected\_hardlinks

Documentation can be found here: [https://www.kernel.org/doc/Documentation/sysctl/fs.txt](https://www.kernel.org/doc/Documentation/sysctl/fs.txt)

For protected\_symlinks:

> when set to "1" symlinks are permitted to be followed only when outside a sticky world-writable directory, or when the uid of the symlink and follower match, or when the directory owner matches the symlink's owner.

This prevents a number of common tempfile race vulnerabilities. Doesn't solve any of the issues mentioned in the article.

protected\_hardlinks makes it so I can't do things like create a new filename for /etc/shadow in my home directory or something like that.  This would prevent me tricking a administrator into giving me access to the shadow file by having him run a chown -R on my home directory.  Among a long list of other potential problems.

Even without protected\_hardlinks you are still not anywhere close to the host of problems that are caused by symbolic links. This is because hardlinks are just a normal part of any file system (it's literally just a filename) and multiple directory hard links are not allowed.

Both of these are going to be enabled by default if you are using a Systemd-based Linux distribution.  For non-systemd systems YMWV.

>but generally speaking in POSIX the same thing applies with hardlinks,

Not really.  A hardlink is just a filename. Every file you can see in a file system will have at least one and the file system will tell you if a file has more then one hardlink.

And multiple hardlinks can't be used for directories.

So while multiple hardlinks can cause problems, it's nothing like the breakage that symbolic links have caused.

There maybe historic Unixes that allowed for directories to have multiple names, but I don't think this has ever been a issue on Linux. I am certainly no Unix historian.
Well, aside from the fact that you are now relying on device files to be literal files stored on-disk, hard links aren't really links at all. A hard link is just a filename and a pointer to an inode. If you create a new hard link, you are creating a new file with a new name and path, except it shares its drive storage backing and file metadata (aside from name) with the file you're linking to. As pointed out, hard links cannot cross file system boundaries, which means no symlinking to network drives, no matter how integrated said drive is with the rest of the system. You are also now relying on the fact that the underlying file system supports separate inode and hard link capabilities, which is true for ext4, but not a guarantee for every file system that Linux operates on. Hard links may also not point to directories, as such a structure would permit cycles, which is not a desirable trait in a tree-like file system model, especially in combination with the inability to differentiate them from "real" files like what symlinks do.
> do symlinks offer any advantages over hard links? 

You can swap out the file at the target and have the symbolic link reference update automatically. The target file/directory does not need to exist, and can even be deleted from the disk without having to iterate across every hardlink.

Renaming also acts entirely differently on hardlink versus symbolic link.
So how do you solve the problem of overlays? What is the alternative to symlinks and dockerized filesystems?

I found the FHS to be a useless "standard". It makes assumptions that needn't be made in the first place.

I am not against a more sophisticated approach; I am just not seeing it with the LWN entry.

/etc/alternatives for instance but also gentoo's overlay approach (I forgot the exact name gentoo uses) are relying on symlinks too. What is the alternative there?

For instance, this statement:

> Banning symlinks entirely would break these use cases, but restricting their creation to the root
> user would most likely suffice

Makes ABSOLUTELY no sense. I fail to see why symlinks should only work for the superuser.
That makes no sense.
In a world where we must constantly deal with other people who have not put in enough thought, symlinks are essential.
That's usually `alternatives` or something similar at play. Root-only symlink creation wouldn't be affected by that, because package management already runs at that privilege level.

(I do think it's a silly idea though)
No offense meant but this is a bad take.

The TOCTTOU file operations vulnerability has been talked about since the 70s, including just about every operating system up to this point, including Windows. This isn't just some Linux thing.

This article is almost an exact rehash of what Matt Bishop wrote about for POSIX  in 1995 in  [*Race Conditions, Files, and Security Flaws; or the Tortoise and the Hare Redux*](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.6525&rep=rep1&type=pdf) (see The Password Program Race Condition \[7\])

Now-a-days this type of attack has a lot of mitigations and in Linux's case, specifically using [opennat2](https://man7.org/linux/man-pages/man2/openat2.2.html) in the API and MACs like SELinux.

The main issue at hand in Chris' article is that privileged applications can unknowingly be exploitable, mostly due to the backwards compatibility nature of the kernel just like Samba was.

In your typical Linux installation, it is unlikely that a user can just arbitrarily escalate utilizing this vector with their own malware.

No worries u/nintendiator2 the sky is definitely not falling.
> Are you trying to imply that root should only be able to modify files owned by root?  If so, that is a interesting take.

I'm not trying to imply that, I'm saying that root modifying files owned by a user is unsafe in POSIX. If you care about security, you have to setuid before modifying user files. Otherwise there will be issues.

> The problem is that through the use of symbolic links a lower privilege process can control the view a high privilege process has of the file system.

As well as hardlinks. Or, normal files. There is nothing unusual about symlinks here. Root and non-root users share the same mutable filesystem. The only unusual thing here is that the symlink can attempt to pretend that a file is owned by a user while it is actually owned by root. Hardlinks can also do that, sometimes. The solution is quite simple: setuid to the user. 

> That's the fundamental problem here. It allows for various number of fairly trivial privilege escalation exploits for unsuspecting programmers (which is the vast majority of them).

The fundamental problem is that they are modifying user files as root.

> protected_hardlinks makes it so I can't do things like create a new filename for /etc/shadow in my home directory or something like that.  This would prevent me tricking a administrator into giving me access to the shadow file by having him run a chown -R on my home directory.  Among a long list of other potential problems.

Right. This is a Linux-specific thing, so not POSIX.

> This is because hardlinks are just a normal part of any file system (it's literally just a filename)

Symlinks are also a normal part of any file system.

> Not really.  A hardlink is just a filename. Every file you can see in a file system will have at least one and the file system will tell you if a file has more then one hardlink.

Here's the fun trick: you can change the link number of an already open file descriptor, so any test for hardlinks is inherently a TOCTOU issue :)
The text before this quote is all true but cast away with my magick wand.

> Hard links may also not point to directories, as such a structure would permit cycles, which is not a desirable trait in a tree-like file system model, especially in combination with the inability to differentiate them from "real" files like what symlinks do

Ah. So it would create a dependency for inode structure in the underlying filesystem, and doesn't work for directories because someone said so because they felt that cycles would be too great a cost.
> Makes ABSOLUTELY no sense. I fail to see why symlinks should only work for the superuser. That makes no sense.

It's a shitty "security" measure. You're root, so you are allowed to create symlinks that can possibly exploit applications.

This sort of thinking needs to die. Requiring root for basic tasks like creating symlinks will inevitably lead to a system where everyone has root nearly all the time. That's not security.
> So how do you solve the problem of overlays? What is the alternative to symlinks and dockerized filesystems?

That's already been solved since the 1990s by Plan 9. Each process has a separate namespace, you bind mount files and directories that you need in that namespace. There are no symlinks.
Cycles absolutely wreck havoc upon the basic assumptions of most tools. Consider `find`. What happens if you point it at a directory that at some subdirectory contains a cycle. Should it loop indefinitely? Surely not; the files it's searching are finite in quantity and size. So, how should it avoid looping? Perhaps by keeping track of all inode numbers it's come across. That way, if it finds a directory with a known inode number, it knows that it's in a loop, and will refrain from going into it. So now find has a worst-case O(n) space complexity and O(n log n) time complexity on n files to search through, disregarding its output. At no point can it forget about inodes it's been to, because that could result in a loop. Now, if you know a directory is cycle-free, then you could have command line option to disable the inode-bookkeeping behaviour, but that's error-prone and will lead to infinite loops when misused.  


Compare that to symlinks and disallowed hard linking directories. You could then implement either depth-first search or breadth-first search with relative ease, and you can drop all previously-visited files from your list as soon as they are visited. This is a valid optimisation because entering a subdirectory is guaranteed to never lead you higher up in the tree. You will never be stuck in a loop. The space complexity is now just O(greatest number of files in single depth of tree) for BFS or O(number of files in biggest subtree) for DFS, both of which are smaller than O(n) for non-degenerate cases. Time complexity is reduced to O(n), because each iteration doesn't have to search all the inode numbers of all previously visited file. What about symlinks? Can't they create loops? Yes, but their nature and destination are encoded within itself. You can tell if a file is a symlink, at which point the default behaviour is to treat it as a file unto itself. You can tell `find` to follow them, though symlinks help here too. In theory, whenever you follow a symlink, you can keep track of its enclosing path. If the destination path is a subpath of the symlink enclosing path, it's a loop. If it's not a loop, still remember that path, and keep going on. For each symlink you come across, you can compare the destination to the enclosing paths of the other symlinks you've passed. Once you reach a symlink-free destination, you can start dropping enclosing paths. The space requirement for this solution then only grows with the number of consecutive symlinks, but still provides the ability to guard against loops.   


This is only possible in an file system that distinguishes between hard-linked "real files" that cannot contain an ancestor, and symlinks that can be identified at point-of-existence.
Thanks for the information! This can all be solved by very simply maintaining two lists internally in every directory: One for files which do not link to a parent directory, and one for files that do.

Now there are zero trade-offs in performance except for when there is a hard link to a parent, while creating hard links, and moving folders. The regular algorithm is used on the first list, the slower parent-hard-link-resistant algorithm on the second.

In fact, when traversing the second list, we have a directory parent to avoided, and that's the original directory that the recursive_directory_iterator belongs to. Any further links will not be followed if pointing to a child of this directory. This only must be checked for directories on the first list. Following any new parent-linking hard link updates this to the directory we last jumped to.

Folder moving gets a bit trickier. If we move a folder to a parent, then we iterate over the second list and demote to the first (we could have gone 3 directories up and now some are no longer links to the parent). A move deeper entails iterating over the first list (slow) and promoting if any directories now link to a parent. Doing both entails both in that order.

Conceivably, a folder could have a "fresh moved" internal flag for when this process is not yet finished, and use the more naive and slower recursive_directory_iterator algorithm which doesn't take advantage of this cache, combining the two lists into one. Now we have perfect performance in all cases except for when a directory is being recursed over after being freshly moved. Though it might be better to just block recursive_directory_iterators until the process is finished and only really do regular directory listings. However, happily, there are no race conditions to worry about since, as each directory gets updated, there is no change to the contents of *both* lists.

I should probably mention that I'm in the (long) process of writing my own OS!
I feel like you are chasing some basic simplicity that cannot be reached without giving up overall simplicity. What do you have against symlinks?

So, who's keeping track of the two lists you talk about? The system? Well, then it needs to store that in the file system. Problem is, no current file system supports thst kind of information inline, so you'd need to create a completely new file system. You could store it out-of-line in a special directory, but now do you keep track of folders from there? With inode numbers? That'd work, but be horribly unorganised and cluttered. With paths? Sounds like you're back to symlinks.

How do you even know if a folder contains loops? The links in it could point anywhere, but have a chain that eventually leads back to become a cycle. To reliably find this property correctly, you do the algorithm I outlined earlier. However, now you have to do so every time you perform an operation that could potentially create cycles. 

Furthermore, why would you want to remember which directories contain a loop? That sounds like a complex hack. True hard linked cycles are universally an error in the file system. They violate the basic shape of the conceptual hierarchical tree model, and require tons of work for no real gain. The property you want is to know about is if a file is a link or a canonical real file. With hard links, this is impossible without out-of-line info. Hard links are directory entries for files that appear just as real as the original files, and indeed are indistinguishable. Symlinks let you differentiate between the "last visited directory" and the parent directory. Symlinks are immediately recognisable, and operate on a higher abstraction layer than hard links.

Said differently, by allowing hard links to directories, you require a Turing machine to figure out if it loops. They create confusing files that appear to exist in one directory, but actually exist in multiple. Symlinks do not have these issues, and even don't have the issues you magic'd away. Those still exist, by the way.

I realise that it sounds tempting to only have hard links. It's fewer types of stuff, after all. However, symlinks exist for a reason. Having different kinds of link makes usage easier and guarantees easier to enforce.
I pretty much agree with everything except that it isn't simpler. I would rather redefine the filesystem model to allow recursive links. Conceptually, _that's what symlinks allow_.

Yes, it would require rewriting the FS, which I was going to do anyway. Maybe it'll be a failed experiment and I'll reach the same conclusion. And yes, it absolutely requires a turing machine to figure out if it loops -- it's a simple algorithm. To put this another way, a filesystem is a node structure. Some node structures allow loops, some do not. 

Every file on UNIX is a hard link to its fd. After all, if you create a hard link and delete the original, the file still exists. It is essentially a shared_ptr, whereas a symlink is a weak_ptr. Fundamentally, the change here is not so great that most apps would have to be completely rewritten or anything. Files can already exist in multiple directories. The only difference is that folders can now exist in multiple directories, which can create loops. To the user, they're used to this -- not everyone even knows what the difference between a "shortcut" and a hard link is. People likely believe this is possible in the first place.

My OS won't distinguish between "original" and "copy". This pretty much means that cross-FS hard links can result in inter-FS dependencies, which sucks. There is some amount of distinguishing in terms of which FS it is stored on, which would be by default the original and moveable to any of them with some operation. However, for example, a system with its /bin on another drive pretty much needs it to boot. Obviously we can attempt to repair a system by deleting the link, or creating some kind of /dev/error/ folder which reports an error if visited.

> How do you even know if a folder contains loops? The links in it could point anywhere, but have a chain that eventually leads back to become a cycle. To reliably find this property correctly, you do the algorithm I outlined earlier. However, now you have to do so every time you perform an operation that could potentially create cycles. 

Hmm.. I might have to modify the algorithm. Either way, it's a cached value.

I definitely see your points, and I'm not saying your wrong per se but we have a difference of opinion. Both systems are within the realm of possibility and I'm interested just to see mine exist if nothing else.
As an educational project, go ahead. I'd be happy to hear the results. As a practical system, strict trees with "weak"  links between branches satisfy the intuitive folder analogy with real life. Folders can contain other folders, but never itself, and paper files must exist in exactly one of the folders. You can create copies and references, but the original remains in situ. Hard linking directories violate this model, and are not technically possible between file systems. Not even two different ext4 partitions.
Or just use cron
HTTP 404...
> HTTP 404...

You didn't use an API key, did you?
I don't really see what it does differently than the other arch derivatives. Given that they also both lack a proper wiki and lack community support, along with the confusing website, I really don't see the benefit for using it.
I think that the website is a big problem. It's not easy to understand the differences between the various versions. I installed arco last week and it took me a few days to understand what was suitable for me. This is my first arch based distro and I am liking it so far. And I agree. It deserves more attention.
The answer is why? What does it do different than any other distribution? If it's something like an Arch tweak tool then you have your answer. No one really cares about some tweak tool that maybe 1/10,000 of users will actually use. Size of community and the backing of a distribution matters which Arco and many other small distributions lack.
My issue with it is all the editions and lack of clear documentation. Why go to all the trouble to read, tweak, mess about when there are so many Arch spins that simplify the process and do the work for me?
Not quite sure myself but I might guess that it brings not enough things to the table. Most arch user would most likely just search for problems in the arch wiki / forum and translate it to the distro they are using. Garuda Linux does have a fair amount of custom tools which are very specific to that distro
Their website is a mess. It's discouraging how many variants it has and the many tutorials and videos the webpage has to even install a simple desktop environment.
Give me one reason to use Arco over regular Arch

Or, if I didn't have time or experience to use Arch, give me one reason to use Arco over a more estabilished Arch spin (Manjaro) or some other distro that's actually good
Its underated. Great for eduaction. User friendly. Lots of documentation. Install the arch linux tweak tool & arch linux kernel manager an have fun.
It's bloated full of keybindings to change wallpaper alone, there is a shitload of redundancy that is unnecessary that is baked in. It's a mess, for those like myself who like a leaner system, I don't want to have to untangle a distro (or unfuck it), deconstructing it from it's defaults right out of the box. It's a totally narcissistic endeavor from Dubois, it's basically "here is my desktop, and this is the way you're going to run my desktop on your system".
A lot of the Linux tubers pretending to use Arch were actually using Arco and still do. Arco is great from what I've seen, and if nothing else; a great way to try out different window managers. Other distros should take note of how things should be done.
ew arch! (I have never used arch, just stirring the pot) 

never heard of it.

number 20 with a bullet on distrowatch.

296 reviews with a 9.0 rating.

must make some people happy!
Maybe what it needs is a better website designer. ;)
I've never checked out their website and hoooooolllyyyy cow. That's a mess. 

Like there's open source "web designer is really just a coder who learned HTML and some CSS" mess and then there's the ArcoLinux website.

If I didn't know what it was, I would have zero idea.

And in google [arcolinux.com](https://arcolinux.com) comes up first which looks like a what to do after you download the installer page? 

I dunno man.
This definitely. The whole site is a mess, it doesn't make a good impression.
I wanted to run arch but believed all the hype that it was hard to install and so the decision was between arco and endeavor. I don‚Äôt recall why, but eliminated endeavor. I liked the idea of arco, learning paths, etc. But after looking at the web site, it was so jumbled up, just finding the iso I wanted wasn‚Äôt simple.

I got frustrated wasting time on their site and just checked out arch. It was pretty straight forward so I gave it a shot. It wasn‚Äôt hard and I really like it. 

So it turned out for me that arco‚Äôs weird site indirectly got me on vanilla arch. Lol.
I'll agree with that. The site is a mess of badly formatted interconneted webpages. No Wiki to speak of. Forum looks like it's from the 90s. Most of the "wiki" type stuff seems to be in video, which is actually a good thing, some people process information better that way. I don't, but some do.
Maybe. And EndeavourOS has a good noob friendly community.
More available DEs

EDIT: and you brought up Manjaro and implied it's a good distro, after all the trouble they've had, I'd say that's debatable.
Sounds a lot like the GNOME guys. :)

*steps back and waits for flames*
Yes. Definitely
I had no many time to install a new distro and have a working machine. My fear was that with pure arch I would risk having too many problems. Maybe in the future.
A video wiki sounds like a special kind of punishment that they'd come up with wherever they send all the people who are too evil for Hell.
Endeavour (which I also like a lot) had the advantage of bringing over the Antergos community, so they had a head start. But yeah, they seem to be very very tolerant. Doesn't help that Arcos has pretty much no Reddit presense either. 

I tell you what though, I installed Endeavour, and then proceeded to grab the Arch (Arcos) Tweak Tool and install it. I kind of wonder if the two OSes have some synergy when you think about it....
> EDIT: and you brought up Manjaro and implied it's a good distro

I see why you could read it that way, but it wasn't my intention. My implication was that Manjaro is a dogshit distro. In other words, "why not manjaro OR a good distro".

> More available DEs

Like the others have said, Arch supports all DEs/WMs
If it's using the arch repositories, then it has the same number of available desktops as any other Arch spin.
Well he has a tweak tool so one can install almost every DE and WM you can shoehorn into a system, and a nuke tool to unfuck it when ya do. It's like one of those "Ubuntu Ultimate" distros where not only does it comes with the kitchen sink, you can also install an entire franchise of Belgian waffle houses. With Sardi icons!
Which is less even when you count the AUR. Arcos has pretty much all of them in their repos.
Jeez, now I'm hungry. 

`yay -S waffle-house-git`

Damn didn't work.
What desktops does arch not have?
If it's using the same repositories as Arch, then Arch has the exact same selection, as I said in the comment you clearly didn't read.

EDIT: Since OP blocked me at the slightest criticism of his hackneyed distro, I'll post my response here.

How am I supposed to find out whether they use their own repos with documentation as bad as this? Seriously, this distro is a mess.
They also have their own repositories, which you clearly didn't bother to find out before saying anything.
If this is correct, it would be helpful to detail how to duplicate it, and which kernel and distribution you're using.

Then we can also test it, and if necessary, file a bug.
Bug?
You posted this same complaint a month ago:

https://www.reddit.com/r/linux/comments/vhpdp3/mount_o_ro_does_not_writeprotect_last_access/

Did you file a bug report for it back then?
Cannot reproduce with 5.17.5-051705-generic: https://pastebin.com/m0C4a2cw
[deleted]
FUBAR man, FUBAR.

is that a priority bug ?
exfat was recently added to the kernel, so its not completely unreasonable that there may be some bugs to work out. while this bug is pretty bad, its not the worst thing in the world. atime isn't a very useful feature anyway, and if you cared that much about the immutability of the filesystem, you likely have an image of it anyway to restore.
Possibly. A week or two ago there was a similar report about NTFS where read-only filesystems were being changed when accessed. At the time noatime seemed to fix the issue. But now it looks like exFAT has the same bug and noatime doesn't work around it. That's really not good as it calls into question the validity of any data recovered from the filesystem, even if it's accessed read-only.
Seems like it. Especially since `noatime` does not protect it.

I used to think mounting as read-only was sacred. Turns out not.
If you read that thread you linked to you can see a bug was in fact filed against the NTFS driver. It is a similar, but not the same issue. This one is in exFAT rather than NTFS, but the behaviour is similar.
> You posted this same complaint a month ago

That was for NTFS, not exFAT.

> Did you file a bug report for it back then?

Someone else did: https://github.com/tuxera/ntfs-3g/issues/43
file metadata (e.g. filename, creation time, modify time...) doesn't influence output of file hashes.

This post is saying that file metadata is changed when it shouldn't.
Who cares? What part of ‚Äúread only‚Äù is so difficult for the kernel to under?
Yes, of course it is. (Edit: A priority. Not FUBAR, obviously.) This is not how Linux should work.

But, we need to know the OP's kernel and distribution, and how to duplicate the bug. Maybe the OP is using an old kernel? I don't know!
> exfat was recently added to the kernel

If I'm not mistaken, exFAT support has been available since kernel 5.4, which was released at the end of 2019. Could it be that you are confusing exFAT with the ntfs3 driver from Paragon? These were released with kernel 5.15 (October 2021).

>while this bug is pretty bad, its not the worst thing in the world.

However, if it is a bug, it should still be reported and fixed.
Just a note, this bug is clearly not in the kernel exFAT driver - mount says it's `fuseblk`, so OP is using the FUSE exFAT implementation instead.
noatime can have large impacts on the performance of CoW filesystems, so it does have its uses.
I‚Äôm not familiar with NTFS, exFAT, FAT16/32 and noatime. Is there any similarities between these?

EDIT: I think there is a security exploit
It‚Äôs still the same issue. File system mounted as read-only, is not actually read-only, because access times are still updated.
File system mounted as read only, but allows writing atime, is the same problem.

Just suggests it‚Äôs not an NTFS driver problem, as you can recreate it with exFAT too.
I didn't hash files. I hashed the underlying \*block device\*, which is also clearly shown in the paste.
True.

And prove we are able to exactly replicate it in another system.

But on other side, having exFat support is mighty convenient.
>This is not how Linux should work.

No, this is very much allowed by posix and linux - a ro mount means no syscall can modify files on it, it does NOT mean that the underlying storage is not modified.

This happens with various filesystems
>if it is a bug, it should still be reported and fixed.

Agreed. That's why I'd like to know how the OP's kernel, distribution and test scenario. But they're using a throwaway account (I don't know why) and appear to have disappeared; maybe they'll reappear later.
> Is there any similarities between these?

Both of the bugs occurred in the respective fuse drivers.

But there is little reason to use them these days given that we have proper kernel drivers for both file systems now.
But not on FAT32 or FAT16. Also, on NTFS, it does only happen on some files, not on all.
Sorry I misread the script, I thought you were hashing a file on the file system.
I think you are using the kernel to mount file system and OP uses exfat-fuse (judging by "fuseblk" in OP's post).

Edit: unable to reproduce with exfat-fuse in Ubuntu 22.04 live system.
In that case, what exactly does "read-only" mean? It obviously doesn't mean what it appears to mean.
It doesn't actually seem to be a throwaway account, just his username.
it means that any syscall which modifies files (creat, unlink, open with O\_APPEND etc.) is disallowed.

The ro mount option is interpreted by VFS, not by the underlying filesystem
Strange username, ha ha!
Thank you
That link needs authentication required.
Gave it a spin on my workstation (Fedora 36 Workstation), and it worked just as well as the build I compiled myself. Looks like they're still building against RHEL 7, so the engine should work on just about every newer distro.

This engine will never be small, but it's a lot easier to swallow ~70GB of storage (20 for the tarball, 50 for the Engine) than over 100 that's required to build from source due to all the dependencies. And a massive time saver from having to clone the behemoth of a git repo and build manually (though ultimately this will depend on the hardware you have and the network bandwidth available).
The attached link does only show a download, there's no juicy details with it except the basics.

Still not feeling great about using Epic's software, they've frequently been hostile towards Linux and have a big link to Tencent - none of this makes me comfortable using their stuff.

However UE support on Linux is big, as it will allow many people to switch their workflows over to Linux, helping our community to grow.

And to be honest, Unreal Engine is really unmatched in many ways, so despite the downsides it is a decent game engine and has things like Quixel, Lumen, Nanite and various other technologies.

If the Linux build turns out to actually be good, I may end up using it a bit.
I wonder how much this is motivation for games and instead for the industrial and commercial purposes that they are trying to get UE used for.
The website is locked behind a login wall
They didn't even have binaries for UE4 right? You had to compile the whole thing from source like a pleb (unless you're into that). Pretty nice
Now if only they'd restart work on UT
This is a HUGE step for game dev on linux, as prior you would have to compile your own UE4 / UE5 from source.

Most devs who are serious about game dev do this regardless, because of things like engine mods & the ability to backport fixes, but to get a project up & running fast, as well as indie / hobby game devs, this is amazing.
O Canada... our home and native land!
This is fantastic, one more piece of software that is part of my workflow coming to Linux!
 [Here's a screenshot before the registration window appears](https://imgur.com/a/sWxhwOj)
It seems to kinda run like crap.
Honestly, this was my biggest gripe with Unreal... stoked! :D
******>!YES!<******
Great, now I can avoid their games on Linux too.
Fucking finally.
Yay! software that's defective by design!
Ok cool. When do we get Linux ports of the games and launcher?
It's a shame that they didn't release those binaries as a fatpak.
FINALLY!
UE has existed for Linux for probably decades, but sadly this doesn't mean that game developers will care about supporting the platform. Hell, today, even asking game devs to support the PC is a stretch.
Yes, this is great news for Linux game developers! For those interested in [Unreal Engine 5](https://github.com/mikeroyal/Unreal-Engine-Guide) here's a guide.
I'm not going to use shit Epic Games do. Full drm, privative bullshit. Not what im looking for when i think about linux, gnu, and free software.

So. Are there other alternatives? I like source (Steam) it's good. I hope they release an engine that fucks this privative chinese bullshit company (Epic Games). And only GOG and Steam stays afloat. 

Steam is moving the gaming community to the FOSS side (Steam Deck, Proton, Source), i will be defending them. This is the way.
For a second I read:

Epic Games releases official binaries for >! UE5 on !< Linux
I‚Äôve been using both Unreal 4 and 5 on EndeavourOS and, for me at least, it‚Äôs not as stable or fast as it is on my Windows partition, I don‚Äôt know if this binaries would help with that but I will give it a try.
Check out /r/linux_gamedev
Does anyone successfully used rider on unreal on Linux?
There's not really any news release on the page, it's just a download page with a link to a zip file.

> **DOWNLOAD INSTRUCTIONS**  
> Unreal Engine for Linux  
> Download the zip file and extract the contents to your preferred installation destination.

> **RECOMMENDED SYSTEM**  
> Ubuntu 22.04 64-bit  
> Graphics card with ‚â•8 GB of memory  
> ~60 GB free disk space  
> [View full development requirements](https://docs.unrealengine.com/5.0/en-US/linux-development-requirements-for-unreal-engine/) ^([note: should work without sign-in])

> ----

> FILE NAME | SIZE | DATE UPLOADED | LINK
> ---|---|----|----
> Linux_Unreal_Engine_5.0.3.zip | 20.26 GB | Jul 12, 2022 | ‚Äî
Classic Epic Games.
Thank you for mentioning the workflow. It is not just that games using the engine are available but that development can happen natively. And when the development is there the threshold for having testing is lower as well.

I don't know how much Unreal is used on Android, but it is there. Not to mention all the other platforms like Steam Deck..
I know of a studio that has been using Windows begrudgingly because of UE, the Linux version is up and running as of this week with happy developers
I wish unity would work towards this but their CEO is busy calling other devs who actually make good games idiots , buying adtech companies and laying of employees. Unreal engine is definitely unmatched for 3d game development , in certain key areas.
> they've frequently been hostile towards Linux 

Ignoring the Epic Games Store, the real issue was that Epic didn't want to supply binaries and worry about each distro out there. It appears these binaries are made with Ubuntu in mind. 

>support on Linux is big, as it will allow many people to switch their workflows over to Linux

The big companies that wanted to change their workflow to Linux have already done so. I think this will really help Linux users thinking about UE5 to try it out rather than help UE5 users try out Linux.

One thing Epic could do, which would be a huge game changer, would be for them to roll their own Linux distro (fork Ubuntu) with all of the game dev tools already setup and installed (with all targets outside of macOS/iOS). Having everything setup in a single click of the button saves a ton of time, gets everybody to a predictable stable environment, and reduces onboarding time to just a matter of hours. But knowing Epic, they will never do this.
Also as bad Epic has been, they're saints compared to the other major proprietary 3D game engine's owner. Unity ugh, what the hell is their execs doing...
UE has been working on Linux for years. This is just to allow developers, artists and studios to use Linux with ease, but it changes nothing for having UE games work on it, as it was already possible.

My take is that Linux is a popular platform for VFX, and UE is becoming a part of the VFX pipeline, so they want to address that.
Gentoo Gaming
And you had to have a GitHub account, which is in the Epic Games group, which required accepting their terms and licenses. You still do for UE5 if you want to compile it
Yes, you had to build the binaries yourself , which took hours to do depending on how good your processor was.
At this point, I don't think I trust them to make a new UT game. I think I'll just play UT2k4 forever instead.
Wasn‚Äôt there a 2020 version (dev or something)?
Oh man, if ever ü•∫ Fast paced Unreal shooter with UE5, I'm not drooling, you are ü•µ
It has been released for Linux for more than 8 years now.
Ubuntu? Is Epic really this ignorant that film production (and UE is taking hold there) is mostly happening on RHEL and derivatives?
Their engine is so ubiquitous
Never. It's for film production where UE takes hold.
Huh? I feel like PC has been getting more support than ever before lately. Even Sony's releasing first party games on Steam.

While I agree with you about general Linux support out of the box, Proton has proved to be quite the silver bullet in that regard.
What drm are you taking about?
>  Steam are free software enthusiast for sure

lol no
I appreciate the effort this took
Can you post the link to the zip, they are disabling access to the page for some reason
The OP could at least post some text from the link because otherwise, there is no news here if one doesn't want to register to read it. I don't game, but it doesn't mean I don't find it interesting.
It's mostly consistent with Blender 3D, which is standard, and Nvidia Omniverse, also Linux compatible.
To be fair, if you had those things, you could have been [good looking](https://github.com/EpicGames/Signup/pull/24)
Yep :/
Then you get email like this https://twitter.com/MorsGames/status/1533324089342951424
The UT "4" pre-alpha was really good IMO. It's a shame they quit, it had come really far. Hell, except for the untextured maps (2-3 maps that were fully complete) I didn't really miss anything at all!
> At this point, I don't think I trust them to make a new UT game.

I sure as fuck don't either. They've been INSANELY disrespectful to the Unreal IP and its fans, and they don't deserve that IP anymore.

> I think I'll just play UT2k4 forever instead.

The Ballistic Weapons Pro team has actually developed ping compensation for ALL weapons too, not just the vanilla weapons as UTComp needs. Basically, anything hitscan now has full ping compensation.
Yes, but they abandoned it completely because Fortnite.
Yes, but only in source. Compiling a huge repository by myself every time there is an update is not a good workflow, sorry. Binaries were exactly what was missing.
unreal engine 5 hasnt been out 8 years
You don't deserve the downvotes, I cannot understand. Having the binaries is not a big deal, having the source code is...
Afaik it actually targets rhel 7.
Booo
Having to be online to play singleplayer games is DRM.
Yeah you are right. I thought proton and source were free software projects.
Most annoying part was trying to figure out if there was a way of using normal brackets in superscript on Reddit, which it doesn't appear to be the case, since the backslash `\` escape character doesn't seem to work for superscript.

Seems like you either have to use square brackets, or one of the alternative bracket characters that look similar but are separate, like:

+ `Ôºà` : U+FF08 : FULLWIDTH LEFT PARENTHESIS
+ `Ôºâ` : U+FF09 : FULLWIDTH RIGHT PARENTHESIS
+ `Ô¥æ` : U+FD3E : ORNATE LEFT PARENTHESIS
+ `Ô¥ø` : U+FD3F : ORNATE RIGHT PARENTHESIS
OP might be already signed in and did not see the auth block. Also, the site is shitty and I could see the content of that page for a half a second before the login modal came up.
Yes‚Ä¶ and almost really any 3d software available works on Linux. Substance, Maya, Houdini, etc. Linux dominates this field in studios
What a hilarious mess that whole thing was...
Thanks i spent two hours reading this thread and then the twitter drama around it. I don't know how I missed this. 

Emails for everyone needs to go. ServiceNOW  does similar things even it emails everyone in the group whenever a new ticket is created. Most organizations don't turn this off. I'm part of several groups at my workplace of about 40k people and i have several rules to sort through the service now notifications just so that i can still use my mailbox and have important notifications from snow not burried under new INC was created and new REQ was created emails
It's amazing. The only downside is that getting the original Linux build to work these days is... problematic. I still haven't gotten sound to work yet. To be fair, sound architecture in Linux has changed a *lot* in the last 18 years.
Is BW Pro development still active?

>	I sure as fuck don‚Äôt either. They‚Äôve been INSANELY disrespectful to the Unreal IP and its fans, and they don‚Äôt deserve that IP anymore.

One can hope they license it to someone. I'd love to see Digital Extremes work on it again.

Hopefully once Fortnite goes out of style we'll see something...
Fair enough. But most development teams would just setup continues deployment to provide binaries or setle on a single stable release throughout development and only need to compile once.
Unreal engine has been available on GitHub from version 4.0.1 all the way till the current 5.0.3 and can be compiled and run on Linux. The news is just that Epic finally provide the compiled binaries themselves.
Oh well, releasing it as open source/free software would be EPIC üòâ
The screenshot says otherwise.
Good thing you don't have to be online..
^(Superscript with [brackets], {accolades} and (parentheses\))

Huh‚Ä¶
This is one of the most annoying issues with Reddit's implementation of Markdown
I wasn't aware of the auth block, sorry. It's nothing that interesting other than a download link though.

Still iffy on using Epic.
Maya works on Linux? That's a new one for me.
My phone wouldn't stop vibrating for like a whole minute straight when that went on. I still have flashbacks.
Somebody somehow embedded a link to Goatse in one of the replies, which then auto-loaded in the email notifications. It was hilarious
Yeah it uses OSS which is pretty wack to get working nowadays.
Sound doesn't work when you launch it with Steam Proton?
> Is BW Pro development still active?

Yep. The madlads have been churning out content like crazy and fixing bugs. They've been really awesome. Do you want a link to their Discord?

> Hopefully once Fortnite goes out of style we'll see something...

If Fortnite starts declining in popularity and then we get Epic crawling back to UT, I am gonna lose what little respect I have for them. At this point, I'd rather they'd just have forgotten the series entirely and be done with it, or as you said, license it out to someone who actually cares about Unreal.
unreal engine 4 being available for long doesnt matter as this is about unreal engine 5.

people consider unreal engine 4 and 5 two different programs completely so your comment of it being released for linux for 8 years doesnt matter.
By most definitions it is neither free or open source software. It is source-available and gratis (free as in free beer), but with royalty model.
I mean the build target for the binary not what they publish as requirements.
[Example image of the problem](https://i.imgur.com/9bqQDME.png) for the sake of clarity, in case some phone apps or whatever correct it.
I'm on "Fuck Epic".

But I'll give them a second chance in about 10 years if they don't do any major fuck-ups between now and then.
Cool, maybe edit your post saying it's a dl link, and post you thoughts on why you are iffy using Epic stuff.
Sure it does!
I have my phone set up with do-not-disturb on a schedule and most of it happened while I was blissfully asleep.

But when I woke and saw how many emails I'd received I... had concerns.
Yeah, I don't feel like screwing everything else up on my system to just pay UT. I thought I read about a OSS -> Pipewire wrapper sometime back but it seems to have been aborted. I'll probably just buy it again on steam some day.
I was trying to use the native Linux build that came on the original discs. I don't think I tried running the windows/dos version through proton.
>	Yep. The madlads have been churning out content like crazy and fixing bugs. They‚Äôve been really awesome. Do you want a link to their Discord?

Of course, it'd be nice to see who's still modding the old rusty engine! BW Pro is still quite unique in style.

>	At this point, I‚Äôd rather they‚Äôd just have forgotten the series entirely and be done with it...

Yeah I get it. I assume you're afraid of them even touching it because it could devolve into a Diablo Immortal situation, which is understandable. I doubt any people other than a select few at Epic care about the Unreal IP.

Which is interesting because AFAIK originally Unreal almost didn't come out and would become to be just a tech demo for the engine until Digital Extremes joined in and finished it.
Ue5 has been available for linux since it launched. Its the same repo as ue4
UE 4 and 5 is the same source code tree on GitHub though.
I know, that's why would be EPIC if it was.

What I meant with having the source code is that you can build it yourself and learn from it.

It's better than being completely closed IMHO.
Another comment says that it's a very big deal that they publish binaries because that's what people use to get stuff off the ground, even though game developers will end up compiling everything at some point.

Film makers are not game developers. They would want to use the binaries and those are for Ubuntu, yet all their other work flow happens under a RHEL derivative.
I just checked, the markdown parser that new reddit uses [renders it correctly](https://i.imgur.com/forITLg.png).

It's kind of stupid to have two separate markdown parsers, with slightly different semantics, on one website.
Reddit is so fucking inconsistent lmao
I can't add text to the post but I'll make a separate comment mentioning it.
Here's some background info about Epic Games CEO and founder: 

> Installing Linux is sort of the equivalent of moving to Canada when one doesn‚Äôt like US political trends.

[Link](https://twitter.com/timsweeneyepic/status/964284402741149698).
I mean you don't really need an exotic Wine/Proton setup to get the Windows binary working, but I can understand if you want the convenience of Proton and Steam.

I think all you need is a registry file/entry for the cd key and a d3d8 wrapper of some kind (d3d8to9) and that should do the trick. The GOG version ships with a wrapper already I think.

Although now you inspired me to try and get it working with a few layers of OSS to Pulse to Pipewire bridges.
I know alsa PA and Jack are supported. Afaik OSS never made it to the project. I'm not sure about the wrapper being built but afaik it was just some discussion, no?
Yeah, the Proton version has now definitely become the de facto way to run a Linux game now, even with an older Linux build available.
It's safe to say that the Unreal IP made Epic what they are today. Nobody cared who they were before that. I've never seen any publisher treat its absolute flagship franchise with such apathy and contempt.
Oh I get it now, I thought you implied that it already was open source because it was epic (in both meanings)
Yes movie production is mostly RHEL (and unofficially also fedora) there days for most part. Ubuntu is popular in data science, engineering workloads (rhel/fedora too) but I've heard that most studios prefer working with red hat/ibm instead. That said most larger studios will have internal IT/ops that should take care of building and maintaining the version used by the studios especially since studios sometimes do end up developing their own plugins. 

I'm also not sure what the service agreement is between epic and the studios for support is for. What they end up going with will also heavily depend on that.
At least this difference isn't exactly super critical, unlike the unnecessary backslashes that new-reddit adds to URLs, but then filters them out and hides them from people using new-reddit.

So people on old-reddit or some apps just get a broken dead URL, without the person on new-reddit knowing why, since they're not shown that there are a bunch of hidden backslashes being added in for no reason.

I've got to assume that one's intentional to break compatibility and encourage people over to new-reddit, the superscript thing just seems like a parsing bug that never got fixed until it got reimplemented for new-reddit.
lol that implies that the only reason i use linux is for privacy. that is definitely one reason, but about #5 on my list of reasons. so arrogant.
The interesting thing is that the AUR has a repo that uses the Steam files, but calls the native binary instead of running through Proton. https://aur.archlinux.org/packages/ut2004-steam

Other than requiring openal, I don't see anything special to make it work. Maybe I fucked something up when I tried last time. I should give it another shot, as according to this, it should work out of the box. Unless Steam has a patched version, which I doubt.
Does anyone aside from a brand new user actually use new Reddit? It's just... So garbage.
Reddit comments are probably the wrong place to ask such questions.

I can understand why someone might consider new Reddit superior for scrolling though posts, it's clearly designed to follow Instagram design patterns and capture those users. I wouldn't be surprised if there are older users who transitioned to new reddit and are happy enough with the new workflow (if not the implementation).  

But Reddit's strong point has always been the discussions in the comments, encouraged by it's innovative (for the time) threading system. New Reddit just makes reading discussions so painful. I assume that no new Reddit user makes it more than two or three layers deep into a discussion thread.
Old reddit has now a (I imagine introduced on purpose) bug where there are posts with thousands of comments but you only get to see a hundred or so. Loading more comments just removes the link.
I jump between old and new on desktop but use sync for Reddit on phone. There are some places where new Reddit UI is better but most places it breaks quite bad (at least on Firefox)
Basically what Google has been doing with AOSP for over a decade, and desktop Linux still hasn't catched up.
The last 4 words in the title are the most important part of it.
Well, here a few thoughts of mine on the Design Goals:

1. I don't think image-based is important here, but more of an implementation detail. Whatever solution you take, it needs to be **easily** reproducible, immutable (at runtime) and cryptographically signed.
2. Yes, but there needs to be a way to turn it off because when you want to hack some stuff around you may not want to deal with signing your stuff (even if you can sign it with your own keys).
3. Sure, but only if my programs can still do stuff with my data when I just lock it. I have colleagues who sometimes need to run simulation which can take a whole weekend. They won't sit in front of their computer the whole time for that and the licensing cost of such software can be multiple times their salary per user. So just buying another one for servers isn't an option either because of the price (if they even have a server version instead of just letting the Desktop version run on a server which has basically the same outcome as when you let it run on your own computer).
4. See 2.
5. Yes.
6. By default, yes. But there needs to be a way to turn it off in case you want to decide yourself when to update. For example I have an RPi where I turn off the network port for 99% of the time and the 1% where it's on it updates, reboots and then receives new data (backup server). And well, with system-decided updates I can't guarantee that it will actually update in that 1%.
7. Yes.
8. Factory-reset, yes. But if it means that I loose access to my user-data (which is what I understand under "sensitive data", no. Sry, but if you properly separate user and system stuff, this should not be possible to happen.
9. Yes.
10. Yes, but I should be able to decide on how it fits to the hardware **if** I want to.
11. Kinda yes, but properly managing localization (not everyone knows English, not even everybody working in IT; so as soon as they need to navigate an English menu, you have failed at that) and making it **possible** for people without a computer (aka "here, install it via this thumb drive") could be kinda hard.
12. Yep, only have the stuff by default which are actually needed instead of like every program for every use-case you may want installed by default.
13. Yes.
14. See 2.
15. Yes.
16. Yes. Having multiple tools for similar (if not even the same) things can become annoying quite quickly.
17. Yes.
I have very mixed feelings about the TPM, I know sooner or later it will be used enforce anti-consumer hostile proprietary garbage.
Hoping immutable oses will catch on
Infrastructure as Code is used to redeploy immutable containers right now, every day.

Fedora Silverblue is an immutable Linux Distro.

Linux badly needs Secure Boot and Full Disk Encryption integrated with TPM2.  

Combine these three things, and you deploy the immutable image, with IaC 'deltas' which customize the image, and keep the mutable stuff separate (separate partition makes a bunch of sense here).  This gives you most of what the article suggests.

The whole image vs package for the applications is something of a philosophical debate.  Personally, I think it depends on the end use.  Enterprise systems should probably be package based.  Containers though, are like dynamically created images.  End user systems also can take advantage of apps that run as containerized images.

> At first glance immutable systems and configuration management don't go that well together

I totally disagree here.  Immutable systems are built using IaC and are significantly less prone to configuration drift.  Do the configuration management on the CODE not the end system.  Then when the updates and/or fixes are applied in code, reimplement the system if it is a server or workstation, or with containers, it automatically goes into effect the next time that container is called.  IaC, CI/CD, DevSecOps can all use immutable systems to better maintain configuration.
Not that fresh news and frankly I don't like the loss of user power this requires.
A GnomeBook, sounds good. Let‚Äôs build together:)
[deleted]
[Repost](https://www.reddit.com/r/linux/comments/um0egs/fitting_everything_together_lets_popularize)
I feel like this could be a selling point for smaller distros! Stop doing like everyone else (all distros are quiet the same) and start implementing those experimental stuffs
Nice try, Microsoft.
https://madaidans-insecurities.github.io/linux.html is an interesting article about linux security
Hey - are we reading a Microsoft employer's blog right now ... =)

To the content: it already fails for me when I read "SecureBoot". I can't
continue past that point because the terminology attempts to insinuate
something I disagree with. If you believe in open source, then I think
you should also believe in open hardware, so it is weird to me that 
non-open hardware is promoted all of a sudden.
Easy gui way?
Why reinvent the wheel? https://ubuntu.com/core
I think it's easier to do on Android, because they could just make changes there that would "reinvent the wheel" in a desktop platform. (Look at how slowly the adoption of XDG-Portals is going, Android had something similar, though way more strict since the very beginning)
Lol, better late than never I guess! For me, I‚Äôm really excited about a Linux distro that would have a ‚Äúfactor reset‚Äù option like android and ios. I know PopOS has a recovery partition, but it‚Äôs basically an installation drive slapped next to the main drive that reinstalls the OS, and the refresh just keeps the home directory and reinstalls everything else.
I mean, all the software involved has been around for a minute. I already do a lot of these. But I agree it would be cool if distros packaged it all nicely.
Fedora Silverblue is an immutable OS, available right now.

Also, "caught" not "catched".
> I don't think image-based is important here, but more of an implementation detail. 

Well the thing is talking about a potential implementation of a OS. And this is one detail about it. So what you said about it being a implementation detail isn't wrong.

> Whatever solution you take, it needs to be easily reproducible, immutable (at runtime) and cryptographically signed.

How would you do that without using a image? 

Installing a bunch of RPMs or deb files or tar'ng out a file system is not going to produce anything close to the same amount of "reproducability" then copying a system image.

> 2. Yes, but there needs to be a way to turn it off because when you want to hack some stuff around you may not want to deal with signing your stuff (even if you can sign it with your own keys).

Personally I would rather have the effort go into making it easy to do things correctly, then making sure it's possible for the user to destroy the security of their own system. 

For example a read-write layer over a immutable image will allow people to easily verify that the underlying OS is unmodified.  Then this will reduce the amount of manually verification effort to only what the user changed.

> 3. Sure, but only if my programs can still do stuff with my data when I just lock it.

If your computer is doing useful work then it's not "at rest". Which means that everything you said doesn't apply to this section. 

> 4. See 2.

I am not sure you get what is the point of this section, because it's pretty significant. There are two general effective ways to verify the integrity of a OS:

1. Reboot with a unbroken full chain of trust
2. Offline verification of the computer contents. 

You can't verify during runtime, because any of the underlying components can trick higher levels components into thinking everything is "OK". 

For example a attacker can install a kernel module to hide file system usage from userland components. So things like virus scanners or malware scanners or rootkit detectors can't see the viruses or payloads or command and control software installed on the machine. 

So, for example, you can't install software on your Linux OS to make sure that the system firmware is signed correctly.  You can't run a program from your terminal that will make sure that the OS hasn't been modified by a attacker, etc. 

So if you had the ability to bootstrap your system at a very low level with read-only media that can go out and verify all the components of your system then that would be a huge advantage. Very big deal. 

Because, right now, if somebody came to the "Linux community" and says: "Hey, my system is behaving oddly.  I think it may be hacked. What do I do?"

The only useful advice people can give is "Delete everything, reinstall from scratch, and restore all your data from backups, but verify the contents of the backup before you restore them".

Which while accurate, is very terrible thing to say to people. 

.......
It would be more like a Chromebook, and a Chromebook is actually a really powerful device:

- You can run almost any app (sandboxed)
- You can run apps inside a container, where you could e. G. develop stuff
- In the example, it would be even more powerful than a Chromebook because you can extend / change core system functionality.
Linux used to be the OS of programmers, developers, tinkerers. People that eventually create things.

These locked down OSes are for people that consume.
Sorry, didn't know that. I read the blog post first ~9 months ago, and rediscovered it just now.
It's ok, I'm new to Linux and hadn't read it first time (wasn't around), i can read it now.
I really like it, but it somewhat seems to not account for some stuff:

- The issues that Flat kill listed are mostly resolved
- Virtualisation-based security can be achieved with QuebesOS
- No one likes X11, and of course it's an insecure mess. This is known by everyone, because of this everybody concerned with security (should) use Wayland
- While spoofing a sudo prompt is easy, spoofing these prompts on other systems is also trivial. Also, on windows the standard account is an admin account, which means you just need to click "Ok" when an app asks for admin privileges, no password required.
- I think the Linux Kernel being monolithic ("bloated") is actually an advantage, because then you don't need a bunch of 3rd party drivers that are unmaintained and incompatible with each other. Also, if you're really really concerned about kernel security, you can compile it yourself with many features disabled (or use linux-hardened, it's on the default repos of Arch iirc) 

However, J think there should be more memory-safety in the kernel. Also Flatpak sandbox escapes are still a thing.
What's more interesting is that anyone still posts that link or takes it seriously.
Madaidans is a very well-known piece of toilet paper (which nevertheless has a few valid points), and should not be referred to for any reason other than criticism
Secure boot / TPM isn't the problem, it's how it's used in Windows by default.
What does non-open hardware have to do with this? The official SecureBoot implementation is part of EDK2, licensed under Apache 2 license, which - to my knowledge - is considered free.

Edit: oh maybe you mean locking in OSes? Fortunately unlike on majority of Linux devices (Android), Microsoft actually requires that BIOS should let the user enroll their own trusted keys on any Windows Logo machine. It's like if Google required phone manufacturers to allow enrolling custom AVB keys in order to get Google Services (not the case, sadly).

But either way, it would've been the manufacturer's fault and not SecureBoot's. Blaming SecureBoot for that would be like blaming HTTPS because it allows locking in to certain root of trust.
It's just a store of cryptography that serves as a root of trust. If manufacturers wanted, they could make their own CA chains.


Do you also complain about the OpenSSL certificate root store? There has to be someone at the top deciding.
Except that Secure Boot doesn‚Äôt have to do with any of that. There is a very similar concept based on it in coreboot too. I want free (as in freedom) hardware too, but most aren‚Äôt whether or not it has Secure Boot or not. It‚Äôs not like without Secure Boot your hardware is open all of a sudden.
I first read this like 9 months ago, he certainly wrote it way before he worked at MS.
It's all just a concept, no one implements what he proposed 1 to 1.

Fedora Silverblue comes really close though.
I think Ubuntu Core is just canonical being canonical again and trying to push their own solutions. It achieves similar stuff to Fedora Silverblue / Fedor CoreOS, however instead of Flatpak for desktop / OCI containers for servers it uses snap.

Edit: I think Ubuntu Core is used in the coreXX (core18, core20, core22) snaps, i. e. it's like the org.freedesktop.Platform but for snap
Because that particular wheel is square.
Android had pain points too when they introduced storage access framework and scoped storage.

They had the benefit of their central repository forcing a minimum API.
You'd have to reinvent the Linux desktop either way to address its architectural issues.
> and the refresh just keeps the home directory and reinstalls everything else

Functionally this is keeping more than an android or (I presume) iOS factory reset, which will wipe your data as well.

Comparable to the windows reset (not the factory reset) except much clearer about you keep (everything in /home, vs windows vaguely describing what you keep)
Check out Silverblue.  It's an immutable Fedora desktop distro.
And its user space parts are not verified, just like on every other distros.

(I use Silverblue myself, BTW)
>Well the thing is talking about a potential implementation of a OS. And this is one detail about it. So what you said about it being a implementation detail isn't wrong.

This is from the "Design Goals" section.

Meaning, you define the "What" you want to reach, not the "How" you want to reach it.

The "How" gets done afterwards.

>How would you do that without using a image?

Even the image get created via putting (a lot of) packages into a directory which gets used as a root directory.

Reproducibility doesn't just include getting the image onto disk, but also building the image.

So if the "creating the image" part of your supply chain is not reliable enough, I don't think it matters if you are copying an image onto a disk or putting multiple package onto a disk (or something else, who knows what people come up with in the future), since "handling the packages" part is not reliable enough.

P√∂ttering discusses here the whole thing, how it works AND how it's build and "creating the image" is part of the latter.

>Personally I would rather have the effort go into making it easy to do things correctly, then making sure it's possible for the user to destroy the security of their own system.

Well, he says that **all** the code needs to be cryptographically verified before it is run.

I don't care if the system/vendor/whatever-supplied code is always verified, as long as the user-supplied code doesn't need to be since this can be a REAL hazzle when you develop or hack stuff.

>I am not sure you get what is the point of this section, because it's pretty significant. ...

Well, I understood under "remote attestation" "verifying the integrity it via a network (probably ssh)".

So it is nearly the same as "verifying a full trust chain", just remotely.

That's why I said, "see 2.", because my points of that still apply here.
[deleted]
[deleted]
Immutable doesn't mean "all changes have to be made by the developer".  It means, "Once I deploy this, it doesn't change until I redeploy it."

Containers are not even remotely new.  Started over 50 years ago with chroot.  https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016. People have been using 'jails', 'containers', 'venv' and vm's in development for this whole time as well.

Containerized development environments are in widespread use.  The Red Hat seminar hands on demonstration for OpenShift includes setting up a small set of containers for development work.  

It would not be impossible to apply IaC concepts to the desktop environment, resulting in an whatever you want to do with your system.  But when you're not hacking on it, it's going to be what you set it up to be. No software is going to break the OS, at least not permanently.

So, if you want to develop, fire up your IDE (image based, runs in a container) and when you commit changes, the CI/CD system will spin up a container to test your changes.  

Same thing for tinkering.  Though if you want to tinker with the OS, you'll need to work on the code and redeploy to test, though this could be done using CI/CD tools as well.  Then when you get everything the way you want, deploy it back onto your system.

Immutable is not the same as the factory lock down, with no right to repair, that we see with phones and other mobile devices.
> Also Flatpak sandbox escapes are still a thing.

They are rare, though. Three in 2021, [listed here](https://github.com/flatpak/flatpak/security/advisories), and [one prior to that](https://github.com/flatpak/flatpak/issues/2782) which for some reason is not listed. It's a pretty good track record overall. I'm glad researchers are investigating it to find these issues.

I would be much less worried about sandbox escapes than I would be about unsandboxed apps (including flatpak apps that create sandbox holes).
>The issues that Flat kill listed are mostly resolved

As long as Flatpak grants read/write access to your home folder to any app that declares it in their manifest, without user consent, it's still a joke.

Same for lack of X11 sandboxing, the unfettered access to your microphone via PulseAudio (which would require all apps to be rewritten to target the native PipeWire APIs to solve), and generally all privacy/security-sensitive permissions.

[https://github.com/flatpak/flatpak/issues/4983](https://github.com/flatpak/flatpak/issues/4983)

&#x200B;

>Virtualisation-based security can be achieved with QuebesOS

VBS is very different from QubesOS. It moves certain security mechanisms, like memory management and signature checks for kernel modules from the NT kernel into Hyper-V, which is based on a microkernel and has significantly less attack surface.

&#x200B;

>No one likes X11, and of course it's an insecure mess. This is known by everyone, because of this everybody concerned with security (should) use Wayland

Yet it is still used by the majority of distros.

&#x200B;

>While spoofing a sudo prompt is easy, spoofing these prompts on other systems is also trivial. Also, on windows the standard account is an admin account, which means you just need to click "Ok" when an app asks for admin privileges, no password required.

It's much easier than on pretty much any other popular OS, including Windows (only if you use a non-admin account). Daily-driving an admin account on Windows is just as stupid as daily-driving an account with sudo privileges on Linux.

&#x200B;

>I think the Linux Kernel being monolithic ("bloated") is actually an advantage, because then you don't need a bunch of 3rd party drivers that are unmaintained and incompatible with each other. Also, if you're really really concerned about kernel security, you can compile it yourself with many features disabled (or use linux-hardened, it's on the default repos of Arch iirc)

Monolithic refers to much more than just hardware drivers. A monolithic kernel turns a compromise of one kernel component (and there is an abundance of those, with a colossal amount of attack surface exposed to userspace) into a compromise of the kernel as a whole.

And DIY hardening only helps a diminishingly small userbase that already knows what they are doing, none of this is suitable for the average user.
true, although i think thats partly because it hasnt been updated as much as it should be (although it has received *some* updates as far as im aware)

> Virtualisation-based security can be achieved with QuebesOS

perhaps, but i think recommending an average user using something like qubes which is.... how should i put this; heavy? hard? unwieldy? is a bit much
Considering everything he says is true why wouldn't people take it seriously?
It's not like he backed it up with tons of sources and citations.
I'm looking at this article for the first time:

 * Strongly agree with all statements in the introduction
 * Mostly agree with everything in section 1. Except flatpak really is the answer to all these problems. We need to remove unsandboxed flatpaks from flathub, and stop using unsandboxed distro-packaged applications.
 * I'll skip section 2 because I'm not familiar with toolchain-level exploit mitigations
 * Mostly agree with the content of section 3, except I'll note that user namespaces are essential for application sandboxing and none of the problems discussed in section 1 are fixable without them.
 * Mostly disagree with section 4: too much hyperbole. Similarly, section 5 is missing the point. If an attacker has the ability to run code with your user account permissions, the game is already over.
 * Strongly agree with section 6. I've never understood why users are not more concerned about this. For every security fix that receives a CVE, far more comparable fixes do not. I won't suggest you avoid stable distros because of it, but understand that security decreases with age.
 * Agree with section 7. Manual anything is useless anyway, since it won't benefit 98% of users.

This definitely isn't trash. If you don't understand the importance of the info in sections 1 and 6, you really need to.
He provided tons of sources to back up his statements in that post.

This reads like a baseless ad-hominem argument.

If you take any issues with his article, please provide evidence that suggests he is wrong, instead of insulting him just because you don't like to hear what he says.

You shouldn't forget that he works on Kicksecure and Whonix:[https://forums.whonix.org/t/fixing-the-desktop-linux-security-model/9172](https://forums.whonix.org/t/fixing-the-desktop-linux-security-model/9172)
The primary issue with secureboot is that it isn't actually secure at all and most "secured" boot systems exist exclusively to prevent users from using their own hardware as they see fit to maintain a monopoly on the closed systems they have created. We don't tolerate this on desktops, laptops and servers, why should be tolerate it for any other platform.

If they wanted to secure the boot, why does uefi need nvram? Keeping state writable from the OS is a huge security issue. And we know none of these boards have firmware written in a defensive manner because CVEs come out on the regular. You better bet most phones don't have OS writable memory for the boot process. Most phones actually have pretty secure boot processes and can not be easily tampered with.

To imply that you can securely boot a system would mean that you have figured out solving many extremely hard problems, for which there is no known solution. The primary one being, how do you stop physical access from being complete access? TPMs have gotten better, but they can by known physical laws never be impossible to defeat.

This is not directly comparable to ssl and the CA system. You can buy certificates for a low nominal cost and most systems even allow you to add additional CAs to the cert store, so you could run your own if you wish.
on the other hand, instead of having multiple tools it uses snap for everything, which is one of the Design Goals here
Fedora Silverblue is an immutable OS.  Not sure how much it would take to implement TPM2 and secure boot.

They didn't completely reinvent the OS, just reorganized the file structure and made the system space immutable while moving the configuration, temporary and user space files into a separate area.

Also, Google and the other cloud companies use immutable systems all the time.  Combined with Infrastructure as Code, IaC, you never update or troubleshoot a running system.  Instead you redeploy it with updated or fixed version.

That's what happens when you upgrade versions in Silverblue.  I just recently upgraded from 34 to 35.  It ran for a time then I logged in and I really couldn't tell the difference.
Yep, I‚Äôm using it right now. Looking forward to what the future holds for it.
> You can run "apps" inside a container, where you (the user) are barred from doing certain trivial things, like accessing external drives.

This would have to be built into the app.  Containers do not have access to the host filesystem by default.  But it is not horribly difficult to make partitions available to the app.
Your ~~~blog?~~~ html2 webpage?

I'm your idiot consumertard.

My keyboard has led lighting that is animated.

I replaced my pager with a cell phone as soon as they became affordable.  I held out on upgrading to a smart phone for a long time, but eventually gave in.  The site gives a ton of uses of a cell phone.  Individually, I don't think any of them specifically would make me upgrade.  It's the aggregation of tools in one package that convinced me.  

One use that the page glosses over is maps.  Sure, I could go buy a strip map for where ever I live.  I used to use a Thomas Guide when I did pizza delivery in the 90s.  Also used to have a road atlas in the car.  The problem with road maps is that they're out of date when they're printed, and cannot be updated.  Online maps are also outdated quickly, BUT they can be easily updated.  And you don't have to go out and buy new maps when you visit a new place.  Furthermore, if you travel abroad, you don't have to also buy a translation book just to use a map.

I don't like web browsing on my phone.  I'm old and I need a bigger screen.  I like smaller phones because they fit in my pocket better.  I too prefer a PC for such things.  I can clear my inbox faster on my phone, but if I want to type up a serious reply (jobhunting or such), I prefer to use my PC.

I like knowing that if I have car trouble, I don't have to go searching for a payphone, or beg to use a stranger's cell.  This is especially true if I'm travelling abroad.

I use my phone less than 90% of the people I'm around.  I do use it for clock/alarm, email, messages, phone calls, random searches while shopping, including price comparisons, and listening to music.  That's 6 devices to do all those (alarm clock/watch, computer, text enabled pager, landline, laptop with GSM card?, and a walkman/ipod).

You want to stick to your landline?  I'll not call you names for it.
The flatpak sandbox is still pretty loose compared to Android and iOS.
I heard that the Steam Flatpak has some sandbox escapes because many games and anit-cheats require access to development syscalls, which can be used for some escapes.
Most Flatpak frontends (gnome-software, the flatpak command line) provide information about the apps before you install them. gnome-software  warns the user with an orange triangle if the app requests permissions like system, etc or home file access. It also warns the user if the app doesn't support Wayland.

Wayland is the default on GNOME and the default on Plasma with AMD / Intel graphics. Percentage-wise most people use something like Ubuntu or Fedora, which both ship GNOME Wayland by default.

The admin account is default in windows, a lot of software even requires it to function correctly (I had issues with the Epic games launcher). I would argue it's worse than an account within the wheel / sudo group on Linux, because on Linux you require your password to do system-level stuff.

How are monolithic kernels compromising? Are you saying everybody should compile their own kernel to only have support for their exact hardware? That sounds like a nightmare to me, you would force everybody and their Grandma to use Gentoo!
>As long as Flatpak grants read/write access to your home folder to any app that declares it in their manifest, without user consent, it's still a joke.

It doesn't. I just installed minecraft from flathub. No home a access there. Not sure what that blog was talking about.
Sure, but it also solves problems that an average user doesn't have.
yeah i was expecting it to be downvoted. People still seem to buy into the myth that linux is very secure by default and that article kinda shatters that notion; it has issues sure, but people who dismiss it out of hand have never given me a straight answer as to why the points in the article are wrong.
Not necessary trying to discredit the bulk of his article, but I do have to say that he‚Äôs overlooking or disingenuously down-playing some critical points. For instance, Window‚Äôs ‚Äúdownload random stuff from the internet, half of which isn‚Äôt even signed‚Äù approach to application distribution pretty much knocks it out of the running entirely from a security standpoint. Further worsened by having no tangible plan for sandboxing (for all its flaws, at least Linux has a game plan with Flatpak) outside of their failed WPF dalliance. So even mentioning all the other kernel-level stuff NT offers is kind of deceptive when we already know it won‚Äôt be enough to save a regular user from the regular way of using Windows.

macOS on the other hand, is a far more worthy contender, and one that is, in many respects, ahead of Linux in security at the moment.

Also, that article fails to mention bug-patching times which recent research has shown Linux has a large advantage over everything else. Waiting to push security patches until the 2nd Tuesday of the month? What a joke.
> He provided tons of sources to back up his statements in that post.

Just because his toilet papers are based on true information (not always true though) does not mean that conclusions are right

> You shouldn't forget that he works on Kicksecure and Whonix

Does not justify his toilet papers
It's more secure than no secure boot

Yes it is not magic. No surprise. 

UEFI needs nvram to be able to be properly configured by the OS.

Many systems do allow you to add your own keys. Direct the rage towards the ones that don't-
I use Silverblue myself, but its approach of layering packages is not compatible with the approach that Android and ChromeOS use.
Its really difficult to retrofit such a strict sandbox to an already existing OS, Android and iOS could be designed around the sandbox and permission system.
Unless you are using sandbox holes -- which are not acceptable -- you have pretty much full host isolation. Not sure what could be done better.

Android and iOS do not allow apps to intentionally disable portions of the sandbox, which is good, but we don't need to change flatpak to achieve that. It would suffice to change which apps are allowed in software centers and app stores, e.g. Flathub and/or GNOME Software.
I think you're confusing sandbox escapes with sandbox holes. A sandbox escape is a major newsworthy event and requires a CVE assignment. A sandbox hole just means the app disabled part of the sandbox.
Flatpak apps can statically declare sandbox holes in their app manifests. The app can effectively disable the entire sandbox.

The sandbox provides security against apps being compromised (if the app does not use sandbox holes) but not against the app being evil (because apps can use sandbox holes). However, higher-level policy could provide such guarantees. For example, we could remove apps that declare certain permissions from Flathub, or refuse to display them in GNOME Software or KDE Discover. I believe it is time to publish a timeline for doing so. App developers need to work on implementing portals to do what is needed, not rely on sandbox holes.

The dumbest possible response to this would be "flatpak is bad because it allows sandbox holes." It's a very big step towards a more secure future. It can be secure today if you don't use the sandbox holes.
Here some of the points are discussed / refuted:

https://www.reddit.com/r/linux/comments/w4g04t/-/ih29x9b
"Ignorance is bliss"
not wrong, just not "on the ground"

I lost interest in maintaining this very soon after I started it, but take a look at https://github.com/xkcd386at/scripts/blob/master/linux-and-windows-insecurities--theory-vs-practice.md
Except that I know that he discussed it with a number of other very reputable security researchers, who confirmed his conclusions.

If you want to ask yourself, feel free to ask on the GrapheneOS chatrooms.
That's not backing up your opinion with anything other than more opinion. Concrete examples or your point is as worthless as you claim his blog to be.
Oh, sorry, I must've misread that somewhere. (I think I saw this on some Flatpak / Steam issue regarding the performance impact of filtering syscalls)
Exactly, most apps don‚Äôt require that, and if they do, then they are just bad flatpaks. The blog should then be saying that those apps are bad not flatpak itself.

I think the reason they‚Äôre even on shown at all (and only provide a warning) in software centers is to ease the transition to flatpak. But soon enough only properly sandboxed apps will be shown by default.
theres a comment directly under that one refuting pretty much every point he raised, i dont agree with every point he makes (flatpak and the wayland/x11 point) but Schmensch-'s comment is seriously flawed

and even if it was spot on, theres still points raised in the article that arent even included in Schmensch-'s comment
> Except that I know that he discussed it with a number of other very reputable security researchers, who confirmed his conclusions.

This explains why his works are toilet papers. They are concerned with theoretical security againts Hollywood movie scenarios, rather than with practical security against real-world threats
The large majority of apps are "bad flatpaks" then.

Theres still other issues with flatpak mentioned in the article.
Like mitigation of heap-memory corruption bugs via hardened\_malloc, a hardened app runtime, a hardened app sandbox, etc.?

https://grapheneos.org/features#exploit-protection

Also ignoring that it is endorsed by Edward Snowden: [https://twitter.com/Snowden/status/1175430722733129729?ref\_src=twsrc%5Etfw](https://twitter.com/Snowden/status/1175430722733129729?ref_src=twsrc%5Etfw)
What large majority? Did you read the article yourself? Only about 30% of flathub have such permissions when the article was written, and that number is decreasing with developers utilizing portals more. Still don‚Äôt see how flatpak itself is related here.

Kitty is awesome.
[Terminator](https://gnome-terminator.org). Because of the tiling function and the many setting options.
Yakuake

dropdown terminals are amazingly convenient
guake or konsole
foot
rxvt-unicode with perl extensions is da bomb.
> Color coded output

You mean syntax highlighting? I'm pretty sure that's a feature of your shell and config. I have it on Zsh.

If you mean just mean the ability to display colors... most any terminal you pick up on Linux will have that.
Still using `xterm` after 30+ years. It does what I need.
WezTerm

Fast, and super customizable Lua based config. It also is cross platform, works on macOS, Linux, windows
[foot](https://codeberg.org/dnkl/foot) is pretty neat.
I use iterm2 on Mac the most, since that's what I work on. Supports all kinds of nice layout options, plus great terminal multiplexing for when I need to broadcast commands to lots of different terminals.

In Linux, I personally love guake since it's always right there to call up and put away. And good old gnome-terminal for everything else.
Tilix, while I'm still in a mostly GTK environment.  ... If I do move to KDE then it will probably be just Konsole.
st
Iterm2

I wish I had a Linux port :(

Edit: does anyone know why it hasn't ever been ported to Linux? It's open source, so I am surprised. It seems like there is a large appetite for it on Linux based systems. 

I can't find any articles why it hasn't been ported.
wezterm
XTerm. Due to my job I need to access a lot of telecom equipments and XTerm is the most compatible terminal emulator I've ever used.
I have been using terminator nice tab terminals
xterm, it opens instantly, is very responsive and supports pretty much anything i might throw at it.
Terminator with Tmux
Terminator, mostly because it has a right click menu that can be easily customized with plugins. Being able to select text with mouse and search for it through context menu is essential for me (super useful for error messages especially). Konsole is another one where I can get this workflow but I prefer the look and customization options of Terminator.
Gnome-terminal, because it's the default on Fedora Workstation and just works.
Konsole with split screen.
tty via serial cable 8,N,1.
Because life's hard and it's all one way.

Also, xterm.
Im just using alacrity
Where is the alacritty love?  Or have all the cool kids moved to something else now?
Try `st`. It's highly customisable but only for advanced users (its customisation file is written in C)

Alternatively try `alacritty` (?)
[rxvt-unicode](http://software.schmorp.de/pkg/rxvt-unicode.html) + [tabbedex](https://github.com/mina86/urxvt-tabbedex/)
I use terminator for 80x24 stuff and tilda as a dropdown terminal.
Tilix
xfce4-terminal. It just works and is easily configurable. I prefer a tiling window manager so most of the time all the extra options for drop down/hiding and splitting windows and tabs and all that are just cheaper built in functionality that I don't really use or need at the application level.
mrxvt
Kitty but sometimes alacritty
I used iterm2 on a Mac for years. Now trying Hyper.
At work, where it is predetermined that we use Windows computers, I use SecureCRT.  When I was on a pure linux network (RHEL 6 and later 7), I used the terminal in gnome.  I liked the tabs.

At home, I have to admit that I'm loving the W11 wt (windows terminal, also available for W10).  But when I'm doing techie stuff, I usually switch to an ubuntu vm with ansible installed, and you guessed it, I run terminal.
[Gnome Console](https://gitlab.gnome.org/GNOME/console) aka kgx.
I rarely use the terminal these days, so either Konsole or the GNOME one does the job.
Kitty ftw.
Terminator for me as well. Or just the basic b\*\*\*\* gnome terminal
I use Terminator.
Yeah I'm liking Kitty too! Took a bit of fiddling to get it set up the way I wanted, but worth it IMO. :)
I'd say Kitty and Wezterm are definitely the two with the best claim to being on par with iTerm2 in overall featureset, though neither is particularly "friendly" in the way iTerm is. You're going to have to read documentation, write config files, and memorize keybindings to get anything out of them.
Not if you run everything except browser in them...
Upvoting this. Konsole provides bookmarks functionality that's really helpful when you work with remote servers.
i use it but:
- only wayland
- no font ligature support
The default shell on mac is now zsh, so OP is probably using that, and yeah, the shell does contribute to syntax highlighting, BUT, the terminal has to support it, AND the terminal can override it.
Wow 30+ years!??
Actually it has better latency when typing then Alacritty ;-D
Isn't it bit buggy?
It is a native macOS app written in Objective C if I'm not mistaken. A Linux port would basically be a complete rewrite.

There aren't many terminals on Linux much are aiming at iTerm2. At lot of popular terminals follow a minimalist philosophy where features are considered  frivolous and a sign of newbie-ness. Personally, I'm not into that which is why I continue to work on my own terminal [Extraterm](https://extraterm.org/) with a maximalist approach. iTerm2 is a great piece of software and something I take a degree of inspiration from.
I‚Äôd suggest using one or another. 
- Tmux/byobu is great to learn for managing other machines via ssh;
- Terminator has a rich mouse interaction experience
hAcKer
Xterm is bloat use st.
The poster was asking for a terminal that actually has features.
May I plug my terminal [Extraterm](https://extraterm.org/) . :-)

It has an iTerm2-like approach to terminal features, and I don't like reading documentation or writing config files or even just remembering things. So I avoid that hard work in Extraterm too. It has a real UI to configure stuff. You see directly what is available and can immediately what it does. You don't have to read docs or hack a config file. You know, "friendly".

Extraterm also has a Command Palette where you can easily search for commands to run without having to remember some obscure keybindings.

Is remembering what to type into the shell too hard? Look up the command line examples from [TL;DR pages](https://github.com/tldr-pages/tldr) from inside Extraterm and have it paste them in directly.
I did run foot on x with cage. It works pretty well ;)
For me, I like the simple default shell/terminal because no matter which system I'm at, they're the same.  I don't have to fight to find a configuration I like.  Our enterprise uses Centos, Ubuntu, and RHEL.  They all have gnome and terminal.  So it's not 'avoiding newbiness', but instead, the lowest common denominator.

From the Windows side, where 95% of my work is done, we have 3 options available.  Putty, SecureCRT and the powershell terminal.  I choose SecureCRT over putty for the tab support mostly, since both let you save sessions.  And I use ssh in powershell terminal (now I understand one reason they created Windows Terminal, it doesn't sound wrong) when I need to transfer files to the local system.
That's a really good explanation. I'll check out your terminal.
I'm a bigger fan of using a multiplexer like tmux, but that's due to server usage. Opening multiple ssh sessions to one server feels unclean.
I never knew him but his death hit me somehow, after so many years of using Debian (Deborah and Ian, his partner at the time and himself). His death seemed fishy, and whether he was experiencing mental health difficulties, police harassment or both isn't entirely clear.
> Later, he contacted me to commission a piece of art -- a rendition of a "Linux family tree" he had conceived of.

Interesting story, do you happen to have any sketches of the proposal for the commission or Ian's mock-up? It looks like a demanding project, it strikes me how Ian was thinking not only about the Linux programming part, but also about its graphical part.

Also, in case you are interesting in contributing some of your artwork Debian, Debian holds an contest for the artwork that the next stable release will feature. The next release will be sometime in 2023, but the contest will be held this year. The call for proposals should open soon. If you are interested, here are some links that might be helpful:

* https://wiki.debian.org/DebianDesktop/Artwork
The package system was not designed to manage software. It was designed to facilitate collaboration. ~ Ian Murdock
His death hits me particularly hard. I live in Indy, grew up here, and I go to Purdue right now. Although he was born in Germany, he was definitely a Hoosier at heart.

RIP.
I think its adorable that Debian is named Debian because of the creators love story. Deb Ian.
The police in surrounding Bay area departments said there was a near zero chance that he killed himself and spoke about how corrupt SF was. Many SF city workers still say he's just another victim
He said that: "I'm bad with dates"
Good old Ian - back when debian used to be cool. I actually learned "Linux" due to debian, in 2004 or so.
That's a cool experience! I haven't met anyone important in the Linux community, but would be cool to.
It was such a sad loss.
What does SF stand for?
/Aww, may his soul Rest in Peace. I am sure your work meant a lot to him. I never met him, but his work wondered into my life in the darkest of moments and brought a silver lining.
Mental health and the police is always a dangerous combination. Most cops don't have the training or patience to correctly help someone who is in crisis. Jails are not mental health treatment centers.
A great man, equal to Bjarne Stroustrup, Jim Clark, Seymour Cray joined Alan Turing, let's not forget
It's OK. Darl McBride is still around.
All cops are bastards.  It shouldn't just matter when they fuck with someone we like, they do this to people *all the time*.
I don't him IRL but I consider myself as dpkg-apostle. So wish him all luck and happiness after life.
This is the first im hearing of Ian, or his harassment by police. Can anyone tell me or point me towards resources to learn more about the situation? This sounds really interesting. Who are the SF police? And what did they want with him?
What the?! He died in 2015! In December...
Paragraphs?
He died over 6.5 years ago ü§î
LPT: Google "do not talk to the police". Might save you some day. It's not as black and white as that taken literally, but you really should talk as little as possible. And do see the videos.
> he died back in 2015

And it just occurred to you and reposted it for internet points. Nice.
His death, and the person who did snes emulation for Higanv and Bsnes for years hit me hard.

Really just resonated with me that people need support networks and people to turn to when life gets hard. And for others who are in good headspaces to be there for others.
I think it's pretty clear that it's both, to be honest. For some reason the postmortem is publicly available and it paints a pretty stark picture of some of the struggles he was going through.
Ian's views on these things always surprise me. It is hard to find people who conceive technical things like this nowadays.
The other one is Adriane Knoppix, an OS with a screen reader made for a blind other half named Adriane.
So sad to hear that.  It didn't sit right w me.  Thank you.
San Francisco
Prisons in their current form manufacture mental health issues. The "healthy" people in power want it that way because if prisons were gentle/reformative, criminals would not be afraid of these places and would commit more crimes. That's the justification given. It is far from what actually motivates / deters criminals, but the majority of voters don't care, so the system remains as broken as the few want it to be.
May neighborhood cats always choose his yard.
PJ has entered the chat
r/ACAB
No they‚Äôre not..

All bastards can be cops?? YES!
All cops are bastards?? NO!
He lived in the bay area/San Francisco at the time.  He was arrested, I believe, for an incident involving a neighbor.  He tweeted later that he was victimized and abused by the police.  Google it and see what you can find.
Is there a limit on how long we are allowed to remember somebody for?
Yes he did.
Return key broken.
I know
I've seen that. I don't talk to the police, call the police, etc. They are not someone to call to resolve situations. Thank you!
Aaron Swartz was the one that hit me hard.
Kiwifarms murdered Near.
There would be no reason to create a false report if it wasn't going to be available to the public. Propaganda only works if people see it.
While having a much different culture than the US, Finland's prison system seems to have one of the lowest reoffend rates. They invest in the prisoner and treat them like a human being. Even the prison cells look like mini apartments.
The idea that "All cops are bastards" is derived from the culture of the police. A cop who tries to remove corrupt fellow cops, question their orders, or disobey bad orders is seen as "not a team player" and is punished for doing so. To be a cop, and stay a cop, means one must be willing to do unsavory things that make them a bastard, like lying in court, or saying nothing when they have evidence of a fellow officer's misdeeds.

While I understand (and agree with) the concept that, of course, not every individual officer is a bastard, you must remember the entirety of "one bad apple": "One bad apple _spoils the whole bunch_." This doesn't mean you should mistreat individual officers or anything like that, but it speaks to the corruption of their internal culture because there's no accountability internally. We can call out the culture and say "ACAB" all we want, but unless there are consequences for that behavior, that toxic culture will continue until there are. I hope that gave you some insight on why people say ACAB and what the purpose of the movement is.

Once again, none of this should be used to harass individual officers unless they have _done something specifically, demonstrably wrong_. Many of them are also unhappy with those unsavory elements of police culture, but have no choice but to play by those rules.
Usually these remembrance posts are done on the anniversary or something
Absolutely not. I would really not have said this if the post started by saying "Ian left us 7 years ago" or "I have just joined this community and it reminded me of Ian" or something similar or if this was on his death anniversary... But that wasn't the case, so I assumed karma bot (which, as everyone know, are playing on people's emotion just like this to build karma). 

I can't say I knew Ian well at all, actually we only ever chatted once but given the lack of information, it just looked exactly like copy-pasting from somewhere else on the internet from 7 years ago to build karma.
So why the post now? ü§î
Made me fucking angry.
I bawled my eyes out at the documentary about him: ‚ÄúThe Internet‚Äôs own boy‚Äù.
I agree. 

Not sure what can be done with those types. There will always be cesspools like KF and others that will conduct themselves like that to be edgy and try to gain acceptance within that community by doing those things. To me, those people are just in as much need for help but for different reasons.
That's a bold claim.
Well yes, you can just apply conspiracist reasoning, start with a conclusion you want and then dismiss any inconvenient evidence as fabrications by the Establishment.
Some of that reform is also seen in German prisons. I think criminals in those countries are the truly criminal due to psychological defects or really bad influence, because overall life is much better due to the higher quality of life, education, healthcare, safety nets, and positive social security systems.

One of the most overlooked principles in the "justice" systems in many countries is _prevention is better than cure_ . If you feed and clothe everyone properly, keep them busy and give them shelter, the number of crimes reduces dramatically. Then only the pathologically greedy, cruel or unstable ones will commit crimes and the rest will not join them in their criminal acts.
We‚Äôre always learning üëç

And yes, I agree with you entirely! I‚Äôm just tired of seeing everyone always mentioning what the cops did wrong..and never at least once, mention a good deed..or a cop that got shot and killed while protecting civilians..

Don‚Äôt get me wrong, I also know my fair share of lousy cops..but I also know a few good men in blue :)
I didn't paste a thing, and why would I care about karma?
Why do you care how much karma people get? It's not like they take it away from you (or have any purpose or value). Whether the post is interesting or not should be more of concern.
Just remembering, and I'm new to this community
For those who haven't seen it, in true spirit of Aaron it is licensed with Creative Commons and free to watch: https://youtu.be/3Q6Fzbgs_Lg

It's a must see!
*bawled
My empathy has its limits. I take no issue with them getting doxxed after that hack, they had that coming. I want them to be afraid to do this to anyone else.
https://nitter.unixfox.eu/near_koukai/status/1408940057235312640
Very true. Nice post. I forgot about Germany; their system is similar to Finland's.
Cops who die in the line of duty are mentioned all the time. Maybe not in some communities but it happens a lot. We don't talk about all the victims of cops tho, only some that happen to trend.
I find using/exploiting people's death just to increase karma disrespectful and distasteful. That's just how I feel and I expressed it. You may not agree, that's fine.

---

>(or have any purpose or value)

Oh oh oh :) look for "buy high karma reddit account" on google: they do have value, very tangible value, in actual dollar (or bitcoins) which is why there are karma-farming bots here which, again in my opinion, which you might not share, makes the whole thing even more disgusting.

So as much as this is probably not it, it still looked like a repost from something someone genuine wrote 7 years ago to gain karma (to resell later).

The fact it's a big lump of text without any paragraphs looks like you typical mindless copy-pasting too.

I was also the first commenter, so no other background information.
Fixed
Agree. 

As my spouse says dealing people with issues that do wrong to others, "It's an explanation, but it's not an excuse."
Thanks for the award :)
> Oh oh oh :) look for "buy high karma reddit account" on google: they do have value, very tangible value, in actual dollar (or bitcoins) which is why there are karma-farming bots here which, again in my opinion, which you might not share, makes the whole thing even more disgusting.

I was not aware of that. This is ridiculous and disgusting.

I understand your reaction given the inital context.
You are very welcome. I enjoyed our discussion. üòä
Every alternative to Google's de-facto monopoly via youtube is good.
*Bonjour*, we are Framasoft.

Our small French nonprofit maintains and develop PeerTube (among 50+ other free-libre projects), with only one (1!) employee working on the PeerTube ecosystem.

This **new feedback tool** can be found on [Ideas.joinpeertube.org](https://ideas.joinpeertube.org). It will help us build the future roadmap for PeerTube (within what we are able and willing to do).

We also hope other developers will see the most wanted features, and contribute to PeerTube's code.

edit: fixed markdown
it would be great if videos from youtube could be uploaded with the same or a deterministic id so a browser addon could be made that automatically checks peertube if you attempt to play it on youtube and presents you with the option to watch it on peertube.
Thanks for sharing this! I will spread the word. :)
Why not just use github?, you can vote with a "thumb up", and then sort by what got the most thumbs up's, this just seem like it could create problematic duplication.
They're an alternative to a monopoly. Why would they go with another monopoly?
>Until now, developers, admins and tech-savvy people could suggest improvements and new features for PeerTube by publishing and commenting issues in the git repository.  
>  
>Nowadays, PeerTube is gaining momentum and users. It is getting out of the "experts" bubble, and that's a great opportunity. So we need to know what content creators, video-lovers and non-tech-savvy people miss from PeerTube
They are already on github (and use it for issue tracking), If we are on the subject if they are not migrating to an open source alternative having a mirror to open source services like lemmy [has](https://github.com/LemmyNet/lemmy/blob/main/README.md) would be nice (It could make the project more discoverable).
Yes I like the build and finish of that machine.

But calling it a "developer edition" is kinda interesting. Who wants to do dev work without physical function keys?
Am I the only person who find XPS kind of expensive? I get the awesome finishing but it is still kind of expensive - especially for us in Asia. Cheaper notebooks from Acer, with 16 or 32GB of RAM with NVME when connected to 30 plus inch external monitor with a good keyboard+mouse combo work out well for most of my work mates. Is there something innately good about XPS that I have missed out? Not trying to be troll but if it is worth the money - I may buy it.
I‚Äôve had the dev edition before and it was okay, it ran Linux okay sometimes. They did however include quite a few bits of hardware which were needlessly a pain in the ass to use on any other distro and their wifi card was a bear to set up. I wound up swapping it out for an intel wifi card and then could run it on practically anything.
ubuntu 22.04 is little bit slow‚Ä¶ especially when selecting another accent color in Appearance settings
This should be called the web dev edition. I seriously don't understand the obsession with removing ports.
The XPS 13 Plus makes me wonder if Dell hired Jony Ive after he left Apple
Markrting. Their linux line has always been designated the dev edition model and it‚Äôs usually identical hardware to the windows version. They just make sure the drivers work with linux
I spend all day in the terminal using a 65% mechanical keyboard that has no function keys and not once have I felt "limited" in any way. 

Just throwing that in there as a datapoint.
uhhhhhh why do companies keep doing this. I use my F keys a lot.
I do, and use the function keys a lot with byobu.

Mainly use it docked though. Frequently have to travel and wanted something light. Got very tired of hauling around my 15in XPS. This is an absolute dream in comparison.
Well, it IS definitely expensive, because it is expected to be sold that way. You have much better options if your requirement is something which worths every penny.
> Acer

As long as I remember they provided quite good "bang for buck" value.
It really is gorgeous
Mechanical keyboard. Huge difference.
I thought the 15 inch XPS is something as heavy as my thinkpad p1 like 4lbs? Do you find carrying it around very troublesome?
Thanks for your reply.
It's cheap and it works well with most distros. I can get a 16GB with 250SSD refurbished for less than US400 or a new one for about 600USD. Cheap ass processors but since most things are dine/stored in clouds anyhow
Yet less functional aka the Ive special
Yeah found it very annoying and cumbersome mainly.

XPS 13 Plus is a huge difference.
>XPS 13 Plus is a huge difference.

Is that? I suppose it is only about one pound differece on machine's weight. And I do wonder if it is performant enough to handle dev tasks without issue.
I'm a full stack web app dev. Most of my work is only single core intensive for any sustained workloads or short lived for any multicore workloads.

This laptop is ideal for my workload.

The main issue I've had is with badly written or packaged apps. For instance MySQL Workbench Snap would hit 100% CPU load while sitting Idle (this leaves the CPU hitting 86C). I've switched over to DBeaver and Idle loads and temps are back to normal.
Well I am more on the C/C++ side and doing some heavy math. I would want a slightly heavier 14" with a decent nvidia card and a performant H-series CPU. Problem is that the thing seems not exist as of today so I will still need to live with my 16" for a while.
Isn't that Mint? (Based on the logo)
"PC Runs Manjaro Linux"

Mint Logo

I used to get then mixed up too lol
Isn't all Tetris technically Soviet Tetris?
Holy fuck, just got hit with a big dose of nostalgia... Thats the exact same model of crt i have played countless hours of cs1.6, and taking to countless lans with... Used that thing until around 2012

Good times :)
Hey, I had that same CRT in the late '90s/early '00s! ;)
That's Mint, not Manjaro.
Probably this: https://lab.dyne.org/OriginalTetrisHowto
Playing tetris with a tracksuit and drinking vodka.
[Manjarno](manjarno.snorlax.sh)
I was just surprised to see the flat screen TV and CD player in use...
In mother Russia, game plays you.
that keyboard ü§§ü§§ü§§
Why does text mode graphics on CRT so reliably give me dopamine ??? I'm like barely 20 and when I was younger computers around me ran like XP n shit.

 great post. <3
In Soviet Russia, tetris plays you
Soviet Mind Game

From Russia with Fun
Thank you for sharing this one :)
Ahhh nostalgia ...
Yikes, I'm an idiot. I don't use either distro. Thanks for the correction!
Looking for Minor Infos..
Great üëç
Thanks for understanding, haha. I was a little bit preoccupied by the guy in front of me that wrote SLAVA CCCP in the guest book.
These binaries were probably built on an [Electronika 60](https://en.m.wikipedia.org/wiki/Electronika_60). Everything else is just a clone.
Only the ones compiled before 1991/12/26.
Came here to post exactly that!
Same, countless thousands of hours spent on that model monitor. Also can‚Äôt forget the CRT de-gaussing with a button combo.
Me too. That thing was a beast.
[Manjarno](https://manjarno.snorlax.sh/)
Just a regular old keyboard wtf
No problem! :)
gigachad was in front of you?
Everything else is just a falling tetrominoes game.
Wasn't as bad as the 21" CRT a friend of mine owned... We called it "Mr. Magneto" as it's degauss (is that a word?) would also degauss *every* other CRT nearby on LAN parties. Along with a nice "throngggg" sound ;-p
Spread the news elsewhere.
it's old, and likely mechanical. I like it.
He was ripped and had lots of tattoos.
Sony Trinitron?
Now if only epic games would go Linux native and make epic games store available on linux
I‚Äôd imagine that‚Äôs the route they‚Äôre going with UE being released on Linux.
> I‚Äôd imagine that‚Äôs the route they‚Äôre going with UE being released on Linux.

Version 4 was also available for Linux, and there has been no indication that this is their plan.

Epic does not give a single flying fuck about Linux customers for their games, they just want the devs.
Does any distro ship with LightDM by default these days? It seems both Gnome (GDM) and KDE (SDDM) have been coupled with their own display manager quite hard.
Can it run on wayland without launching xorg? Sddm supposedly is to support that soon, but its releases are also so slow that it's sad it is the default when installing kde nowadays.
Will be testing it I have had mixed result using lightdm, most of which is inability to login despite correct password
OpenSuse and Debian should use lightDM as default if you select Xfce during installation. Presumably some other distributions do the same.
I think Mint with Cinnamon does.
Nop, lightdm always run its greeters on xorg, sadly.
X has been crapping out on me as well, like if your monitor is disconnected and you login with TeamViewer, your whole system freezes, no shell even.
To be honest I wish Gnome and KDE didn't integrate so much with their DE specific display manager... Call me outdated but the idea of a modular DM that you can swap *without losing functionality* would be just swell!

And not to mention the scathing and morbidly enjoyable comments about GDM's lack of security from the legendary author of XScreensaver...
https://github.com/max-moser/lightdm-elephant-greeter

This greeter runs cage, a wlroots based wayland kiosk compositor
At one time I had to login through ssh in order to disable lightdm, because I was pratically locked out of my system
Is that lack of security due to X11? If so, shouldn‚Äôt the Wayland session be fine?
IIRC it wasn't about X11 but rather how GDM was just poorly coded with bugs such as it unlocking itself if you just hold space bar long enough or similar.
"SECURITY CONCERNS
XScreenSaver has a decades-long track record of securely locking your screen. However, there are many things that can go wrong. X11 is a very old system, and has a number of design flaws that make it susceptible to foot-shooting."

https://www.jwz.org/xscreensaver/man1.html

That's not to say that he doesn't also have qualms with other lock screens. It's a bit (or a lot) of both.
The last time i used xscreensaver i was under 18. I was thinking of setting it up on a spare laptop i have but this guy actively recommends switching back to X & disabling multiple useful features of X11 just for a lock screen.

 Built in locking on gnome/kde had it's fair share of issues but they are mature options now with features like lock screen notifications, Media playback controls (any day now gnome-shell :P), calendars etc. 

Also the constant bashing of other solutions on the man page comes across as rather crass to me it sounds like someone who lost their "popularity" and is now whining for no good reason.
I was referring [to this](https://www.jwz.org/xscreensaver/faq.html#gnome-screensaver):

> I'm running GNOME or KDE, and everything's broken! What's wrong?

> Probably you are not running XScreenSaver at all, but but are running "gnome-screensaver" or "kscreensaver" instead. You should stop.
> 
> XScreenSaver is secure, stable, and mature; whereas gnome-screensaver is bug-ridden, unreliable, and an ongoing security disaster (for all the reasons outlined in the On Toolkits page).
> 
> Gnome-screensaver was created in 2005 by people who thought it was a better idea to rewrite all of XScreenSaver from scratch instead of sending me patches for the changes they desired in XScreenSaver itself.
> 
> In 2011 or so, "gnome-screensaver" was forked and renamed as both "mate-screensaver" and "cinnamon-screensaver", re-arranging the deck chairs while fixing none of their fundamental design problems.
> 
> Then in 2013, Ubuntu Unity re-wrote the screen saver engine from scratch again, introducing numerous security holes that would be hilarious if they weren't so sad.
> 
> The only way to securely lock your screen under X11 is to turn off gnome-screensaver, mate-screensaver, cinnamon-screensaver, etc. and use XScreenSaver instead. How you go about this is explained in the "Installing on GNOME" section of the XScreenSaver manual.
> 
> Remember:
> 
> Once is happenstance. Twice is coincidence. Three times is enemy action. Four times is Official GNOME Policy.
For the record, GNOME no longer uses gnome-screensaver as a lock screen, it uses GDM (or whatever display manager you're using).

Gnome just no longer has fancy screensaver effects at all.
Hey all - just some more context here‚Ä¶

I‚Äôve recently started as the content manager for an embedded software company (Witekio) and I really want to make these events useful and entertaining.

Is there any examples of people who do it amazingly that I can learn from? What type of content do you think NEVER gets enough coverage?
Given I have no idea what Yocto is or does, and the LinedIn presentation blurb does nothing to inform me farther, it therefore does not pique my interest and loses my attention.
Nothing works in depain.
How does MicroOS compare to Silverblue? Seems like installing 'native' packages is a bit easier and more flexible at least?

Is there a concept of base OS 'commits'? Like a known good base configuration from OpenSuse, that you layer your own packages over?
Ok so I daily drive Silverblue, hadn't heard about MicroOS so I tried it in a VM.

My first impressions is that the installer is great and the software selection fantastic, no fanfare in the base system. I also love that unfiltered Flathub is available by default as well. The partitioning is incredibly elegant as well, /boot/efi on fat32 and everything else on btrfs subvolumes.

I also like that each "deployment" is a btrfs subvolume, makes booting old systems bulletproof. The only thing that could kill a MicroOS install is btrfs self destructing.

Its good that Gnome Software only has to deal with Flatpaks now, as I have never found it reliable handling system packages. Still, I hope its faster than I'm used to. The app for updating the system doesn't have an icon though and looks half-baked. I didn't get to test it functionally.

Will have to try it a bit longer to make accurate impressions though. I have never used the SUSE ecosystem so I'm not familiar with it (though with Podman and Flatpak I can do anything anyways). I will be installing it on a pendrive to take it for a spin.
Similar feelings, I have for NixOS  
I am glad that there are distros, where updates are simply, painless, and won't result in unbootable PC :D
What trusted application sources are you talking about?
Hey, question:

How does Gnome store work with Flatpak under MicroOS? I use Tumbleweed with KDE, and Discover keep screaming everytime I need to update anything through it. Apparently it's because OpenSUSE doesn't implement PackageKit correctly or something, so I'm wondering how it works on a system that needs to rely on PackageKit frontends
[This](https://microos.opensuse.org/)? Gonna try it on an EOL Chromebook.
MicroOS is really neat.

I do favor it over Silverblue, in my experience, only downsize was booting time on my Rpis were a bit long as it run a scrub check (and btrfs is btrfs, so *kind of* slow).

Now, I'd say it's all fine, I'm curious to see how far it will go. I like the openSUSE, I don't know why.

> applications are installed only from a trusted store

We already had that with classical repos (until you used PPA), sure *it can get better*, but it wasn't not there before.
I am not an expert on this but I've heard different things. Is it true that:

* there is but a single, global namespace for all packages by default?
* packages are not properly isolated at installation, i.e. packages which are incidentally installed from the same `tui` update share the same fate, to the effect that rolling back from their common subvolume 'removes' not one but *both* packages?
* there is a heavy reliance on shell scripts and third-party tools like *toolbox* and *cockpit* to introspect the system and deliver on the promise of immutability?
* KDE Plasma is not officially supported?

I think I need to squint more to behold the advantage(s) over NixOS, but I am happy to keep an open mind.
How are the application installed from only a trusted store? I'm not sure I would consider downloading a bunch of random binary .deb files from all over the internet that are extracted for flatpaks to be a "trusted store"..

Flatpak+Flathub is essentially what I call "Mullet Package Management". Business in the front, Party in the back.

https://github.com/search?p=2&q=org%3Aflathub+.deb&type=Code
OpenSUSE MicroOS? Does it use RPM? I've been interested in immutable OS for a while now but I still use a bunch of random apps ranging from proprietary like FDM and Authy to random open source ones like qtscrcpy and syncplay. Nevermind getting VFIO or vmware working for some of my Windows gaming.

I'm currently testing out Nobara and I keenly feel the headache of one day you just want to install an app but it doesn't have a ready to install .rpm (or if they do, there's no auto-update).

I've tried Silverblue+Kinoite and Endless OS as well, but the toolbox approach didn't work that smoothly for me and I have no idea how to install a Debian or Arch-base toolbox (with working graphics and sound).

There's Junest I suppose but it doesn't work that well for me with GUI apps and even then some apps like Surfshark still doesn't work because of permission issues.

I'd be down for immutable OS honestly if it has an easy way to get most of my usecase sorted out, hell, if there's even an easy list of terminal command to get an Ubuntu or Arch toolbox set up perfectly, but otherwise I'd be too frustrated to use it.

Also, "normal users" expect to be able to install random apps they need to do what they want. For example, my co-workers would expect to be able to just run my country's tax app and my company's database app. I don't think Wine is even listed in Flathub and Bottles is still not "normie-tier Just Works" yet (plus most people expecting to just be able to click an .exe in file manager and it'll run perfectly).

This is why I am partial to Arch-based because I could just tell them to open "Add/Remove Softwares" and thanks to AUR (and Flatpak+snaps, still no AppImage options unfortunately) or "Applications (Bauh)" (has AppImage and Web App support) they're more likely to find what they need/want. And at least on a normal Ubuntu-based, you can just get a .deb which works almost like a .exe (hopefully deb-get could get some sort of gui integration soon).

So *I'm* interested in it, but I don't think I'd install it for *my parents* when their Win7 laptop inevitably breaks. Not yet at least.
AHHHHHHH GNOMES

&#x200B;

I TOOK MY MEDS AND THE GNOMES ARE STILL HERE
Is is based on Debian or Fedora?
Same experience here with Silverblue! I really think that‚Äôs where Linux desktop is headed towards with flatpak and everything. It‚Äôll give users a stable and predictable experience because all installation are the same, and gives developers a stable environment to work on by flatpak.
Gnome is great, but it's quite sluggish in loading apps compared to KDE/Plasma.
[removed]
Well, it didn't like Virtual Machine.

Failed to mount /etc

Dependency failed for Rule-based Manager for Device Events and Files.
That "trusted store" is going to be used for censorship in the next decade or so. Mark my words.


Also it's missing arch advantages.


And GNOME. Yuck, that memory hog that isn't customizable without hacks.
That "trusted store" is going to be used for censorship in the next decade or so. Mark my words.


Also it's missing arch advantages.


And GNOME. Yuck, that memory hog that isn't customizable without hacks.
Unlike Silverblue which uses OSTree, MicroOS relies on the filesystem directly: it uses btrfs snapshots. So you can manage your snapshots, every transactional action creates a new snapshot. For a non-technical end user, I would say it's very similar to use.
I do think distros like Silverblue/MicroOS are going to be the future of the Linux desktop. I've been running Silverblue on a backup PC and things have worked well for the most part. I'm probably going to install MicroOS on that same PC and see how it goes. One of my concerns with these distros is the Nvidia proprietary driver, but it did work fine in Silverblue. Another one is the availability of applications as Flatpaks (preferably created by the application devs), but it seems to be getting better.

I really like [Distrobox](https://github.com/89luca89/distrobox) and I think that's available for MicroOS as well.
So I installed it on a usb drive, on metal the installation was a mess. The installer had an enormous font size, and the dialog boxes didn't have a background, so all the text from each level of dialog just blended together and it was really hard to read. I'm not even using an uncommon resolution, 14" 1920x1080.

The installer couldn't recognize my WiFi adapter and had no internet connectivity, even though after install it worked with no tinkering (Intel AX200). Maybe I missed a button somewhere but it was impossible to navigate the network submenu since there was so much crap on the screen.

The install was also pretty slow, but that was expected since I was installing to a USB drive.

After installing it worked pretty much as I had seen on the VM though. I tried installing stuff with GNOME Software and it probably was the snappiest I have ever seen Software work. Nonetheless, Software believes it can still install rpms and when it tries it fails and complains. If it can't install anything it should not show rpms at all.

Some extra complaints about the installer I didn't mention before, its not possible to disable the root account and it wont let you use an empty password either.

Another complain about the way the system is managed is that it should always be possible to rollback to "default" system with no extra packages. Maybe keeping the base system in its own subvolume, the modifications in another and then using something like mergefs to mount both into a single folder? It should also be possible to see a list of all the modifications done to the system in a simple list.
Sadly not the experience I had on nixos, my full disk encryption configuration was valid on 21.05 (?) but broke with 22.05 (?). I was able to fix it with some help from the forum though.
On NixOS even channel switches (NixOS way to dist-upgrade) are painless. Heck, I even have some packages from nixpkgs-unstable and it‚Äôs a bliss to manage. Sadly though, I feel like some packages are maintained worse than others so if anyone feels like jumping onto the nixpkgs maintainers list, please do :D
In gnome we‚Äôve given up with package kit - the transactional-update stack does it‚Äôs thing fully automatically and the gnome store is just for flatpaks
I think that one might be an older website version.

Here's the [current](http://get.opensuse.org/microos/) I downloaded from.
That disk usage though
Do yourself a favor a disable btrfs scrub (see how at [https://en.opensuse.org/SDB:Disable\_btrfsmaintenance](https://en.opensuse.org/SDB:Disable_btrfsmaintenance)). Without a mirroring drive or profile its benefits are limited.
Plasma is "supported" in the same way as GNOME, just currently with less resources and work behind the MicroOS specific parts.
>Does it use RPM? I've been interested in immutable OS for a while now but I still use a bunch of random apps ranging from proprietary like FDM and Authy to random open source ones like qtscrcpy and syncplay.

MicroOS for desktop is going all in with Flatpaks. So we'll have to wait for flatpak to gain more adoption before a bunch of apps are available on a truly immutable OS. But MicroOS actually pulls from Tumbleweed, so in practice you can install any .rpm that runs on Tumbleweed in MicroOS. And although openSUSE is not as popular as Ubuntu, there's a good chance of finding a repo providing missing packages at software.opensuse.org.
Distrobox is similar to toolbox but it supports many distros.
OpenSUSE
I agree but that will only happen if Flatpak becomes a true replacement for all the conventional packaging formats, which it currently is not.
that's not normal. I don't have this problem on my machine from 2013.
I‚Äôve found the opposite to be true in my experience, but I haven‚Äôt tried MicroOS.
For me it isnt, and my pc is a potato. I‚Äôve had the opposite.
bro `touch grass`
This comment has been removed due to receiving too many reports from users. The mods have been notified and will re-approve if this removal was inappropriate, or leave it removed.

This is most likely because:

* Your post belongs in r/linuxquestions or r/linux4noobs
* Your post belongs in r/linuxmemes
* Your post is considered "fluff" - things like a Tux plushie or old Linux CDs are an example and, while they may be popular vote wise, they are not considered on topic
* Your post is otherwise deemed not appropriate for the subreddit


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/linux) if you have any questions or concerns.*
> That "trusted store" is going to be used for censorship in the next decade or so. Mark my words.

With your logic, Flathub and every other Linux repositories and stores are going to be used for censorship and Linux will become absolutely proprietary.

> Also it's missing arch advantages.

As if every distro aims to be Arch.

> And GNOME. Yuck, that memory hog that isn't customizable without hacks.

GNOME isn't a memory hog anymore. What year are you living in, 2011? Ever since GNOME 40, it's very smooth on my potato PC, and GNOME is very customisable. You can use Extensions and GNOME Tweaks to configure it a lot. If you think Extensions are hacks, then plasmoids on KDE are also hacks :)
> That "trusted store" is going to be used for censorship in the next decade or so. Mark my words.

With your logic, Flathub and every other Linux repositories and stores are going to be used for censorship and Linux will become absolutely proprietary.

> Also it's missing arch advantages.

As if every distro aims to be Arch.

> And GNOME. Yuck, that memory hog that isn't customizable without hacks.

GNOME isn't a memory hog anymore. What year are you living in, 2011? Ever since GNOME 40, it's very smooth on my potato PC, and GNOME is very customisable. You can use Extensions and GNOME Tweaks to configure it a lot. If you think Extensions are hacks, then plasmoids on KDE are also hacks :)
gnome bad arch good epic le reddit moment
>And GNOME. Yuck, that memory hog that isn't customizable without hacks.

Actually I prefer Plasma myself but GNOME has better support in MicroOS at this moment. But FWIW GNOME does the job too.

>Also it's missing arch advantages.

Which advantages?
Least braindead arch user
But with no way to check a snapshot vs some official known good state, correct?

My server runs tumbleweed and btrfs, so MicroOS seems like the logical next step. It definitely felt faster than silverblue the last time I tried it, which makes sense keeping the technology in mind. 

The ability to switch to a new upstream image is the only thing I'd miss from Silverblue.
Yeah Distrobox is fantastic, I use it on Silverblue as a replacement for Toolbox to keep separate homes for each container.

MicroOS comes with Flatpak, Podman and Toolbox, so you basically have everything you need to install anything without touching the main system at all. I have seen some scripts to change the home of Toolbox as well, maybe I don't need distrobox?
Yeah, me, too. Long time Linux user here, I never installed an immutable one. But boy, ever since so got a steam deck (SteamOS is an arch based immutable distro) this concept started to appeal to me.
It keeps problems away fr the enduser, resulting in less possible screwups an admin has to fix. Love the idea and will checkout silverblue and MicroOS
>So I installed it on a usb drive, on metal the installation was a mess.   
The installer had an enormous font size, and the dialog boxes didn't   
have a background, so all the text from each level of dialog just   
blended together and it was really hard to read. I'm not even using an   
uncommon resolution, 14" 1920x1080.

Yeah, I've been experiencing what you describe on the tumbleweed installer also. I don't know if it's already reported or not, but it's a few months old bug.
Ironically that is the opposite of my experience. I was running multi disk full disk encryption on 21.05, and it did not work at all so I had to make a bunch of hacks to make it even remotely usable. By 22.05, it worked without a hitch using the builtin solutions.
Same version, sadly, GUI wouldn't load on my old Chromebook.
sounds like fedora silver blue
GUI didn't load, so...
I saw it was doable, but I didn't dived yet into it. I mean the RPis are servers, they haven't rebooted in a while (64Gb disks scrubs aren't the longest shit ever).

Thanks for the link tho and the headup, Dunno what a drive profile (or btrfs profile?) is, so need to look that up.

EDIT: "the scrub command does nothing other than tell you there is a problem. ", yeah I guess, indeed, screw that.
"Supported in the same way as" to mean "is out of beta just like Gnome"? Or to mean something else?
I'll have to try that. It's been annoying not having the Surfshark GUI, and Junest is just such a jank solution for all the other issues I have.

VFIO is probably forever out, though. And I wonder if you can install Gnome Boxes under distrobox to have USB redirection.
Yeah, but for most apps and games it already is. For something like discord however‚Ä¶
Well depends on if you have a firewall or not using gnome I just feel a bit more insecure and the App Store is not as regulated even signing in google they don‚Äôt allow for app specific passwords.
I tried to `touch grass` but it returned a `Keep off the grass` error. When I tried it with `sudo` it said my account doesn't have that privilege and the incident will be reported. I'm never taking advice about touching grass ever again.
Here's the catch:  
Plasmoids don't break on KDE every time it gets an update. 

Extensions do, however, break every update on GNOME.
Deja vu
I can configure KDE a lot with just the settings alone.


Plasmoids don't break on every KDE versions unlike GNOME extensions.


Also GNOME still takes up more memory. More than some other DEs do.
AUR, a decent package manager, being lightweight by design.


Oh and, up2date packages.
I have some prototype code for checking a MicroOS system state compared to a known good state

But as users can heavily customise their system state it is quite easy to diverge from what we expect while still being fully functional and supportable
A bit late but here's the reported bug for those interested [https://bugzilla.opensuse.org/show\_bug.cgi?id=1199020](https://bugzilla.opensuse.org/show_bug.cgi?id=1199020)
When the btrfs filesystem has a copy of data (in a dup profile or raid), scrub will replace defective blocks from the copy. When there's no such copy it only reports an error in journal, but such check happens anyway on data read.
VFIO is possible by layering on Fedora Silverblue or installing on MicroOS.

Passing usb through to distrobox might not work, at least not if the container doesn't run as root (podman is rootless by default).
That is an exaggeration, they may fail version checks on major revisions but not minor. Also that "breakage" 90% of the time it is only due to that version check, rarely it is due to actual code changes with major revisions. The version check can be turned off but is enabled by default.
Gnome extensions don't break on every update. They break when there is a big change. Besides, plasmoids need to he compiled, gnome extensions can be installed easily.
GNOME only uses 800 mb of ram.
MicroOS is based on tumbleweed which often gets updates quicker than arch.  

Also Flatpak isnt Snap, it has support for 3rd party repos so no censorship here.
There are MANY criticisms that I could put against MicroOS and Silverblue. But this aint it. 

Arch have a lot of really problematic issues, currently the biggest one are scrubs who fantasize that its "difficult" to install and feel smug about doing just that and behaving like tits towards others.           
It isn't. 

Arch is simple to install and keep - but not suitable for a wealth of different things, and motivations. Something everyone but the most clueless fanboys who can barely grasp the reasoning behind the engineering choices of Arch are able to understand.

But on the other hand, you could be a troll too - mocking the "I am 16 and just managed the 'trick' of installing Arch" persona... who am I to tell?
The AUR is a security nightmare with little to no auditing. This is well known. Pacman is not a decent package manager. Despite Arch being rolling release, they use pacman which cannot resolve/detect broken shlibs for partial upgrade. Additionally the package manager/package format cannot handle upgrades from backdated machines. Last but not least, no, arch is not minimal. Sure your package count is low, but only for the reason that arch's package maintainers throw devel and doc packages all into a single package. These facts combined with the fact that most binaries and libraries ship with all debug symbols just top the cake.

Arch is a testbed OS designed for software development and testing upstream software. Its not a stable OS designed for end users.

I personally dont care for the whole minimalism fix people hype over, i use gnome and flatpak daily. What i dont like is people having this false belief that arch is minimal, that minimalism hold any merits and that other distributions are poor because their desktop environments use 40mb more of your 8/16gb system ram. You want minimalism? Go install LFS, compile your own upstream packages with O3 and strip every binary.

Gentoo, void, alpine, fedora, debian, ect. They're all linux, they all have stable package managers with stable package formats that survive delayed upgrades, partial upgrades (where possible), and have sane package seperation.
You mean up to date kernel. I love the simplicity of Arch, but it isn't really useful for practical purposes. A lot of packages are indeed way behind upsteam stable (Inkscape), some at least a month behind their release dates(Libreoffice).

Now I don't blame the Arch devs, since those packages may not be the things Arch users use on a regular basis.
Op is also using rolling release distro. If someone use netinstall, he can also pick and choose what goes into his system.

And about AUR. It's a 2 edged sword. If you are careful enough, good for you. Otherwise, it can messed up your system pretty badly. In Op's case, even if something bad happen he can rollback to previous image. You can do that to some degree with btrfs snapshot. But other immutable OS like Silverblue/Kinoite users can go back much further if they have pin a previous image.
Average Arch elitist.

The AUR is not really special imo. It's like Ubuntu PPAs but you have to compile it or use AUR helpers. Pacman, is decent, but package managers like dnf is also decent. APT is really old and very behind but it's okay honestly. Being lightweight by design... yeah any distro can be made lightweight.

You can also get up to date packages on distros like Fedora and Ubuntu Non-LTS.
What does MicroOS do above a normal workstation install that has snapper/timeshift/btrbk snapshotting? Is it better integration between package manager and the snapshotting tool which is key?
Right, I've just tested Distrobox myself today. Looks like it works pretty well, surprisingly. That said, I failed with Surfshark GUI so it looks like things that requires true control of port and socket on the host (instead of just piping stuff, like the display and audio server) fails.

Still, for the most part it does cover up the weakness of Fedora/.rpm distro - I can finally have non-jank solution to getting Authy that doesn't involve snap, and I got a lot of the stuff I missed from Arch too. Thanks for the tips.
Every "number change" is a big update.
Snap won‚Äôt get censorship either. Just because it‚Äôs closed source doesn‚Äôt mean it can get censored although it would be nice to have external repos in Snap.
Because arch puts them in testing and staging first.


If you were to use the staging repos on arch you would get updates instantly lol
EndeavourOS also exists and so does 
```
archinstall 
```


But I prefer EndeavourOS for some reasons:


Community is not elitist.
Some extra goodies in their repo.
(I don't use NVIDIA, but if you do) Better NVIDIA support.
Wow I really love Arch and minimalism so I installed kde-desktop to go along with it.
AUR is not a "security nightmare". That's like saying all windows programs not in the MS store are security nightmares.


Don't expect the AUR to be as well monitored as the official repos.


It's the 


Arch
User
Repository

Not the


Arch
Developer
Repository!



All you have to do is just read the goddamn PKGBUILD. Just like when downloading exes on windwoes, .dmgs or .pkgs on macOS, and downloading appimages on Linux, of course you would not read the PKGBUILD in any of these cases since it doesn't exist, but rather you would look at the company, see if it's legit, scan with AV, etc.



Pacman is a decent package manager, people just do partition upgrades, user's fault.



Also the whole "if you don't update every week your system will break" is fud. You need to update the keyrings first.



Arch is stable if you use it correctly.


And if your definition of stable is not "does not crash or kernel panic" but rather "does not change" then Windows 98 is the most stable OS ever.



Arch actually does things quite well. Arch can survive a delayed update IF AND ONLY IF YOU update the keyrings.
And also spell et cetera right. It's
```
etc.
```

not 

```
ect.
```


:D


Also debian, here on out known as depain, is bad because packages get delayed for literal years, and apt is so bad Linux Mint (uses apt btw) couldn't handle a version upgrade from 19.1 to 19.3.



Gentoo is bad because you have to compile almost everything from source, which takes forever and only provides a smidge extra performance.




Arch is not a testbed os. It's designed to provide bleeding edge software.



Funnily enough, you mentioned a testbed os yourself, Fedora.



Fedora is a testbed for stuff that will eventually end up in REHL, so stop the hypocrisy.




If it's not designed for people like you and me, who is it designed for? Governments?



Which is easier?



Remodeling your dead rich uncle's mansion?


Or 


Remodeling your aunt's typical house which had everything moved out of it.



Flatpaks, while not ideal (theming and such sucks) it's better than snap lol.
You are probably using libreoffice-still.


"A lot" is absolutely untrue.


I didn't have to wait for long to get KDE 5.25.


Also you never mentioned where you got them? Lol?


AUR, built-in repos, Chaotic-AUR, or some sort of third party repo? Lol.
>APT is really old and very behind but it's okay honestly. 

In what aspects would you say it is "very behind"?
Ubuntu and fedora are definitely not rolling release. While stable packages has it's uses, if you want up to date packages you go for arch or tumbleweed. 

Also, I'd say the aur is better than ppa, cause it sucks to add a ppa! Aur has the convenience of "everything's right there"
PPAs absolutely suck cuz they delay distro upgrades and cause conflicts.


Also chaotic-aur rocks lol
That, plus a read only root filesystem, plus the automatic updates working strait out of the box , and the use of ignition/combustion to handle the first boot configuration

And the most important thing is that the updates happen ‚Äútransactionally‚Äù

They do not run on the running system, that‚Äôs utterly immutable. Instead we take a snapshot first, and patch that snapshot, before declaring it the default for the next boot

So, it‚Äôs kind of like the traditional way but backwards, where we patch the running system but snapshot it before and after.. the problem with that of course is the running system can impact the update.. that‚Äôs never going to happen on MicroOS
Nope

Many Gnome extensions simply don‚Äôt break, the developer has to add the version number to the JSON metadata file.
No lol

Sometimes updates come to arch much later than something like fedora. For example, gnome gets on arch after two or three months after release.
> AUR is not a "security nightmare". That's like saying all windows programs not in the MS store are security nightmares.
> Don't expect the AUR to be as well monitored as the official repos.

Tell me you don't know what you are talking about without telling me you don't know what you are talking about.
> Pacman is a decent package manager, people just do partition upgrades, user's fault.

Meanwhile other package managers just fix the issue if some user reported that their system broke because of a partial upgrade instead of blaming the users.
By packages, I mean distro-specific packages from the main repositories that come with the default installation. No AUR or third-party repos.
No history function, that's one.
Not OP, but I can state my grievance: no parallel downloads.

The other thing is that you have to add a new PPA to install the latest release of lots of software . . . Want to use a version of Blender that's less than 12 months old? New PPA. Bazel? Same thing. Then you end up with one or two dozen package repositories that you have to synchronize each time you run `apt update`. That's less of an issue with aptitude as it is with the repository maintainers, but it's exacerbated by the lack of parallel downloads.

That alone is enough to make me want to use dnf or pacman.
I didn‚Äôt say Ubuntu and Fedora were rolling release, I meant that Ubuntu and fedora get newer software when their new versions release.
> Also chaotic-aur rocks lol

Lol, indeed.
Ah that sounds fantastic! I'm definitely interested in trying MicroOS.

I've been using Silverblue for half a year now, and it is working great for me and I do like it. Sometimes I do wish for more "freedom" with installing packages. Toolbx is great, but its not completely seamless. This sounds like a great balance between immutability and transactional updates, and still having flexible software install options.
Well that's just GNOME not preparing the arch versions of their packages. And where is your proof?
Tellll me you are a jerk without telling me you are a jerk.
Arch is a DIY K.I.S.S distribution.


It's simple not to do a partial upgrade, just add a u to that -Sy


 :D
In some cases the AUR version of a package can be more up2date.


Also we know arch can't be the most up2date ever. The way to stay up2date on everything is to compile absolutely everything. But that's not practical. I can just wait a few hours for it to go 2 the repos.
This is not particular to APT. None of the other package managers, except for DNF, have it.
APT already downloads from different repos in parallel. For the same repo see https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=158486

>The other thing is that you have to add a new PPA to install the latest release of lots of software . . . 

This has nothing to do with APT, but with the way fixed release distributions like Debian are designed. APT can handle newer packges without issues in Debian Testing/Sid. You can also backport newer packages or use official backports to get newer software.
right, but they aren't as up to date as rolling release. versioned distros are stable because they don't update as often. rolling release has no qualms about that.
Have you tried [Distrobox](https://itsfoss.com/distrobox/)? Works great on Silverblue for me!
We still use (our own fork) of toolbox on MicroOS, and infact on my systems I use it as my default shell

But some of those rough edges of toolbox are really a non-issue when you can just call up a regular shell on MicroOS and still install packages like on a traditional Linux

I personally prefer the MicroOS way where good practice on immutable systems (eg. don't change the base too much, use containers/toolboxes, etc) is encoraged by design, but not necessarily enforced

Folk still shouldn't be installing random packages all the time on MicroOS..and if they do they're going to get annoyed by all the reboots they'll have to do..but they can if they want, and that's cool in my book.

It's also nice that we get to use pretty much any package in the whole of openSUSE Tumbleweed, which sure makes things more flexible than I see on Silverblue.
> And where is your proof?

If you have used GNOME on Arch, you will know it. The only way to prove it yourself is to wait until GNOME 43 releases, wait for a week and check. It will be on 42.
Well, your whole thread on this post is already an example of this.
> Arch is a DIY K.I.S.S distribution.

How much of K.I.S.S. is good to the point of it being detrimental to the users? It doesn't have to be this way for Pacman.

And by partial upgrade, I don't mean people intentionally upgrading some of their packages, but unaware users that accidentally did `pacman -S` before doing a full system upgrade ‚Äî or a "full package sync" in Pacman's terms.
And you open up a fresh can of worms just by stating AUR. It is a user repository with zero oversight either from Arch devs or any other person. Also, since the builds are binary, there is no way to check for malware, and even package authentication. And thus, we are again going back to the Windows model (install anything from anywhere).

Oh, and don't get me started with texlive! The installer is broken since 2019 (even the Arch Wiki says so!)

Practically, I see zero real-life use cases for Arch Linux, when the same technology and toolset is provided to me by other distros that don't screw up practical packages. It is great as a concept, but the community it has fostered has made it a meme!
This is where Nala steps in with the history for apt

https://gitlab.com/volian/nala
right.
I second this, I find Distrobox way better than Toolbox. Especially, it has more possible distributions by default (mainly because it doesn't need a "special image" to start a box, unlike Toolbox).
I have and I use it often. Fantastic tool, love it!
I feel exactly the same. It also helps that I'm a big fan of btrfs so the underlying system feels immediately familiar and sound.

The only thing that prevented me from going with MicroOS during my last server install was uncertainty about how well it would handle remote unlocking of luks system encryption for which I'm using [dracut-sshd](https://github.com/gsauthof/dracut-sshd). 

Especially the system setup wasn't immediately obvious to me.

I think I'll research a bit more and move to MicroOS soon though, it's just very tempting.
That's GNOME being slow with releasing the archified version.
No, not really, I was trying to tell people GNOME was trash lol, I get down voted by the GNOME shills I guess.


Tell me you use Ubuntu without telling me you use Ubuntu.
Partial upgrade is known to mean
```
doas pacman -Sy somepackage
```

And as a general rule of thumb I never run -S by itself.

Whenever I install smth with pacman I always run
```
doas pacman -Syu somepackage
```

It's easier to just do -Syu all the time.


As they say, breaking your system is a learning point.


Now now now, before you say that I'm a troll that's going to tell people to rm -rf their root partition, I know that learning by breaking isn't the best way.


The arch wiki exists though lol.

Unaware users should read it before doing the idiocy of what most how to do x articles for arch tell you.

```
sudo pacman -S somepackage
```

Is an attrocity that most how to articles for arch recommend.


First of all it might cause your situation.


Second of all, maybe the package relies on a dependency which exists in the arch repos but not on your system?


It might install it but then your system is in a partial upgrade.



And sudo.


Sudo is bloat, opendoas is better.


opendoas-sudo also exists.



So blame the articles, and btw Arch is also an RTFM distro.



The "unaware" users should at least know how pacman works.


Babying the user is not an option, this isn't Ubuntu.
The internet is also not centralized with zero oversite either.



Just read the pkgbuild, make sure it is legit, bam.


In your beloved distros, malware could also in theory make it to the repos. You always have to have due diligence.

The only binary build on the AUR are marked with -bin. Now stop the fud.



There are builds from source on the AUR.


I never mentioned texlive.


You are just spreading fud at the point. Arch works, isn't boated, and keeps it simple. DIY, and KISS.
do you think that making packages for Arch is GNOME's job?
Sure buddy, whatever keeps your delusion alive and kicking.
> Second of all, maybe the package relies on a dependency which exists in the arch repos but not on your system?
> It might install it but then your system is in a partial upgrade.

That's the problem, a dependency resolver should understand that if a package requires the version 2 of a library, installed applications that require the incompatible version 1 must not be removed and instead signal a dependency exception.

Having a smarter (and stricter) dependency resolver does not equal to absolving the user of all package management responsibilities.
>The internet is also not centralized with zero oversite either.

Nope, the internet is governed by multiple bodies who dictate multiple things from how the data needs to be transmitted, to where and how to resolve domain names.

>Just read the pkgbuild, make sure it is legit, bam.

How do you know if a is legit or not? Legit packages need to have some form of package integrity check, which can only be done during the installation process with `makepkg`, since the PKGBUILD only contains the signing key for the uploader, which is the easiest to spoof.

Also, why would one refer to the user repository for packages that are already part of the core distribution.

>The only binary build on the AUR are marked with -bin. Now stop the fud.

Nope. Almost all binary packages in the AUR are not marked with -bin. Case in point:

* [Libreoffice](https://aur.archlinux.org/packages/libreoffice-dev-en-gb). This package *pulls* other binary packages, which are not marked with `-bin` at the end.
* [Google Chrome](https://aur.archlinux.org/packages/google-chrome). Again, binary package not marked with `-bin`.

You don't even know how AUR works, and you're recommending it! Wow, that take the cake for being really ignorant!

>There are builds from source on the AUR.

Then why not use Gentoo or LFS?

>I never mentioned texlive.

Yeah, you never did. I am doing it, since an important package that is used by real people (academicians, engineers, businesses) is FUBAR at the moment. Heck, for such an important package, the only solution is to use a hackneyed version from the AUR which takes over the repository packages using spoofs, which again conflict with other tools such as `tlmgr`.

>You are just spreading fud at the point. Arch works, isn't boated, and keeps it simple. DIY, and KISS.

So, pointing out flaws is now spreading FUD? Wow, that's a bit rich coming from an ignorant person who doesn't even know how their favourite system works.

Arch only works if you're going for neofetch screenshots on Reddit! And guess what, ALL DISTROS COME WITH NEOFETCH! Regarding bloat, all you get is an ultra-minimal installation base with Arch, and you can get the same with Fedora with the right installation medium. After it is installed, you get the exact same stuff as any other distro!

Arch is just a meme at this point. The only thing it has of value is the Wiki.
Yes and no.



GNOME isn't purposely going to break arch support lol.
It's not a delusion.


You think arch should baby the user? Lol, it's easy not to do a partial upgrade.



Anyways, shut up, at the end of the day I'm not using apt lol, even if you paid me a million dollars.
Rule of thumb is always upgrade your system when installing packages. Done.


Easy as just doing

```
doas pacman -Syu quakespasm-spiked-git 
```
Rather than 

```
sudo pacman -S quakespasm-spiked-git
```


Which how to articles LOVE to suggest and I HATE that.

Or even worse,


```
sudo pacman -Sy quakespasm-spiked-git
```


The system you suggested exists. It's called upgrading and installing the package. One of the UNIX philosophies is to make a program do one thing and do it well. We can extend this to command line options and say that -S should only install packages and dependencies (and their versions). Nothing else. Because then you start getting into not being transparent to the user territory.
Na, your beloved distros are also flawed in the sense that they try to be rolling release with little to no software.



Arch isn't a meme, it genuinely works.


I would have tried some of your beloved distros, but I won't due to you spreading fud.
>You think arch should baby the user? Lol, it's easy not to do a partial upgrade.

LMAO

>Anyways, shut up, at the end of the day I'm not using apt lol, even if you paid me a million dollars.

Hahaha.
I guess each to their own. Looks like implementing a better dependency solver for Pacman is an interesting task to do anyway, I'll probably take a shot at doing it -- and maybe it'll get into Pacman 7.0 or something.

>! That's -- of course, if I have done with my pile of projects in the backburner. !<
Lolz! Neither Fedora, nor Debian (my favourite distros) are rolling releases. Fedora is upstream to RHEL, and Debian is stable AF!

Arch works when the stuff people need starts working by default. How is KISS philosophy telling people to go to a web page, download a git link and do `makepkg`? That's the antithesis of KISS!

Anyways, I don't need Arch, neither do I recommend it to anyone with some sense of sanity.
So all you are doing is just laughing, in that case i consider myself to have won the debate.
I agree that a dependency solver is nice, and a better one like you suggested is nice to have but ultimately uneccesary if you aren't stupid.
AUR helpers exist. That is KISS.

```
pikaur opendoas
```

Done!



Debian is not stable because apt is very breaky.


Rolling release ftw. More up2date packages without being a beta tester for a company.


Also Fedora is a testbed for RHEL so every one that uses it is a beta tester.


Oh, and if you definition of stable isn't "does not crash or kernel panic" but rather "apps and toolkit do not change" then Windows XP is the most stable OS.
Sure, sure.
>AUR helpers exist. That is KISS.

That is the core definition of bloat! Why have a second package manager, which invokes the first package manager to do its stuff???

>Debian is not stable because apt is very breaky.

Apt is stable. Incorrect implementation of `apt` by Ubuntu, which in turn was passed down to other Ubuntu-based distros like Pop OS caused it to have 1 major issue, which was then rectified.

>Rolling release ftw. More up2date packages without being a beta tester for a company.

Rolling release isn't an issue. Even Tumbleweed is rolling release. The issue is not having proper build chains. Arch only has a proper build chain for the kernel, not the rest of the packages needed for a usable computing experience.

>Also Fedora is a testbed for RHEL so every one that uses it is a beta tester.

Lol again! You are ignorant about beta testing! Beta testers need to provide **feedback**. Red Hat or IBM does not collect any data about my computer to get the feedback from!

>Oh, and if you definition of stable isn't "does not crash or kernel panic" but rather "apps and toolkit do not change" then Windows XP is the most stable OS.

No, by stable, it means the system needs to do 3 things:

1. Can run without external interference (no automatic crashing or kernel panic).
2. Having software that is thoroughly tested to run with the given hardware platform (software can't behave differently on Intel x86_64 vs AMD x86_64).
3. Update to newer patches (usually security) that doesn't conflict with existing APIs and software.

All rolling release distros fail on #3, while Arch fails on both #2 and #3.

For example, in the Arch-compiled version of GCC, all `short` variables are converted to `int` on AMD systems, but not on Intel systems. No other distro has this issue!

>Windows XP is the most stable OS.

Also, we're talking about Linux here, not Windows. BTW, Windows XP, Vista, 7, 8, 8.1, 10 and 11 all fail on all 3 points mentioned above!

You can shill for Arch all you want, I don't care. After using Arch for multiple years on multiple computers including dev servers and personal devices; I have come to the conclusion that the Arch build chain is broken, and the community isn't interested in any package outside of a handful of packages that they *deem* interesting. While they do have the liberty to do that, I too have the liberty of switching to a distro that works, and isn't a broken mess. That is what software freedom is all about.
I get the sarcasm lol, you aren't making any points.



Checkmate.
```
pikaur is not a complete package manger, it's a mere wrapper that automates stuff.
```

Fedora has 2 package managers, yum and dnf, now that is bloat!



Again, where's your source for the short 2 int problem?


Also at least quote the whole line.


Ints can hold more than shorts afaik.


3 is a lie, arch updates don't break all computers in existence.


1 only happens to windwoes lol.



The gcc problem only happens if you are downloading gcc from the repos and compiling every thing you use with it. At that rate use Gentoo. :D




You can shill for Fedora all you want, arch still has benefits.
Checkmate indeed.
>pikaur is not a complete package manger, it's a mere wrapper that automates stuff.

So, my package manager needs a wrapper, just to do the same things, and it isn't even GUI or TUI to enhance user experience? That's bloat. Heck, even from the default installation, I cannot install **any** of the external package managers that are listed on the Wiki (pikaur, pacaur, yay, paru etc.)

>Fedora has 2 package managers, yum and dnf, now that is bloat!

Yes, Fedora as of now has 2 package managers, yum and dnf. DNF is the latest version, since it does a lot of things (parallel downloads, enhanced security) better than yum. Yum is kept mainly for compatibility with older versions (RHEL 7), something Arch users need to understand.

>3 is a lie, arch updates don't break all computers in existence.

You didn't even read #3. Arch updates do tend to break toolchain compatibility, (kernel does not match compiler, therefore unable to compile external device drivers, even those downloaded from AUR itself [V4L2 Loopback](https://aur.archlinux.org/packages/v4l2loopback-dc-dkms))

>The gcc problem only happens if you are downloading gcc from the repos and compiling every thing you use with it. At that rate use Gentoo. :D

You've never used GCC by the looks of it. When custom-compiling GCC, the user can set the size of variable types: the default is `short` 2 bytes, `int` 4 bytes and `long` 8 bytes (64-bit). In Arch Linux, both `short` and `int` are set to 4 bytes each. On a program that generates 1 million `short` values; compiled on an Arch system, it is already using 1MB more RAM than any other distro, even the infamous *bloated* Ubuntu! On a real-world program that generates billions of `short` values like a web browser; compiling it with Arch and running it (on any system) would use 1GB more RAM than if it was compiled natively with the default GCC settings (distro doesn't matter).

>You can shill for Fedora all you want, arch still has benefits.

I am not shilling for Fedora. Arch would have had its uses, if the community weren't hung up on its "holier than thou" and fixed the important bits that make usable. I care for things that work, and where I can do my work without the OS getting in the way.

Arch is only good for babysitting a Linux distribution, which is laughable at best. No wonder it is a meme!
1. e4
Ok. Where do I start.



Arch is not a meme. Many people use it daily.



Arch doesn't always break compatibility, and people have had literal decades to shift from yum.



And btw, pacman is actually a wrapper for libalpm.



Also for a GUI, just use pamac. Again, it wraps libalpm.



Again, I do not plan on using GCC, and I won't be compiling even GNU Hello.





Arch is usable. Fedora on the otherhand, claims to be up2date and all, but then you are stuck with month old software lol! And just delete yum alr lol. Yum as in Yellowdog Update Modified, not as in me lol, it's a coincidence.



Back when I chose my name for basically everywhere I didn't even know fedora existed.



Plus who cares? Arch is aimed towards people who use binaries, not for source code junkies. Gentoo exists for them.
So your checkmate move is an opening move? That explains a lot of things about you. Check yourself, buddy.
>Arch is not a meme. Many people use it daily.

Just because people daily drive it, doesn't stop it from becoming a meme.

>Arch doesn't always break compatibility, and people have had literal decades to shift from yum.

Note the word *always*. It does break compatibility often that it becomes a chore to babysit an operating system -- something that enables you to get things running, not troubleshoot it.

>And btw, pacman is actually a wrapper for libalpm.

>Also for a GUI, just use pamac. Again, it wraps libalpm.

Libalpm is a library, and pacman and pamac are programs that use the library. Doesn't mean pacman is a *front-end* to the library!

>Arch is usable. Fedora on the otherhand, claims to be up2date and all, but then you are stuck with month old software lol! And just delete yum alr lol. Yum as in Yellowdog Update Modified, not as in me lol, it's a coincidence

Fedora never claims to be up to date. It packs in the latest packages 3 months before the release date, freezes them, ensures security and integrity, and then releases it to the public. Arch does no security checks, no integrity checks, rarely releases up-to-date packages on time (heck, Tumbleweed does it faster); yet claims to be bleeding edge!
Coming to yum, it is a backward compatibility feature for systems that run the software I am typing on -- you know, something that generates real life *value* -- unlike the neofetch screenshots that's done for worthless Reddit karma!

>Plus who cares? Arch is aimed towards people who use binaries, not for source code junkies. Gentoo exists for them.

Arch is aimed for people who are too ignorant to read LFS. As long as the Arch community doesn't care about things that people need on a daily basis, but rather prefer to be stuck on superfluous internet brownie points, it has no practical use cases.
> Again, I do not plan on using GCC, and I won't be compiling even GNU Hello.

You may personally not plan on using it, but the same compiler is being used by Arch Linux to build almost every software that runs on an Arch system.
No, that's just starting a new game. Don't jump to conclusions.


Nxna7 forces mate.


Joking lol


[at least i dont play at elo 1!](https://youtu.be/7S5Plq6RC24)
I main arch and all you need to do is update and it won't break.



LFS is aimed at people who want to really get nitty gritty with the stuff.



Arch is aimed at people who want a DIY KISS distro that is also bleeding edge.
So what? Report the bug and it's get fixed!



When fedora gets a bug I'll make sure to overplay it and cause drama like you did.
Take it easy, Magnus Carlsen.
> When fedora gets a bug I'll make sure to overplay it 

Sure. I don't take spreading awareness of an issue as a problem.


> and cause drama like you did.

Did I cause a drama? I didn't notice but sorry if you think I indeed did so.
I'm not Magnus carlsen, and I never claimed to be, but I do see the sarcasm. Watch the video, it's hilarious. 


>!and somehow white wins!!<
Bitwig also support PipeWire and to top it all off they're working on a new, more open and extensible audio plugin format called CLAP. So, they get a lot of points in my book.
And looks like it's a specialized build for the Freedesktop Flatpak runtime, no less! Finally a proprietary software that does builds specifically for Flatpak.
Bitwig is a fantastic piece of software.
I am not in the music production business but I hope they sell tons and become really profitable, this is proprietary software done right.

I know this is a controversial topic in this sub but I think we need more of this for the user base to grow on Desktop GNU/Linux.
Who submitted what?
Na&iuml;vely one would think that _Flat_&zwj;pak wouldn't be a good technology for music software...
Never tried bitwig. Is it good? Great? Super awesome? What does it offer that reaper don't? Or ardour?
Same as Conduktor, they even have it on their main site:

https://www.conduktor.io/download/
I really want to afford Bitwig. They're doing everything right on Linux. It's just barely too expensive to justify for my hobby-level curiosity.
Music production software. It's nice because apps that know about Flatpak work better, i.e. they use proper file-open protocols that don't break anything when it's sandboxed
The eternal problem of /r/linux.

$project did something! What is $project? Don‚Äôt be silly, all Linux users know all software.
https://www.google.com/search?q=bitwig

https://www.google.com/search?q=daw
Why not?
i havnt been using it for too long but i really love the modulators. 

https://www.bitwig.com/learnings/an-introduction-to-modulators-45

i used Reason for about 10 years or so where you would have to add multiple rack devices just to do sound design type stuff with LFOs, but with bitwig they are just tiny little squares that take up hardly any screen space and they are lot quicker to program
It's been my favorite DAW back when I used Windows, which made switching to Linux much easier for me. It's closer to Ableton Live than Cubase, Logic, Reaper or Ardour though.
> Same as Conduktor, they even have it on their main site:
> 
> https://www.conduktor.io/download/

They link to Flathub (which is good) but the packaging work does not appear to have been done by them: https://github.com/flathub/io.conduktor.Conduktor/graphs/contributors/

That's one individual who doesn't seem to be involved with upstream and Flathub's own bot. Doesn't look like an automated process that's triggered by whoever runs the release script in-house at Conduktor.
They have a limited 16-track version that's cheaper but has most if not all of the same features i think. There's also a free trial period of a month on the full version, but it kind of stinks because you can't render/export with it. (They're also on some rent-to-own website, but I don't know if there's some sort of catch or something.)

Overall I'm really happy with Bitwig. It's by no means perfect and there are some features from other DAWs that I wish it had, but it's certainly powerful enough for me and has supported Linux natively for years. I'd say that the things it does, it does pretty well.
> It's just barely too expensive to justify for my hobby-level curiosity.

Have you had a look at Reaper? Made by former Winamp devs. Their discounted license is only USD 60.00 and the discount is pretty broad. You can even make an annual profit of USD 20k: http://reaper.fm/purchase.php

Linux support left beta a while ago and they even officially support 32 and 64bit ARM architectures for RasPi: http://reaper.fm/download.php

Not on Flathub, though. EDIT: Not yet: https://github.com/flathub/flathub/pull/3222
> I really want to afford Bitwig. It's just barely too expensive

It's currently 100‚Ç¨ off (I guess it's similar in other currencies as well). It's still a lot but maybe that's enough for you?
I wonder if they properly apply the selinux profile too (on the bitwig side), does anybody know?
> $project did something! What is $project? Don‚Äôt be silly, all Linux users know all software.

I figured that the two mouse clicks that after the first click land you on the mail page of the repo and then secondly under "About" lead you to Flathub's description would be so low of a hurdle, I would not need a special introductory text.
One would think that inevitably the result would be out of tune. Think about it.
I first thought so too, and I wanted to complain about a missing footnote but it turned out that upstream has approved all of it.

https://github.com/flathub/io.conduktor.Conduktor/issues/59

So they might not have done it themselves, but it's certainly official.
> There's also a free trial period of a month on the full version, but it kind of stinks because you can't render/export with it.

Is this really the case? They say "no limitations" and before they offered the one-month full edition they offered a version that was the way you describe it, so I _think_ the current one month trial does allow export.
I've been trying Reaper and Waveform. The main thing I've run into is that Reaper and Vital don't interact well; Vital has nasty rendering glitches making it unusable. This is probably Vital's fault and I need to report the bug.

Reaper seems more stable than Waveform. Waveform 11's stock plugins are kinda bleh, I hear that's improved with 12 but haven't tried it.

My other observation is that Waveform's UI is much more approachable for me. Reaper is packed with features but many of them are in menu trees that could be broken out into UI. It's not the end of the world, but it increases the learning curve in a way that Waveform (and I suspect Bitwig) don't have.

The price of Reaper is great for the features, though. I know bitwig has a lower tier but track limits rub me the wrong way.
Reaper is functionally free if you don't mind waiting a few seconds each time you start it. It isn't open source but Linux support is solid and it's developed almost entirely by two passionate developers. It's also in the Arch repos.  


Reaper's biggest strength is how unbelievably configurable it is, but this comes with the tradeoff that to get the most out of it you have to spend more time learning what's possible and tweaking it to work the way you like.  


It has way more features than could possibly be mapped to the user facing interface all at once. So many times I've wanted a particular obscure feature (say, the ability to live-update midi files after editing them in external software instead of having to manually reload them) and it's just an option, or an action, or a script.  


For EDM production there are probably more focused tools out there, but if you want to integrate a DAW into a custom workflow Reaper is very hard to beat.
Do you honestly click every unclear title you see on Reddit on the off chance that it's of interest to you? And many FOSS projects aren't actually very good at explaining themselves.

Some time ago I saw a post on this sub saying that people posting about some project should include a short description in the title. Everyone enthusiastically agreed, but nothing came of it. Alas.
You could say they have diminished their feature set.
> Reaper is packed with features but many of them are in menu trees that could be broken out into UI. It's not the end of the world, but it increases the learning curve in a way that Waveform (and I suspect Bitwig) don't have.

I've heard from podcasters that they really like Reaper because its GUI can be adapted via skins such as Ultraschall (*ultrasound*) from the German podcasting community. Perhaps other skins to emulate different DAWs are also available.
I've had occasional rendering errors with Vital on Reaper but nothing that has prevented me from using it. Reaper can run both the VST3 and LV2 version, so maybe try the other one to what you've already tried and see if it works any better?
The title says it's about a DAW which is a description, not part of the name. Well, now plenty of comments explain it.
I don't know, seems like a valid mode to distribute music software.
Would android be vulnerable too or doesn't it use netfilter? ü§î
Sad... But lets hope this doesn't get used by the NSA (/s) lol, and let's also hope it does get fixed
How's he gets Cap_Net_admin?
Did you read the article before commenting ?
And in which way exactly this overblown Go crap is better than good old `fancontrol`?
c'mon, be nicer!
So if I'm reading this correctly, it's not really sharing to a projector but sharing from one device to another device's web browser?
This is really nice! I honestly think I use this at work. They installed TV's that only support AirPlay because the majority owns mac's. But all the devs have Linux machines so we can't use the TV during meetings. There are HDMI ports, but very hard to reach and we only have very short cables.
So thanks for this! üòÉ
10/10 icon
Does this work with a Rpi zero W. This would be awesome.

Would there be a way to create a virtual second screen on the client so that e.g. presentation software would be able to use two screens, one for presentation and another one for the presenter view? This would be even more awesome üòú
Can I cast my pickle with this?
It appears to be a server that streams the host device's screen to a webpage that can be accessed through the local network.
I don't quite understand the second question.  Most browsers let you choose the specific window you want to share, so you can just keep your notes in a separate window.
It should.  Basically any device that has video out and can run Python.
It's the opposite actually.  You browse to webpage on your personal device and you can share you screen to e.g. a conference room projector.
Hmmm then I did not get it fully. I thought it shares the entire screen, that is all and everything I have on my computer monitor. In that case I was wondering what would happen if I e.g. start a presentation. On a PC with a single screen, I will get into a full screen presentation mode.
On a dual screen setup (like a laptop and a video projector) one gets full screen presentation on the video projector and a presenter view giving additional Infos on the laptop.

I was wondering, could picklecast mimic a second screen so users get that behavior.
I'd phrase that differently:

Can the user, instead of selecting one of the existing monitors / windows, let the browser create an extra virtual monitor, onto which windows would be moved?

(And the answer is that browsers don't support this, and if they wanted to, they'd run up against quite some difficulties in making the OS get the concept. Browsers usually assume for WebRTC that you don't see the streaming destination, so it makes little sense to them.)
I was more concerned performance like if a Rpi zero W would be up for the task.
Oh, I see.
At least on Firefox and Chrome, you can also choose entire monitors, not just windows.  So you would start sharing a specific monitor, then start your presentation and only the presentation would be cast.
Consider reaching out to the author of [http://xwp8users.com/](http://xwp8users.com/)

Yes, I understand that is the graphical version.  

However, like yourself, he has also worked on the character instance of version 7, acquired from eBay.  

Assuming that you are the author of the above project, in my understanding, you got some things working that he did not; he also got some things working that you did not.  I can imagine a fruitful correspondence.
Op crosspost that to /r/DataHoarder 

There might be someone having it.
perhaps here:  http://ftp.dreamtime.org/pub/linux/wp8/
What's a character mode?
I have a copy Caldera Open Linux with WordPerfect 8.  

Would that be the correct version of WordPerfect you are looking for?
This is really intriguing. I assume the server version would have been a lot rarer, mostly focused on enterprise sales, no? My father uses WP religiously, and used WP9 forever before I got him a newer version. I did buy a physical box for WP8 via Corel Linux, even got a Tux plush!
Will do that. Thank you. Tavis Ormandy is the one working on the project - I'm just posting details on how to make it work for first-time users. Thank you again.
Thank you! I've done exactly that.
It turns out that r/DataHoarder is about storage, and the mods rejected the crosspost...
Thank you - unfortunately it's the GUI version, not the terminal-based one.
It's what you see in the terminal, with the standard monospace font.
Unfortunately, that won't have it - that version only has the GUI version. It seems that the character-based version only shipped with "Corel WordPerfect 8 for Linux Server Edition" - but thank you!
The server version would have cost at least USD500, so, yes, probably very uncommon...
sorry...
I have been continuously upgrading same Debian/Sid setup since ~2000. I think I started with Potato. Still running it now typing this comment.

Moving to new hardware was done by shoving an old hard-drive in, booting up and fixing anything that's broken. Never a clean reinstall.
distro of theseus
Ian has some balls on him.  

I would have had all of this virtualized and had duplicated the vm and performed the upgrades there first.  Once confirmed as everything working I would have migrated the data over during down time and done the switchover.

Ian is obviously way more skilled than the average sysadmin.
Can confirm this works if you do not customize too much. That being said you do need to be aware how config files may change. Otherwise never really had a problem upgrading Debian usually hardware fails before I have that problem.
Impressive. Personally I've never been able to upgrade my Ubuntu install without a ton of issues that forced me to do a clean install afterwards. Now most recently from 21.10 to 22.04
I have been o the same install since Jessie, don‚Äôt even remember the year maybe 2015 or 2017.
i started from Wheez..
Respect buddy
Same chiark that hosts PuTTY? That is a truly respectable uptime!
Fuck man I'm new to Linux and I literally JUST installed fedora last night, this is making me want to use Debian lmao
Not exactly the same as this Debian example, but I've upgraded my Proxmox VE cluster in-place since 2.3 and it's now 7.1 (IIRC). Boy can I appreciate being able to upgrade key systems in-place. Good work Debian!
It's my goal to do something like this with the Arch installs on my gaming PC and server mini PC.  So far so good, but it would really be a feat. 

You have to respect Debian for what it's done and continues to do.
Why is this a desirable thing? Aren't we trying to move to less stateful and more reproducible?
I've been using Linux daily for 22 years.  I've never upgraded in place.  Always just backup and do a fresh install. ü§∑üèº‚Äç‚ôÇÔ∏è
Yep. I retired my Frankenstein debian machine because I moved to arm. From a p3 up to a 4770k with no reinstall.
Same here, started around the same tine even, and never had much trouble with the upgrades. Debian does a great job of making upgrades work with minimal hassle.

 The only bad one I dealt with was an x86 to amd64 crossgrade, which at the time wasn't well documented and wasn't part of the normal upgrade path. Even then, though, I was able to make it work. Thanks to that, my Debian desktop that started on a 99mhz AMD K5 with 8mb of RAM is now running today on an 8c/16t Ryzen 7 1700 with 64gb of RAM. It really grew up over the years. :)
Why
Same here since lenny. My first kernel was either 2.6.26 or 2.6.22. still carrying the same installation around.
Heh...no Arch user can say that. Arch wasn't even around in 2000.
I had a system like that once. I called it TheseOS. It was originally installed in 2002 as a Red Hat Linux 8.0 install, but never actually upgraded - the package manager broke at some point so I ended up just installing everything from source. I upgraded quite a few packages that way, even the kernel (from 2.4.something to 2.6.5). I eventually reinstalled a new OS when I upgraded to a 64 bit processor.
distro of the sus
I dunno, the stuff you described yourself doing seems like exactly the sort of thing a skilled sysadmin would, and *should* do.

I think Ian just wanted to pull off a fun stunt. Note he also mentioned he practiced beforehand, but in chroots rather than VMs.
There was no virtualization or containers like we have today back then
I don't think anyone really knew how skilled he truly was.
Debian's conffile flag usually catches these changes and let me review them.  The one biggest problem I had was at some point the fstab/crypttab options for encrypted temp mounts changed formats and systemd's mount didn't understand the old format (and failed to boot until I managed to film the error scrolling past in the boot screen since it couldn't be logged anywhere) while the old mountall did.
Same here. I've been using Ubuntu since 8.10 and I have never had an Ubuntu upgrade that didn't cause major issues.

Next time I reinstall it's going to be another distro.
I personally have no trouble upgrading Ubuntu from one LTS release to another, but point releases always gave me trouble.
What were the issues in that case? I just recently performed the same upgrade for someone else. What should I ask them about?
This will tell you when you installed your system


stat / | grep "Birth" | sed 's/Birth: //g' | cut -b 2-11
I think fedora is a much better desktop experience than Debian stable because everything is significantly newer. Especially the desktop stuff has improved drastically in the last two years, and Debian stable doesn't have any of those improvements. If you don't need the obscene levels stability (i.e. things not changing, not "no bugs" stability) that Debian provides, I'd say stick to Fedora

On the other hand, distro hopping seems to be a right of passage in this community, so up to you
Come to the dark side, we have a social contract.
Please use Debian or a Debian Stable-based distro like MX Linux. Fedora is a bleeding edge testing distro. It is not meant for workhorse systems and beginning users.
Arch is a rolling release, so it doesn't take anything special to so this. Just a Pacman ‚ÄîSyu
Its cool from a "look what I can do" kind of perspective, but yes, as a sysadmin I shudder at the thought of all the unknown, unmanaged, and potentially abandoned configuration lurking on that machine. Any of my personal machines could be thrown into the ocean and I'd be able to get an identical setup running within an hour since I maintain all my machines via ansible playbooks.
Wow, you‚Äôre really doing it wrong. I‚Äôve reinstalled my Debian twice in about twenty years, because it was easier than doing the 64 bit migration or encrypted disk migration in place - though both would be possible.
Moved to ARM? May I ask what system you are using atm?
Science isn't about why, it's about why not!

That and baiting out a hundred "Ship of theseus" comments apparently.
Back in the way it was easier to upgrade Debian to a new version than it was to actually do a fresh install. 

Plus you *could* when most distros couldn't.  It was also a time when people were unironically suggesting reinstalling Windows every 6-12 months to make it function properly and doing a Windows update from one version to another usually broke the OS, so having a distro that could safely upgrade was amazing.

Then it just became a matter of convenience.  Why rebuild my tweaked and customised system from scratch every few years when it just keeps upgrading and working?
Because transferring all the customized configuration to a clean install would have been problematic and a waste of time. There are probably hundreds of small tweaks on my system which I would miss if I started from scratch.

And because I could since I was running Debian :)
[Suse, yes please](https://www.youtube.com/watch?v=M9bq_alk-sw)
That;s not how any of this works.  Its as trivial as virtualize from bare metal, toss it on a vm host and go.
Yeah. The default of keep old config while useful, can lead to bad habits. I remember a similar issue with a mail server running dovecot. Keeping the custom config would just barf because many options were deprecated and the config file went from one large config to several smaller ones more specific to each function.

Really I should have read the documentation and change logs ahead of any upgrade but ever seasoned admins get lazy after a while.
Yeah LTS from 20.04.5 to 22.04.1 style of upgrades have always been smooth and hassle free for me.
I RAN THIS AND NOW MY DISK IS FULL OF FURRY PORN WTF DUDE NOT COOL
There are advantages to stable release schedules that don't introduce the newest software. We're running a growing number of workstations with debian and Ubuntu stable versions, because we can rely on the installs working over long periods of time as expected. I would be less comfortable with rolling releases because I've been bit with the changing behavior bug on occasion, requiring updating scripts and other settings. That gives the time as the support window ends to spin up new versions, test them and then roll them out. This can be even more important with servers where you want the maximum in predictable behavior. I don't mind rolling releases on smart phones and even my own desktop, but I want servers and staff workstations to be consistent over longer periods of time.
I agree. Stay with Fedora, I find it to be the greatest if not one of the greatest desktop experience I've ever had in Linux, even better than Ubuntu. It rarely breaks. Not sure if Ubuntu has gotten better but Fedora is basically "the stock Android of Linux" to me.
Fedora is NOT stable for fuck's sake. It's NOT for new users. Every install is a roll of the dice whether it's going to work without any issue because it's a bleeding edge distro by its very definition!

Go ahead and downvote me all you want, but Debian Stable and Debian Stable-based distros are what should be recommended to beginners who want a true hassle-free experience. Not a freaking testing distro. THIS is partly why Linux keeps getting a bad name for itself because people keep recommending these distros with less than stable bases. And then those users get bugs and/or crashes and/or weird incompatibilities and before we know it, we have another Linus Tech Tips situation. "OMG, Linux is SO unstable. Fuck this, I'm going back to Windows."

> I think fedora is a much better desktop experience than Debian stable because everything is significantly newer.

Just use Flatpaks if you want the latest and greatest apps. MX Linux (which is based on Debian Stable) specifically supports Flatpaks natively through their package manager so it's super easy.

> If you don't need the obscene levels stability (i.e. things not changing, not "no bugs" stability) that Debian provides

No, it also means... Maybe not "no bugs" because that's an impossible goal for a repo, but it DOES mean as few bugs as possible on top of things not changing. With that said though, MX Linux again will update their own repos with new packages once they THOROUGHLY test them to make sure they're fully and completely stable.
Seconding Fedora on desktop, its near-rolling nature and the fact the Gnome it uses is both recent and vanilla (un-fucked-with by Ubuntu/Pop extensions) gives it a stability of a different kind. Up to date software also means bugfixed software.

I update almost every day. I‚Äôve had one (bluetooth) issue show up from an alsa library update, which i easily rolled back and it was patched two days later.

New Gnome is just better than old Gnome, a lot of people‚Äôs complaints have been solved. Wayland is good. Etc etc.

Using Debian as a server is nice since it changes so rarely you don‚Äôt have to worry much a bout different servers being in different phases of update.

But I‚Äôve been using Rocky Linux because I want a stable distro that‚Äôs not Ubuntu that actually comes with basic stuff like curl installed out of the box.
But I wanted to use Fedora. If something breaks, I want it to so that I can learn how to fix it. I'm not using Linux because I want a stable Windows alternative, I'm using Linux because I want to truly get to know my system, and as stupid as it sounds I \*want\* problems to occur so that I can fix them.
Yeah, for sure. But to have an install of *any* OS go that many years is an accomplishment.
not slamming Ansible or tools like it, but where is your actual DATA?

Example: where do you host your playbooks?
That's a secondary system, not my main. Has some files, ftp and git.

 My main is on arch and it's AMD64.
I have always heard it called "grandpas axe" or some anecdote like that. I never knew the actual name of it, thanks!
The first intel CPUs to support virtualization were announced in 2005. It was a long way from there to today's QEMU.
I look at the diff every time. 90% it's just a version number in a comment at the top or it's something I didn't even change and don't care about so I take the new config.  The rest I have to edit.
üòÇ sorry about that
Right and that kind of environment is perfect for something like Debian. I'm not saying Debian has no place/use case. I just wouldn't use it as a general purpose everyday desktop distro on a personal computer that's all
In my experience Fedora has been rock solid, and Debian Stable has been a buggy desktop experience. The problem with Debian is that it's using ancient DE packages, that are no longer being maintained, and have 2+ year old bugs that have long been fixed upstream and released in a newer version that Debian N will never get (until you upgrade to Debian N+1 and jump 4+ major DE releases, that is)
> If something breaks, I want it to so that I can learn how to fix it.

Alright, but just know some of these problems are going to be problems that require you to have some programming knowledge to fix. If that's what you want, then full steam ahead I say.
> and as stupid as it sounds I *want* problems to occur so that I can fix them.

You might as well try Debian Unstable if you want to face that scenario.
My data is in git repositories which I host on a personal gitea instance in kubernetes. So assuming I've pushed all my branches, recovering my data is just a matter of cloning the repos again. Sensitive data is encrypted with gpg for a key that only exists on my security tokens.
Ah, ok. And is it some kind of a dev board (Raspberry), or something completely different?
Grandpa's axe is a valid metaphor, it is just simpler than Theseus's ship.
You are completely misunderstanding everything.

Bare metal virtualization is just taking an image of the os on a machine and copying the vm to a virtualization host that can run it.
Yeah, Fedora fixes old bugs of course, but it also introduces new ones, and you have no idea which ones they're going to be until they happen.
No better way to learn than by doing!
if you throw all of your personal hardware into the ocean, how do you use your security tokens and how do you clone your repos?

If you have cloud repos then you still have a unique data store.

Once again, not slamming ansible or proper deployment practices, just when I hear people say "everything should be stateless" I'm like "your data is still stored somewhere" and that somewhere needs to be backed up/protected.

My personal data is replicated to multiple places on multiple machines to protect against someone throwing all my machines in the ocean.
It's a banana pi with sata disk. I had a raid 5 on it with the module you can use, but it's getting backed up automatically nowadays and I don't need it to be that robust.
The first part is what I call dding the OS. The 2nd part DID NOT EXIST BACK THEN. You DID NOT HAVE A VIRTUALIZATION HOST.  
and if you mean running it baremetal, then you're just cloning your OS.  
I've done similar things dozens of times. I've been moving most my OSes around for more than a decade. I do know what I'm talking about.
I was speaking more about if anyone throws any single personal machine of mine into the ocean rather than all of them all at once, but you raise a valid point. My git repos are "cloud" based in that my gitea instance on kubernetes is hosted in vultr. So the possible scenarios are:

1. Someone throws all my personal machines in the ocean but not my security tokens nor do they destroy my kuberneres cluster in the cloud: I buy a new laptop, plug in my security token which serves both as my gpg decryption key and also as my ssh key, clone my repos, and run ansible. Everything is fixed.
2. Someone nukes the data center hosting my cloud instance but leaves my laptops alone: since git is dvcs, I have "backups" of my git repos in the form of the clones on my personal machines.
3. Someone throws all of my personal machines into the ocean and either nukes the data center or they find and destroy all my security tokens: I am boned.
Are you even reading the article?   He did not upgrade from 0.93, he was already upgraded to Debian 2017 or whatever.  Your post is so stupid I won't even bother to look it up if you can't be bothered to even read the article.

And to be perfectly fair, even if your post was actually true in any way, in what goddamn world does it matter if dd did not exist in 1993 but it does now?  How could that in any way possible impact a sysadmins ability to dd an existing drive no matter the age as long as the kernel supports the filesystem?

C'mon. 

I know what you are saying but this is getting silly.
Fair enough, and the fact that you've pondered these scenarios means the likelihood is pretty low.

Having done backups for a living, I realize that one of the first steps in creating the plan is knowing what data is actually important and what data isn't.

Would I be inconvenienced if my collection of iso's got lost? Sure. Would it be catastrophic? Nah.

Getting back to the OP's "achievement" of infinite upgrades, I find that a fresh install of the os once in a while is a good thing. I use it to go over the list of stuff - do I really need $app installed that I tried once and never used again?
I didn't read the article because it's behind a fucking cloudflare server that won't open for me.  
and the title said skip-skip-cross, that's debian 3(2003).  
Fuck the article and whoever the fuck wrote the title if they can't write the version.
Ahhhhh ... okay my bad.  Apologies for being rude.  For reference, here is the sentence: Two weeks ago I upgraded chiark from Debian jessie i386 to bullseye amd64, after nearly 30 years running Debian i386. This went really quite well, in fact!

I don't use cloudflare for that very reason, only cloudflare gives me trouble like your self.
The title said:

>'skip-skip-cross-upgraded' a Debian machine that has had Debian since version 0.93R5 (released in 1993)

It didn't say it was 'skip-skip-cross-upgraded' *from* version 0.93R5. And it was from 1993, not 2003. It is not my problem that you don't have access to the post. Learn to read and abstain from comenting things you are unable to read, you schmuck.
1st. 0.93+2 major versions=2003.  Learn semantic versioning and reading, because I did say that.  
2nd. It is your problem for misconfiguring cloudflare.  
3rd. learn to write properly. you wrote 2 lines of text with 2 bits of data, one of which is wrong. the rest is babble.   
4th. You're the schmuck here with your title and your site.
From the article you are incapable of reading (but keep commenting on as if you know everything)

>chiark‚Äôs OS install dates to 1993, when I installed Debian 0.93R5

Now shut it, schmuck.

>4th. You're the schmuck here with your title and your site.

LOL, I am not Ian Jackson.
Some people like to use software that you do not like.
People learned them when their performance was a major selling point and now just stick with what they know.  Some people still use csh over a modern shell.

I presume others like their clunky and retro look and feel too.

But taking fvwm as an example, it hasn't really been updated majorly in years so it's not like it's a high priority for the open source community.
If there was a single company developing these WMs, I would agree with you, in that case they should be reorganized and joined in more efficient and merged teams/products. But we're talking about free communities, people who come together in their private time to develop something because they like it. That's the diversity open source enables and which is the reason we have hundreds of linux distros. If someone disagrees with any aspect of a project, even if it's just a single line of code or code style, they are free to fork it.
Because people use them ...
Why not? People use them for whatever reason... Be it they just like them, are used to them, resources constraints... and for those which are still maintained (and not simply repackaged) the developers do it because they love doing it, or other reasons.

When I started Linux back then, one thing I found great was the diversity of the GUIs I could try, it was so fun! Playing with KDE3 with Karamba, then e16, Fluxbox and others...

The "They shouldn't exist" thing troubles me... :/ People just do what they want, in their own way.
"or could someone tell me that you started Linux with something not fully DE like Gnome, KDE, XFCE, Cinnamon..."

Sure. I started Linux with TTY. Same as I started using Microsoft OSes back in the good old MS-DOG days. I did leave Linux for Windows later (my work back then literally included playing games - freelance game reviewer for well known PC Gaming magazines), but on Windows I replaced the default DE with a sort of OpenBox clone I don't remember the name of. (Back in the day where all you had to do was stop explorer.exe from starting, and start a different WM instead.)

Their "selling point" is when people want a simple system, but prefer stacking window management. That simple. Having to edit a configuration in a text file is, in that case, actually a good thing - you're limited by what you can write in a text file, not by what someone decided to build a UI to let you toggle. Different ones may have slightly different capabilities, be configured in different ways, and thus there's a reason to have multiple ones. (Compare how some might prefer BSPWM's approach where control is left to a separate process, typically sxhkd, while i3 configures the WM _and_ control in the same config.) Different stacking WMs will also have different styles of menu systems available - or not - which can impact your choice.

A more extreme version is, depending on who you are, the fact that something like DWM is configured directly in the source code can be an advantage - no need for a config file handler, making the system simpler, and your only limitation is what you can implement. If that floats your boat, it is what you should use. If it doesn't, you shouldn't.

As for Enlightenment, you'll suddenly lose a heck of a lot of your Smart TV interfaces and so on if they "should not exist". They're the upstream for almost all Samsung TV UI's, if I remember right. (Though I do remember preferring them to Gnome or KDE back in the day, because they were the only DE that didn't look like a donkey's anus back then.)
"maintained today" ???

No, they are just being packaged today. There are ZERO maintenance. For example, openbox's last release is 7 years ago.

They exist in repo simply because people still use them. People use them because sometimes you don't need a DE.

And lightweight isn't a synonym for fast or efficient. It might happened to be fast in the past though.

A preference for something simple is just a preference. It doesn't mean being obsessed with saving some MB of disk usage. Minimalism is just another life style.
Sometimes you just want something simple without many deps. These days I usually just use gnome or another DE, but sometimes I just want to quickly setup a simple WM that mostly works like something I'm familiar with. In that case I'd roll with openbox (or labwc these days) since its default behavior is similar to what I'm used to, and I find the config syntax easy to work with.
I use a stacking WM (openbox).  My workflow doesn't support a tiling WM which is just fine with me.

The reason I run a custom DE is that no prepackaged DE uses anywhere close to the applications I prefer when using Linux.

I use spacefm as a GUI file manager and I don't think any DE comes with spacefm as default.  My image viewer and calculator come from deepin, my panel, task manager and some theming tools come from LXDE and so on.

Why would I want to use someone else's idea of a DE when I can roll my own?
With open source, if you don't like a piece of software, you can join the project and fix it, or start a project to create a fork and try to improve it, or even start a project to create a better solution.  This works because it encourages competition, spurring innovation and improvement.

As time progresses, projects lose development attention, and eventually they become stale.  Some of them putter along, with singular maintainer or only a small group.

On the other hand, ideas from a successful fork can be merged back into the original when everyone agrees that the differences are an improvement.

Bottom line, successful OSS software receives maintenance and development attention and stays 'active'.  The rest dies off.  It seems natural to me.
I think that this perspective is similar to using Gentoo for "performance" reasons.  Yes, indeed Gentoo is great for building a system where the features a user doesn't want or need can be stripped out.  However, the true reason (in the opinion of someone who has used it since it was called Enoch) is customisability.  That's the same reason that I use OpenBox.  I don't care *at all* what something looks like.  I want to be able to set my key & mouse bindings, have control over window placement, et cetera.  I like applications (window managers included) that get out of my way.
Some of them are also historical pieces of software when they were improvement over what existed at the day and stuff like Gnome 1 was buggy...

Some people don't like eye candy. Also most of the tiling wms are ugly out of the box. 

Also some DEs use or used to use them - XFCE used to, Lxde still uses openbox by default.

Some of them are just personal projects to have some fun programming.

Some of them don't see much development anymore, because they are kind of complete.
You work on the flawed premise that what is best for you must be best for everybody else. 

Have you thought about a career as missionary, zealot, or dictator?
The \*boxen are just nice, lightweight window managers that can be simple right-click-to-launch-something shells that still support systray icons.

It's irritating to see people complain about choice in an ecosystem that wouldn't allow for their preferred choice if not for the ability to choose to begin with.
While I agree with most commenters here that while they are barely maintained anymore and thanks to Moore's law and optimization on the X/wayland & big DE world GNOME and KDE (and Mate and Xfce and....) all run snappy enough on modern hardware and use barely any resources for WM, all the while providing simple pleasant compositing with shadows etc, but that does not mean people are not using these minimal WMs.

And OP's language on "why do these exist" is a quite harsh (maybe one could call it silly?), but it is an interesting tought experiment tough: What would happen if lets say Suse (so not Ubuntu, not Fedora, not Debian, not Arch) would decide that "Hey its too much of a hassle to package Openbox, and barely anyone uses it anyway, so next release will not have that package anymore, k thx bai." How many people would be inconvenienced? 100? 10? It wouldn't be a 1000 people, or would it?
I think you are mixing to different discussions here:
- Stacking WMs vs Tiling WMs
- Most of the stacking WMs you mention seems to be dead (unmantained).

I don't know the reasons for this, maybe because Tiling WMs have became more popular lately?

UPDATE:
JWM and FVWM seem to be maintained though, in contrast to Openbox and Fluxbox.
I started Linux on TTY in mid 90s... ;)

Until recently I mained tiling WMs for years, mainly i3-gaps, sway and dwm on various machines. Recently, on the laptop side, I fell in love with new GNOME on Wayland and its' fantastic trackpoint gestures. It's my preferred DE nowadays for almost any purpose, yet it falls flat on gaming, as a lot of games seem less stable/functional on Wayland for me. Sure, xorg fixes that, but I find new GNOME without trackpoint gestures to be... far less pleasing to use. Not to mention resource heavy, on my low-spec laptop with only 4 GB RAM.

The vast majority of gaming applications such as Steam spawn some side/popup windows, and open the games in a new window. This makes them look ugly and disproportionate on tiling WMs. Stacking/floating window managers to the rescue.

IceWM is nice, almost like a lightweight DE. Cold booting into it, only about 300-ish MB RAM usage on my system. I don't main it, but I use it for games when I want to run them.
You cant understand why some people like to only use a few meg of ram to display pixels?

You didn't even mention my preference, WindowMaker. You would probably think it super ugly, possibly sickening. 

Well, the reason why I use WindowMaker and other lightweight window managers is because I think your "eye candy" desktop environments are ugly and annoying, weras WindowMaker is sleek, refined and pleasant to look at.

You listed a whole host of reasons which were actually opinion, your opinion, as to why these window managers are a "problem". All of which only were relevant to people who love DE's with eye candy and swishing rotating icons animating themselves simply to waste cup and gpu time.

As for the titling wm's, well I tried some of those and although an interesting concept I found they got annoying. How they work and what they do sounds great and I liked it, till I got bored of it. I want my windows to be the size I want them to be and not to resize everything. But, I use emacs, which also has a tiling interface which I use frequently so I do use such UI's just only in emacs where it feels better.

Another thing I hate about modern DE's is all the wasted space, titling solves that but like I said I only really like titling in emacs where I rarely go for the mouse. I also absolutely loathe flat UI elements. I hate them all.

Buttons should be 3D.

Buttons should depress when clicked. 

Shadows should exist. 

Things should be above other things and *distinguishable* from what is beneath them so you can tell they have focus.

So to really make it clear how these window managers are still developed, it's because people like me use them.

In fact, you want to see what I love possibly more than windomaker? What looks divine, clean, efficient and can a say beautifully full of lines, edges and clear 3D elements?

My dream UI? Well get ready to loose your lunch if you haven't already but here is what I think ricing cant hold a candle to:

https://en.m.wikipedia.org/wiki/Common_Desktop_Environment

Guess what, that beauty was updated only last year.
A rebel without a cause tilting at windmills. 

Talk about things that should not exist.
light WMs are really useful, because you probably don't realize it, but gnome, kde etc ... are extremely heavy and complex software, gnome even uses a JavaScript engine and some part are writen in JavaScript ! I tested many desktop environments and the complexity of these environments often makes them unstable and some curious bugs appear, it depends on a lot of things and can quickly break it if something in the system is modified. While light VMs, like openbox, fluxbox etc... are simple and much more stable. There are people who don't need all the modern features of current DE and who prefer to use a simple and stable WM that doesn't depend on dozens of libraries and uses a lot of ram and cpu.
Open source means old people who don‚Äôt like change can stick to their ancient technology far longer than is healthy
I use those on low powered machines, like raspberry pi's and old PCs
There's a pleasure in seeking out how to use less ram.

Icewm is nice after changing themes! And  I personally think enlightenment is visually pleasing plus it is its own thing!
Performance, on text only works. No images permitted in any shape or form. All texts encrypted for transmission if necessary over old telephone lines in emergency. When internet goes down (or is cut off by political actions!) communication is still possible with these.
If openbox is too hard for you to figure out, I'd suggest you not ask questions like this.
Okay thank you. Maybe I ran emerge -p --depclean too much
fvwm was pretty good last time I used it.
> but on Windows I replaced the default DE with a sort of OpenBox clone I don't remember the name of.

Probably either bb4win or one of it's forks (bblean, xoblite) or *possibly* litestep.  I kinda miss those days of Windows, but Linux has filled that void.
>It might happened to be fast in the past though.

I lost many words in dev software. But I totally agree with your sentences: It was or used to be fast.
dictator, or even fascist. I like performance guy, not good guy. Btw, you said the truth!
Sorry, I dont like middle choice.
Very much agreed.

When I try to make KDE Plasma, or Gnome, look the way I want to, it's all nice and fun and it works. For a little bit. Then some bug crops up that nukes some of my stuff, just because I dared to pacman -Syu.

With lightweight window managers, things have only ever broken for me when _I_ did something to it. They've never broken on me. I happen to prefer Tiling ones (especially on laptops), but if I was a stacking WM person, that would be a big deal for me.

(Though my gaming computer does use Gnome - after a lot of surgical removal of all the things I _don't_ use. But it does annoy me that to get the things I want I have to use extensions that just... keep... breaking... I used to use KDE Plasma on that machine, but then they broke the animation framework they relied on, so for a month (until the next actual Plasma release) it was a truly horrible experience.)

Remember kids - you cannot have bugs in code that does not exist!
I3 virgin: "Hold my beer"
"LOOK AT ME! I RICE!"
/me goes to grab his stick to give the youngins a lesson.
Well, most of the WMs he mentioned *are* ancient and unmaintained, I cannot say the same for things like Sway and i3.
Yeah it's excellent in its class.  Was my daily WM for over a decade.
I did use litestep for a while too, actually, now that you mention it. But I achieved performance nirvana with what, most likely, was indeed bb4win. Thanks!
They ARE fast & stable today as well. It may not matter to everyone but it matters to some. I actually use Openbox in my main pc.

And zero maintenance is not a problem if there are no bugs.
Your issues seems more related to Arch packages than to KDE or Gnome
Not everyone likes tiling. "Workflow" and "look and feel" are pretty much the most subjective part of software.
>"LOOK AT ME! I RICE!"

U rice, I cheese. Modern problem require modern solution

I willing to use Debian Stable, not Oldstable
No. With KDE, what happened was that they depend on a specific library (released separately) to drive their animations. It's part of the same project, just not part of the same suite. Aaaaaand... they have a completely different release cadence. So they ended up in a situation where they had to choose "what do we break"? (If you track back a few years, there's commentary on the issue on their dev blogs. The summary was "meh, most distros won't update the package until a year or two anyway, so who cares". Ah, yes, the joy of developing on the assumption that no-one will use released software for years anyway...)

Now sure, it's "Arch packages" in the sense of "we package the latest official releases". Same as Tumbleweed etc etc.

For Gnome extensions, they're not even packaged, so that's a complete and total non-sequitur. But yes, sure, being on a distro that does ship the latest released versions, it's kinda sucky that most extensions assume I'm running a years old version...

Because most people building and maintaining extensions will NOT be using the latest version of Gnome. Hell, the latest version of Gnome might be years in their future whenever they update their LTS release of Ubuntu or whatnot.

But sure, I take it your argument is something like "I did it to myself through using the latest release of Gnome". Well done sir. Shame on me for expecting release software to, you know... work with its own ecosystem.
I see you don't understand that Linux is about the kernel, not so much whatever desktop or window manager (or neither), which is why there is still no "Year of the Linux Desktop" yet, just as Torvalds, all he uses is a few terminals and a web browser.
And what point relates to WM/DE in topic? I don't use BSD not bc they're bad, because no one uses flatpak in BSD when I need some apps just-run in flatpak. Torvalds is not the God in Linux world, noone needs to use fedora as he does, is that right? Or you wanna tell me every fluff topic in this sub only about linux kernel?
That's quite an incoherent word salad you're tossing.
Two reasons come to mind:
1. You prefer the UI/UX of one or the other distro (Ubuntu has lots of options, gnome is the flagship, Mint has Cinnamon as the flagship DE and offers a couple alternatives as well)
2. You have a reason to prefer traditional .Deb packages over self contained alternatives like Ubuntu's Snap packages which have some drawbacks as well as some strengths.
Mint doesn't just throw in controversial changes for change sake like Canonical seem to do for years now. Mint is stable enough that I installed it for my Dad on his laptop and mom on her desktop and I never have to touch them. Been using it for years and they keep adding quality of life improvements that are much appreciated. Plus if you like Cinnamon desktop it doesn't get any better or newer than when it comes directly from the developers. I love Mint.
It doesn't come with GNOME-Shell or snap.
Mostly a preference for the default GUI
They would use linux mint over ubuntu because of their personal preference and because they like it better. The capabilities, libraries, firmware, etc. of both systems are identical since they link to the same repos and use the same base system.
snap
I did use Ubuntu years ago. Ubuntu was the default "just works" distro at the time. Then they went rogue with the UI. I switched to Mint which had a more traditional UI feel through Cinnamon. Never looked back since and kept rolling with Mint.
Linux Mint is Ubuntu's Desktop edition with most of the issues fixed and better integration and utilities. There is no reason to use Ubuntu Desktop over Mint, in my opinion.

However, there is no server edition of Mint so if you wanted to run a server OS you'd use Ubuntu.
In addition to not having snap and having Cinnamon I like the file manager Nemo of Mint because of the easy way you can extend it using scripts.
Just use Debian already.
I like ubuntu over Mint because Ubuntu is updated way more often (for better or for worse). Mint is great tho. Ubuntu also had better drivers than any other linux I tried for my laptop. Worked on install and all that no need to mess around. I didn't try mint but I did Debian and it was a pain to get the same drivers working.

Overall any distro is better than windows or MacOS lol
It seems to stay in support longer too.
Well, basically it is just the look and feel of the machine. Both are Ubuntu based under the hood but Mint comes with different / more / less packages pre installed than Ubuntu. If you like the feel or look or packages or services that come in mint preinstalled / preconfigured more than the ones in Ubuntu than Mint is your choice. (vice versa ofc)
I haven't tried Mint for a good few years, but when I did, the reason I wanted to use it was that by default it looked beautiful out of the box.

Though annoyingly I never got to use it because it just wouldn't work with the computer I had at the time and I never got to the bottom of why that was.
To your first question: this may be a personal preference. I think mint is a bit better than ubuntu for semi-advanced users. I like its default looks more than ubuntu too. And overall the mint team seems to be a bit more professional. This may also have to do with me not liking GNOME, nor unity before. The Linux Mint desktop is ok-ish (I think it was cinnamon or something).

Your mileage may vary of course.

For the practical point of view there should not be a huge difference. You can switch on both systems and both use debian at the end of the day. 

Firmware etc... should not be any issue - it works equally well or bad on both. But you really should not have any problem there. I tend to use mpv+ffmpeg and mpv+ffmpeg kind of work with just about everything, at the least when it is supported (which matches to most what is popular; I can not recall having any problems with codecs in decades).
Back then for me was that mint came with vlc as a videoplayer and then cinnamon
Because Mint is the better Ubuntu with many issues fixed and removing all the proprietary stuff that canonical pushes in Ubuntu.
On the desktop/laptop I prefer Ubuntu LTS for the stability and the 22.04 made me switch from KDE to Gnome.

The last 20+ years I have worked with linux on servers, mostly rpm based. When I'm off duty, I need something that just run.
The issue with snaps is usually more philosophical than practical (this doesn't mean it is irrational).

While snaps are slower to run from cold boot, it's true that it's getting significantly improved to a point it shouldn't matter too much, but some people prefer the more community-oriented Flatpaks or the traditional debs for distro building. Given snap is replacing debs for Firefox and Chromium, some people are being turned off by Ubuntu.
Especially snap.
>	The capabilities, libraries, firmware, etc. of both systems are identical since they link to the same repos and use the same base system.

That‚Äôs true when comparing Linux Mint and Ubuntu LTS, but Ubuntu also offers non-LTS versions that have newer packages.
Yep same for me. Been with mint for years.
what are ubuntu snaps?
Why?  I like my software to be at least somewhat up to date.  If you don't, then that's fine as well.
no
or Linux Mint Debian Edition.
what did you about windows and mac os?
It's a software package system. Mint has abadonned it because of it being commercial. [https://linuxmint-user-guide.readthedocs.io/en/latest/snap.html](https://linuxmint-user-guide.readthedocs.io/en/latest/snap.html)  
(you can still use snap in Mint if you want)
Debian testing is the best distro for me. Newer packages, stable, no weird cruft added by most downstream distros.
Ubuntu and thus standard Mint are based on Debian testing.

I prefer LMDE, which is Linux Mint, based on current Debian stable, it is rock solid. Who cares it's not as cutting edge as Ubuntu based, it's not that it is old or something. It is just the best way. You have Mint Cinnamon plus the no nonsense stability of Debian.
I was getting good sound with pulse effects but found it was using a lot of system resources. I only had an EQ and a Bass Boost (or something along those lines) enabled. Would def use again if anyone knows how to ask pulse effects to take it easy on system resources.
Jamesdsp?
It's good it has some interesting presets and adaptive bass 
You could try it
Easyeffects (pipewire only) is probably better as it uses native pipewire filters instead of gstreamer.
Do you have any experience with replacing ALSA with Pipewire? For Ubuntu ALSA seems to be the default.

Edit: It seems more like a replacement for PulseAudio. I just don't want to run into issues that may be circumvented from the start.
I've been using Pipewire and Easyeffects for a few months now and I really like it! It was a little twitchy at first but it seems to have settled down quite a bit in the last few updates. It's at the point for me now where I don't have to think about it, it just does its thing in the background. :)
Pipewire replaces pulse and jackd, ALSA stays. I found the best way to replace the legacy systems is to upgrade to a distro that supports Pipewire, like Fedora
This is actually great for open source, this is the point of open source...

They will scour the most important code bases to the government and ensure no bad actor has added code that can be exploited to attack their operations...

We also should not trust them though, it works both ways. They may want to sneak in ways to attack their enemies, and technological innovation should not be hampered by these petty squabbles. Humanity should be allowed to continue reaching into the future regardless of ideologies, engaging morality independent of belief system.

So, I welcome their investment in the community, but their basic motive is hazardous to humanity generally. They protect the status quo and thus are fundamentally against the real drive of open source. It has nothing to do with them being American, it is true of any military or government agency getting involved.

They do not desire the best for you and me, but their paranoia and fear is useful as security is extremely important generally.
You have issues out there like the npm package that wiped hard drives linked to Russia.  How many more times can a single developer tantrum just show up and do something stupid like wipe hard drives??  

https://www.bleepingcomputer.com/news/security/big-sabotage-famous-npm-package-deletes-files-to-protest-ukraine-war/amp/

So much of Open Source software is only managed by a single person or maybe a dozen people?   Granted, who has the time to plant hacks in their code when teams are small? But when they do, it's a major issue.
It's a rather fair question when it comes to Linux security but a broad one. While I'd like to say that every Small and Medium size business could handle the capacity to sift through and document all prior updates being deployed, the truth is...the capacity to do so in its entirety is just not there. If they want the clear-cut answer that security can't be 100% verified for opensource, then yes that is true. However, it should not be understated that there are tools are in place in the sphere of business architecture for that very reason.

I would have thought they understood that security was "one" of the primary drivers behind Cloud-Infrastructure and Data Centers today. That was a huge given. Modern cloud infrastructure for basic businesses today have methods of introducing layered security through "verified" cloud-based applications on the front-end and log aggregation and reporting in the backend. Some of the tools include Oracle's Cloud Infrastructure, ELK, DarkTrace, etc..

So, while most applications on the open-source end of the spectrum can conclusively be unsecure there are measures of layered security today to keep things secured as best as possible. My general rule is "In the digital space, nothing is secure, and it should never be assumed as secure even by design". 

Adapt or Die.
I hate these hyperbolic headlines, everything about it is wrong. It's not like DARAPA doesn't already currently use a shit ton of linux servers. Every gov agency does. The gov is just CYA, it's been on linux for a long time and will continue to be. I hate how it's making it sound like the gov is unsure if it should try this new fangled thing called linux. It's probably red hats biggest customer.
I am more worried whether I can trust DARPA. Or "Secure Boot(ing)" or "Trusted Computing". These honesty-inspiring words inspire distrust in me.
>DARPA is worried about how well it can be trusted

i mean thats fair , its  like theirs has been example swhere open-source devs have poisnined their own repos before , 

i think theirs a saying , 

trust but verify , most people dont do the second , people trust but never look though code
A roundabout way of saying "I want more power and control".
I am worried how well can I trust DARPA.
> To do this, the researchers will use tools such as sentiment analysis to analyze the social interactions within open-source communities such as the Linux kernel mailing list, which should help identify who is being positive or constructive and who is being negative and destructive. 
>
> The researchers want insight into what kinds of events and behavior can disrupt or hurt open-source communities, which members are trustworthy, and whether there are particular groups that justify extra vigilance.

Well this seems stupid...  
"Haha I'm destructive on the public mailing list, look I'm a bad actor !"  
DARPA definitely is aware of social engineering and "being nice to achieve goals".

This article feels wrong... Maybe it's because I've been reading Snowden lately.
That's funny. I'm worried how well DARPA can be trusted...
They can and should be worried about all third-party and first-party software. The good news is that unlike closed-source third-party software, they and every other military and non-military organisation can exhaustively review it as much as they have time for and can afford. With their resources I don't believe they're only just getting round to it now.
100% they will try to push backdoors
Coincidentally, the rest of the world is worried about how much the US military can be trusted.
That's funny.  I'm not real clear on how far DARPA can be trusted.  I guess it is mutual.
Technology Review is a complete joke of a publication. Their hitpiece on Steve Kirsch and their failure to redact it‚Ä¶my god what a joke
Didn't the NSA do this already? I know they put a lot of code back into the Linux kernel (SELinux amongst other things).
Incidentally I think that's also a case against running shit you don't know without at least a container
This article has a really weird tone. None of this is news, and people aren't just "realizing now" that most of the computing world is underpinned by Linux. The US military has had its eyes on Linux for a long time, too, as the NSA has contributed a lot to the kernel. SELinux, for example, is originally NSA's work.
I've had decades of work experience doing classified software engineering and development both inside the government and for contractors. In all that time I never once encountered a serious objector to open source software, \*except\* for those people who were basically in the pocket of closed-source vendors.
If they're so worried about it, they should assign devs to contribute security fixes to it!

God knows the CIA and NSA have enough cybersec focused developers on hand who are, likely, already familiar with the code and it's exploits.
Huawei is the biggest contributor to the linux kernel!
I think there shoud be state run code auditing agencys which also fix found bugs in open source code but also be allowed to audit every proprietary code and fine companies for every found bug
It's a monolithic kernel with a practically unauditable codebase as a function of the size of its trusted surface (it has problems with [ambient authority](https://en.wikipedia.org/wiki/Ambient_authority)). That's all you need to know about how trustworthy it is.

What we need is work on making things like [seL4](https://en.wikipedia.org/wiki/L4_microkernel_family#High_assurance:_seL4) & [OSes based on it more user-friendly](https://en.wikipedia.org/wiki/Security-focused_operating_system#Object-capability_systems), while reusing Linux & other such systems for their driver support.
Speaking as one of the guys who introduced OSS to DARPA, I want that job.
They better start contributing then.
I am all for this government intervention but have the same doubts on their integrity as everyone else. I mean what if they try to put some sneaky backdoors into the kernel which make linux more prone to hackers. Also it will be an invasion of privacy and so i think that if the governments are to participate in such things then they should do so ethivally
Really nice that they are investing in review.

However, no software can be trusted without review, neither open nor closed source. So stupid to believe: "I paid a lot for that piece of software, it must very good"
if they hoard 0days this would help FOSS
Anyone else think that article was way too short?  That's a big topic and that article took a very small bite.
Oh go back to windows then.
This is great news, a lot of systems are compromised daily and stiff regulations and punishment for habitual offenders will lead to better coding. It could help us rather than hinder us from doing our jobs.
what if we open source the govnerment #woah
[deleted]
> We also should not trust them though, it works both ways.

I've had a lot of nasty things to say about the NSA's level of spying over the years, but they also do a good job with their hardening linux guide. 

It's rather surreal to go from using a tiny OS that few mainstream people have heard of to multiple US governmental departments talking about it.
Well, the NSA already does all that.   
They have two missions: break everyone else's security, and make sure US civilian and military security is robust.        
Sometimes these are in conflict, but it also often results in good things, like quality ciphers and security patches for Linux.    

This sounds like DARPA wants to build an understanding of who's contributing what to open source projects, probably to understand stuff like "these individual contributors are working as a team in secret", and "this group is discreetly making significant changes to this core bit of code, but hiding it behind many disparate changes".   

It sounds like an interesting thing to look into, although maybe a bit fuzzy to draw hard conclusions from.
They want the best for the government, which sometimes just so happens to overlap with what's best for you and me.
" reaching into the future regardless of ideologies, engaging morality independent of belief system."  


This, in and of itself, would count as an ideology though....
They'll make private copies that they edit for use, while poisoning the public copies with spyware and bloat.
>This is actually great for open source

sorry wait you think theyre gonna add happy fun time benefits to Linux kernel on the American taxpayer's dime? is that what capitalism does? benefit others and at cost to its self?
>You have issues out there like the npm package that wiped hard drives linked to Russia. How many more times can a single developer tantrum just show up and do something stupid like wipe hard drives??

It feels like this point really hasn't been taken as seriously by people as it should have been. Actually now that I think about it, I wonder if that saboteur has been prosecuted (since to me that seems like it'd be a clear violation of the CFAA)

[Also relevant XKCD](https://xkcd.com/2347/)
We need systems that consider isolation a primary feature, not an afterthought that gets ignored for lack of budget.
If you think open source is bad, imagine buying hardware you have to reverse engineer and disassemble üò±

DARPA can worry all they want, and make whatever contributions they want but don‚Äôt let anyone suggest that open source is some kind of issue ü§£

The code is open for all to see, whereas binary blobs, firmware and protected hardware is far more insidious.

Something for sure needs to happen to reduce potential exposure for widely used libraries, but beyond that I‚Äôd worry far more about the things you can‚Äôt see. I‚Äôm just a n00b though so ü§∑‚Äç‚ôÇÔ∏è
In Canada, what this dev did is a criminal code offence even if the destruction took place in another country. If they were Canadian they could have received a sentence of up to ten years.

https://laws-lois.justice.gc.ca/eng/acts/c-46/section-430.html
Your quote. And maybe its my own misunderstanding but got me to thinking: when it comes to industry it's just not a question of are but where are the vulnerabilities. Going from there I feel most federal networks/software admins or project managers, not sure what you'd call those trigger pullers, are so stuck in their ways that this alone creates vulnerabilities. It's so easy to blame software and in some cases hardware that the human element hardly seems to be a concern when it comes to this type of analysis.

Even with all the money in the world they use third party solutions when something can easily be done in house, which also provides better understanding to offer training, and ultimately creates a more secure space. 

Let's dump mills into some research boys, gotta shred this cheese to increase bonu- I mean budgets next year
The linux kernal is 27.8 million lines of code.

I personally think open source for classified networks could end up being a bad thing. If bad actors have access to the source code which your infrastructure running on top of I feel as if it would be easier to find an exploit.
Yup, I read this as: "the NSA just decided to add all prominent open source developers to their list of domestic surveillance targets."

The cynical side of my also presumes that this realisation that the "enemy" could have plants in the open source community likely coincided with a realisation that the NSA could have plants in the open source community, so that's probably something we can all look forward to.
Secure boot and TPM related standards are open and not some black box you have to trust. The only thing you have to trust is that the vendor of your TPM hasn't added a backdoor, likewise for your motherboard firmware and Intel's ME / AMD's PSP.

If you want to revoke Microsoft's keys that you probably have by default and install your own keys that you generated yourself and sign your own bootloader and kernels then you can absolutely do that. All Secure Boot does is move from a model of "trust any unsigned code to boot" to "only trust code signed by an approved key" where your UEFI config should be letting you specify your own keys.
ill go to my grave claiming this until someone disproves it: Intel purposely created or did not fix the speculative execution attacks because they allow only the most powerful of institutions an easy backdoor into almost any and every system that went to market. Maybe AMD did too, but Intel I know works closely with US government and I have no doubt they let SE remain a serious issue to gain favor. 

This applies to other governments too since, again, it's only a consistent, efficient benefit to governments because they can easily seize equipment for any bullshit reason. That also means it's region-locked, in a sense; even if the same flaw effects US CN RU equipment, one still needs some strong institutional backing to seize the equipment in the first place. otherwise it must literally be physically stolen which is a huge risk for a million reasons (even if you succeed the victim is now aware). so, effectively, its something that can effect literally every government without actually effecting them too severely, not so severely as to give up the huge and obvious benefit of spying on their own citizens.
You can very likely trust DARPA that they won't sell your data to random commercial entities, but you can't trust them not to use or collect your data at all, for their own purposes. Honestly, I'm fine with the latter, as shitty as it may be.
I really feel like they should be checking it as a standard procedure. They're large, and important, enough they can afford it, and it'd probably be significantly easier than negotiating with a proprietary company to show theirs. I don't see the problem they're having.
>i mean thats fair , its like theirs has been example swhere open-source devs have poisnined their own repos before

Https://github.com/advisories/GHSA-97m3-w2cp-4xx6 for example.

>most people dont do the second , people trust but never look though code

On the one hand, because they assume that someone else will do it. But also because you have neither the necessary knowledge nor the necessary time. I myself belong to the latter group.
> trust but verify , most people dont do the second

left pad for the win!

To infinity pad!!!
I don‚Äôt do it because I trust you for reviewing all softwares you run on your machine, which are a big part of the most common softwares we all use.

Thank you for your dedication, it must be hard to spend so much time reviewing millions of line of code while being jobless because it takes so much personal investment for the community.
Yeah, most people who swear by this stuff don't actually understand anything beyond "Microsoft spies on you" and many proponents of FOSS don't - and can't - actually explain to people what's going on programmatically.
Machines should be doing the looking through code, with a human pilot.
Came here to quote the same paragraphs. Well a subset of it:
> To do this, the researchers will use tools such as sentiment analysis to analyze the social interactions within open-source communities such as the Linux kernel mailing list, which should help identify who is being positive or constructive and who is being negative and destructive.¬†

I see Torvalds, Kroa-Hartman, and several individual top contributors ending up top of the list for "negativity".

That automated negativity detection sends some bad 1984 shudders down my spine.
> who is being negative [‚Ä¶]

Linus Thorvalds is going to get banned from his own project
No need to worry *how well* DARPA can be trusted. They've been very forthcoming about the fact they're untrustworthy.
That was my first thought when I read it.  They're awarding all this money on contracts to essentially see who's trust worth and who's not (using AI, of course :/), when I'm interested in if they could just redirect that money to some security researchers and maybe subsidize some bug bounties.

I mean, I hope they realize trust worthy people could also introduce bugs inadvertently that's hard to spot but nonetheless very damaging.
Source?
Found an article. That is disgusting to know. But thankfully with open source code, people at Intel would know if there was any Chinese malware being added to the kernal.
Android?
This is why it's good to have competing governments around the same code, they will both look for each others stuff... and any good actors will take the best of both and throw away the bad for a net benefit.

We also need to continue building technologies that address vulnerabilities after the fact, SELinux is probably the best for this among a wide variety of technologies... we should not be telling people to just turn it off rather than telling them how to actually manage SELinux correctly... it would be better for them to just run a Windows server otherwise.

I don't rely on distro's that just tell me about updates for this reason, nor do I like things like AppArmor because it only protects what you tell it to which is just the wrong approach entirely... it should only allow what you tell it to because it's less work than trying to lock down every single application individually.

Any security strategy has to be multi-pronged because even good actors make mistakes that can be exploited.
After watching Redmond accept binary-only deliverables from the 3 main law enforcement TLAs, sign them, and ship them with the OS... I can only imagine this is the government trying to find a way to do that with Linux.
I've never heard about the Linux kernel being compromised by a government. Have a CVE or wiki page I can read more on it?
we should just require China to review American PRs and vice versa
> A few have been so crafty they made it in only to be found later.

Source?
>made it in only to be found later

`if (userCountry = RUSSIA || CHINA) then spy()`

Actually, do you have a sources to read on it? I'm really interested, how these exploits look like, so that they've managed to sneak by dozens (hundreds?) reviewers, who know kernel code very well
I assume Heartbleed and the flaw in x86 chips that allowed exploits was their work as well.

If I was a foreign power I wouldn't allow foreign chips in my country.
That's why hardware backdoors are the way to go as a government.
I feel a lot of this could be connected to the OPM data base link. I know recently a good friend of mine who is a Linux Admin got a great job offer working for a contractor connected with the US Govt. 

If you don't know the OPM database leak was where China installed spyware on a Linux server that sent over the background checks of everyone with TS clearances to china. The spyware was cheekly named Norton Anti-Virus. It was a junior Admin that discovered the process running and thought it was strange there was a program called North Anti-Virus on a Linux machine.
Most of the NSA and DISA hardening guides are really good.  Combine it with NIST policy guidelines and you are well on your way to a decently hardened enclave.
The reality is for a variety of situations it simply doesn't make sense not to use the open source solution anymore... corporations see that they can't compete separately so they try to add value on top rather than doing their own thing.

Unfortunately the value they ordinarily add is pretty buttons, so users gravitate there...
To be fair few mainstream people have heard of it still...

It is generally hidden behind corporate brands, but I guess it doesn't really matter provided they continue to advance the technologies.

The reality is most just don't care about details like this, they just wanna get on ig or watch videos and play games.
They do not want civilian security to be robust at all, they explicitly want to make it illegal to make civilian systems particularly secure at all.

No one has gathered more information on citizens than this organization, how do you think they accomplish this?

Do not think that contribution tracking is a bad thing, this is one of the most important features of git: we explicitly know who's responsible for every change, there is direct responsibility for every commit, necessarily.

We know it happens though, and we know because people are actually looking at the code that don't want any such exploits at all.

This situation should improve as new code is increasingly written in Rust but it may be one of the biggest weaknesses of Linux right now, C is just hard to actually secure.

That is why containers are more and more popular, isolation creates more layers of safety.

I would not use a system that uses traditional packaging ever again, the only other viable option to me seems to be Nix currently...

I am absolutely shocked that people still use X knowing that developers literally quit the project because it is impossible to secure.

It is crazy just how insecure the Linux systems we used to boast about security for actually were... and people still essentially use the same systems today.

It's actually shocking how insecure Linux can really be by default on many distros.
I don't mean just a few either... I mean like X hasn't had any support at all in a long time outside distros fixing bugs or whatever if that even still goes to any particular upstream... there is basically no one maintaining it on purpose anymore.
Weaponized important infrastructure projects like the Linux kernel isn't good for anybody. Any short term success by any government, regardless of how well  intended will backfire over time.

Creating distrust, forking the codebase, switching to balkanized national/private kernels. None of that would be good.
No, it never benefits you and me, not really... 

They sometimes strive to appease us so that we don't interfere with their power, but that's not the same thing... we're still disadvantaged by their power as such.
I intend it as a practical matter...

You can certainly create an ideology around such ideas, but my intent is simply to work together to innovate increasingly effectively without alienating anyone that is capable of contributing.

Ideologies are more absolute, abstract... what I'm saying is entirely relative and can be a function of various ideologies for each participant individually.
I mean, the government loves benefiting the rich and they're the main ones actually using this software generally...

I think they would like to have all these services be more secure because it is likely their own information is on these sites...

I don't think it benefits you and me much because we just aren't viable targets as individuals unless we're engaged in something important...

Certainly, I have nothing private on my computer or in any way connected to the internet.

Everything available is something I'd share with a stranger on a first conversation... because that's how I treat the internet.
A quick read on its wiki page is telling me that law is centered around unauthorized access and theft.

an evil library was invited, and this one wasn't stealing anything.
Damn I would love to have a open source motherboard and cpu for x86. I would pay ten grand for a ryzen blackbird.
Hence the reason they are in the state they are in now. I won't got into details, but Mil spec architecture is getting better, it could be better, but the red tape (rightfully so) isn't conducive to the pace of software and hardware development today and in the future. They need to start making some heavy sacrifices and layer up if they seek to modernize and steadily compete in these coming years.
The latest TPM, Pluton, is 100% a black box. Look up the history of "Palladium" to get to know the roots of the projects that became TPM.
That's great and all, but since we can't verify any of that...
>If you want to revoke Microsoft's keys that you probably have by default and install your own keys that you generated yourself and sign your own bootloader and kernels then you can absolutely do that.

Which would kill 3 of the four devices I have here because we are already at the firmware needs to be signed by MS keys step here with increasing amounts of work done to make circumventing this impossible. Are you seriously thinking this will get better in the future? Just look at the new processors. Hello proprietary blobs sitting on a lower level than any open source code you can run.

And how is Secure Boot not a blackbox when every vendor seems to have his own implementation and only the in- and outputs are standardized with these openly available standards?
I've never seen any mention of a public information source with the specs of TPM or Secure Boot. Care to cite?
How long was the bug that allowed x86 chips to get rooted open and in the public?
I'm not. These assholes keep trying to outlaw use of strong cryptography, which would leave the entire country a sitting duck for cybercriminals around the world, solely for their own convenience. They can go straight to hell.
How can you be fine with being spied on by a hostile government?
Unfortunately you have many documented cases where NSA knows about and keeps zero-day exploits for themselves, or goes as far as intercepting and modifying Cisco hardware to add exploits (and these days, they just mandate that cloud service providers build their cloud with backdoors to save them the trouble).

Just imagine if they were using all that talent to patch and close these vulnerabilities instead of what they do now...
Dear Reddit Android App,

It would have been nice if I could have highlighted just the text that is a url/link to copy/paste into my browser.

Thanks,

Someone Who Hates People Who Block Copy/Paste Text Feature
According to the article.
[removed]
What about Israeli-American malware?
Unfortunately no, Android has the same monolithic kernel problems (and most of its language-based security applies only to its JVM APIs, which unsurprisingly leads to its C & C++ parts being responsible for 70~90% of vulns on it) , a bunch of corporate malware added (no need for compromise if the system is malicious by design) (all the proprietary drivers also benefit from the ambient authority problem) and some serious user lock-in problems (the user should be god as far as the system is concerned - which isn't the case for Android - but arbitrary programs shouldn't be).
I don‚Äôt remember the details, but wasn‚Äôt there some weird code change made by on government  that opened a very weird exploit on a very remote scenario, which another government caught, kept, and introduced another slight mod in some other source file that would give them the possibility to disable said exploit without the first gov knowing they have been caught.

I only remember very vaguely, though. Might as well been a movie in the 90s‚Ä¶
Both AppArmor and SELinux are good, if you use them correctly. Having work with them both, I find that SELinux generally is switched off or set to permissive primarily due some requirements of expensive  "Enterprise" applications. CxOs who paid for some expensive stack of apps  aren't going to listen to some Engineer why killing SELinux is bad.
Fun fact that you may already be aware of: SELinux originated at the NSA.
There are no good government actors. They are all in it for themselves. The best we can hope for is the spider man meme version of them all pointing at each others exploits.
wouldn't a different government just make a patch for themselves and then use the exploit themselves?
[disappears]
> I assume Heartbleed and the flaw in x86 chips that allowed exploits was their work as well.

They are most certainly not. Everything about these two things screams typical incompetency rather than malice.
Oh, this is the first I've heard of it. That sounds really scary that they had their fingers in a government server. I wonder what else they got. If it were me, I'd do a full rsync of the drives and see what I got later. 

I'm worried about people spying on my computer. I do all my banking on there and have some files related to my business. (Also I'm the only one who should know how much time I spend on reddit looking at pictures of cute baby animals... other than reddit themselves, of course.)
Hyperbole much?

An organization is perfectly capable of having grossly conflicting primary objectives.   

No one is saying the NSA is some bastion of privacy respecting security auditors.    
They're snoops, hackers and security auditors.   

You misunderstand how bureaucracy works if you think that a massive agency can't simultaneously work to fix the flaws in a security protocol, work to add new ones, and advocate for the protocol to not be used.     

"No one should use end to end encryption because it's expensive for us to bypass.    If they do, it should be free from easy to find weaknesses that APTs can exploit to harm Americans.  As an APT, we want to be the only ones who can effectively do that."
Basically, desktop Linux is the least secure system currently? Even Windows fates better?

That world be ironic since Linux was always touted as "more secure than Windows", and yeah  it might have been in the nineties and the naughts, but times change.
> switching to balkanized national/private kernels

That's idea is quite intriguing though. It'll be like a Tower of Babel tale unfolding right before our eyes.
>the rich and they're the main ones actually using this software generally

i have to imagine rich people are the people who use Linux the absolute least. if you mean they get use out of, okay sure. but they dont use Linux anymore than they do their own fucking house chores. 

The US government clearly doesn't care about security they care about snooping. Evidence strong investment in cybersecurity since 9/11. Not a COLA adjustment to the budget for cybersecurity Im talking an investment that shows the US actually cares about that kind of thing. I'd love to see it.

And all the nonsense at the end isnt really worth responding too, sorry. if youre just googling furrie costume tips all day long that's fine but dont tell me how i get to use my computer. im not letting anyone see shit, there's no justification in the US. innocent until proven guilty not vice-versa. press charges then look, not look then press charges which is what youre saying. blech
It's not just about that - it also covers distribution of malicious code intentionally (which is what the developer that modified that library did):

>Whoever knowingly causes the transmission of a program, information, code, or command, and as a result of such conduct, intentionally causes damage without authorization, to a protected computer;

https://www.law.cornell.edu/uscode/text/18/1030#a\_5
Couldn't agree more, and I have noticed this rise in security minded development, both software and hardware. It's pretty cool too see but leaves me wondering if I'm late or on time to the game. I'm excited for it but the rush inspires the question: is this push just fluff?

Heavy decisions are for the youth and unfortunately there's too much money to be ignored to keep folks on board with, likely debilitating, flaws that are easier to ignore at higher costs. Gatekeeping is grandfathered in. Y'allready know shit rolls down hill so it makes it even harder to trust "the best" outside of encryption and network protocols
But the interface to it is not a black box. It's a mostly isolated component sitting on the LPC or SPI bus. It's not like it has direct DMA access to all of RAM or the ability to run arbitrary code on the rest of the computer.
The same problems are present without secure boot. It's not like you didn't have to trust all of that before.
The spec is insanely big, open and the result of collaboration between pretty much all companies (and a bunch of security agencies and universities) with an interest in hardware.

[https://trustedcomputinggroup.org/work-groups/trusted-platform-module/](https://trustedcomputinggroup.org/work-groups/trusted-platform-module/)

[https://trustedcomputinggroup.org/membership/member-companies/](https://trustedcomputinggroup.org/membership/member-companies/)
EU here. We have our own shit to deal with, but this one isn't it. I'd rather fight US *companies* trying to lobby and strongarm their way into europe to enforce their BS that is already enforced in USA.

I'm more concerned about rampant capitalism, seeping in, than US intelligence having tabs on me. Former impacts me greatly, while latter, despite sounding scary, does not.

Things like net neutrality (the lack of thereof), SOPA/PIPA/ACTA and their permutations... these are the scary stuff you keep exporting.
Even if you're spied on by a non-hostile government, I doubt the non-hostile status is permanent.
By leaving backdoor bugs in the code :)
Hostile?
Yeah, I missed that, thanks. From some further reading it looks like there are some broken incentive structures in place at Huawei leading to the really high number of contributions, lots of which are just simple fixes and spelling corrections to pump up numbers and make their involvement appear more substantial.
Your submission was automatically removed because you linked to the mobile version of a website using Google AMP. Please post the original article, generally this is done by removing amp in the URL.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/linux) if you have any questions or concerns.*
You are right. the fact that Israeli-American malware exists makes it OK for China to do it. I was more concerned originally about Chinese malware, as their piece of shit government requires all companies work for their state in order to accomplish their terrible ideals, but instead I am now more worried about malware written in order to take out nuclear reactors from another rogue state.
Yes, Android would be really bad for a Desktop OS right now, but is has a security concept that could be a cool addition for a Desktop OS if properly implemented. 

Like apps can by default access nothing, and ask the user for everything they might want to do or every directory they want to access. But of course it still must be possible to run a program with full homedir or even root permissions. 

Flatpaks might be a way in the right direction, but their sandbox right now has many holes to sneak through.
It's stupid that I sat here for almost a minute even trying to think of a 90's movie...

And the one I thought of, I don't think is 90's... flight of the navigator is 80's I think now?

It's good though, the brain is making room for more relevant things.
I'm saying though, AppArmor is designed to be used incorrectly.

SELinux is turned off because your average admin is a lazy bastard.

Most CxOs tend to be sold on it when you tell them their data is trivial to access without it.
>listen to some Engineer why killing SELinux is bad.

That engineer should be setting correct policies for SELinux though. Worst case that engineer should be containerizing it if there's some really weird edge case scenario that SELinux policies can't fix for some reason.
And Red Hat... yes...

And the lead developer still works at Red Hat long after NSA ceased investment...

I believe they genuinely wanted to protect their systems exactly because they're very aware of how easily most systems can be accessed...

I think that their SELinux rules are probably absolutely nuts and they would never share exactly how they set it up for obvious reasons, but the technology is necessarily highly advanced for this very reason and available to everyone because it was necessary due to the license.

Even if it was purely maintained by NSA though AppArmor is still stupidly designed.
I mean, everyone who contributes to open source is in it for themselves.

The best motivation is peer acknowledgement.

No one outside core teams should be trusted, everything committed should be reviewed.

We should still take measures to stop any bad actions by these people, or simply missing something.

And we should even check that, and that should be checked, and really nothing should ever be trusted because anything can let others in.

Ultimately complete security is probably impossible for a system connected to another.
They might want to make sure that companies in their country aren‚Äôt vulnerable to a foreign government‚Äôs backdoor.
I already said this but I ramble a lot so you probably missed it... 

It's why I also brought up AI approaches such as CodeNet and other approaches to tackling advanced QA...

Humans should not be trusted generally, the only good motivation in the community is showing off and trying to wow peers.

If they're doing it as an assignment for work they're probably sloppy and thus dangerous.
Heartbleed we know exactly where the it was introduced and by who, and it wasn't the government. It's wild people make up these accusations when the git history is right there.

On heartbleeds case it was simple: for as widely used bad openssl is, there were essentially only two full time (I think volunteer) devs working on it with a code base of over 500k loc, with no ability for security audits or anything else. Really an example of OSS abuse - for a project as important as openssl, it should have had all the big players contributing with not only engineers but even at least periodic security reviews and audits.
They got a shit ton of SF86 forms which is the form you fill out to get a top secret clearance that you have to list all your dirty laundry on. It was bad, it was by far the worst data breach the US govt has ever dealt with...by alot.
I feel like I'm actually understating the problem in an effort to remain civil...
I currently use Fedora Silverblue, this utilizes things like SELinux while being a completely containerized system with an atomic core... that is hardened by SELinux... and systemd takes extensive advantage of SELinux to mitigate its problems, at least potentially and as implemented on Fedora...

Using sysv and x is infinitely worse than Mac and Windows, I honestly can't express to you how terrible both are. It's honestly like they just didn't consider security at all, and it really probably wasn't a thing when they were designed.
Yeah, it's not.   

Both systems have a very complicated security footprint.    
One system can actually be publicly audited, so the bugs get found faster, and fixed faster.  

It's not like windows isn't written in C++, and it's not like rust is a magic security blanket.     
It provides protection against certain classes of memory based exploits, in cases where those protections don't need to be disabled for functionality.
I mean, most businesses rely heavily on open source... and this is the largest collection of rich people in the country today.
They don't define authorization in that law...

The author authorized it.  See it's even in the root word of it...
Pluton is on-die so it can't be bus-sniffed.

They 100% can host arbitrary code and I've seen them used to store back-door access in a couple of defcon talks now

Intrusive DRM requires intrusive user-hostile access.

https://www.forbes.com/sites/richardstiennon/2013/11/16/trusted-computing-must-repudiate-the-nsa/?sh=1bc0ed9c7eff
> the interface to it is not a black box

oh i can see the cows go in then ??? and then they come out hamburger. i have no further questions, thank you.
So it's different from PSP/IME?
the public part of the interface is not a black box.

It's not like it has known direct DMA acces to all of RAM or the ability to run arbitrary code on the rest of the computer.
Yeah, but I also didn't have the TPM chip running arbitrary code.
As an American, I wish those concerns were separate. The US intelligence agencies can and do just buy data collected by the various corporations run amok.

In reality, surveillance state and surveillance capitalism are two sides of the same coin. It's impossible to eliminate one so long as the other lives. Maybe it's different there, since you've got the GDPR, but I somehow doubt it.
I've heard of European governments making noise about banning encryption too. Don't get too comfortable over there.
They come hand in hand.
If the question is "Can I trust the government?" the answer is "No." Doesn't matter what government it is or if you voted for the dickhead(s) in charge or not. It's _always_ "No."
Yes
Sounds like it's working as intended
Why worry about a what a country that MAY make companies develop malware as a national asset.

When you can worry about a country that HAS used the state to development malware as a corporate asset.
Always look at the bright side!
There's Enemy of The State from the 90s I think IIRC, that one's about NSA and they clashed with the FBI at the end.
If you have deployed SAP or some expensive Big Data shit then you will know what I mean. Lazy? Hardly. Don't be too harsh, nobody is putting their pay cheque in question facing off with their bosses. Especially if you have mouths to feed and mortgages to pay.
Hate to me sometimes. I get paid doing what the customers want. I don't like killing SELinux but I make sure my customers are aware what could happen in writing and if they want to move forward, ok then. Mercenary yes, but need to pay bills. Philosophies are good but they don't pay. Then again Hadoop and SAP people really dislike SELinux.
Sorry if my comment came off as a criticism of SELinux or a defense of App Armor, that wasn‚Äôt my intent. 

I don‚Äôt think SELinux is bad because it came from the NSA, I just thought it was an interesting fact that was apt to this comment thread that some readers may not be aware of. 

I agree with you that SELinux is better than AppArmor, even if SELinux has caused me pain at previous jobs :)
To be fair, if the government contributed malicious changes to an open source project, it would be a random person who has been given hush-hush money, not a git commit signed "The Government".

Still, super unlikely that heartbleed was a result of this.
Yikes! That means China could potentially blackmail those people in an attempt to get secrets for the rest of their lives. I see what you were saying that the hardening guide probably grew out of that then. It was probably one of the things they proposed when they did a postmortem.
Why on earth would you be uncivil in a conversation about open source software?

If you're so caught up that you feel the need to be mean to strangers on the internet, maybe it's time to take a step back.
A lot of people dislike these things because it curbs computing freedoms like picking and choosing your own hot key daemons. One commenter even told me that s/he doesn't want Linuxto be more mainstream because then it'll be just like Android: locked down and no way to customise it. 

They grew up in a computing environment where they had command upon the whole sytem. But times where they could do that without danger are long over.

As for me I cautiously looking forward to immutable systems, and thinking about moving to Fedora due to Ubuntu gotten stale in the last few years.
On Silverblue, the security comes from SELinux, Flatpak and using Wayland instead of X?
Instead, you had a motherboard firmware running arbitrary code. And GPU firmware. And winmodem - anyone remembers winmodems?
Had an Intel ME or AMD AMT the whole time running arbitrary code tho
The TPM can't run arbitrary code on the rest of your computer, it's also completely optional and generally removable. Others have already pointed out the real threat being firmware and management engine code which is a completely separate issue. Unless you're running something ancient enough to support Libreboot, you're trusting binary blobs from third parties. None of this has anything to do with secure boot or TPMs in general.
Yeah. Everyone should be pro privacy. Government or corporation.
Can you trust the system? No. Can you trust people? Maybe...


I'm sure people get caught performing victimless crimes by system employees, but they don't feel like filing paperwork unless there's a personal vendeta with said employee or said offender is a high offender and they need to be made an example out of.


I just don't feel safe with the system, I have no reason to believe such leeway is permanent or if their is tracking of something that isn't a crime now, but might be in a future and make it retroactive.
Don't feel it.
I prefer to think of it as being realistic.

Not looking at a whole side will create incredible ignorance in you.

This will create fear where acceptance and intelligent action is more effective.

I remember on a possitivity binge I watched a video of wolves chasing and killing a buffalo and what my mind interpreted and commented on was that it was cool these two species were playing together in nature. It took someone else to actually point at what had really been the case, and this sort of delusion is obviously really problematic when applied to activity in the real world.
On a Red Hat/Fedora machine it is quite trivial even if it looks complicated immediately, if you know you want a particular application to work it is just a quick command.

Opening up every other aspect of your system because you don't want to read documentation is insane and should lose you a job in IT. If you actually care about your job you should have a more fine grained rule for each important piece of software, it should only do what you want it to do because everything else is bad.

This is the nature of real IT work... you can't be lazy, the point of your job is to protect the company against developers errors so that hackers can't access company intellectual property.
Your statement here is actually more insane than that though...

You are basically arguing that giving away trade secrets is the best way to keep your job... 

How have you even typed these words without realizing they're dumb?
I don't know what kind of contracts you're getting where they're not withholding at least partial payment for non-conformance unless you're actually mitigating SELinux setting to permissive or something.

I guess if you're talking to a CxO about this instead of Engineering/Architects then they must be small clients. I generally work for F500s now and I'd definitely partial withhold and write a vendor non-conformance so we don't do business with you again. I don't mean that disrespectfully but if you're just tossing your hands up, I'd basically see it as my org having to finish your work for you.

But I guess if it is working out for you, then you do you. I just hope you don't do it to me.

I mean SAP/ERP is a pain in the ass but that's the job and I'd hire an integrator for it that specializes in it just specifically so I wouldn't have to worry about stuff like this.
SELinux has not been touched by the NSA since I started using it.

Its maintainer team is shockingly small and old, it is especially boring work.

That is who I trust though.
It could be that this is such an obvious fact to me that your attempt to inform me has gone over my head.
Sure but in this case we know it was a German PhD student implementing the Heartbeat extension from RFC 6520. I'm not about to accuse that person of being a government shill and purposefully introducing the exploit - and I think it's rather garbage for anyone else to do so with not even a shred of evidence.
Correct
Because actually engaging the position that Xorg is perfectly secure is an insane prospect to me.

The notion it is a viable piece of software is completely untenable.

It is a function of ignorance, and ignorance is so annoying cuz the information is so easily available but no one cares about details.
For me the benefit of open source is realized when we come together to work towards common goals, everyone doing their own thing just creates duplicated efforts.

I am not against all that that, if they really don't want to work on the accepted technology they can do whatever they want with their systems... but by streamlining things we are creating much better and more maintainable code, and users don't appreciate this enough.

Ultimately this is a huge reason the desktop has never taken off, it's just too difficult to target for a developer whose interests are outside operating system design.

Barriers to entry are not features.
I mean, there are a lot of security measures acting together... systemd takes advantage of selinux in loading every service, it also provides a foundation for containers along with the kernel cgroups feature that allows Flatpak to separate every application both from other applications and the system itself, using dbus to communicate between containers and even things like pipewire are made specifically for container requirements... then wayland is the way applications communicate with the screen, again working around containers so that nothing can communicate with anything else...

It is essentially the same system design as iOS and Android, and enables things like app permissions to give users greater control of what software can do... this is currently managed through an application called flatseal for instance. So really a lot of this is just modernizing the software stack for Linux, but certainly security is a key factor in this paradigm.
There's also all this stuff that's just standard and carries over:

https://fedoraproject.org/wiki/Security\_Features
"It's okay because it was already there"
You are not wrong, but truly how many people actually listen to techs? SAP, Hadoop or those really expensive application stack -  in many cases those people's official stand is that either you make SELinux permissive or kill it. We can argue until the second coming but again, if the bosses have been made aware in writing that is happening and they are happy with it and the vendors are standing by their written recommendations, just do it.
You are taking this very personally and I don't know why. Grow the hell up. It is just tech and tools. I don't care. I am done talking with you since you obviously haven't had enough experience in most things.
Jeez man you still don't get it. If you want it I will do it. That's how I get paid. I don't get paid going against what my customer says
Well again I didn‚Äôt mean to infer that it was inherently untrustworthy because of the NSAs involvement. If it were inherently untrustworthy due to government involvement we have much bigger fish to fry than SELinux. 

Also I know they haven‚Äôt been involved in ages , hence why I used the word originated. It‚Äôs long evolved past that. I‚Äôve worked on government systems with people who didn‚Äôt know SELinuxs origins so it was just a fact that popped in my head.
Well that‚Äôs why I said ‚Äúyou may already know‚Äù. Can‚Äôt tell what‚Äôs an obvious fact to who from a paragraph on the internet
That sounds interesting. I think now that it is year 2022, it is finally time to ditch the MS DOS security model and get something better. Right now, I run some programs with Flatpak, and also did revoke some programs /home or internet access, but I know with X11, there is not much security if a sandboxed program really pokes against its sandbox. (And I really need to read about what permissions exactly allow what)
Citation needed for sap , since there is a special guide for sap on rhel.
I really hope I never hire someone like you, and I quite hope nobody in my company ever hires someone like you.

prem_gnosis is saying you're useless. I would take that further. You're an active danger to whatever company you work for, and your security team should be beating down HR's door to fire you.

It would be with cause.
I would quit.

You're the reason IT work is being passed to developers.

The career is basically going extinct because you're useless.

Well done.
I once wanted to be part of the field...

It was the most exciting reason to move to Linux in the first place some 25 years ago... 

Your entire job can be done better by a well written Operator...

It allows real laziness because it's completely automated...

You serve no purpose in the field any more...

It is because at least the Operator isn't going to open the company up to a thousand exploits so that his job is easier.

Real value is provided by developers already, what is the point of you?

I hate your whole attitude, how does it count as experience?

You're literally telling me you didn't bother even learning a tool because it's too hard, and you want respect for your position.

You're literally useless, but I'm not taking it personally.

You're why DevOps is such a great movement...
No, but there is a thing called integrity.

You have to make it clear where you know something isn't right, and give better options. If they still go down that route, they choose it, but if you don't try, for an easy life, it's not a quality service.

However, corner cutting consultancies keep people in jobs as there is always a need to clean up after the "yes boss,  we do it for cheap".

You will find that if you do push back and are honest, you develop more trust with your clients and they feel they can rely on you more.
They basically just sponsored Red Hat until it was feature complete for what they wanted... and the guy who did most of the work is still there, used to hang out on IRC a lot and is probably the most intelligent person I've ever interacted with.
I mean, DOS basically just gave you an interface to the hardware...

It is incredibly simple, it didn't even expect to have internet access...

Even when Microsoft went to NT it wasn't great about security...

A UNIX system is a little more complex and secure than this... just due to it being multi-user it at least tries to keep things apart...

DOS is single user.

It didn't even have X... that is what Windows originally was... a graphical system for DOS...

Xlib sucks though so we started using Qt but the license is lame and the company isn't really pro community so GTK exists...
Several of my customers and current SAP Admins I am working with cited these -

 https://access.redhat.com/solutions/2489391

RHEL 8 for SAP

https://acceas.redhat.com/articles/5946171

I don't work with SAP HANA but do prepare systems so that customers can work with them.
Come on, pile on the insults. You boys think I can make decisions on what goes in, that's cute. You don't get to make decisions on Enterprise systems unless your pay grade says so. I was once very fired up like you lot. Lost more than a couple of jobs because of my philosophies. Fine when I was single with zero mortgages. I am not now and I don't care what people say or do, I go in, do my job as the customer wants it, make sure they know the best practices and give ecommendations in writing, that's it, if the refuse to hear it, ain't my problem. I need to indemnify myself. You don't force customers to listen to you. If you are truly a business owner you would know what I mean.
Thank you. My career is going well and have travelled to different continents doing and get paid quite well doing it. You need help.
I believe it. As you said in a previous comment it is a necessarily complex system and for a small team of developers to develop and maintain it at the level it is is incredibly impressive
With MS Dos security model, I did refer to this post https://theinvisiblethings.blogspot.com/2010/08/ms-dos-security-model.html

It is from 2010, and things are about to change slowly, but I think what is described there is still very accurate.
I'm looking at the sap note and they do say that they support selinux and its up to the admin ( https://launchpad.support.sap.com/#/notes/2777782 )

However, I think there was have been a time when the performance penalty was unknown and this unknown was not something that they were comfortable making a judgement call on.

(from https://www.redhat.com/en/blog/security-recommendations-sap-hana-rhel )

----

In February 2022, after extensive testing on RHEL 8.2, 8.4, 8.6 and 9 using the SAP HANA validation test suite (which is used by SAP to assess if a system can run SAP HANA at acceptable performance levels), Red Hat‚Äôs engineering team concluded that SELinux can run in Enforcing mode (the recommended mode) with a minimal impact to database performance (around 2%, which is acceptable and in line with SAP standards). 

----

Additional note:

The above link should point to access.redhat.com.

Edit: formatting.
It sucks cuz everything you've said suggests you're bad at it and someone is going to replace you one day and have to fix it.... at least it won't be me.
What's most impressive to me is how simple the tools are considering the complexity of what is being done.
That's why it's such a shame companies just flippantly tell people to turn it off...

It will literally just build a rule set for you based on the software you're trying to use... they've made it so you don't even have to understand it!

Still it's too hard  :(
Actually the error right in the logs tells you how to not have the error anymore... lol

Still people don't bother.
It can't do it automatically else there's no point having SELinux...

It's technically not an error, it's just working inconveniently.
That... isn't a security model at all...

Although it is the approach of X...

Coincidentally, both originate prior to the internet as we know it.

Because UNIX is a multi-user system X is at least limited to accessing the users stuff by default, but if it's running as root itself any exploits gives you access to everything... 

Gets pretty DOS'y from there.
That's interesting, I will use that. Again as I have said, customers want it that way so I do it that way. I don't need to like or agree with it. So RHEL for HANA 7x still wants it dead. OK. Killing SELinux for me is setting it to permissive. Thanks for the info.
Hopefully they replace him sooner rather than later.
No problem.

Yeah it looks like there isnt a good profile for 7.x yet, and 7.x is in the maintence part of the product lifecycle so it will likely never get a selinux profile.

I haven't benchmarked this difference myself, I wonder how much difference performance hit there would be between permissive and enforcing.

Thank you for talking about this like an adult.
At the end of the day we are here to share information, as if the world needs more strife. Cheers man
Has Windows passed Linux in terms of stability? Maybe I‚Äôm just suffering from old habit, but I still find myself rebooting my Win10 work laptop on a fairly frequent basis in order to get rid of performance problems. Whereas, I reboot my Linux laptop and server after a kernel upgrade ‚Ä¶ sometimes.

I‚Äôll grant that Windows stability has gotten a lot better over the years, but I‚Äôm not ready to say it‚Äôs better than Linux is.
I was initially drawn to linux *because* it was considered vastly more stable, as it was derived from unix.   
"Windows always crashes" was just accepted as the norm; it was the general mantra to always be sure to 'ctrl+s' save your work every few minutes, lest a crash wipe it all out.   


\-this was in the windows 95 era, mind you. Crashes were indeed common. Very, very common.
Literally 2 years after the book came out, Neal Stephenson switched to Mac OS. At least he's still using Emacs.
> I recall the consensus on a different post asking 'what does windows offer that linux can't' to be stability

Whenever I hear terms like "stability" and "security" in the context of comparing two OS, I always ask what "stability" and "security" means. I always get subjective answers :)
> Where along the way did Windows surpass Linux in terms of stability? Or are these perhaps differing definitions or contexts of what exactly is meant by 'stable'?

Almost certainly different definitions.

Developers use the word "stable" to describe a concept closely related to [semantic versioning](https://semver.org/).  A stable release is a sequence of updates in which no breaking changes are expected to appear.

In the commercial software world, where customers pay for support, it's common for contracts to require long term support for a stable release of a product.  However, in the Free Software world, there often aren't any support contracts between the developers of an application, library, or distribution, and the customers who use it.  And without a contract to outline a support lifetime, an awful lot of developers don't seem to feel any obligation to maintain interfaces long term.  So this is where the post you mention indicated that Windows offers more stability (probably).  Windows' programming interfaces are stable for far longer than many interfaces on common GNU/Linux systems.  (And that is why software built for one distribution very often won't work on another distribution released at around the same time.  There are many many small libraries each evolving at their own pace and those changes work their way into distributions at different times, such that each distribution presents a unique collage of programming interfaces.)

But while developers typically mean that interfaces don't change in incompatible ways when they use the word "stable", lay persons generally just mean "reliable" when they use that word.  So, I'm going to guess that the people in the post you referenced who talked about Windows stability were probably professional developers or engineers, but as you describe yourself as not technically experienced and fairly young, you probably inferred a different meaning than they implied.

Years ago, it was certainly true that Windows was much *much* less reliable than GNU/Linux systems, but that's much less true today than it used to be.  But it has always been the case that Microsoft's programming interfaces were more stable than many Free Software interfaces.
I have never heard of a server room environment where the average uptime of individual Windows servers exceeded that of Linux or Unix.  Shit we used to joke that our time since last boot on our Linux servers was frequently over a year while the Windows servers were lucky to make it a month or two.  Nothing has changed in the last 2 decades that leads me to believe Windows is **more** stable than Linux but Windows itself is more stable than it used to be.  Linux has improved too supporting far more hardware and becoming much easier to install and set up.

The main items Windows offers are better hardware support, potentially a higher level of vendor support (debatable for sure) and greater numbers of users familiar with the product.  Also not sure how the graphics subsystems in Windows were built but holy crap the ability to just throw random graphics cards from different vendors and eras and it just fucking works is significantly better than Linux.

Eventually I think Windows will fall by the wayside completely and it will happen in much the same fashion that IE fell.
I remember very well how horribly buggy WinNT was - 4.0 was almost unmanageable in a production environment. And how Win98 advised to restart the system in no more than every 49 days.  That was the official request from Microsoft.  Linux, even back in the day, had uptimes of years - without a hiccup or even a notice.  It still does now, and I am fairly certain that if there was a bare to the metal benchmark, Linux would still win handily.
Next read Cryptonomicon.  Better plot and characters.

(Kidding--you said you were a fan :)

(Also, anyone else who hasn't read this book: go read it.)
Funny and entertaining book. I've used Windows since 3.1 and Linux since it had a GUI. We measured Linux in uptime days. We measured Windows in reboots per day. You had to reboot Windows to install or do almost anything. Often multiple times. And BSODs were a mainstay of NT.
I think we have arrived at a time where both are very stable. I remember windows 95/98/ME were very, very unstable and having some process use 100% CPU basically always made them freeze and the taskmanager was useless/barely existant. This got MUCH better when MS switched from DOS-based to NT-based with 2000/XP. Since then I could make all my stability issues go away with fixing some specific issue. I can say the same for all the linuxes I used. If it crashes, there's an issue somewhere, not "linux is generally unstable" or "windows is generally unstable"

(stuff that made issues appear like using a -git kernel, weird compiler options, windows insider versions, beta/modded graphics drivers, unsupported, old hardware with drivers manually fiddled in... but that's where the fun lies, right? :D )
36,329 word essay.  Published in 1999.

Source: https://web.archive.org/web/20180218045352/http://www.cryptonomicon.com/beginning.html
My wife has a windows gaming machine and I have a linux one.  We both use suspend about as often, run into problems about the same (very rarely), and use the systems extensively.   

There may have been a stability issue in windows before, but if my wife's experience is anything to go by, there's not one inherently in windows now. 

Personal opinion is that I think linux users tend to be more conscious about what they're doing and windows users tend to be more "fly by night oooh, an exe on a dubious page, let's run it." and that might have led to stability being an issue in windows.
>he makes Linux systems to be unstoppable and Windows NT very buggy. However I recall the consensus on a different post asking 'what does windows offer that linux can't' to be stability

the problem is that the word "stability" in windows-land and linux-land does not mean the same thing *at all*. With windows people, if you say "stable software", it means "this software does not crash / have bugs". With linux people, if you say "stable software" (as in, Debian stable), it means "this software does not change". e.g. Debian stable won't accept an update to a software that fixes bugs but also change, say, the software's user interface as it's expected that if you install Debian stable, it means that you don't want your computing experience / workflow / ... to change at all. Conversely, on windows no one really cares about things "not changing" - every version of windows changes the UI deeply and people are expected to roll with it.
I recommend the chapter on drills to my students in CS classes. It's kind of hard to summarize why I'm so adamant about Linux as a tool. That chapter puts about the right feeling into the heart.
NT before NT 4.0 was not easy to get stable.

NT 4.0 was much better.

Windows has become more stable over time.  At some point they mostly removed the BSOD.  If you don't do overclocking and use quality drivers, Windows can be very stable.

Linux always had more uptime, which people conflate with stability.  The reason for the longer uptimes is that, unless you update the kernel, you don't need to restart the system.  Even so, Linux crashed less than Windows.

In the modern age of cybersecurity, every patch cycle includes kernel updates, so both Windows and Linux systems typically need to reboot at least monthly.  And both are significantly more stable than either was in the NT 4.0 days.
Stability for Linux ... or most of the UNIX variants was/is in years. Even today, my win 10 box requires a reboot about once a month, or I get lockups.
In the beginning there was Jack and Jack had a groove
At work, we have to reboot Windows machines much more frequently than Linux ones. Windows is certainly better than it once was, but no, it hasn't become more stable than Linux, at least not in the server space. If I don't reboot my Windows desktop at least once a week, it starts to get a bit "whimsical".
If anyone described Windows as more "stable" than Linux they meant binary compatibility. You can take old .exe and it'll most probably run on newer Windows. On Linux it would require a recompilation or tweaks. That's just an edge case for abandoned proprietary binaries.
Windows probably hasn't surpassed linux (though this is complicated by many different distros) in terms of stability, but it is a juggernaut because of the monopolistic practices it uses to force windows as the default via backdoor relationships with computer hardware manufacturers.

Windows is the story of a predatory monopolistic business model managing to hit network effects before competitors could come online. It's a shitty, mediocre operating system built by an organization that doesn't give a shit and couldn't find its ass with two hands, but it's the default.

Also, Melinda French divorced Bill Gates after it was revealed that he had extensive and multiple encounters with Jefferey Epstein. Gates, for all his philanthropy, is in all likelihood a pedophile.
I've got Linux servers that get rebooted once a year for cleaning, they're never idle and I don't have to do a lot to maintain them. To me that is stability. These days I could see windows boxes having good uptime like that but for some reason one of the easiest and most effective methods of trouble shooting issues with windows is rebooting it.
I run both, windows mainly for vr, but every time I boot windows its a different issue. Its honestly amazing how it manages to break itself when its not even running. I have a (probably horribly mis-configured) arch install and rarely do I have an issue. I occasionally have an issue with bluetooth that makes me reboot, and some issues with some games, but thats it.
I haven't used windows in years now but yeah im in the same mind on this
I run both Linux (btw I us... nm) and W10/11. I think both are equally stable. I think its been more than a decade since I saw a BSOD? Yes Windows does require a reboot more often, but thats more due to some design choices, it has nothing to do with making it run better and makes no difference. And its only once every 1-2 months and they no longer reboot without consent, all the old issues are all gone.
Ah, the spring cleaning, including reinstalling Windows.

And the winter cleaning...
Well, still less hypocritical than switching to Windows would have been, what with Mac OS's under-the-hood Unix-ness.
Have you even read the essay?
That's what I was getting at in the original post
What exactly do you even mean by 'programming interface'? I've never seen someone use that term before. Are you talking specifically about programming suites for software engineers? Seems like a bit of a misnomer...

Beyond that, I guess routine software updates are logistically more difficult in large commercial settings than I realize as an individual user.
[WE DO NOT BREAK USERSPACE!](https://lkml.org/lkml/2012/12/23/75)
Wait you'd say hardware support is better on windows beyond just graphics cards? I can get Nvidia aren't playing the linux game nicely and I don't blame them per se since the market and support/porting for the gaming industry isn't there... And to be fair I've never tried building a Windows PC but damn I've always been blown away by the ability any distro I've used has had to just make hardware work without fucking around. 

Interesting last statement, too. How exactly did IE die? And have you read the essay? Why hasn't Windows already died?
Hahah yeah Bobby shaftoe was a bit more of a badass than jobs and gates combined
I dare not suggest Lawrence Waterhouse was more accomplished than Richard stallman
Yeah I think people just really have no idea how unreliable early Windows was. Never lost an hour or more of work because of a crash. And for power users, bash scripting was easily more powerful than any built in Windows scripting language. I remember plenty of times I had to download a 3rd party tool to do something I could do in Linux on the command line in a few keystrokes. 

To be fair, Windows had Office, and the Linux options at that time were rudimentary at best.
What is suspend?
Also im pretty sure when system admins say windows isn't as reliable, they aren't randomly running questionable executibles...
Yes he's a skilled writer in terms of making effective analogies. I really appreciated Cryptonomicon for his ability to approach complex math and computer science in a way that keeps it accessible without previous knowledge
The uptime flex is a real thing. I‚Äôm using Fedora, which wants to reboot after almost every update, so I can‚Äôt flex with that, but my Solus machine‚Ä¶ oy, blin can she rack up some uptime. My old Windows 7 was stable until it wasn‚Äôt, but even in best form it couldn‚Äôt match  Solus.
My teams too to calling those therapeutic reboots. You don't *have* to reboot, but snagging a new kernel and drivers every so often can help.
Sounds like you're not getting kernel updates.  Might reconsider that, given the number of security related patches, and the frequency of release.
This has been my experience with Windows 10/11 both as a home user (yes, I'm a filthy dual booter) and a professional software developer (I had a job in a Windows shop and, no, it wasn't in C#).

Sure restarts on updates are common in Windows. But that's because of how updates interact with the infamous Windows registry. Restarts don't happen without my say so, either manually or scheduled. And modern Windows reboots pretty darn fast.

But I haven't seen a BSOD in Windows in *years*.

As far as people's Windows computer from their job being more finnicky than their personal computer running Linux... I don't know about your work computer, but my company put a lot of bullshit security theater software on it that's CONSTANTLY running in the background and using system resources. Of course my home computer running Linux is faster/more reliable! It doesn't have any of that crap on it!

And guess what - my personal Windows 11 SSD doesn't have any of those problems either. Again, I'm not running a bunch of stupid security apps on it all the time.

Windows requires a little more maintenance, in some ways, than a desktop Linux system. But it's not very much, honestly, and it's no longer necessary to reinstall the OS every year or whatever people were doing.

I have a lot of criticisms of Windows and I use my Linux install (Fedora 36) 98% of the time. But it's only rational to acknowledge the ways in which Windows has (vastly) improved over the years. And the things that Windows does better serve to highlight areas where desktop Linux still has a lot of room for improvement.
whats hypocritical about it
They are referring to an API - application programming interface. 

https://en.wikipedia.org/wiki/API

For a simple example, accessing files is a very common thing that programs need to do. So I make a library that other programs can include to simplify the process. In that library, I add a function called `readfile` that takes one parameter that is the name of the file. Programs can include my library, call the readfile function, pass it a file name, and my library will open the file and return the contents. 

I release the library, and programmers start using it. Well, it turns out my readfile function is awful with huge files because it reads the whole thing in one go. So I want to add a second parameter to specify a number of bytes to read - that way a program can read the file in smaller chunks at a time. 

Now I have a bit of a dilemma. Option A is to add a second parameter to readfile and release a new version of the library. The problem is that older programs won‚Äôt work with my new version because they don‚Äôt know that they now have to specify a second parameter.

Option B is to add a new function, `readchunk`that takes 2 parameters, and leave the original readfile as it is taking only 1 parameter. The problem with this is that I now have a messier interface, but older programs will still work without changing their code. 

Option A creates an unstable interface, but it keeps the interface clean. Option B maintains a stable interface, but after years (and decades) of additions, it can become more difficult to understand and use the library because of so many similar functions.
Yes, the kernels interface is very stable, but most of user space doesn't make system calls directly. Most calls go through libc.
> Wait you'd say hardware support is better on windows beyond just graphics cards? 

Been 18 months since my last Linux install but I can remember having to use a wired connection to get the wifi driver installed, I suppose that's more of an ease of installation issue.  

> Interesting last statement, too. How exactly did IE die?

I suppose IE is still alive in name at least but it has had it's heart ripped out because Trident is well and truly dead.  This may indeed be how Windows dies too, at some point it just becomes a Linux distro with a Microsoft desktop/window manager running on Linux kernel with maybe a Microsoft curated package manager as an alternative to apt or yum.

> And have you read the essay? Why hasn't Windows already died?

Nope, have a link?
>I remember plenty of times I had to download a 3rd party tool to do something I could do in Linux on the command line in a few keystrokes. 
>
>To be fair, Windows had Office, and the Linux options at that time were rudimentary at best. 

Both of those this are still very true.

After daily driving Linux and having access to the core utils and shell, trying to so simple tasks in windows can be a nightmare. You *have* to use third party tools. Or I guess learn PowerShell but thr


Open source/free office software still pales in comparison to office. It's matured to the point where you can get by for almost all tasks but once you get into advanced uses is falls flat.
Suspend, and it's a power mode.  And I don't think we were talking about sysadmins.
They're only have access to a debian mirror for updates and run stable releases, they do get rebooted for dist upgrades and run unattended upgrades, I'm also subscribed to the security mailing lists and keep an eye on them.
Ok so what does windows do better than a desktop linux system?
Nothing... the essay is more a comparison and an evaluation of merits and culture than a rejection of any singular OS. Not sure what these two are getting at...
Thanks that's an awesome explanation
IE was EOL June 15 2022.

There were rumors that Windows was trying to go to a linux kernel.  Then we got WSL.

As long as it has the desktop market share that it does, Windows will keep on tickin.

Windows Server is a juggernaut as well.

When the shiny wears off 'cloud', we're going to see hybrid become standard.  Some services will be from the cloud, and some will be on-prem.  Linux may run the cloud, but MS excels in the whole AD + System Center + workstations realm.
Suspend. Are you just talking about turning off your computer?
Multi-monitor support is the easy answer there. You got different screen resolutions? Different refresh rates? Need fractional scaling? Windows just works. Linux doesn't.

Oh, and let's add font rendering that doesn't make you think you need new glasses.

Also, Windows offers a single target for third party app developers in a way that the hyper-fractured Linux desktop land doesn't. Hopefully Flatpak (or something l like it) will save us from that but we aren't there yet.
I think locally installed MS office apps are going to die before the shiny wears off the cloud, but that's just my take.
No.  Suspending is putting your computer to sleep in a low power mode in RAM.  

Hibernating is doing similar but even lower power mode and potentially going to disk.  

Suspend to disk is a hybrid approach where you go into a low power mode to RAM for a limited time, and then go to disk and that even lower power mode after a set time.

I rarely turn my computer off nowadays
Ahhh ok that's one thing I have yet to try on linux. Does that come down to GPU drivers? Or software that manages outputs?
It's mostly an X11 problem, from what I understand. Wayland is supposed to make some of the display issues better but I think currently your mileage will vary  a lot depending on GPU hardware, desktop environment, etc.

That's the thing - I think Linux will be better than Windows at everything one day. But it's taken a long time to get to where we are now and it will take a long time from now to get where we want to be. But I believe we will get there.

I think one of the things that has always held desktop Linux back is fact that there's no money in Linux on the desktop. The money in Linux is in the world of servers and data centers. Desktop Linux has always been a second class citizen.
I guess that's the same thing
No. You're pragmatic.
I can sell you an indulgence and wipe away your sin for the low price of $29.99. My indulgences are high quality and gpl licenced.
When I first installed Linux on my whopping 25mhz 486sx with 8mb of RAM in 1993, it was because I wanted a Unix workstation, and the only other affordable Unix-like OSs were Coherent and Minix, and neither supported virtual memory. I certainly didn't have the dough to buy a license for an actual variant of Unix.
Part of the beauty is the many senses of the word "free" that Linux supports.  You don't have to embrace them all, but they're all there for you if you want them.
Not at all, it‚Äôs your time you trade away for money, if Linux suits your needs and your wallet, by all means embrace less expensive!
Nope. You're much cooler than people who buy a Mac just to be trendy and have more office cred status.
no such thing as a bad linux user, unless that linux user is hitler or something
There's a path to the Great Way in every operating system, if only we can find it.
I would suggest the only 'bad' Linux user would be one with malicious intent for the OS. Much like any other tool.

If I buy a hammer to smash someone's window, I'm definitely a bad hammer user. If I scavenged a hammer from a trash heap to use it for DIY instead of buying one, I'm definitely not a bad hammer user. I might not be using it because of its particular brand or model or anything it represents, but I'm using it to do what I need.

You don't need to be part of the culture to use something. If it's what you need, use it.
No, mate
I'm similar, I first used linux to extend the life of an old computer that could no longer run Windows, and I doubt I'm alone in that entry path. Installing linux on an old computer that you would/will otherwise toss out is a no-risk proposition so the fear that often comes with wiping an operational Windows hard drive to install linux just isn't there.
that makes you a bad Windows user
I am not sure why you get downvoted, but the answer is a clear no.

You made a pragmatic decision which make perfect sense even for ppl that are not hardcore open source fans
It's true that "You get what you pay for", but it's also true that "Software is like sex. It's better when it's free."
Wait a second. People actually buy Windows??
Are you happy using Linux?  Are you employing it to use your hardware as you choose, as the **owner** of your PC? (I assume so, because that was 10 years ago and you stuck with it through the lean times.)

Then you're a good Linux user.
Nah man. You had a problem. You found a solution. 

You don't need to be steeped in the underlying ideology that created to find it useful, though it is worth recognizing that ideology is what keeps the ecosystem freely available and usable.
Windows blows anyways. I jumped ship right after that when Vista hit the fan. I sent that back to Microsoft for a full $140 refund I'll never forget it. I'm so glad I did too because I have been on Linux exclusively for 13 years now. Also thanks to Ubuntu. Best operating systems on the planet are Linux based. So my question to you is, are you also glad that you couldn't use Windows? According to Steve "developer developer developer" Ballmer I chose internet cancer over Windows. :)
You could've downloaded win7 the same way you did with ubuntu. What could you not afford about that?
no linux user is bad unless you use chrome or edge.
Until 2014 I had never paid for a copy of windows and in 2014 I only did so because of issues I had upgrading.

But, I did happily buy a copy of redhead off the shelf!
There is no such thing as "bad Linux user". It's just and operating system, a tool that you uses for whatever reason. Believing in open source is not the only valid reason for using Linux.
I've never used the terminal and don't expect to use it, does that make me a bad linux user?
Good and valid reason in my books.

One of linux' greatest strengths is its lack of price.
No, that makes you a very sensible Linux user.
I would classify a "bad Linux user" as someone who's constantly flailing and unable to do the things that he needs to do, and could do them easily on Windows or a Mac.  But if you're productive with Linux then hey, you're fine.
I use Linux because Windows doesn't really have any good tiling window manager solutions. An OS is meant to be used in service of a practical goal, not a moral cause, IMO.
No it does not make you a bad Linux user. It makes you a good one. You could have  went with a pirated copy of windows, but instead chose Linux. 

That said, the Linux community is a pretty welcoming community. It does not matter why you came to Linux, it matters that you came. 

If you still feel guilty, try making a small donation, or spread the word.
In no way does that make you a bad Linux User. Come for the ‚ÄúFree‚Äù, stay for the ‚ÄúFreedom‚Äù, I say. üòÉ
No, that makes you a bad Windows customer.
No, being free as in free to download and install without some nagging 'please buy me' pop-ups is one of the advantages of Linux

You don't have to pay for an OS for a computer you already own
Ways to be a bad Linux user:

- Take open source Linux based projects and resell them as closed source proprietary projects.

- Use Linux to organize a terrorist organization.

- Beat someone to death by using a computer that has Linux installed on it.

- Contribute to Red Star OS.

- Use Linux to exchange child pornography or do other terrible things on the internet.

I don't think you are doing any of these things, so you are probably good.
>I got into Linux because I couldn't afford Windows 7, does that make me a bad Linux user?

Wait. What? Why would that make you a bad Linux user? People make choices of what to buy and use, and available money (or simply frugality) is one of the factors that determines that choice.

In the past I've seen lots of people build a ‚Ç¨2000 computer, and then pirate Windows because they didn't have the ‚Ç¨100 to buy the OEM/System Builder version. If you are in the situation that you can only build / buy a cheap ‚Ç¨300 desktop that comes without an OS, it's perfectly fine to install Linux on it because you don't want to pay ‚Ç¨135 (nowadays) for Windows.

I'd rather see people do that and use other open source software such as LibreOffice, than pirate Windows and Office.

My girlfriend has a laptop that is now 2 years old. In another 2 years, Windows 10 will go out of support and she REALLY doesn't like the forced "must have an MS-account" for Windows 11. Possibly, the laptop also cannot run Windows 11; I'll have to check on TPM and CPU.

When the EOL life of Windows 10 rolls around, that laptop is just 4 years old, and probably MORE than capable enough for her internet/e-mail/discord/LibreOffice needs. It will probably be re-installed with Debian Stable, KDE, a few flatpaks, and ONE Windows-program through Wine. Then that laptop will run until it physically breaks.

I'm not going to trash a perfectly good piece of hardware because MS says that Windows 11 won't run on it, if the programs used on that hardware are also available for Linux. (Except for one; which does run in Wine.)
Why? It's absolutely fine. 

As for you get what you paid for: IBM, Google and other huge corpos have paid a lot of money for Linux development. We might ride for free, but a lot of devs earn their living by it and corpos make money.
What will determine if you're a good or bad Linux user is your cli-fu, not your self-awareness issues. Also if you can't exit VIM you are a bad Linux user.
Johann Tetzel, begone! 

He was the friar guy in the 15th century who created a system of "indulgences" that you could buy to get to Heaven.
Shut up we know that your comment is a /s but op might not lol /s
I avoid anything Apple due to planned obsolescence. I still have my old iPad 1, I use it to watch YouTube videos on Invidious.

One of the reasons why I love Linux is because you can reuse old hardware, like for example, my brother hand me downed his 2006 laptop, 1GB RAM (soldered unfortunately) and I installed Zorin OS Lite on it, I had to increase the swapfile size to compensate for the 1GB RAM. I tried Bodhi Linux on it, it sucked.
The number of linux devices in an average house is incredible, smartphones, smart tv's, tablets, smartwatches, IoT devices ...

&#x200B;

linux is probably used in some sort of weapons, ... "et alors".
Downfall meme incoming 3, 2, 1...
I feel like that because I came into Linux for financial reasons and not due to open source and FOSS principals.
> "Software is like sex. It's better when it's free."

I heard that Linus Torvalds is the one who said this lol. What does he mean by this?
I don't think the adage with getting what you pay for works at all for software, since the only pieces of paid software that are obviously superior are the ones who are running a capitalistic near-monopoly on their specific field, and/or used other underhanded tactics to achieve recognition.
Everyone who has a PC does. It is just included in the price. I mean, did you buy a stick of ram, a cpu, a mother board, and a case or a desktop?
Yeah, it would have been cheaper by 60$ to get my laptop without the license.
Some do. Those building their own computer for gaming often buy a copy of Windows. With like real money. I know, it's crazy, right?
Yes, I'm happy using Linux, it opened me up to a whole new computing world that I wouldn't have known about otherwise.
> So my question to you is, are you also glad that you couldn't use Windows?

Yeah, I'm glad that I couldn't use Windows because it opened me up to a whole new computing world that I never would have known about otherwise. Been a Linux user since 2012 and never looked back.
Really? At the time, I had no idea you could download it from Microsoft website. Windows 7 was available in computer stores in DVD medium.
While I use Firefox as my daily driver, I also use Chromium to visit government websites because for some reason gov't websites don't run great on Firefox.
Why though? Both of them are just chromium under the hood :)
Edge has superior vertical tabs though. Firefox went and made themselves all trendy with interface upgrades and then didn‚Äôt add useful interface features.
I discovered and got into Linux specifically from my software pirating activities, 20+ years ago.  The FOSS principles soon followed.
i guarantee half or more of users from poor or developing nations and/or backgrounds came to linux for the same reasons. it's why they've held onto 32 bit support for so long, in some diatributions...including my daily driver. not everyone can afford the shiny new computer, game, or phone. it's as good a reason as any.
Leave that to the lunatics of the sub. Just use what works for you. Giving back is always good, but optional.
Probbaly that if you are doing something wrong if you have to pay someone to hvae sex with you. People who love each other do it for free, because they love each other. Or something like that. (I am not too sure...)
Or provide some value that requires ongoing labor that's not programming. Like, a piece of software that provides quality sales leads might be worth paying for. Not because of the actual code, but because of the work that goes into maintaining that data. Or, say, software that helps you through an audit with very specific requirements. Or tax preparation software for businesses!
is that some kind of sarcasm?
This is known as the Microsoft Tax.
> did you buy a stick of ram, a cpu, a mother board

Yes, several times. It would never even occur to me to buy a pre-built desktop, it just makes zero sense as far as I'm concerned (laptops notwithstanding, but that's a different story).
chrome=/=chromium
looks like the tax filling site of my country which runs on windows server is programmed to not to work on linux on purpose once you log-in  (chrome and firefox)
True, but what's on top of it?

I'm sure I'll be downvoted for this (and I don't care, as your downvotes are meaningless in you know... reality) But I always find it amusing those who scream and yell about Google or MS, then use their browser.

One of the many reasons I use Linux is because I don't like MS or Google watching my every move. So why in the world would I use their browser that does exactly that? Why not just use Chromium then.. Which is what I do. I only have it when some sites won't work with Firefox.
But they're not "just" chromium, are they? They're also a lot of other crap.
its disguisting spyware thats not foss.
oh hell yaeah!!!! giving up my privacy (a human right) for cool ass vertical tabs!!!!! yay!!!!!!!!
Well I don't know about "wrong", just that it feels better for most people if they're shtupping someone they actually care about in some way. The comparison is silly anyway.
Yeah, I should have specified that I meant typical end user software, not like, production stuff, since I have no idea about those at all. But honestly, as far as your "typical" to "advanced" user software goes (excluding videogames, since that's a whole another category and debate), the only examples I can think of is stuff that generally engaged or currently engages in predatory or anticompetitive practices. Adobe, Microsoft, arguably Apple... That's about it? I know the CAD people prefer paid software, but I can't speak to that myself.
No
Unless you built your PC yourself, you indirectly paid for Windows. Companies have to pay for each Windows license. They probably get some kind of reduced rate (otherwise we'd see a bunch of PC's selling without the OS), but it's not free to them. Then, they pass that cost onto the customer.
r/woooosh
does user agent spoofing work?
Thats a completely reasonable explanation and upvoting on that. But there is also an opposite party who just hates these without reason, and that is simply sad.
Yeah buddy
not always the case, there are pre-builts and laptops that come with freedos or in some really rare instances with linux
Yes, there will be some who go around hating because they just like doing so. But that isn't the same issue at hand and is irrelevant to using their browsers. And in grand scheme of things, it doesn't matter if some do that. It doesn't make their browsers better because someone won't or can't give you a reason. The browsers are still tracking you.
It depends though, for one, I heard that Microsoft Edge on Windows collects telemetry, and if this is true for the Linux version of Edge, that's bad in my opinion.
While this is true the company will have paid a flat fee to install Windows across their machines. I doubt very much that companies such as Dell and Lenovo will have to account for each and every Windows install. So while they offer options for laptops with no OS the price of the machine still has a small amount of the cost of a Windows license accounted for in it.
They both collect your information regardless of any OS.... Do you seriously think they only collect on Windows and not Linux?
I know that, but this is from [Mozilla's Data Privacy FAQ](https://www.mozilla.org/en-US/privacy/faq/):

> **I use Firefox for almost everything on the web. You folks at Mozilla must know a ton of stuff about me, right?**
> 
> Firefox, the web browser that runs on your device or computer, is your gateway to the internet. Your browser will manage a lot of information about the websites you visit, but that information stays on your device. Mozilla, the company that makes Firefox, doesn‚Äôt collect it (unless you ask us to).

I'm more inclined to trust Mozilla more than Microsoft to be honest because one is FOSS and the other is closed sourced.
> In cases where you determine the pricing for your product or in-app purchases, all pricing, including sales or discounting, for your digital products or services must:
>
> Not be priced irrationally high relative to the features and functionality provided by your product.

Well that's clear as mud...
I think people are taking this the wrong way. Don't get me wrong, Microsoft does scummy things, but also have you seen the Microsoft store? It's a fuckin disaster lol.

Sure you can legally take open source software, repackage it, smear a shitty logo on it and upload it as your own. But if you owned a store would you want that on your shelf along with 500 equally shitty expired clones of it?

I'm not sure if it's still there, but you used to be able to, say, search for GIMP and find a bunch of gimp clones called things like "GIMP PRO" and "GIMP premium" some of which actually cost money. That stuff has to go.
I think that's the part in policies:
> All content in your product and associated metadata must be either originally created by the application provider, appropriately licensed from the third-party rights holder, used as permitted by the rights holder, or used as otherwise permitted by law. Reporting infringement complaints can be done via our online form: Report intellectual property infringement.
I'm just gonna say it. They need to curate the store more if they want it to survive and actually be used. I get that it's fully legal for people to resell/redistribute the software if it is licensed as such, but if grandma is told to get vlc from videolan.org by the grandkids and then buys something else from the Microsoft store because they typed it in the start menu, that can be a bad experience or reason to not buy from or use in the future. Microsoft wants to be how the App store is to a MacBook.
What the heck is windows store?
I use Linux.  I don't care about their store.
i think it have a sense for MS Store. But in the other hand - nothing can't help MS Store
So, as far as I get it, since Microsoft actually sells free software through their store (e.g. GIMP), it wants to be the only side who sells this kind of software? OK, their alleged goal is to protect people from scamming, but isn't it scamming when you sell an executable which can be actually downloaded for free? (I.e. chargeless, as in free beer). I mean, there are plenty of ppl who don't know that there is actually free and chargeless software.
Good, don't care about licenses, scammers should get banned and get the bullet too.
good, ive actually been using the new store recently and i have to say its decent, i have a few apps i install from there (vscode, discord etc) and its improved in recent years especially with the windows 11 update. hopefully this will improve it even more
The MS store is a scammer infested joke. Who cares anyways we be running linux up in here lol. Last time I looked in it it had a bunch of rebranded foss software, total joke.
TLDR: If we think your price it too high for something, we can remove it from the store until you lower it to a price we decide.

All large companies love having one or two super vague, "If we think X then it is in violation" clauses. Great way to deal with products/companies/ideas they do not like or that compete to closely with their own stuff.
It's intentionally vague to push back against sellers who repackage FLOSS projects and sell them at stupidly high prices hoping to catch a whale or two.  All markets reserve the right to end sales of any product.
> Not be priced irrationally high relative to the features and functionality provided by your product.

So if free software can provide the same functionality for free (or a minimal cost), then MSFT and other companies aren't allowed to overcharge folks for proprietary alternatives?
Literally 3ds max charge 2k a year for using it, is that irrational?
It‚Äôs to prevent a random person from paying $999 for an app that only shows a picture of a diamond. 

There‚Äôs nothing nefarious about a clause like that
The original purpose of the changes was to keep scammers out who simply repackaged OSS and sold it.
Still not good enough. If I maintain a fork of a project, this does not allow me to sell it as far as I understand this.
>What the heck is windows ~~store?~~

\- Fixed, you are welcome
It's Microsoft's attempt to create a walled-garden like the Apple app store that they're 100% in control of, instead of just letting the PC be an open ecosystem like it generally has been. 

I said it in some other thread, but for a company that <3's open source they sure do a lot of things that seem to fuck over open source. :)
I mainly use Windows and don‚Äôt use their store. And why is this news in the Linux subreddit anyways.
>since Microsoft actually sells free software through their store (e.g. GIMP)

when did MS sell GIMP on their store?
> Last time I looked in it it had a bunch of rebranded foss software,

That's exactly the problem they are trying to address.
I think they want to prevent something like this: [https://en.wikipedia.org/wiki/I\_Am\_Rich](https://en.wikipedia.org/wiki/I_Am_Rich)
Yeah, so now it looks like someone has to report it and it will be taken care of.
So now Microsoft gets to determine who is a scammer.

If a license is not in violation there is nothing wrong with selling repackaged software. Anyone who has a problem with that should license their software accordingly.
OSS allows you to do this though.
Dumb question but how is this scamming?  As long as they meet the contractual obligations of the license, I wouldn't really call it scamming.

Moral? Questionable, depending on the specifics, such as support, marketing and what not.
It does! Depending on the license. As they state in the update. Some software is literally completely public, and free for you to take, put, completely unaltered, in a box, and sell to someone. You just have to ensure that the original project's licensing permits that you do that. This policy just references "appropriate licensing" and "use as permitted by the rights holder" which is just the whoever owns the project you would be forking.
From what i understand as long as the original licence allowed it you can sell it, though you have to be careful about trademarks, i.e i can make a fork of blender and sell it but i can't use the brand if blender while doing it
> used as permitted by the rights holder

FOSS licenses do allow you to do that
>And why is this news in the Linux subreddit anyways.

This is a continuation of this previous post: https://www.reddit.com/r/linux/comments/vtxr9r/software\_freedom\_conservancy\_heads\_up\_microsoft
I hear more Microsoft news from /r/linux than just about any other subreddit lol.
Well, this guy (Mental Outlaw) actually shows this in his video: [https://www.youtube.com/watch?v=N73VkaqalWQ&t=182s](https://www.youtube.com/watch?v=N73VkaqalWQ&t=182s)
Then good on them I guess, better late than never, last time I looked was win 8 so many years ago. Guess at least they finally figured it out.
Scam call centers also use app stores to launder money. They put up some low effort (or repackaged FOSS) apps for a high price and buy it with their victim's money.
Yes, but with the great power granted by the vague wording comes great responsibility. Responsibility I doubt MS is up to. Even if they remove dozens of bad apps under this rule, I would still rather not have to rule if it was used to remove even one undeserving app.

The main problem is rules like this are rife for abuse; it's less of a question of it but more a question of when. Unless of course the Windows App Store fails to take off, like it has for the past I don't really know how many years.
Kind of the original NFT although there are a couple of copies of it
Yeah, because MSFT has never charged someone a thousand dollars for something worthless. ;)
[deleted]
Microsoft gets to determine what software they allow on their store, yes. They are not actually scammers, since the people do receive the software, but it amounts to spam.

The updated wording continues to allow this though, if they are compliant with the entire license (which most are not because the code is not available).
The problem isn't so much repackaged software, but repackaged software under the same or similar name as the original to give the appearance of being the original authors, which is misleading to users, and unethical, especially when it comes to soliciiting payment or donations.
Yeah, but we must take in account some practical problems here: spamming and malicious repacks (who happens to be the majority of these repacks).

The thing is: the probability of a software to be a malicious installer is far smaller if restricting the right to sell software to its original authors. And MS is in it's right to gate keep whoever is going to sell software on their store. Walled gardens suck, but they only exist because developers buy into it. If Windows developers (incluiding OSS devels doing work for Windows) wants to be at MS mercy even to be allowed to take a pee, so be it.

Furthermore, the ones doing spamming are actually violating GPL: they don't even give the proper credits, sometimes rebranding the software entirely (so not to clash names) and forget the source.

And even if someone resell there doing all the right stuff (proper credits and pointing to the sources), there's no point in allowing 300 versions (with some abandoned) of the same thing on their store.

When FSF did that enormous article describing why you can sell GNU (or OSS) software, centralized OS integrated software stores didn't existed. It was done in the rationale that someone selling CD ROMs with the source are actually doing good to the community by "spreading the word" while not sending himself to bankruptcy. Distributing software used to be a major problem for the free software movement before this omnipresent internet that we have today.

People spamming a online store, clogging a direct monetary channel that can fund genuine developers, are just leechers. Fuck them all.
Bingo. Like it or not, that's one of the freedoms granted by the license in many cases. Hell, RMS encourages selling copies if people will pay you for them.
If MS is not violating their EULA / TOS there is nothing wrong with preventing selling repackaged software in their store. Anyone who has a problem with that should not use windows store.
Yes, but Microsoft doesn't have to
IANAL, but I think the updated wording acknowledges that.
It does literally say "appropriately licensed from the third party rights holder", so where did you get this that it doesn't allow that?
Microsoft doesnt knows how OSS works xD
I always like comparing ourselves with Windows and MacOS to compete, but honestly this news is pretty irrelevant to Linux.
This is actually what MS is trying to prevent here. That GIMP is just someone who took the open source code, slapped PRO in the name and uploaded it with a price tag.
Maybe I'm missing something, but I don't see in that video where Microsoft themselves selling GIMP copy
I'm fine with that. But MS is also free to say, that they don't want their store to be involved in such a transaction.
sounds like a trademark issue, not a license one.
They are the single biggest or second biggest company contributor to OSS depending on if you measure number of engineers working on OSS or commits made to OSS projects. Google is the other 1st / 2nd depending on the measurement used.
But then you don‚Äôt get your daily dose of anti-Microsoft circlejerking!
It is. But the point is that it sounds like Microsoft are basically saying that the previous store policies were worded poorly to exclude all OSS from charging, when really their intent was to stop complete rip offs. For example, some rando putting their own "Libre Office" onto the store and charging $20 for it, even though they have the right to use the code, they are not The Document Foundation, and have no right to use their product name (even though they can take the code) and sell it without their permission.

It's a similar situation where Iceweasel which is a browser which is basically unbranded Firefox.
If the license of LibreOffice allows that that's perfectly fine, my guess is that a rando can use the code and create a "BestOffice" based on that, but not release it under the name of LibreOffice if they modified the code. Which again is fine.

Also Mircosoft could make a better search if you look for LibreOffice they should show the differences and show the ones that are downloaded by the most people. There are solutions other than imposing restrictions that are not in the license in the first place.
But that's exactly what I'm saying. The name is the problem not the code. They initially made a mistake by making a rule about acceptable code licence, now they have corrected that mistake by making it about trademarks. It seems that the intent from the start was to guard against trademark abuse, not to limit open source projects. 

Whoever wrote it at the time probably thought that open source = free beer, so if someone uploads it to the store and asks for money then it's probably a scam, not considering that some open source projects would like to sell a paid version on the store.

The problem is solved now. Microsoft solved it. FOSS projects are free to charge now. Hypothetical BestOffice is free to copy LibreOffice 1:1 code with a name change... And charge for it .. Or if Libre authors allow it, do so without a name change.

Your solution to deprioritise listings is also a bad one, users should not be able to find dodgy apps imitating another in a curated store at all, even at the end of the results list.
How do you define dodgy? I can take the code of LibreOffice make a fix that I think it's important and publish it, who decides if that code is dodgy or not? Or if I can charge for it or not?
If you take the code of LibreOffice and make a genuine fix and then publish it to a storefront whilst pretending it to be the official version or authors of LibreOffice (The Document Foundation) by re-using their name/branding, then that is dodgy.
What if I call it "TheBestOffice"? Is that dodgy?

We get to my initial point, it's a matter of trademark protection that is badly applied.
Yes rebadging thebestoffice is fine, even if you charge. As long as you still adhere to the LibreOffice open source license it is not dodgy. 

Yes the trademark protection was bad initially and now it's fixed. As I keep saying
Installing a VM without GPU acceleration: 1

Installing a VM with GPU acceleration: 7
Never had printer issues on Linux, so 2
Fedora 1 ez pz

Manual arch install , I can do it pretty fast most annoying part is partitioning the hdd.

Installing Windows 11 with a reg hack to install without TPM is the stupidest thing I ever did. I need Windows "just in case" for firmware and forces you to use regedit to disable TPM. I god damn hate regedit. The windows 11 bar is absolutely stupid, I went to MSFT support and suggest I can change the windows 11 back to windows 10 with regedit. Those instructions destroyed my entire Windows install. Downloading random exes to install a driver scares me a bit. I just installed Windows 10 on a crappy 100 buck PC and lays dormant until I need it. 

To be quiet honest. Microsoft can roll their own Linux distro, similar to Valve's SteamOS, stick their own Microsoft store proprietary repo, support wine project to transition, fire entire windows team and hire linux devs. 2 easy install - 6 when something goes bad.

edit: bad wording
I'm curious to see the comparison using this scale of:

 * Arch
 * Gentoo
 * LFS
>Anything to do with printers is a 6.

It is? I have printer with WiFi support and the only thing I need to do is to be on same network as printer and it is automatically detected and configured. Well, at least on Fedora, on openSUSE I had to manually setup it. Not very hard either with some tutorial (luckily I found one for my printer).
Installing Fedora was a 2 for me, getting the drivers (nvidia) up and running was tedious but not difficult. Since I am using Wayland, I did not need to configure anything xorg related, that saved a lot of my time. If Fedora managed all the driver stuff before like Pop or Ubuntu, it would've been an easy 1 from me.
since I am all AMD hardware installing/using Fedora WE was 1 for me, using Fedora Server Edition was 3 (I had some troubles with gdm autostarting and some things missing), using NixOS was 4-5 for me, Arch before archinstall 4... ubuntu never worked for me so 6... and installing gentoo was 5
Increasing file system size in virtual machine: 6
Printers on Linux are 1, maybe 2. 

On windows.... From 1 to 7.
EndeavourOS and Garuda Linux: 3

[Manjarno: 777](manjarno.snorlax.sh)
Linux Format magazine has a special on GPU virtualisation this month
Its only difficult the first time while learn about CPU pinning and giving an VM direct access to a NVMe I found, however the second time was pretty straight forward.

Does this match your experience?
For me, it's easy to "print", but much harder if I want to print with full resolution, color modes, data reporting (trays, toner, etc).
It‚Äôs subjective so if you‚Äôve done them you can score it
5 for Gentoo 

Enticing
I have bookmarked the howto, but did not try it yet. I am not even sure if my notebook can do it, I have an integrated gpu and a dedicated, I can select in my UEFI setup if I want to use only the dedicated Nvidia or both. I think I will try it some time!
Completely valid point.  So, from my biased perspective:

 * Arch - 3
 * Gentoo - 2
 * LFS - 4
5 for installing gentoo not using it... I am at stage that I am really thinking about switching to gentoo for every day use but I only installed it and do some very basic usage like updating it... without gui, only cli... also I didnt modified kernel, my only goal for now was to get it to boot up
Just set aside a day when no one will bother you and you'll have done no problem.

A pot of coffee is also helpful.
Gentoo easier than Arch, eh?
Well, just goes to show. 
I might try it - note its absence from my own experiences.
I do not drink coffee, my caffeine is Cola Zero =) but yeah, I will do it when I have a few days off, it is nothing to do on one weekend.
For me, yes.  I have used Gentoo since before it was called "Gentoo" (originally it was called "Enoch Linux"), and have been a developer for years.  Arch is frustrating to me because of binary dependencies, systemd, and some other aspects but those are all probably because they're not as familiar to me.  LFS is extremely difficult the first few times, but does get to be a bit easier with practice. :)
You can't break the stereotypes!!

Remember r/vfio exists if you get really stuck though.
Inspiring 
I note the Gentoo flair now you mention it
Nala is a frontend for `apt` so that we can have prettier output, faster downloads of packages, and a history!

You can find the source code, installation instructions and more information here [https://gitlab.com/volian/nala](https://gitlab.com/volian/nala)

This is also where we track bugs/feature requests. If you have one please don't hesitate to open an issue.

Thanks for stopping by! This release is a lot of bug fixes, and a few small features. Development is going to slow down for a bit after this release.

On top of being busy in my personal life, I'll be working on rewriting Nala in rust. Another project of mine `rust-apt` has been coming along rather nicely, and I think it's time I can start using it to rebuild Nala. You can find the project here. [https://gitlab.com/volian/rust-apt](https://gitlab.com/volian/rust-apt)

**\[ Changes \]**

* Add config option to pass arguments to install hooks
* Use man pages for `--help` menu
* Add `assume_yes` configuration option

**\[ Bug Fixes \]**

* `upgrade` fails sometimes when using install hooks
* Local .debs cannot be installed
* `fetch` sometimes crashing on a bad mirror
* Lowered referesh rate to minimize flicker
* .debs without a reported size are uninstallable
* `history` sometimes overwritting entries
* `upgrade --exclude` not working at all
* Local .debs not installing recommended pkgs
What shell are you using? I like this visual, commands separation and coloring.
Thank you for sharing your work. Looks very comprehensive.

What made you want to rewrite it in Rust?
Does it look like this out of the box or is that a zsh/fish shell theme? Because that looks sexy as hell
Looks really nice mate!
For your info, TitusTech just did a video on it : https://www.youtube.com/watch?v=oroSkR4Nn\_w
Damn dude, I wanted smth like that several years ago when I was on debian. Looks really great !!
If someone made something like this for dnf, it would be really cool.
Please make for yay and Pac-Man!
do you plan on publishing nala to pypi? since it's fully written in python, being able to download on `pip` would also be nice
Installed this on my Ubuntu server a while ago and I simply love it. Good work!
yay!
This shit is awesome a history !!!!  
Tumbs up
This looks phenomenal!
really cool...
I installed this some time ago and have been using it ever since as my primary update method.

Great work.
That looks actually quite nice, from both a usefulness and aesthetic standpoint. I really like how it looks
I love nice tui apps and this looks lovely. It‚Äôs a damn shame I don‚Äôt use Debian, else I‚Äôd be rocking this.
Any chance this would work on Ubuntu/Debian derivatives like Mint and Pop_OS?
Receiving the following errors when executing sudo nala fetch  
`Fetching Debian mirrors‚Ä¶`  
`unhandled exception during asyncio.run() shutdown`  
`task: <Task finished name='Task-124' coro=<MirrorTest.net_select() done, defined at /build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/fetch.py:105> exception=ValueError('list.remove(x): x not in list')>`  
`Traceback (most recent call last):`  
  `File "/usr/lib/python3/dist-packages/httpcore/_async/connection_pool.py", line 227, in handle_async_request`  
  `File "/usr/lib/python3/dist-packages/httpcore/_async/connection_pool.py", line 34, in wait_for_connection`  
  `File "/usr/lib/python3/dist-packages/httpcore/_synchronization.py", line 38, in wait`  
  `File "/usr/lib/python3/dist-packages/anyio/_backends/_asyncio.py", line 1842, in wait`  
  `File "/usr/lib/python3.9/asyncio/locks.py", line 226, in wait`  
`await fut`  
`asyncio.exceptions.CancelledError`  
  
`During handling of the above exception, another exception occurred:`  
  
`Traceback (most recent call last):`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/fetch.py", line 119, in net_select`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/fetch.py", line 125, in netping`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1751, in get`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1527, in request`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1614, in send`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1642, in _send_handling_auth`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1679, in _send_handling_redirects`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1716, in _send_single_request`  
  `File "/usr/lib/python3/dist-packages/httpx/_transports/default.py", line 353, in handle_async_request`  
  `File "/usr/lib/python3/dist-packages/httpcore/_async/connection_pool.py", line 233, in handle_async_request`  
`ValueError: list.remove(x): x not in list`  
`Traceback (most recent call last):`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala-cli.py", line 32, in <module>`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/__main__.py", line 41, in main`  
  `File "/usr/lib/python3/dist-packages/typer/main.py", line 214, in __call__`  
  `File "/usr/lib/python3/dist-packages/click/core.py", line 1128, in __call__`  
  `File "/usr/lib/python3/dist-packages/click/core.py", line 1053, in main`  
  `File "/usr/lib/python3/dist-packages/click/core.py", line 1659, in invoke`  
  `File "/usr/lib/python3/dist-packages/click/core.py", line 1395, in invoke`  
  `File "/usr/lib/python3/dist-packages/click/core.py", line 754, in invoke`  
  `File "/usr/lib/python3/dist-packages/typer/main.py", line 500, in wrapper`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/fetch.py", line 661, in fetch`  
  `File "/usr/lib/python3.9/asyncio/runners.py", line 44, in run`  
`return loop.run_until_complete(main)`  
  `File "/usr/lib/python3.9/asyncio/base_events.py", line 642, in run_until_complete`  
`return future.result()`  
  `File "/build/nala-legacy-jxjPNq/nala-legacy-0.10.0/nala/fetch.py", line 94, in run_test`  
  `File "/usr/lib/python3/dist-packages/httpx/_client.py", line 1997, in __aexit__`  
  `File "/usr/lib/python3/dist-packages/httpx/_transports/default.py", line 332, in __aexit__`  
  `File "/usr/lib/python3/dist-packages/httpcore/_async/connection_pool.py", line 326, in __aexit__`  
  `File "/usr/lib/python3/dist-packages/httpcore/_async/connection_pool.py", line 312, in aclose`  
`RuntimeError: The connection pool was closed while 51 HTTP requests/responses were still in-flight.`
I've been dreaming for a fork of aptitude that supported flatpak...
Why is anything earlier than 22.04 considered "legacy"? Linux Mint hasn't even been updated to the latest LTS, so I suppose I'm stuck on "legacy"?

What sort of features am I missing out on?
i fucking hate apt but this shit cool asf
Nice one, just wget it
great job . I like to rust slowly getting in to Linux :)
Wow, this is beautiful!   
Is there anything similar for Arch/Pacman/AUR?

&#x200B;

I just switched to Arch Linux but I would totally put this on any future Debian boxes.
I didn't realize that Nala was an apt thing. At first I thought it was just a front end for a shell.  Which I thought was cool as hell.  Does something like that exist?  My distro doesn't even use apt.
Apt still sucks.
will nala ever come in fedora dnf
Why do people insist on using deb based distros if you have to download a wrapper, for your package manager wrapper?
This looks sick! I love it!

Imagine having one for every distribution out there. Although not as easy as it sounds but really well done. Amazing.
What are the differences between this and `aptitude` (different from `apt`)?
I'm getting a weird behavior when I try to install a local `.deb` through `nala`.

It just doesn't install, show the whole process but doesn't install, no errors are reported.
Wow, such a cool good looking thing! Sure it's suggested by Chris, but can you make an AUR helper for this?
heya

I just installed nala 0.10.0 (legacy) for a Ubuntu Server 20.04.4 LTS running in a VM

the problem: when I run `sudo nala fatch` it crashes

Fetching Ubuntu mirrors‚Ä¶

unhandled exception during asyncio.run() shutdown

[....]

RuntimeError: The connection pool was closed while 50 HTTP requests/responses were still in-flight.

---

fixed it with `sudo nala fetch --country RO`

ty!

---

can we do `nala fetch --region EU` or something similar to widen the mirror range ??
Just installed it on MX Linux 21, using the legacy version. Got the fetch error, adding <--country XX> worked like a charm.

Looks great, and the colours match my pastel text theme.
Hi, did you get a bit of inspiration from dnf to build nala? The commands and features all look very similar to dnf actually...
>https://gitlab.com/volian/nala

Installed on Debian 11 and error:  
\`\`\`  
ImportError: cannot import name 'Group' from 'rich.console' (/usr/lib/python3/dist-packages/rich/console.py)

\`\`\`
A bash function for a simple apt history:

    function apt-history(){
          case "$1" in
            install)
                  grep 'install ' /var/log/dpkg.log
                  ;;
            upgrade|remove)
                  grep $1 /var/log/dpkg.log
                  ;;
            rollback)
                  grep upgrade /var/log/dpkg.log | \
                      grep "$2" -A10000000 | \
                      grep "$3" -B10000000 | \
                      awk '{print $4"="$5}'
                  ;;
            *)
                  cat /var/log/dpkg.log
                  ;;
          esac
    }
Keep up the great work! :)
The shell used is not so important. You can customise the prompt under any shell.  [This prompt](https://unix.stackexchange.com/questions/558690/how-to-change-linux-terminal-prompt-to-match-parrotos/558709#558709), for example, is pretty close to the video.
I use xonsh. It's a shell written in python. It's pretty good but slow in comparison to others.
[deleted]
One of the features of this is performance. Since it is currently written in Python, which is also one of the slowest languages there is, I would assume the Rust is to make it even faster!
This is out of the box. KDE Breeze Dark theme on Konsole.
Nice, just watched it. He does a good job of showcasing it. Thanks for the link!
Thank you! Nala has made my own Debian experience much better. A personal favorite of mine is the `--exclude` switch when upgrading. Granted this can be done similarly in apt with pinning, but it's nice to just have a quick switch to use.
Doesn't dnf/rpm already have all these features?  Well, except the colorization and border lines.
Why? Dnf can appear rather slow, yes, but an additional frontend wouldn't really help with that as it already supports concurrent downloads.  
Also, the visuals of Nala were inspired by dnf and the developer wanted the same level of visual clarity for apt.
Unfortunately it's not that easy. Maybe a long way down the line something like this can happen. But for right now operations are very tightly integrated with `libapt-pkg`
I was also looking for something nicer for Arch, but haven't found anything as nice as Nala. For now, I switched to [pikaur](https://github.com/actionless/pikaur), which at least displays updates in a much clearer way.
Unfortunately not. Pip isn't very good for installing system apps like this. Additionally it requires `python-apt` which isn't available on Pypi.

If you have `python-apt` installed, you can most definitely install it with pip through our git repo if you so choose. But you won't have some extra things like translations and man pages. It requires python 3.9 + if you plan on installing it this way.
I'm glad you enjoy it. Thank you for using an early release package manager. Even though it's really stable, some people are put off by something like this. The improvements made simply from people using Nala are a lot.
Thanks, I believe it is!
Thank you! I appreciate it. I'm glad you're liking Nala.
Thank you,

A lot of work has gone into making sure the format is beautiful and easy to read.
Thank you,

Maybe it's time that you switched to the dark side üòà

It's not really a TUI though. I some how created a bastardized pseudo TUI. Although I'm considering eventually changing it to a real TUI.
Yea should be no problem. For Pop_OS they are still tied to the Ubuntu repos I believe, so even `fetch` should work. A lot of people use Nala on Pop already.

For mint I would imagine everything would work but `fetch`. Haven't heard of too many people using it, but see no reason it wouldn't work.
Please see this issue with steps on how you can solve it  
https://gitlab.com/volian/nala/-/issues/92
Well not a fork of Aptitude, but you can bet Nala will be the best libapt-pkg manager.  


When you say support, what would you like it to do?  


I don't think this is in scope for a while if at all, but I'm curious.

Right now we have to focus on the rewrite, and then feature parity with apt so we can take over the world.
It's just a name. The reason being is that anything older than 22.04 does not have the packages required to run Nala as a pure python program.

For nala-legacy the code is exactly the same but we use nuitka to compile Nala and all of it's libraries into a single binary so that it can work on the older distros.

There are no features that you're missing out on by using it. I chose this method over setting up a separate repo for it. Eventually the compatibility build will be deprecated, but I'm not sure when. As long as a lot of people are still using 20.04 I'll maintain it.
Not that I'm aware of. I don't use Arch very often. You just have to come back to the Dark Side.
r/usernamechecksout well yum has been replaced by dnf so  suppose it wasn't that good after all...
It's unlikely to happen anytime soon, if at all. Nala is tightly integrated with `libapt-pkg`. For each different package manager it would essentially be writing a new one.
you don't have to?
People don't insist on this, and you don't have to use a wrapper to use a Debian based distribution. Also, what is wrong with this wrapper? I think Nala is quite a nice option for people who prefer another way for interacting with APT.
> why do people insist on buying economy vehicles when they want them to look nice!

It's a preference. Why are you so pressed and obsessed about what state other people prefer to use?
Thanks! Maybe one day in the future it could be. Or it could become it's own distro agnostic package manager. Who knows, the possibilities are endless. If I could work on Nala full-time and support my family, I'd imagine I could get so much done, it's really just time restraints.

After Nala is rewritten in rust, I'd assume a nice GUI for it would probably be the next project.
I haven't personally used `aptitude` a whole lot. But for one `aptitude` uses an actual TUI I believe. Nala isn't really a TUI, although in the future it may become one.

Additional features (I'm not sure how many of these `aptitude` has)

1. `fetch` command. If you're familiar with `netselect` and `netselect-apt` this is some what similar. Fetch will parse the respective master mirror list and then grab their release files and see what mirrors are the fastest for you. Then it will write them to `/etc/apt/sources.list.d/nala-sources.list`. Nala will then use the extra mirrors to speed up pkg downloads.

2. Parallel downloads. While `apt` can have this functionality if you configure it, Nala requires no real configuration. It will use any mirrors you have defined to try and find a package, and then split the connections between all the mirrors to improve download speeds. Additionally if a download from a mirror fails Nala will continue to the next mirror until all options are exhausted.

3. `history` command. This works pretty similar to `dnf history`, which it was modeled after. `nala history` will print a quick summary of all past transations. `nala history info 2` will print what was done for the 2nd transaction in the history. `nala history undo/redo 2` will then either undo or redo whatever was done. If it was an install, undo will remove the packages. Currently rolling back upgrades is not supported, but maybe in the future it could be, but it will always be dangerous.

There are some other features that I have planned whenever I can get them fully hashed out. Most notable a `backup` and `restore` command. Nala will dump the current state of the system, installed packages, configuration files, 3rd party repos, etc to a file. This file could then be used as an argument to `nala restore` and it would restore all your installed packages. You could use this for somewhat of a preseed. This is quite a bit away though. It's probably about 1/3 of the way there.
This is supposed to be fixed.  


What version of Nala are you on?

Is it `nala` or `nala-legacy`?

What is your distro and release?

and finally what `.deb` are you trying to install?

After it fails to install would you send me the contents of `cat /var/log/nala/dpkg-debug.log`?
It may be possible to do something like that. I'll have to check into what it would take.

I do have a fix ready for this error so it'll be resolved in the next release.
Actually forgot that this is a feature but you can specify the `--country` switch multiple times. `nala fetch -c RO -c US -c ETC`
Yep, `dnf` was the main inspiration for the information formatting and then of course the `nala history` command.

As a Systems Developer we use `dnf` a whole lot, and I wanted `apt` to be as easy to read because Debian is my preferred distro to use in my personal life.
For Debian 11 you need to install the nala-legacy package. That one will work for you.
Why do you use that shell if its slower?
Would you mind sharing how your xonsh.rc is set up to get your prompt to look that way?
However, the terminal emulator used usually has no effect on how the prompt looks, for example.
This is correct. Python is just too slow for my taste.

Most of the improvements will not really even be noticeable to the average person, but I have to do it for me.

An example:

`nala list --upgradable` currently takes `0.76` seconds to complete.

`apt list --upgradable` takes `0.30`

With my current `rust-apt` progress this can complete in just `0.06` seconds.

This is a rather extreme example of improvements and not everything will be this much faster, but I've managed to make some operations more efficient than `apt` itself.

`rust-apt` is designed to do everything lazy. It does not initialize any part of the `apt` ecosystem unless it's actually necessary. `python-apt` initializes everything when you load the cache.

another big reason to rewrite is that I'm not a huge fan of the way `python-apt` operates, and doesn't seem like it's very well maintained.

With `python-apt` it's basically not possible to implement `nala install --reinstall` without a patch.

with Nala being written in rust, and me also writing `rust-apt`, I will have more control over lower level functions in the code. This will eventually allow for better improvements than I could ever dream to accomplish using `python-apt`.

edit: mmstick from Pop-OS has worked with me a lot on `rust-apt`. He has taught me so much about rust, and allowed me to pick it up very quickly. Once the library is more mature they plan to use it with their rust tools as well. This will open the opportunity for them to create things such as maybe a really nice rust gui for installing and upgrading.

The Makedeb author is currently working with me on it as well. He has plans to use it in a project of his. After completion I think the library can have some real use to the community in helping to improve Debian systems.
But isn't it limited by your network speed?
Well, python has big advantage here. Since Apt is written in it, you can hook directly into it and skip parsing it's output... Also it doesn't matter much in what such thing is written in as you spend most of the time waiting on network, disk or Apt itself.
r/notopbutok
I find it interesting that portage of all things is written in Python. I can‚Äôt imagine a package manager that has to do more work to install things. Lots of dependency resolution.
Holy shit, dude. This is beautiful. Any chance this can run on anything Arch? üòÇ
You are welcome and thank you for Nala ;-)
I thought dnf's speed will improve, lol. Looks like I was wrong. Sorry, My bad.
I've been through a decent bit of the `dnf` source code while initially writing Nala just to get some pointers on what I should be doing.

Where I think most of their performance issues lie is not necessarily that it is written in python itself, but more that they store their entire local package cache in a sqlite database.

While this is an easy backend that already exists, it's rather slow for a lot of things. Namely this is why their tab completion feels non existent for packages, it's just that slow for them to access everything in the database. And a simple front-end "wrapper" here wouldn't really solve that.

In this regard `apt` has a custom binary format for their cache that seems really well optimized for what it needs to be.

With Nala using the solid foundation that `apt` has laid down, and adding some quality of life improvements such as `fetch`, and the `history` from `dnf`, I believe it to be the best package manager in existence. I'm absolutely biased in this regard though.

Eventually I want to include easy access to `eatmydata` with maybe a `--fast` switch or something. While a little more dangerous this will greatly speed up long installs, upgrades etc.

Ideally, it would probably be superior to rewrite `apt`'s orchestration to `dpkg` and try to do installs and upgrades in parallel. I haven't looked much into it, but this would require some hacks to work, or a total new implementation of `dpkg` (it could probably use one at this point). This is actually a topic I've talked with mmstick on and we both agree this would be great, but a huge undertaking that neither of us have time to hit just yet.

I also believe that `apt`'s cache binary could be improved maybe with modern tools. Right now it's very fast to read from, but it takes quite a while to generate the binary after an update on the package lists.
>Why?

I said that because dnf is slow otherwise it's perfect. If additional frontend wouldn't really help then I think dnf don't need one. I thought using something like nala for dnf would improve dnf's speed I guess I was wrong.
This looks pretty great I‚Äôll have to give it a try
I just saw that Nala is in the official Debian repos, I think I will take it for a spin! :-)
Ya know I DO have a aarch64 Debian vm on my Mac, I‚Äôll have to install this on there and check it out. And there‚Äôs nothing wrong with being bastardized, it‚Äôs still legit! üòÇ
Thanks. That fixed!
Basically if you searched for Firefox, for example, and you would get firefox:amd64, firefox:i386, and firefox:flatpak. Then you install whichever version you want, or even all versions, if that was your fancy. Obviously, you would now also have to depend on libflatpak (or whatever its name is).
I will say once again,
# ***I HAVE NEVER USED YUM, DNF, OR FEDORA FOR THAT MATTER!***
Why do you need another way of interacting with APT? Surely that means apt isn't good enough by itself?

Like the biggest deal about a distro is basically the packages / the package manager. Why would you use something where that part isn't good enough by itself?

Also nothing is wrong with the wrapper :D But it shouldn't be neccesary right?
I'm on latest version of nala from the apt repository, Kubuntu 22.04.
That's a good question. I've been developing python for a while so that is initially why I tried it. I really like some of the features of xonsh. And in the shell you can just write python code to do things instead of having to do it the traditional posix/bash way.  


The xonsh.rc file is also written in python, so for me, it's much easier to configure things the way I want it. But I do know of xonsh's downfalls. If I had enough time I'd like to configure zsh the way I'd want it, or maybe try to use one of the new shells written in rust. I still use bash a lot just because sometimes it's easier to do what I need to do, or I'm avoiding a bug in xonsh.  


TL:DR, I probably would not recommend xonsh to someone who is not a python developer.
Sure thing. I uploaded it as a github gist.  
https://gist.github.com/volitank/b25e4e6da45e3c16e42fa796aaeb1a7a
A bit like the difference between using the classic Neofetch and something like macchina (Rust reimplemenation), for a CLI tool the difference between waiting a second for a tool complete and it completing before your screen's even finished rendering a frame is pretty noticeable.  For macchina it makes a bigger difference because you can literally run it every time you open a terminal emulator window and it won't slow things down, but if you're running a ton of nala commands in one sitting it being more efficient is quite nice.
I believe apt is written in C++, not Python.

Though there is a Python interface to apt libraries
No it's not.

https://salsa.debian.org/apt-team/apt

I'll agree that most of the time will be waiting on network IO, but that doesn't mean that you can't make your tool more efficient. 

And Python is not efficient. If you know Rust, why not use it? 

Or, if you don't know Rust, why not challenge yourself to learn something new?
TIL Apt is written in Python
wait, does it uses apt or apt-get and it's friends?
Thank you!

I think it's unlikely. You'd have to install `dpkg`, `apt`, `libapt-pkg` and then use Debian repos for the packages. I'm sure it's not impossible, but at that point just use Debian imo.

I'd love to get it to work distro agnostic, but the time and effort necessary for this would be absolutely gigantic.

In the future I do have plans on making a rolling release Debian distro to go with all the projects I have. But this is much down the line after Nala is completely stable.

The goal for the distro will basically be a rolling Debian with `firewalld` and `SELinux` by default instead of `apparmor`. But this is quite some ways away. `SELinux` on Debian currently needs a LOT of work. I have an installer half-written, but it's kind of trash and I'll redo it when I have enough time.
Wow, thank you for the insight.  
It's obvious that you put a lot of effort into Nala and I'm almost envious of your motivation to do all of this.  

Do you think dnf devs are aware of the "issue" with the sqlite database? If you think it'd be possible to optimize apt's cache binary, could that new solution be used for dnf as well?  

Sorry for all these dnf related questions when you're probably more experienced with apt. I'm just using Fedora and would love some optimization wherever possible.
If it was possible to increase dnf's speed with an additional program, I assume that could just be integrated into dnf itself.  
And I sure hope so, because it really is rather slow in comparison.
Yes it is. Right now it looks like `0.9.1` is in Testing and the latest version `0.10.0` is in Sid.
Is it Debian Sid or stable? If it's not Sid I guess I better get on releasing the \`nala-legacy\` packages for arm64 lol
Your Username Sure suggests otherwise
>Why do you need another way of interacting with APT? 

Who says it is needed? This is not an official project from the APT developers. It is developed independently because its creator thought it was worth the effort, and some people may find this useful.

>Surely that means apt isn't good enough by itself?

Not really. What makes you think that? For all of APT's existence, how many wrappers have been developed that remain relevant? Perhaps aptitude, and -up until very recently- Nala?

>Also nothing is wrong with the wrapper :D But it shouldn't be neccesary right?

It is not neccesary.
Hmm I just booted up an Ubuntu 22.04 test machine and it works. I have heard a lot of issues from Kubuntu specifically. It shouldn't be any different though. They use the Ubuntu Jammy repos right?
Thanks for answering, I thought that would be your reasoning after checking out xonsh website.

Looks like it would be a decent shell to use for someone getting started with python development to aid the learning process.
I wonder how many of python applications performance problems are due to the relatively slow default implementation, it would have been nice if debian had a way to easily switch implementation , there is a relatively large [list](https://www.openhub.net/tags?names=python_implementation) of python implementations and some of them seem like they could have a higher performance.
Fellow xonsh user!

Imo being a Python dev isn't the only case where xonsh is good, but if you *dislike* Python I definitely wouldn't recommend it. I'd recommend xonsh to people that are looking for a modern shell that isn't fish, and doesn't try to *fundamentally* change what a shell is (like things like nushell).

I also rarely notice any issues in xonsh's performance ‚Äî it starts a little slower than a well tuned zsh setups, but also definitely way faster than some undertuned bash setups I've used. ü§∑‚Äç‚ôÄÔ∏è

Also it works very well on native (non-WSL, non-MSYS) Windows with only Python, which is a huge plus for me since I end up doing a non-trivial amount of dev on native Windows. Having a bash-ish shell on Windows without an environment that messes with build systems like MSYS or WSL is extremely convenient and helps make Windows dev a lot less painful.
Thank you very much!
Even if it's not really that big of a deal, I want to make Nala the very best that it can be. I'm determined.

for neofetch, it actually annoys me how slow it is. My RedHat server is the worst cause neofetch has to get the total package number from dnf. So it's double slow.

I have it in a login script that runs neofetch and does some other things. Gets old being so slow lol.
My bad, it is c++ indeed
Interesting. Good to know.

Anyway i don't think rust will give much better results in this case. Anyway, do it in whatever language you want. I just don't see any benefit of doing so, technically speaking. Of course personal preference is important, but it is not technical reason.
APT written in c++. Nala uses python-apt to interact with it.
I see. No worries. I appreciate the work you ~~fo~~ do and best of luck on the Debian project. I can test for you down the road if you needed a tester. I have a crappy HP sitting in my basement that I can nuke and test stuff on it.

Also, can this Nala project be forked and made compatible with Arch, or would it be a giant undertaking?
Anything is possible. I'm sure the `dnf` devs are aware, idk how they couldn't be.

I don't think it's something that is really a priority for them. A big issue with mainline package managers such as `apt` and `dnf` is that the entire ecosystem kind of depends on them. A change in the cache format would be a HUGE change for them, and potentially impact stability.

I'd imagine improvements are possible on both sides of the cache. I don't think that an optimized `apt` cache would be able to be used on `fedora`. The package systems work too differently to really share a common cache optimized fully for both.

For `apt`'s cache I'm not 100% on how they have it setup, I haven't really dug in deep on the cache format. One day I will, but right now something this low that just works will remain the same.

One thing that is Beneficial in Nala's development is that I can make any kind of change or feature that I want, and your system will still have `apt` to fall back on if there is an issue. Technically I think this was the design goal with `apt` vs `apt-get` but they don't really use that to their advantage.
dnf is not slow. It's just doing different things to apt. By default, dnf will check packages with the remote repositories every time, while apt only consults a local cache. It's a tradeoff between speed and correctness. dnf (or rather, yum before it) chose one, apt chose the other. From memory, you can tell dnf to cache the package list locally and only use that to get a similar behaviour to apt, but I've never needed to - not least because software upgrades are not something that's typically on the critical path, so performance is rarely a major consideration.
Stable. I only have built it for bootstrapping a Slackware install and I‚Äôve yet to finish my job so it‚Äôs still there. I‚Äôll take a look when I‚Äôve got a minute though, don‚Äôt rush anything, if it‚Äôs not currently avail I‚Äôll live!
Check my flair.
I did once try to test `pypy`, but since `python-apt` has a lot of C++ code and uses the standard cpython api it doesn't work with other implementations.

I'm not sure that python can ever be as fast as a compiled language such as the C's or rust. A lot of Nala's performance limitations come from how `python-apt` was designed in the first place. If it was written with a little more love it wouldn't be much slower than `apt`.

For xonsh I have no idea. The only thing that is ever noticeably slow for me is tab completion with long lists such as `apt install [TAB][TAB]`
Most of the things I don't like about Xonsh, outside of the performance I notice when tab completing large lists, are how they change some functions of the shell.

For example you can do `grep "Some String" $(find /my/dir -type f)` with bash.

It's mostly the same with xonsh, but it's `grep "Some String" @$(find /my/dir -type f)` which I find annoying.

Additionally I use a lot of environment variables in my work and it's kind of unfortunate that I have to define it like

`$MY_DIR_VAR="/home/volitank/my-dir" command *args` rather than

`MY_DIR_VAR=/home/volitank/my-dir command *args`

These are small nuances that I don't like with it. But I continue using it because for the most part I just like how it works better.

and writing the `.rc` file in python is just so much more intuitive imo.

Last point I can think of off the top of my head was getting xonsh to work properly with `poetry shell` when developing python stuff wasn't very trivial, where bash, zsh and such it just werks.

Just small things like this make it sometimes annoying because you can't always copy/paste commands from websites.
Aside from performance reasons, the biggest reason is that by authoring the library Nala uses to bridge the gap between itself and `apt`'s c++ code, I will be able to control more with how the program functions. Updates and bug fixes for the code can be streamlined.

Right now if there is a bug with Nala, and it's actually with `python-apt`, there is little to no chance that will be fixed.

a big example is that implementing `nala install --reinstall` is not currently possible without a patch to `python-apt`

edit: well it's technically possible, but there is no way to check to ensure the package will be reinstalled. We would just have to say that it is and hope for the best. I don't really like this solution. There is a list of benefits to writting \`rust-apt\` whether Nala itself gets rewritten or not. Pop-OS will be using this library a lot when it's more mature.
It would be a giant undertaking, or else I'd do it real quick. Most of the code would probably have to be thrown out, or at least refactored to be more general.

For formatting the output of `dpkg` I'm honestly surprised I ever got it to work in the first place. We have to subprocess out the operations to `dpkg` and put it into a pseudo terminal, where I then wrote basically a small, `dpkg` specific, terminal parser to read what `dpkg` says, and then output it in a pretty fashion.

By doing it this way (I couldn't think of a better solution), all of the support for things like `apt-listchanges` are basically hardcoded. So if you have a personal install hook that requires user interaction it will hang up likely giving no indication of what is wrong. This is why the `--raw-dpkg` switch exists. It does away with the fancy parsing and just throws the info into the terminal like `apt` does.

I am still impressed with myself that I was able to get `debconf` prompts and window resizes to work properly during `dpkg` operations. This will undoubtedly be the most difficult part to rewrite in rust.
Alright, thank you!
dnf *is* slow. You've just explained why.
dnf uses local caches. It's just more eager to update those caches. You can either change that to use older caches or update the cache in the background on a timer.
Ah, that mostly explains it.  
Thanks for clearing that up!  
Though, I still have the impression that checking the repositories using dnf takes significantly longer than with apt. Is it just me?
Well you can get `0.9.1`, Just not the latest version. It'll still work for most things. Only thing SUPER buggy in `0.9.1` is that you can't really install local debs at all. I'm definitely going to get the new version out there by tomorrow though.

And just a quick note, on stable you'll have to install `nala-legacy`. It's a weird compatibility build for older systems that don't have the required python version and libraries, but it works.
You dont have a flair
Needing to strip subprocess substitution for all practical purposes is the one pet peeve that still grates on me at this point. I got used to typing `env VAR=val` for one-off commands at least, and prepending with a dollar sign for persistent changes isn't something that happens enough to bother me. And worth it to be able to do Python-y math in my shell with no overhead lol.

Completely agreed that writing the rc files in Python feels so much better though.

It's definitely not a *seamless* transition from bash/zsh to xonsh, but imo the bash influence is strong enough that switching to xonsh had way less friction than most other modern shells or even fish. And tab-completion supporting bash completion scripts by default is the icing on the cake. That and prompt_toolkit is awesome.

Still definitely not for everyone, but it's nice to see it does get some use~
Yeah, my mistake i thought apt is written in python given it has many python plugins. Bug in binding is good technical reason though.
Sound like a lot of work to me. I'm going to try it on pop os on my other machine.
Great! I‚Äôll give it a whirl!
Glorious EndeavourOS. Thought I was on a diff sub lol
Honestly the main reason I started using xonsh is so I can just type `1 + 1` into the shell and get 2 lol.

I'm mostly use to the little quirks that bother me, so it's not too bad. You're right though, xonsh supporting bash completions is honestly God Tier.

I am also a big fan of their history. I have my history backend set to sqlite. and then an alias `aliases['xhistory'] = 'history show all'` Perfect.

Overall I do like it better than bash, but I could totally see someone trying to use it an Noping out, it took me a little bit to get use to some of it's differences.

edit: I'm pretty sure this alias is what made `poetry shell` work. If you ever run into it doing python stuff.

`aliases\['.'\] = 'source-bash --non-interactive`
No worries. There is a lot of `apt` stuff running around in python, perl, etc.  

`dnf`, however, is written in python.
Yea‚Ä¶but is this really ‚Äúbuilding your own‚Äù or more ‚Äúuse somebody else‚Äôs script‚Äù?
I have tried again  with the lxqt and  xfce profile and it works perfectly fine 100% !!!The first time might had a problem maybe because i lowered its priority ? but now it works perfectly fine !!!

Its gonna have a part 2 

and obviously a  part 3 :-)
I only watched about 10 seconds of the video, but I think the whole point is you can build your own iso file with everything already on it. 

I'm not entirely sure that it's really all that useful in any way, a customized build script that pulls the latest packages down seems like a much better idea in my opinion, but I guess if you wanted to do a bunch of installs all at once it could be semi-helpful.
Yes its about building your own i will explain more in next videos !

You can add your packages your config files and customize every parameter of course

You can create your own desktop profiles
>	Yes its about building your own i will explain more in next videos !

You can add your packages your config files and customize every parameter of course

You can create your own desktop profiles

Using someone else‚Äôs script
And probably you can modify the script as you like and fork it and create your own.

Also it just uses Arch Iso so its standard and used by many popular Arch Linux distros from what i know.

Anyway this is the way to go you can try just using archiso if you want but you will end up with the same results.
Has anyone ever attempted multi-core support to increase the speed of packet installation if there are multiple packets that could be installed in parallel?
Rewritten into python!!!

(Don't perl me!)

Edit: Actually ... I was not so far off.

See at 35:02 where apt-sign is implemented in Python and C++, based on OpenSSL. Guess perl is slowly marching into legacy land ...
Delta updates would be a great addition, specially in countries where Internet is expensive or when using a mobile data plan with a cap
I would assume that package installation is mostly networking and disk I/O. Is there any point at which multi-core might give significant gains?
That actually IS a good question. It has also been my experience that things such as extracting archives, copying files etc... is the slowest part of package installations (if we ignore having to download anything which can take even longer).
Why not use the QMK firmware?
A possible alternative is to reverse engineer the windows software and write your own Linux version. That is what i did for the older GMMK keyboards: https://github.com/dokutan/rgb_keyboard, and what other projects like OpenRGB do.
Their software *sucks.*  Buy a Pro and use Via or Vial if you want a GUI or upstream QMK and write your config yourself
I have no idea what this is or why I'd want it. The site wasn't clear either. Mind sharing some info?
75 already. I always admire the amount of dedication from this sub. keep it up :)
Let's go!! Mac got 70 votes and they are taking action!

We have 30 votes, its not so far!!
Just wanted to ask this, doesn't it support qmk?

Edit: updooted anyway, because, why not...
Yes! That's what I'm currently using and it works great, but it would be cool if we had first party support and got a company to acknowledge Linux as another relevant platform
That's true, but look at the page I linked: they are planning a revamped app for Q4!
Sure! It's the software that controls all hardware products from Glorious Gaming, such as the GMMK keyboards...

It would be a deal-breaker for us Linux users!! üò±üò±üò±

EDIT: Because currently the only thing we can control in Linux is the lighting, and that's done through a third-party cli app
They make socketed switch keyboard where you get a working assembled base board, switches and keycaps separately.

They are pretty much the only provider of full size iso boards so they are kind of useful in my case.

I need to replace my keyboard and I researched the subject a bit, I don't need the fancy RGB but I kind of like the idea of custom switches and, within my constraints, they seem to be the best choice.
I agree, but am not very optimistic we will receive a FOSS Linux version. The best i personally hope for is a closed source Linux port of their software, which i wouldn't consider ideal.

In addidtion, asking for (and receiving) official documentation of their USB protocol would make the development of alternatives to their official software much easier.
https://github.com/libratbag/piper

Looks like this should support it somewhat - having one application that controlsmany keyboard models is better than having many propietary brand-specific application imo

If anything we should ask them to contribute to libratbag
Thanks for answering. I hope things work out for the users then!
Actually I wrote them about this, that they should either get to work or release their code so that it can be integrated into some other project like piper.

The answer was that they are trying to improve the one they have...


I tried :(
Personally I would say "fuck them" and not buy stuff from them ever again

In practical terms maybe you could help that project by reporting missing features on your keyboard?

We don't need the manufacturer to make better software than them - it would just make it a lot easier.
Noting to self not to buy anything from Glorious Gaming, whatever that specifically is.
Hmm I partially agree, but don't you think it would be better to encourage more and more companies to adopt OSS practices and to make them acknowledge Linux as a platform with lots of users?

Third-party software is cool and all, but the final goal is to get first-party things.

Also yes, with the current state of their software I haven't bought anything else from them, and I clearly stated that in the email I sent them...
>but don't you think it would be better to encourage more and more companies to adopt OSS practices and to make them acknowledge Linux as a platform with lots of users?

How is you buying their products despite giving you 0 support encouraging them to do anything but not support you?

>Third-party software is cool and all, but the final goal is to get first-party things. 

I disagree, I'd rather have one open source application across multiple manufacturers.

The goal should be that manufacturers contribute to libratbag and we no longer have to deal with those bs applications
> The goal should be that manufacturers contribute to libratbag and we no longer have to deal with those bs applications 

That's another option, that would be really cool too
It's not an option, it's how we (Linux/OSS using people) live our lives for the most part. That's how Linux kernel works, etc

It's just that people coming from windows are used to crappy ad/spyware laden pieces of poop from their printer, scanner, keyboard, mouse, gpu etc vendors. Each having their piece of your cpu and personal data. No way I want that type of "support" in my world. Leave it for people running windows.
Wish they'd just port IrfanView, it's just such an excellent light-duty photo processor and editor. I haven' found a direct equivalent yet, they either fall short or are too much.
Gwenview from KDE does some of those things.

For renaming exiftool and bash work really well.
try nomacs or xnviewmp (this one also handles color management) 
https://www.xnview.com/en/xnviewmp/
Digikam has most if not all the tools you are looking for  (and more), though it is not lightweight.
[deleted]
I will mention I have used irfanview ages ago under linux with WINE.

But it was not an ideal solution. 

But at the time i needed to do a task that irfanview was ideal for, so i used what i knew to get the job done. :)
Have a look at [gThumb](https://wiki.gnome.org/Apps/Gthumb). Fits in nicely with GNOME, launches fast, and has a solid feature set.
I've looked at so many alternatives and frankly none of them do enough.

These days I run IrfanView under Wine and just deal with it.
Gwenview and XnViewMP (both from flatpak) (gThumb too but it's kind of buggy for me and it shows you only images, not the folders, so you're gonna have to use your mouse to navagatve folders instead of just the keyboard)

I'm coming from FSViewer and these are the only ones that meet my standards. An image viewer should have:

* Its own fileexplorer
* Extremely configurable shortcuts
* Ability to edit images (size, crop, rotate, brightness etc.)

Gwenview is by far the best one IMO. Though if you're using Xorg/X11, you will have to run this command after you install it, so it starts faster:

    flatpak override org.kde.gwenview --user --share=network

For some reason the inability to access the network slows down its start time under X11. Wayland appears to be fine though.

The only thing I don't like about Gwenview is that mapping the "Next" and "Previous" shortcuts to left and right arrow prevents you from highlighting the next / previous folder in the file explorer. Without it, you have to bring back the image to its original size first and then go to the next / previous one with the default built in shortcut, which is probably something like "Next / Prev image in preview". 

The point of wanting to map Next and Previous to left and right instead is so that you can go to the next image without zooming out first. (there is a special configurable shortcut to bring the image back to its full size)
Flameshot
gThumb
None. That's why it's been around for 30 years. Just run it in wine, it works.
Why not just use [Irfanview](https://i.imgur.com/bmkwtKz.png)?

I installed my system default Wine and used the [WineGUI](https://gitlab.melroy.org/melroy/winegui) application to install the program.  Seems to work quite well for me.  I use the 32-bit version of Irfanview, version 4.58 installed in a WinXP bottle with the WineGUI application.
I think GIMP does all of these though I'm not sure about the 3rd point
Hmm. Kolourpaint may be an alternative. Not as feature full as irfanview but simpler than gimp.

By the way you can also use wine + irfan view. I used this back on 32 bit systems. (I can't get wine to work ever since I moved to 64bit systems, it's over my head to figure out why...)
Photoflare
Image Glass.

edit: mind was on Windows.
Our best hope is supporting Gwenview development. It's steadily improving a lot and since it's a core KDE app it has more development manpower behind it.
Digikam
Nomacs is very good, seems to be the kind of software that is actually written by someone who uses it a lot and knows exactly what pain points people have with other programs.
it's completely written in win32 api (whatever this is called, I'm not a programmer), like foobar2000, it's not easily ported to anything non-windows
Xnviewmp does most of the same things. It's *way* more powerful than the half-assed crapware viewers like Eye of Gnome and all the others.
Will take a look, thank you
‚ÄúAnd more‚Äù is the problem with all Linux software alternatives.
Tyvm for the detailed answer. That looks like a perfect fit! Converting can easily be done on terminal anyway, I guess.
Thanks so much for that. gThumb doesn't work that well and Shotwell constantly crashes on my machine. This was needed.
+1 for nomacs. Has fairly advanced features, while not being too much.

But also I mainly use viewnior as a image viewer, as it is as simple as possible for my needs. (Though it has basic rotate/flip editing capabilitites, I do not use them for editing.)

And ofc GIMP for more advanced stuff.
Thank you for this suggestion!
\+1 for nomacs. Haven't looked back since I switched to it years back. Very lightweight, good performance.
Good point.
Although, I'd like it to be a bit more lightweight than GIMP is. Will add this to my question.
Not very lightweight though
Have you tried nomacs then?
That wasn't added to the question when I wrote that comment. Read OP's reply to my comment

Idk why people keep down voting this
Haven't heard of it yet. Will look into it, thank you
This happened more than 2 years ago. KDE does not appear to have died yet.

And, to be fully accurate, LTS releases were only delayed for open-source. Meanwhile you have https://community.kde.org/Qt5PatchCollection, which backports stuff from Qt 6 to 5.15.
The answers to your questions [from a KDE developer.](https://youtube.com/watch?v=s5vnlFFAp7Q)

EDIT: sorry for the link mix-up. I initially posted it from mobile.
Qt LTS releases are not closed source. The extended support is commercial only, the releases themselves are still open source.

No, this won't affect KDE. KDE already maintains patches for old Qt LTS releases.

Of course they won't switch to GTK. Two main reasons: 1. that would require re-writing the entire desktop. 2. GTK doesn't have LTS releases at all so that wouldn't provide any benefit.

Qt is still the best possible toolkit for developing cross-platform. Especially if you want stability and native looks. Again, GTK doesn't have LTS releases so Qt isn't any worse off than GTK has always been.

It sounds like you have never actually looked into anything to do with Qt, GTK, or KDE. Maybe start with some basics before divind head first into the wild conspiracy theories and knee-jerk reactions?
I don't really know how many users KDE has today. 

But from the user's point of view they need to rethink the whole desktop and concepts. 

Actually there should  be a question and a research of how  modern Desktops should evolve back to simplicity.
Did they ever post text explaining it?  I‚Äôm not able to listen to a YouTube video.
Someone got confused with their links...

EDIT: hehehe :)
Maybe don't be a dick Mr. Smarty Pants.  He or she just asked a question.  You were free to ignore it and move on.  But that would get in the way of you waving your elite Linux expertise around.  Can't have that.
Frankly, it works fine. And it has wobbly windows.
Why not just use Gnome, they have the uniqueness and 'simplicity' thing going on. Not everything needs to be rethought. Just leave my workflow and myriad of options alone.
[Here's](https://tsdgeos.blogspot.com/2020/01/the-qt-company-is-stopping-qt-lts.html) a blog post from a KDE dev about that. IIRC Niccolo's video is more thorough, but that's all I found in written form about this 2 years old topic.
He or she asked a pretty uninformed question about a fairly advanced topic, and the parent comment was a much nicer response than you'd get from most people with an understanding of the subject matter.
Gnome works well, macOS works well....

And then I've found myself 20 years later, even more towards 25 years later, since 1998 when I saw KDE 1.\*... Oh long story short - almost 25 years later I consider myself to be happy with Gnome and Android desktops but I don't  love them at all.

Lol, RetroSpector78's video about RedHAt 5.2. I look at the mirror and see myself saying "Wwwwoooooowwww  Enlightenment!! Coooll!! wooow!.  25 years later trying E. - what the heck is it and how do I use it for daily  tasks?? üòÆüòÆüòÆ
Oh no, it doesn't work that way, you know!

Opposite camps want to convert each other and show the only right way! 

Actually I am very agnostic to DEs, but curiosity leads to try a few popular desktops. 

For me KDE looked overcomplicated in comparison with Gnome, Mac, Win.

 I would put it somewhere nearby XFCE.  But I liked KDE-1 look and feel, way better than Win98.
Thanks.  YouTube isn‚Äôt a particularly great way to share what is essentially text.  There‚Äôs no value in the video.
It's a bit (understatement) of a hack, but you can download the (automatically generated) closed captions from youtube with yt-dlp or some shady website and read them as text. Did it for this video:

https://pastebin.com/Ari2vxiK

Haven't proofread it or whatever and the formatting is garbage, but it *may* be better than watching the video. :)
Shipping a unified kernel image is an interesting concept and useable for virtual machines but for a normal setup you have partition UUID's, lvm and cryptsetup that need to be part of the initramfs giving the integrated initramfs a big size for no reason (they can't be customized for each machine) and just not working because the cmdline is included in the unified image and static which would force everyone to use the exact same partition layout and UUID's.
> they can't be customized for each machine

Why not? We already generate an initramfs on every machine. That doesn't need to change.

> and just not working because the cmdline is included in the unified image and static which would force everyone to use the exact same partition layout and UUID's.

The idea is that you would not use a UUIDs to identify filesystems, but instead use well-known partition-type UUIDs instead.

But whether or not you use the same layout seems beside the point. Unified images could be used for any layout.
>  you have partition UUID's, lvm and cryptsetup that need to be part of the initramfs giving the integrated initramfs a big size for no reason (they can't be customized for each machine)

This isn't really a problem, your initrd should get these from somewhere else than the kernel cmdline. EFI vars should be perfect for non-confidential system configuration.  
My main gripe with unified kernel images is that they are very storage wasteful when pulled in a filesystem based VCS VFS like ostree (files based dedup, not chunks).

I was planning to switch from my custom scripted grub bootloader, but systemd-boot doesn't really cut it. 

- Auto-detection and creation of boot entries support is minimal, it's just for unified kernel images. In other words, if you want to boot a previous system tree checkout, then you need to edit the bootloader config, and that's the last thing I want to do. Unless there's an update for bootloader, I don't even want to mount the EFI partition, no-to-mention writing to it.
- You can't bundle a config with the bootloader's EFI binary.
- And the worst one is that there's no way to globally append kernel parameters to the cmdline.
> Why not? We already generate an initramfs on every machine. That doesn't need to change.   

Correct, we currently do which would be stopped if it was instead statically shipped inside the unified kernel image which cannot be changed without generating a new one which would invalidate all advantage they bring because then they wouldn't be a known constant on all machines.

> The idea is that you would not use a UUIDs to identify filesystems, but instead use well-known partition-type UUIDs instead   

This would still cause problems with lvm, raid, crypto and so on that need customized values.   
I can think of ways to fix this but that would increase the initram size and make this more complicated than it needs to be.

Edit: 
> But whether or not you use the same layout seems beside the point. Unified images could be used for any layout.    

What about restoring a system subvolume, Fedora and a few other distros have btrfs or some other filesystem set up to boot into a seperate subvolume in case the system doesn't boot which requires a different cmdline only if the system fails to boot so in this case you'd have to have two UK images.
> Correct, we currently do which would be stopped if it was instead statically shipped inside the unified kernel image which cannot be changed without generating a new one which would invalidate all advantage they bring because then they wouldn't be a known constant on all machines.

There's still the advantage that the initramfs can be signed with a per-machine key.

In fact, the biggest advantage I see is that it's probably the easiest way to get the ludicrous behemoth that GRUB is out of the picture.

> This would still cause problems with lvm, raid, crypto and so on that need customized values.

Do they? Most Fedora systems use a lot of that and get by with a single `rd.auto` parameter. There has been a _lot_ of work in making the initramfs detect everything automatically.

And even if they don't... so what? You can customize the default command-line and sign that.

> What about restoring a system subvolume, Fedora and a few other distros have btrfs or some other filesystem set up to boot into a seperate subvolume in case the system doesn't boot which requires a different cmdline only if the system fails to boot so in this case you'd have to have two UK images.

I don't know enough about btrfs to answer that.

But given that there are [discussions on the `fedora-devel`](https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/A56767DM3BLNWUN6WUZ66SNTW3D66UZQ/#PNZXE364J2665HMVQKUM4TZLKG4LR32Y) mailing list on how unified kernel images might be usable on Fedora, I'm sure that _if_ it is ever adopted on Fedora ‚Äî don't worry, it's still an if! ‚Äî it will work in Fedora. Why wouldn't it?

Stop being so negative! :-)
> Do they? Most Fedora systems use a lot of that and get by with a single `rd.auto` parameter. There has been a _lot_ of work in making the initramfs detect everything automatically.

They do, it's a mount parameter and it's not what you think it is, normally it mounts a path in the filesystem '-o subvol=@' but if it fails it instead mounts '-o subvol=@snapshots/...' both mount points work but one is before, for example, a system update and one is the current system and if the update fails/is interrupted/causes you to not be able to log in/some other error it mounts the other, older, sysroot.

> And even if they don't... so what? You can customize the default command-line and sign that.   

I'm not saying UKI are bad, i use them fore secure boot. I'm saying shipping them through the package manager is a bad idea.
> I'm saying shipping them through the package manager is a bad idea.

I think the plan would be to ship a "base" initramfs via the package manager, and use overlay initramfses for local customisations.

This is already possible. You just concatenate multiple initramfses together.
> This is already possible. You just concatenate multiple initramfses together.    

Is there a reason for not just generating the UKI on the PC?
That's what I _expect_ to happen.

Heck, that's what was demonstrated in my link! Did you not read it?
> The whole idea of using a drastically simplified boot loader instead of grub, **along with _pre-built_ unified kernel images**

\^This is what my main points were about 
> Shipping a unified kernel image is an interesting concept and useable for virtual machines **but for a normal setup** [...]

Edit:
> Heck, that's what was demonstrated in my link! Did you not read it?

I did, apparently you didn't.
Sure, if a pre-built image works for 90% of users, that's a win. The remaining 10% might still need to overlay it with extra things.

Seriously... when did the Linux community become so damn _conservative_? This post was just a quick demonstration of how you can try out UKIs yourself. It's not the downfall of civilisation!
> Seriously... when did the Linux community become so damn _conservative_? This post was just a quick demonstration of how you can try out UKIs yourself. It's not the downfall of civilisation!

I'm not being conservative, I've been using unified kernel images since before this article was posted I'm just against the idea of forcing everyone to use a giant initramfs that has to include all kernel modules, be unoptimized for the pc and isn't an upgrade from instead generating it locally which can already be done completely automatically with a bit of setup.
https://github.com/lima-vm/lima/
Yup, docker as development (and deployment) tooling is a common pattern these days, I regularly use docker as an interactive shell to test and develop things, it's great stuff!
I do something very similar on my work Mac, except with podman. I have a collection of scripts that build, run, start, stop, and destroy containers based on environment variables. I have dockerfiles for various versions of Ubuntu, Alma Linux, and CentOS. I use Alma9 as my daily work distribution, but can experiment with just about anything quickly and easily.
I do all my development on a mac in mounted volumes in docker containers with various flavors of linux
I guess installing linux isnt an option?
Valgrind runs native on MacOS though
If you install docker on Macos it runs in an alpine image already.

I personally prefer using podman, which comes with a Fedora CoreOS image to run docker.

From there it's the same thing as you are doing p
For M* Macs, Asahi Linux exists.
I'm a Mac owner who runs Linux.  SSD drives are so cheap now (1TB for $99), I recommend buying a second SSD drive & dual-booting with your favorite Linux distro.  There's no need to mess with the overhead of virtual machines unless you're interested in VMs.
Wow. Thanks for the link, looks like a really cool project.
What's lima?
No we don't have administrative privileges at school. At home I'm using Linux for my PC and my Laptop.
At school we don't have administrative access, so VM-s are the best choice.
Asahi is a failed project. Apple won't open their GPU specs (or any other) in observable future. Sure devs can have some fun with it, but sorry, it will be even worse than Nouveau.
Otherwise, it's always possible to set up VMs manually too of course.  
https://github.com/utmapp/UTM/ is great for that; it's a really nice front-end for QEMU.
> Lima launches Linux virtual machines with automatic file sharing and port forwarding (similar to WSL2), and containerd.
>
> Lima can be considered as a some sort of unofficial "containerd for Mac".
>
> Lima is expected to be used on macOS hosts, but can be used on Linux hosts as well.

Must be hard clicking on Links...
You‚Äôll probably find it good practice to develop stuff with docker at home, too - all my home application servers are containerised because it makes them portable to any host OS, and sometimes even any CPU architecture (e.g, bar one container, all the docker-compose files on my arm pi can be run on my x86 desktop).

It also makes a good start for infrastructure-as-code, and helps you get to grips with those patterns and concepts.
You mean VMs.
It's still in it's early days. Meanwhile nouveau has been around for a zillion years.
Just make sure to use an aarch64/arm64/armhf distro with UTM on M1 to get the benefit of kvm paravirt.
*sigh* must be hard understanding a reference to a childish joke / internet meme.
Yeah, I corrected it
Yeah, emulating x86_64 is *slow*.

If you need to run x86_64 apps, use Ventura's Rosetta in Linux.
Oh lol I get it now. I thought you were just curious.
The joke is "ligma" and it wasn't really that funny when it was popular how many years ago?
...did you though? Not to be pedantic, but your post now says "vm-s" as opposed to VMs.
But it has not been released yet.
You must be fun at parties.
To be frank if I hear someone making a "lick my ass" joke in a party I would immediate find another crowd to join.
Only the highbrow crowd for... JockstrapCummies
Continously updated list of stuff that works and does not work:

https://github.com/AsahiLinux/docs/wiki/Feature-Support

Kernel version > already upstreamed

asahi-linux > Working and soonish to be upstreamed

WIP > Not working very well yet
What package manager does asahi Linux use?
Very exciting work. I'm constantly impressed by these guys. I'm probably getting an M2 MacBook Air in the next few months, so being able to run Linux on it will be a huge huge plus.
can this distro run proton?
wait, so Asahi Lina isn't marcan after all? Who she is then?
I am so fucking hype for this. My macbook with the m1 pro is literally the nicest piece of hardware I've ever owned, nothing comes close to apple hardware imo, but mac os gets very annoying over time.
Could we hope what asahi brings to be mainstream and integrated to regular distributions ?

Following this project is amazing ! :)
That was such a fantastic, fascinating update blog post. Thanks for posting it!
I'm following closely.
Why are talented open source people wasting effort in porting open source software for a company that not only makes it the hardest feasibly possible for them to conduct this porting, but is also philosophically diametrically opposed to their spirit and work, so much so that it would not be far fetched to say this company hates them for creating more choice in their anti-choice environment?
How is the GPU and the speakers on the base M1 MBP? Only thing holding me on to macOS
It's based on Arch, so pacman
Don't know why you got downvoted since that's a legitimate question.

AFAIK you probably need Rosetta2 to run amd64 binaries on M1 and that's still an Apple/Mac thing although Apple has recently make it [available for Linux](https://developer.apple.com/documentation/virtualization/running_intel_binaries_in_linux_vms_with_rosetta?language=objc) but its too new and it was intended for Linux VMs on top of MacOS. However it seems to work on other environments and even other ARM CPUs. [Somebody got it running on Amazon's ARM CPU](https://twitter.com/never_released/status/1534127641082593281).
"can this distro run proton"

if you are talking about valve proton, then idk but probably no I don't think an arm version exists.
Possibly via Box86_64 but that would probably be a horrible experience
Some beast. Asahi Lina is what she goes by everywhere though https://github.com/asahilina
Why do you think it's not Marcan?
It's such a shame that brilliant hardware like the M1 and M2 is so tightly shackled and locked down with garbage like iOS and macOS.
Everything is being upstreamed, it just takes time.
>makes it the hardest feasibly possible for them to conduct this porting,

This is not the case.
>a company that not only makes it the hardest feasibly possible for them to conduct this porting

If Apple truly "hated them for creating more choice" and wanted to make it as hard as possible to port Linux, they could easily lock the MacOS boot process, like they do on iOS and ipadOS (it's very similar apparently), and leave the Asahi people looking for jailbreaks.

Instead, they provide tools specifically for the purpose of booting 'other OSes', and last November they actually improved this process, for no apparent gain for MacOS. True, they don't make Linux porting \*easy\* by publishing hardware specs, but they absolutely don't try to stop it.

The M1/M2 series hardware appears to have a uniformity which once the initial support is there will make it relatively easy to support Linux on future models; this should actually be better than (say) some random laptop where they could, and often do, stuff like substitute a different WiFi controller with no Linux support without even changing the model number.

Much non-Apple hardware has no official Linux drivers and has to be reverse engineered like the Asahi people are doing.

I'd just add that I am not a fan of Apple as a company and have exactly zero Apple products, but do find the Mx devices and the Asahi project interesting.
Because a lot of companies supply only macbooks (yeah, horrible) and macbooks pro have an excellent exterior quality (trackpad, screen, weight, battery and so on).
Read the article
GPU will be one of the last things completed. Unfortunately you'll be waiting a very long time (unless they get more funding for more devs)
Rosetta2 doesn't do much special when it comes to emulation. All of the special tricks that Apple Silicon Macs have for improving x86 emulation performance are a part of newer versions of the ARM spec (and those tricks only account for a relatively small performance increase. The chip is just fast enough to brute force x86 emulation well. [Source](https://twitter.com/marcan42/status/1534053625110351872)). Rosetta2 for Linux isn't going to be some game changer.

FEX is likely going to be the go-to for x86 emulation (rather than the slower QEMU based offerings). The biggest problem is going to be with 16k vs 4k page sizes. XNU has limited support for running different processes with either a 4k or 16k page table size. Linux likely never will have that support (as page table size is treated as a constant in many places of the kernel. [Source](https://news.ycombinator.com/item?id=30731383)). x86/amd64 only supports 4k page table sizes. Many ARM devices only support 4k page table sizes (eg Rpi4). Many programs only support setting page table size at compile time (formally Chromium. Linux kernel). You can run Linux on Apple Silicon Macs in 4k mode, but you give up performance and require some hacks to get IOMMU to work (a version of Asahi will likely be available in future that supports 4k page tables). FEX requires 4k page tables as that is what x86 apps expect ([Source](https://github.com/FEX-Emu/FEX/issues/1221). Rosetta2 runs in 4k page table mode)

FEX does support Proton (with changes, detailed [here](https://wiki.fex-emu.org/index.php/Proton)). Before Asahi supports a 4k kernel, or FEX adds 16k to 4k emulation, or Linux adds support for running different page table sizes per process, then QEMU User mode is probably your best bet (I've heard that it's been integrated into Flatpak to "just work" when it comes to emulating Flatpaks of a different architecture). Anyway, gaming support is limited until full GPU support is added (alpha support coming as soon as in a few months).
The distro can, the architecture can't‚Ä¶
[FEX](https://github.com/FEX-Emu/FEX) should be faster - x86 emulation works well on macOS, I don't see why it wouldn't work on Linux
Only Box64 will ever be supported. Box86 will never be supported as Apple as excluded 32 bit ARM support for years now. FEX however does 32 bit to 64 bit translation, but it doesn't support 16k page tables. Box64 will always be limited as many games have still include a bit of 32 bit libraries somewhere. The reason why Wine64 is not usually used (which also has no support for any 32 bit binaries).
Why horrible? Actually Box64 is already quite fast and there is still room for optimization.
Because this is Marcan's GH: https://github.com/marcan

So unless (s)he codes under a different pseudonym in his sleep, it's a different person
That's part of why it's able to be great tho, apple just made a chip with everything *they* wanted. Compatability be damned.
I‚Äôm a Linux guy. But I‚Äôm just an average user not a developer. Can you elaborate on how you don‚Äôt get on with macOS? Is it a dev thing?
Actually it's locked only by lack of official support for other operating systems. Compared to some other ARM hardware, Apple doesn't block running any third party operating system on their ARM hardware. They could easily block it just like on iOS hardware as their boot process is similar but they didn't choose to do that. There are many more locked ARM hardware.
Good to know, thank you for the answer :)
Yup.

OSX is a pain. So much so that my work-issued MBP is in a corner after I convinced the CTO to let me get an XPS.

Getting the build quality of an Apple product with the usability and power of a proper operating system would be a dream.

The attractiveness of Apple is in the ecosystem. If you're running a heterogeneous environment, there really isn't much to set it apart.
Additional info:

>You can't just mix 4K and 16K pages. It's not like huge pages, where you can have a mix of huge and normal pages in an address space. Page size is a global CPU config option (two, actually: one for userspace and one for kernelspace), and it changes *all* the page sizes, including huge pages, along with it.  
>  
>What macOS did is add an abstraction layer that allows it to manage processes in 16K mode and 4K mode, while the kernel always uses 16K pages. This is a nontrivial amount of work, and involves deep surgery in the virtual memory subsystem. Then the kernel has to flip the CPU global page size setting every time it does a context switch. It's a pretty ugly hack, and probably has a significant performance cost on other CPUs (e.g. might need a full TLB flush); presumably Apple designed the M1 so that this kind of thing at least performs well.  
>  
>So Linux assumes that the kernel and every userspace process runs with the same page size, everywhere. Breaking this assumption would require a huge amount of deep kernel rework, likely affecting all architectures, not just ARM64. Turns out it's a lot easier to rework the kernel to be able to have different IOMMU and CPU page sizes, so that's what we're doing instead.
Fex looks pretty cool, but personally I think box86/box64 are more impressive, at least for now. 
 
https://box86.org/2022/03/box86-box64-vs-qemu-vs-fex-vs-rosetta2/
There's no Vulkan driver. OpenGL ES 2 Support is slowly getting there but GL ES 2 basically represents the feature set of a GPU in the early 2000s.
FEX doesn't work on 16k pages. In order to use it on Asahi, a MASSIVE rewrite to how the kernel handles paging would need to be made.
Benchmarks show box86 as being faster than fex though, no?
https://box86.org/2022/03/box86-box64-vs-qemu-vs-fex-vs-rosetta2/
I was talking about Box64, just got the name wrong
Because it can't run 32bit apps and op was asking about Proton
And what exactly prevents him from making another account?
Things I would say a non-dev would find annoying about macOS if you're coming from either Windows or Linux:

1. The very concept of maximising windows is just absent. You get two replacements that are similar but different enough to require relearning, which is the worst kind of similar.
2. Applications keep running despite all visible windows are closed. Theoretically you can make an argument for this paradigm, but I fail to see a situation where you want a program to be invisibly running when no content context is actually present. The macOS paradigm implies that a GUI program can serve some purpose by not having anything interactive present in any windows or tray icons. Which is... impossible in practice.
3. Every now and then Apple introduces very silly additions to the OS that makes your life miserable in an invisible way. The most recent one I can remember is the whole notorisation thing where they sort of check the signature of every program you run with their servers. Normie end users wouldn't know the background of this half baked feature and would only notice that their programs somehow take ages to launch when their internet connection is poor.
4. The desktop macOS is just growing more and more like the mobile iOS. Personally I hate this. Who the fuck wants to talk to Siri in order to find a document? It's just as stupid as Microsoft's push for Cortana on desktop Windows.
>FEX doesn't work on 16k pages. In order to use it on Asahi, a MASSIVE rewrite to how the kernel handles paging would need to be made.

A massive rewrite would be needed to make mixed CPU page sizes possible, but Asahi is planning on making 4k pages work by making it possible to have different page sizes for IOMMUs than the configured CPU page size, which is a lot less work.
Huh I didn‚Äôt know, interesting!
It can't but if we are speaking about Proton there is chance that it won't need in near future. Wine is finishing their PE transition and after that it can be possible to run 32 bit Windows software without 32 bit libraries on host.
Nothing, but being able to steward the Asahi project, run a VTube channel, two Twitter accounts, and develop Asahi under two different aliases with only 24 hours in a day seems extreme
I agree with all of these things except number 2. To understand this, you have to consider how things evolved over the years. There was a long time ago when computers were slow and macOS was not multitasking. So you‚Äôd start up Microsoft Word, wait half a minute and then you were in that application exclusively. There was no desktop anymore. There were no other applications in the background. You‚Äôd write one report, save it, then close it to then start a new document. So closing that document didn‚Äôt mean you were done with the application. You might have been a secretary who spent all day in Microsoft Word. Imagine having to keep loading up Word every time you moved on to a new document. 

Fast forward a few years later and computers had gotten a little faster and macOS was now multitasking. But applications had become significantly more complex. Now it was Photoshop that took half a minute to load. A photographer may have worked in Photoshop so much that they kept it open all the time to save the half a minute load time. So even if they were in another application (e.g. Pagemaker), Photoshop was there in the background when needed. Surely the user shouldn‚Äôt have had to have a document open in Photoshop at all times for this workflow?

Of course computers have become fast enough these days that it‚Äôs rare any application would take half a minute to load. But even 5-10 seconds can be irritating (I‚Äôm looking at you Firefox snap).
I think I heard something about how making 4k pages available would slow down performance. Is it true or am I misremembering?
>I think I heard something about how making 4k pages available would slow down performance. Is it true or am I misremembering.

I don't recall.
Never heard of anyone complaining of cat, even on the slowest systems something else will most likely be the bottleneck, it uses system default blocksize about 128k so it can retrieve a megabyte of data with just 8 syscalls, if there isn't a way to read multiple blocks in a single syscall for the input device.
What about cat's other options? Although even these could be done faster using memory mapping

&#x200B;

 \-A, --show-all           equivalent to -vET

\-b, --number-nonblank    number nonempty output lines, overrides -n -e                       equivalent to -vE -E, 

\--show-ends          display $ at end of each line 

\-n, --number             number all output lines 

\-s, --squeeze-blank      suppress repeated empty output lines 

\-t equivalent to -vT 

\-T, --show-tabs display TAB characters as \^I 

\-u (ignored) 

\-v, --show-nonprinting use \^ and M- notation, except for LFD and TAB --help display this help and exit --version output version information and exit
Terminal speed is another possible bottleneck.

I know this because I daily drive Gentoo and it's a bit of a life hack to compile your software silently, because during compilation GCC spews so much text on the screen that it actually slows down the process a little bit depending on your terminal.

If cat, the kernel and the devices read speed is fast enough the terminal will likely be the next up bottleneck.
Those are all GNU extensions. IMO, entirely unrelated to the (con)cat program.
Somewhat similar, if you‚Äôre writing a long python script with a big loop printing after every iteration can noticeably slow down your script.
I'm pretty sure -n, -v and -s all predate the existence of the GNU project.
Gonna date myself, but got a copy of Redhat in a pc magazine.  Then got a copy of openSuse from another magazine.
Raspberry Pi!
Windows kept crashing and it pissed me off
I'm older than most here: My 7 year old kept getting virus' on my windows machine playing dumb kid games on websites. I had a windows machine because I had needed a new PC and that's what it came with. Prior to that, I had been using OS/2 Warp but IBM had dropped support for non-commercial versions.

About the third time I had to reinstall Windows to clean it, I went looking for something else and found Mandrake KDE Linux. I bought it and installed it from 3.5 floppy disks - about 12 IIRC. That would have been 1998. KDE was version 1.0
Two guys with ugly glasses and beards came into the computer shop to buy a hard drive or something.

They gave me a CD with Slackware 1.0 on it... and also mentioned the mysterious "Perl" language...
When a dude on comp.os.minix wrote about his new hobby os.
My dad forced me to use it since I was 7-8. Learned a huge amount of knowledge about computers because of it.

Love ya dad.
Had to install Linux for a course, but we ended up never using it 
After messing around with my distro I was like "Why would I go back to Windows" and here I am
I started using IBM AIX and SunOS Unix in 1991 in high school.  Started college in 1993 and it was mostly Sun SPARC Solaris, DEC Ultrix / MIPS, HP-UX on PA-RISC machines.  I wanted that myself.  DOS was garbage.  One program at a time.  Just one user. No memory protection.   Windows was horrible too.  Constantly crashing.

A friend a year older told me about Linux and showed it to me on his PC.  He gave me an account on his machine and I would login to his PC from the computer lab just for fun.

I saved up my money from my summer job and got a Pentium 90 in summer 1994.  Within a week of school starting I had partitioned my hard drive and installed Slackware.  

I used DOS for some games but even Doom ran on Linux back then.
I got my first laptop at the age of 13 by doing small work over the summer
I got one with no OS and my cousin told me he'd get me 2 OS's (üôÄ) aka dual boot Windows and Ubuntu 16.04 LTS
I never really cared for that second OS until some day I decided to check it out for some reason. I decided to boot it up because I'm a nosey little shit. I tried to update it but it said that there where broken packages. After some googling I came up with the idea to delete them... After deleting, as far as I remember, there came up a few billion (exaggerated tens) of dependencies... I decided to reinstall... Then I got into the vacuum which can be summed up like this: Ubuntu, get into r/linuxmemes, try Linux mint, no wifi, try pop os, yes wifi, try debian, fuck everything up, retry Windows/Ubuntu but accidentally convert GPT into MBR, unable to dual boot, blame it on Debian, purge windows, try pop os again, try mint again and learn about drivers, download wifi driver with Ethernet, enjoy Linux Mint, get curious again, try installing freebsd, fail numerous times, discover that the necessary driver for my wifi isn't even available on freebsd, give up, try Manjaro, try installing Arch, I made it, still not satisfied, waste 4 months trying to get Windows back into the dual boot only to learn that the partitioning table needs to be GPT and that is only possible with UEFI mode, after some days, get dual booting back, go back to mint, go back to pop os, go back to Ubuntu,  go back to pop os, try trisquel twice, it's fucking unusable for me, distrohop for a bit more, I hate myself, go back to windows, I hate myself more, dual boot Ubuntu, miss space in SSD, delete Ubuntu, unable to merge partitions, hate myself x3, dual boot Windows and pop os, hate myself x‚àû, gonna try to dual boot Windows/Arch with KDE now knowing a lot more about Linux than the first time, gonna try ricing i3 wm

TL;DR in my country (if nowhere else) they say that curiosity killed the cat, thankfully, I am not a cat but my happiness must have been lmao
Around 2015 or 2016, a friend from my uni had a laptop with Ubuntu installed on it. I thought that was pretty cool, so I asked him about it and here I am.
Some day in the 90s i heared about a free alternative to windows... too bad there was no internet to get it and no driver or programms for it.

1999 I tried to install Corel Linux (was an addition of a computer magazine), but it did not work on my machine.

In 2006 I heared ybout Ubuntu, making Linux easy. I tried it, but still wasnt completly satisfied... took some years to get into it, slowly slided to Ubuntu and Mint after that ;)
My City has a project where they take old Laptops and Desktops, clear their Hard drives with DBaN and install Ubuntu on them, so they can give them for no charge to People in need
My gpu in the imac broke
I ended up starting to use Linux mint after I had installed it on my dad's computer. He had the habit of downloading mallware and viruses on his computer.  He had bricked two hard drive cause of this. Installed mint on all of my computers and haven't looked back since. About the only time I use windows anymore is at work.
I had a spare PC and obtained a copy of Red Hat 7.0.  Yes, Red Hat 7.0. 2.2 Kernel. Xfree86. Manual xf86config editing. GNOME complete with the foot. rpm -ivh, oops, missing dependency, go search the internet, download dependency, rpm -ivh, oops, missing a dependency for the depenency, rinse, repeat, it's a wonder I still use Linux. :)
Grew up with Unix, used Linux through university in the early 90s, have been a fan ever since. Redhat and Mandrake were my first. Couldn't go back to any other OS now without significant pain relearning the limitations imposed upon you. Linux = Freedom (to learn, fuck up, then learn some more).
Got opensuse on Linux format (uk magazine). When linux was on its early days
I was into modding Nextels back then. I was looking up mods for the PS2 and found how to install Fedora IIRC. Then I looked up fedora and found an entire new world.
A friend at that time, quite a nerd, had bought a box with Suse Linux 6.x. I borrowed the box from him out of interest. That must have been around 1999. Or 1998? I don't know.
Sometime in the mid to late 90s I got into MUDs- Multi-User Dungeons. Think text based MMOs before MMOs we‚Äôre a thing, and a smaller scale. Also, many MUD, MUSH, etc codebases were open source. 

I tried really hard to get my own server running under Windows 3.11 I think it was? But I was limited, if I recall right it‚Äôs because I could only run 16-bit executables, and these codebases weren‚Äôt really designed to run under Windows anyway- I was using Cygwin and clumsy hacking to get them to work. 

I don‚Äôt remember what my first distro was at all, but I finally got frustrated enough to get one on floppy disks and install it. It‚Äôs been a wild ride ever since.
Roughly 2018, I wanted to try something other than Windows since I hated the bloat and preferred older versions. I'm getting a laptop again, soon, and I won't even touch Windows. Open-source and privacy-friendly >>>>>>.
I initially wanted to make a Hackintosh install because I was simply curious of trying another OS than Windows XP. Turns out it was rather complicated and that Ubuntu Live-CD thing was quite impressive.

What is it you say? Free software? As in you're not just trying to lure me in then lock features away so I pay for the license later? Like, really free, free? I can do whatever I want with it and it exists only because a bunch of people wanted to do something for the common greater good?

Sure Compiz and the cube desktop was fire but open-source software philosophy blew 16 year old me away. Still does 15 years later.
I asked my computer class teacher if there were any other OS besides Windows and DOS, and he mentioned a few things, but the only word i remembered was Linux. A few years later i heard that the Ubuntu free install CD did reach my country, so i ordered it right away and a few months later i had it in my hand. After that it was all history...
OP specified no time period.

Probably in 90s it was a word from one to another and exchanging CDs. Those who didn't have Internet, but had modems - there was FIDOnet almost for free of charge. 

In 2000s every IT magazine tried to post a few articles in a year and of course the  news ribbon. So if you are in the  World of computers somehow - you have a Linux CD in  your collection.

&#x200B;

PS:

Oh no! Read comments below, felt myself oldy-pants-shaking-dust.

Excuse me, but if you didn't hear about Linux since 2005 and onwards - that was your problem why you didn't hear about Linux. üòõüòÉ
I first tried Linux with Ubuntu 8. Bought burnable discs just to try it out. I quickly realized it couldn't play my favorite games, so I switched back to Windows until this year haha. All the annoyances around Windows 11 and Proton being awesome  finally gave me the push I needed. Over the years, though, I've built a few different desktop PCs and almost always used Linux to get them up and running and test the hardware. It's just so much quicker and easier to get going.
Quake 1 test was out for Linux before Windows/DOS.
had an acer netbook that I loved. it ran windows xp untill it died. tried taking it someplace to get it reinstalled, but they wanted over a hundred dollars. ended up getting a new laptop that ran windows 8 and it was awful. after venting to some furries on irc I looked at ubuntu and then into mint cinnamon and have been with it ever since
ChromeOS out of all things. Now I use Arch on a Corebooted sunlighted Chromebook.
I discovered Linux via the Warez scene on Usenet, back around 2000. I first discovered BeOS that way, and became interested in alternate operating systems. This was back in the days of dial-up, and the one one or two distros I downloaded did not work or boot. Probably PEBKAC, I am sure.

I was in a bookstore browsing magazines, and discovered an issue of Maximum PC (or Maximum Linux?) that had a couple of Linux CDs. One had a couple of distros I can't recall, which of course did not load or install.

The other was a Mandrake 7.0 that DID boot and install just fine.  I never went back to Windows fully after that, with my second-ever PC, which was my first home-built system,  was Linux and BeOS -only.

So, in a strange way, my searching for free Photoshop compatible plugins for my crappy Adobe PhotoDeluxe software led me to to Linux.
Vista.
I followed alt.os.linux (IIRC), I'd been primed with Minix(ran on i386).  I'd used Unix and knew what I wanted....  This would have been in c. 1991.  Jumped to linux at home in the following year and then work in the next 6-12 mo.
First encountered UNIX (sunos) at uni in the early/mid 90s and had an epiphany, "so the shell expands glob patterns, not the programs that get invoked - that's absolutely the right way to do it, and the opposite of DOS! I love it!". Spent weeks absorbing manual pages. Started writing little utility scripts. Played pranks on friends with write(1) (was big multi-user system).  

Amateur radio housemate installed slackware on a PC for me.  Faltered a bit, went back to Win9x for a couple of years. Finally installed SuSE from a magazine cover disk and stopped booting Windows in the late 90s.  

There have been times when I have cursed it (xf86config anyone?), but overall I feel so much relief at being out of the Windows ecosystem. Dabbled with OSX when the iBooks first came out (because unix with nice GUI), but it didn't stick because of the relentlessly expensive upgrade cycle.

I'd do it all again.
I started very late, because from my school days I was heavily trained on Windows. I have used windows for 15 years for all kinds of tasks, from using paint in standard sixth to booting Kali linux to learn ethical hacking. The first time I encountered Linux was when I joined a research lab in PhD. All the systems were atleast five years behind with so many issues with update. I had problem even installing chrome browser and hence I figured that Linux is way to difficult(it is not actually, it was not maintained correctly). Didn't touch it for another six months until windows decided to create huge issues for me. Extremely slow and buggy updates irritated me so much so that, I impulsively dual booted it. Used both of them for sometime until one day, I decided to take the leap of faith and removed everything and booted Pop OS. No more windows as a crutch. 

Turns out I was pretty good with Linux, bash and sysAdmin stuffs. Eventually I kind of fixed the issues with my lab servers. I still use windows 11 sometimes to stay updated with what is going on but will never ever go back there. It was a torture. 

p.s : For someone who might think Linux is tough from my experience, let me tell you it is not. Its just that I was unfortunate enough to get a badly maintained old system. Newer distros are way way more friendlier than you can think.
i started using it in 1995, i‚Äôd heard about unix from reading the cuckoo‚Äôs egg and byte magazine. then when logged into a bbs, took some geek quiz that mentioned ‚Äúa free x86 unix clone called linux‚Äù. i investigated more, found walnut creek that sold slackware linux and freebsd on cd, ordered it, and tried them both. stuck with slackware because i couldn‚Äôt get x11 to work in freebsd on my crappy packard bell 486.

i was a sophomore in high school then, needless to say i was not one of the popular kids!
Me a 13 year old computer nerd got a copy of Redhat 4 or 5 as a birthday gift from my uncle.......as a box set
Brazilian public education system used to use a distro made by the Ministry of Education called Linux Educacional on yellow PCs. When I used it we were on version 3.0, with KDE 4 and based off of Ubuntu 8.something.

I loved using it at school and always thought "Why doesn't XP look this good?" I kept using Windows XP and then 7 at home, but after having problems with 10 I gave Ubuntu 16.04 a shot. 

I was blown away, looked so nice, was so much faster and most of my games worked (I was basically only playing Valve stuff and emulators back then). Can't go back to Windows now.

I have used Windows a lot since then, at work, on my dad's laptop, etc and I don't hate it at all, I actually really liked Windows 11 - but man, I can't imagine myself using something other than Fedora now.
I found out about it long before using it. I tried to install some type of Suse on a desktop but the NIC didn't work and I gave up.

I started using it properly when Windows 8 came out in 2012. I tried it and HATED it. This was before MS rolled back with 8.1 and I thought if this is the way they want to go, I am out.

At this point hardware support in Linux seemed to have crossed a threshold where it 'just worked' in 99.5% of cases. This would have been Mint 13.
Read about it in some magazine back in 1996 (I was 11yo), in 1997 I managed to get in my hands a copy  of SuSE Linux, tinkered with it, didn't understand crap, went back to Windows, next year a copy of Corel Linux came in a magazine, gave it another try, destroyed my hard drive in the process (my beautiful 2GB drive!) by 2002 I understood more concepts and started distro hopping like crazy, 2004 I installed Gentoo head on, then moved to Fedora, then to Arch Linux (since 2010 or so) and that's about it.
For me, it was only about two years ago. I was looking to buy a Chromebook, not really sure why in hindsight, but I wanted to try one out first. I found online, an OS you could boot off a USB as a live session to try, that was based on chromium. It was cloudready, by Neverware (from what I understood Google bought them out and then made Chrome Flex?? Something like this).

Anywho, I enjoyed the little experiment, and started to see just how little I actually needed windows specifically, so I browsed around online for other OS that could be tested through USB...and I found everything Linux. Started off with Ubuntu, then Mint, then Pop Os, then all the Ubuntu flavours before landing on Peppermint.
Had an XP drive completely die out a day after I bought a laptop.  Had to buy a new drive and didn't have the spare loot for a Windows install.  Booted up that Ubuntu disk I'd been eyeing for a while.
Bought a PC with Windows Vista in 2007 and it was stuttering right out of the box;

it's still in daily use fifteen years later with Linux Mint.
I'd heard of Linux in high school, but it was described as this arcane thing that only magicians and hackers used.  A few years later, I got my first college laptop, and it came with Windows Vista.  It took less that two weeks to start looking for alternatives.  I decided to try an Ubuntu live CD, and a few days later I installed it.  It took about 8 mouse clicks. I felt like a god.  

That was 14 years ago.  Basically everything in my house runs some flavor of Linux now.  I don't claim that Linux is objectively better or anything, but it's like slipping into an old pair of shoes at this point.  Familiar, comfortable, predictable, and easy.
I heard about it in a programming tutorial, but I was skeptical about it then windows did what it does best i.e. I had a ubuntu live boot usb , installed Ubuntu 20.04 as my first distro, fell in love with Linux and Never looked back.
Was going through a rough phase, laptop broke down. Tried using dusted old PC from school time but it was too old to run latest windows and was sluggish. Decided to try Ubuntu 20.04 (knew about it from college times) and it works like a charm.
Deviantart. I wanted to use macOS but as a teenager I didn't have the financial capabilities to do so and saw all kinds of cool desktop screenshots on Deviantart that looked like macOS rip-off‚Äôs. It turned out to be Ubuntu so I started playing with that and have been using Linux since then alongside macOS and Windows :)
I asked a question on the Windows forum
Well, I first found out around 2007. Primarily I had a PuppyLinux USB for rescuing computers. I tried to switch to Ubuntu but it was pretty horrid at the time.

My next run in was around 2016. I went on an "optimization" spree with my underpowered (6th gen i3 U processor, 4 gigs of RAM) laptop and decided to try out Linux because I couldn't get my RAM usage on Windows under 1 gig (yeah, I was kind of clueless about caching). I think I tried Linux Mint, Ubuntu, and Fedora. The touchpad didn't work properly on half of them, gestures didn't work on any of them and had to be manually enabled with Touchegg. Still, I stuck with, I think, Linux Mint MATE for a while because wow 500 megs of RAM usage must mean it's light, right until I reinstalled Windows and, what do you know, the performance on Windows 10 was seemingly *better* than on Linux Mint, probably because I didn't even know that browsers on Linux didn't have hardware accel, which was why my browsing experience was so bad. Battery life was certainly *noticeably* better on Win 10.

My current run is going much better and I'm sticking to Fedora 36 on both my ultrabook and main PC, and, to be clear, I am only sticking to it because Windows are *most likely* cutting Windows Update support to my country and I would rather sacrifice some creature comforts and gaming performance then be stuck with an OS I cannot update without a VPN. Still, it is undeniable that Linux desktop has come a long way. The current experience is almost acceptable for wider adoption. Wayland was perhaps the biggest wow factor for me, as it finally meant that the desktop experience was actually smooth, but unfortunately I cannot use it thanks to a couple of GNOME limitations which make my workflow an absolute pain in the ass with it.
Started using Redhat at the end of the 90s because it was a college course we had to go through... Setting up servers and so forth.. Ended up using opensuse after a while because  i liked yast.. Slott og cd-copying at that timeüòÄ
Well I was set on going into CS so it would eventually come the day for me to switch.

But if I'm honest, I just did it because a friend of mine did it first and a) it looked like a nice challenge and b) I really liked the Pacman update progress bar aesthetics on the terminal x)

This was 2008 so I started with Ubuntu to ramp up the difficulty, and only made the transition to arch a couple years later.
I found out about it by reading comp.os.linux.* almost thirty years ago. First installed and switched to it completely (from OS/2) in 2007.
It was GNU/Linux Mint(mate) for me about 7 years ago, but very soon after that I switched to Debian stable. After a couple of years of using it I switched to Trisquel with a libre kernel and I am still using it as my daily driver. I like the thing that I am able to use it on my job and that the kernel is deblobbed.
Many years ago now so i don't remember specifics. Was having an issue with my Windows machine. Did some research online and i kept seeing Knoppix over and over again. Boot into Knoppix and try xyz. It worked and i was like what is Knoppix? So i did more reading. Turns out Windows and Mac werent the only operating systems around.
About eight years ago I was interested in getting a job at a local web hosting company, saw that they used "linux". I was intrigued, decided to give it a try, and downloaded Linux Mint.

It was a good thing I did, since around that time my laptop suddenly had severe heat issues and Windows couldn't run with overheating and shutting down. Linux was apparently lightweight enough that it didn't cause the same problems.

I later fixed the heat issues with a good cleaning, and there were things I couldn't figure out how to do that I needed so I switched back to Windows. It'd be six more years and three more attempts to switch before I finally dove in and switched to Linux for good. Boy am I loving it though.
I have worked in IT since the early 70s. I used Multics in the late 70s. It is the precursor to Linux. I have been aware of Linux since it was created.  much later, in the 200s, I was recovering from a virus on my home Windows Pc. I was struggling for 3 days to put things back together.  My daughter suggested I install Ubuntu. It took minutes and I have been happy with it ever since.
Mandrake Linux CDs in a magazine.
Raspberry Pi 3. I was blown away by the versatility of the little device.

The UI (Pixel DE based on LXDE iirc) blew me away and I remember thinking it was so good for the 1GB onboard RAM.

People often ask the secret to pushing Linux mainstream. My answer would be Raspberry Pi. Yes, you get only 1 kind of people then (those who wanna mess around with electronics, are computer science enthusiast etc.) but imo it is the most effective way.
Through an Ex. Installed it couldn't do anything, went back to Windows, but won't lie it's been a lifesaver for the past couple of weeks.
Put new video card in Win95 (or 98? maybe) computer, drivers were shite, constant blue screens, sometimes at boot (but not every time).

Read an article about Caldera Linux and how easy it was to install and use. (This was well before it became part of SCO.) Tried it out. Dual booted for a while, mostly for gaming. Within a year or so was Linux only. Did a lot of distro hopping just to check out new things.

BTW, while I was dual booting (so around 1999 / 2000), Quake III Arena had better performance on Linux than it did on MS Windows on my hardware. Just saying.
I've used Linux for a good 11 years. I remember searching for something different to Windows because I was bored. And I stumbled upon Ubuntu. I had a very low spec Pc at the time: 2GB RAM, 128Gb HDD. I installed it and my computer went `whoosh!` like a rocket. I was impressed. After that I got a bit bored of Ubuntu (3 years later) so I searched around and found Arch. Tried that and I was not bored for the next 6 years.


After that I've used a lot of OSes like openSUSE, Gentoo and whatnot. I've just settled on using NixOS. It's a great distro in my opinion. 

I like rolling release distros a lot, that's why I used a lot of rolling release distros in my Linux days. Even NixOS is rolling release.
I found an old copy of Linux for Dummies in a remainders bookshop. It had 2 CDs on the front with what claimed to be a free operating system. I ended up installing it on my old computer when I bought a new one the next year. I became a full-time Linux user around two years later.
College.

The computer lab in college only had Ubuntu. I would use putty to log into the lab computers to work on assignments from home. It was how the instructors enforced using same compiler and versions of everything.

It wasn't until my first job as a software engineer where I started actually using it on my personal computers. Started with Ubuntu for a few years, then Pop when snaps became mandatory, then mint because my new job uses that. All my servers run Debian though.
I found out it existed when I read a newspaper article around 1995 about Red Hat.

The first time I actually tried to install Linux was in 1999, but it wasn't until around 2008 when I first did anything useful with it, installing Puppy on very old hardware.
2002 running xandros as the file server for my small business.
Class in 1993.  Student mentioned Linux.  Prof mentioned OS/2 (why hasn't this been open sourced?).  Downloaded some boot media (on floppy) and booted my Windows 3.1 box.  Couldn't figure anything.  Ordered the Slackware CD.  Had to get a job were I had to work on UNIX a bit before I started to officially switch, which was 1999.
In 1994/1995, I was a kid running DOS, and occasionally starting up Windows 3.11. I spent a lot of time on local BBS's, and started to hear some details of the eventually to be released "Windows 95" in the forums and via chat. Apparently it would replace DOS and the system was going to boot straight into Windows... Yikes. But I'd also seen something called "Mini Linux" (lives in a FAT directory, 'boots' from DOS) on the board, which I played around with for a bit before taking the plunge and downloading the full set of Slackware disk images and copying them to a giant stack of floppies. For the next few years, I had systems set up to dual boot just in case I needed DOS/Windows for something, but had quit bothering with dual boot that never actually got used by around 2000.
I found out linux existed when one of my cousins came over. When she turned on her laptop GRUB came ip and I asked what that was for. Fast forward a couple years and I now daily drive Void.
Windows bluescreened left and right. And sometimes it just corrupted the whole filesystem. I had really bad luck with it. So I tried Linux (first ubuntu in a vm and then zorin os) and it worked flawlessly. Now I use arch btw.
I‚Äôve always kind of knew about Linux. However, it wasn‚Äôt until my boyfriend explained to me how a laptop like mine (a 2nd gen AMD laptop) could run far superior on Linux that I actually looked into it.
First was when me and my friend started watching Mr. Robot, so we installed Kali Linux. It was fun, but aside from "wow we are le epic h4ck3rs" we did nothing actually.

Years later in university one of my profs (a very kind and helpful man, but he liked to tease us and joke with us, nothing negative or offending)  stood next to me in lab, looked at my laptop, and said "Wow, thatRoland, really, you are using Windows? Of all people? What a shame." So I decided that yeah, I'm gonna show it to him, I've installed Ubuntu to dualboot. In next lab, he saw that I used Linux, stood next to me, and said: "Wow, you are really using Ubuntu? That's too easy." In the end, I actually started liking it, so I stayed on Ubuntu for a while.
 I heard some things from friends on how cool linux was.  I was 14ish my step dad got me a debian CD.  This was was around 93 / 94 or so.  After numerous attempts I finally got it installed and was greeted with a command prompt.   I was like wtf is so cool about this. But I've been using it since, and I've actually been working at SUSE Linux for nearly 12 years.    I've been windows free for 20 years.
installed slackware in 1993
That creepy early 2000s IBM commercial with people lecturing a frightened-looking blonde child in a blank white room while a voiceover goes on about how awesome he is for just existing. https://www.youtube.com/watch?v=x7ozaFbqg00
I was in a mental health facility for around 60 days and was able to use my mom's chromebook which was lent to me for my school work. In the free time I had I wanted to try and download world of warcraft on that laptop to play as well and went down a rabbit hole of trying to learn how to use ubunut to download wine etc etc. The rest is history
Apologies in advance if I err. This was more than half my lifetime ago! Memory a little sketchy - especially during my early 30's working in the city (too much good stuff = blank spots).

The year was 1993 - long hot summers with little thought of global warming. I was 22 (so nearly 30 years ago!) and working for Logitech in the UK.

We were visited by the global head of tech support. He didn't have an RS-232 cable for the modem, so fashioned one from a few (maybe 3 or 4) paperclips, connecting his DB9 on his laptop to the DB25 on the USRobotics 9600 modem. 

I was fascinated by his screen - just a login: followed by password: prompt. A few commands and I watched as he issued AT commands directly to the modem. 

My access to the internet was at that time very limited. Connection was via CIX and latterly Compuserve, before (some years later) having a demon account - yes, I was larcombe.demon.co.uk :-) 

Anyway, discussions of Linus ensued. A video of Richard Stallman with his long hair, beard and sandals, just dancing with himself a few inches from (and facing) a brick wall... that was it - my journey had begun.

I ordered the disks (and I think they might have been FREE) to be delivered to the Logitech office. I could have been Slackware (but I am not 100% sure), and they were 3.5" 720Mb SDs (not even the 1.44 HDs!).

The first dozen or so attempts were painful - but I was using a spare tech support PC, so didn't really care. My POS Windows 3.1 machine was still available for me to do my day job.

When I first got my linux terminal to connect via PPP to demon I was in heaven. ELM for mail. ftp directly from my terminal - no staging area to download what I had already downloaded. Linux made me (my machine) part of the Internet - instead of just a consumer. 

Since then, I have been linux all the way!
twitch streamer was  using ubuntu to play this game and i had a windows pc that i was about to toss, installed ubuntu which quite literally breathed new life into what i thought was trash, changed everything i thought about computers and now im in IT
Started just last year. My computer had Windows on it, but the performance was lacking, even though I had pretty modern hardware. I thought long and hard about using Linux instead; I already had experience with it in my extracurriculars, but I didn't know if I was ready to use it as a daily driver. Finally bit the bullet one day and installed a Linux distro (started with MX Linux) and everything worked like a charm. There were a few things like certain Steam games or Discord screen share with audio not working, but they were trade offs I was willing to live with.

Fast forward to today, and I've hopped to many distros since, like Arch, Pop!_OS, Fedora, etc. and am now on openSUSE. I've never looked back.
About 2 years ago, I started having problems with my 2018 Dell laptop, which ran windows 10. I was a college student and didn't have time for my computer to fail, so I took a 2009 Sony Vaio laptop and put Mint XFCE on it to see if the problems I had with it would be fixed. Surprisingly, the Sony worked better than the Dell, however, I was limited on the hardware in the Sony. Still use both depending on what I'm doing.
when I was about 11-12, I used my 8yr (it was 8yr old that time) old Lenovo laptop(hand me down from parents) and it was average performance, so I booted up Linux after discovering and learning basically everything in about a year
I remember it vividly.  It was about 1996, I was thumbing through my Popular Science and saw two pics of Windows alternative desktop environments and I had to have them.  I already hated Windows back then.  So, I was on a quest.  I joined a local Linux Users Group.  We would meet up and nerd out.  I was a practicing Chiropractor at the time and would compile Linux kernels between patients.  Ultimately, it lead me to change changing careers.  I went back to school and got a CS degree.  This is 100% due to Kim Kommando's article in that Popular Science issue, which I still have.  Kim Kommando literally changed my life with that article.
I've only been using Linux for about 3 years now, I was building a gaming PC and didn't want to pay for Windows, and decided to try Linux instead, and oh my gosh I was amazed by how nice it is, I quickly converted to open source and have never once thought about going back
The field I entered for grad school leads to a lot of heavy number crunching or data science, which often leads to super computers, which mostly use Linux. So, it was a requirement that we installed Linux on our machines. I installed Linux Mint on my laptop, and then proceeded to almost never use it for several months, opting to use WSL instead. Eventually, I started seeing some of the things you could do with Linux on the desktop side via YouTube and gave it more of a chance. Wound up making the switch over to using it as my main OS after distro hopping from Mint to Pop to Manjaro and then to Arch. A few years later, and I honestly don't see myself going back.
A former housemate of mine ran BSD on her computer and suggested I give it a try. She was really big into privacy and having full control over her own computer.
My first was an old variation of Knoppix. I think that it was 3.4 maybe  and I was hooked.
Found out from a good friend whose been using Linux for 10+ years. Switched shortly after Microsoft started pushing Windows 11, haven't looked back since.
1997 I started studying computer science, taking my first programming class. Programming assignments were all in C++ on Sun workstations, which meant spending an ungodly amount of time in the lab. I wanted to work on my assignments at home, but didn't have a dedicated phone line, so telnet sessions were being constantly interrupted. I searched USENET for "unix on pc" and found Linux. I downloaded slackware onto fourteen 3.5 inch floppy disks and went home to install it. After half a dozen attempts I got it installed. Working from home was luxurious. I still had to spend a little time in the lab making it work on the Sun workstations. It took me about a month to figure out how to make slackware dial in to the campus network, but once I could do that I could upload my code to a Sun server and quickly test/tweak it to make sure it worked there, and submit it to my professor.
I had a teacher in high school that was into Linux and took the chance to talk about it in class a few times. It was a computer basics class but she did a good job of explaining the bigger-picture stuff behind the basics. I had already heard about Linux before, but we got to talking about it one day and I got really interested. I think I went home and installed Slackware. Talk about jumping into the deep end lol. I eventually came to like it a lot, learned about things like slapt-get (is that still a thing?) and how to compile stuff I found.

I mostly use Linux for pragmatic instead of idealogical reasons, though with how Windows is getting these days I'm starting to appreciate the idealogical differences more and more.
comp.os.minix
I took the privacy pill.
was using os/2 v3. saw all this stuff about win95 coming. i tried it, hated it. os/2 v4 was still another year away, and looked around at what things existed. figured out unix was a thing that expanded more than just huge servers, and linux was there.

so that was 1994-95 or so. started with slackware (download and write to floppies), but then found out you could buy mandrake for 10 bucks on a cd at bestbuy which made life so much easier.

i started actually working in IT in 1999. quickly focused on linux. nowadays I do 90% OpenShift/kubernetes.
My dad was a programmer in the 80's and we had dumb terminal in the house so he could do the occasional remote work. He worked primarily with VAX systems running VMS , which I suppose is Unix adjacent. Anyways, he would let me sit on his lap while he worked sometimes.

At one point in the late 90's or maybe it was early 2000's, my daily driver was a Sun Microsystems sparc 10 that I got for like 40 bucks at a computer show.

Anyways, I'm a big old nerd.
First time I learned about Linux was when I was young teenager (maybe 12, 14) from my chiropractor doctor who looked like old grandpa who was evaluating my spine and he shared a fascinating rant story about how Bill Gates is a thief, Microsoft monopoly, how a monkey can easily without any effort go through Windows installation and urged me to learn Linux and added that Scandinavian countries in government agencies use Linux Desktop not Windows.
I was 5 years old and my dad made me play with super tux kart. Now i am here 13 years old and still using it. :)
was interested in the dark web so i flashed linux mint to a USB stick abd installed Tor on it. booted it on school laptops when i had free time (also made some chiptunes on there as well with openmpt). customization was nice so i kept using it.

i then proceeded to wipe a windows partition while trying to install a distro onto another usb stick, once i realised the win partition was replaced wit hgrub i just put the distro on the thing. been using linux ever since (i'm on arch now)
Dual booting an old iMac with gentoo. After a few months, I just removed the HFS+ bullshit I no longer needed.
Use OSX for a while, got used to the terminal, zsh and the like. Got addicted, ditched windows and installed Debian on a laptop.

4 years later, I have 4 linux machines at home, a personal infra running nginx to reverse proxy a ton of stuff, and I ain't looking back. Only using win10 on the desktop gaming rig.
I can't remember when I first heard of Linux, I imagine a very long time ago but I started actually using Linux in 2013 I think. I was unimpressed with the performance of my HP Pavilion so I decided to dual boot it. Soon enough I never booted into Windows so I just wiped it and stuck with Linux, hopping through a few distros.
I wanted  to learn programming, and I discover Linux, because of that, and I decided to try, and after that I never looked at windows again.
I had a friend at school who knew about Linux. One day I told him how slow Windows is and told me about cloudready. I was confused about it and later on I discovered Ubuntu. He also knew what Ubuntu was and I began using Linux from that point forward.
Free cd from pc magazine when internet so slow.
In '17 I inherited my father-in-law's lap top.  The hard drive was fried, so I had to buy a new one.  Since it was going to have no OS, someone recommended installing Chrome or Linux onto it.

Being Google-averse, (I use it only when I ***absolutely, absolutely*** have no other alternative,) I looked into Linux.  As, they say, down the rabbit hole I went.  Found a really nice article that kinda-sorta demystified the installation process and made some solid recommendations for non-technomancers like me.  What really sold me was the overwhelming amount of choices I had.

Since it was an older machine, I ended up installing Peppermint (which was ok, imo. A good place to start, at least for me.)  But since then, I've got a new(er) old laptop, and have tried: Bodhi, Mint (installed on old desktop on a SSD I bought), Zorin, Lite, Pop!, elementary, and deepin...which I liken to that captivating face across the room that you cannot take your eyes off of. (Still like to download and try others every now and then.)

Currently using Ubuntu.  I like it a lot.
My story is fun: I had a cousin (he passed away about 10 years or so ago now) who would take contract jobs with the Army in the early 2000s in war zones to deploy computers for civilian use. The OS of choice? Red Hat. 

He knew I was into computers but at the time I was studying to be a Windows admin while also learning digital photography/videography on Mac OS X. But man, that expose to Red Hat at that time. I got a \*nix-based system at zero cost (because this was pre-RHEL) and all of this software and I didn't need to go on Kazaa to get it?!?!?! I was blown away.
When I was 15 or something I built a fli4l-router for our home internet. Routers were expensive back then, and my dad had an old 486 and a few ISA-network cards. No harddisk, the machine booted from a 3.5" Floppy, that contained Linux and everything. I didn't really understand much but I got it working. I kept using windows for many years and tried Desktop Linux a few times out of curiosity, but there always were issues, until ten years ago or something I just switched and I'm not going back. I still have a Windows computer for games, but I don't use it that often.
When I was 12 my best friend's dad had a computer running Ubuntu 9.10. I was given an old computer by a family member later that year - my first computer all to myself that I could do anything I wanted with - and after getting bored tinkering with Windows 98 and XP, I downloaded Ubuntu 10.04 and began the journey of teaching myself Linux. I still primarily used Windows for years after that point but I distrohopped and ran Linux in VMs and dual booted it and ran it on my server and took a Linux class in high school and so on until I eventually relegated Windows entirely to VMs.
My big sister, who isn't a computer expert, asked me about it in the early/mid 2000s because she heard it was for advanced (read: cool) people. How she heard of it, I have no idea. I got my start on Fedora Core 5 by burning the .iso to a CD, probably by using the program Alcohol 120% or maybe ImgBurn. I mostly used Linux in an on/off capacity on laptops until 2019, when I finally switched everything permanently. It has had some annoyances, but nothing is more annoying than running a proprietary operating system.
Originally the ifconfig command got me interested into Linux. I was preparing a presentation on networking devices and wanted to get a photo of it running on Linux. So I booted ubuntu, got a photo of the command in action, and then ultimately stayed on it because I liked it that much. 


I'm a pretty new user btw, started in Jan of this year.
after watching a youtube test quiz about "how tech savvy you are" one of the questions was: "have you ever used linux?"
For me, I discovered it when I tried to convert my laptop into a hackintosh. I failed cuz i have an amd iGPU, so mac os wouldn't run on it. After that I discovered solus. I used solus for about a year in 2020. I switched to arch linux in 2021
my dad bought a laptop in 2013 if im not mistaken, dell vostro 5460... it came with ubuntu 14, i didn't even know its different OS until i need photosop and corel and can't install it, i switch to windows 8.1 6 month later
Got tired of Windows ME crap. Walking through CompUSA and saw SuSE.
Someordinarygamer is the one who convinced me to start using Linux but I know about it way before him though I can't remember where from. Chose it for privacy reasons and because windows kept giving me audio issues
98/99 in Community college.
Slackware.
All I can say is we've come a long way.
First time I heard about it was when windows on our family PC bricked itself and my dad had Ubuntu on a flash drive and we used it for a couple day till he fixed the windows install. Across the years I've found out that servers run on Linux, I became interested in programming, made a discord bot, didn't want to pay $40 a month for a Windows server so I dual booted Ubuntu, ported it from .NET Framework to Mono and I subconsciously was enjoying Linux way more so I stuck with Linux
Raspberry pi, but I really only used retropie on that. I really got into it when mental outlaw was recommended to me on YouTube, and I've been Foss and privacy focused since
My brother made me dual boot fedora with cinnamon and I got hooked.
About 14 years ago, I was taking my high school electrical engineering class. Loved that class more than anything else I was into at the time, built all sorts of crazy stuff. Wanted to kick it up a notch, and my teacher knew this. He recommended I get into computer stuff, and almost like a shady deal, he pulled me into his office and showed me the goods. He rebooted his computer and through grub, launched Ubuntu instead of windows. I was hooked. After a few weeks, we played with all sorts of Ubuntu based distributions and I got a real good idea of what I was working with. My first self installed distro was Ubuntu 8.10. I‚Äôve been running Linux as my daily driver on almost every computer I‚Äôve owned since. My main pc dual boots windows 10 and Garuda, my laptop runs Linux exclusively. Only reason windows is still around is for when I want to game. My old pc at the time was dual booted windows ME and Ubuntu 8.10 and I spent way more time in linux than I did in windows. I‚Äôm by no stretch a genius when it comes to Linux, but I know how to make it do what I want and I know how to learn/find how to make it do more. Wasn‚Äôt until a few years ago that I learned about ricing and tiling window managers and stuff. Finally made my home in Garuda xfce with bspwm as my window manager. One of these years I‚Äôll do a traditional arch install, but I‚Äôm far too lazy for that now
Had to use a particular software framework for a project in a university club. Started by dual booting Ubuntu, later switched to arch Linux
After goofing off for the better part of the 80's chasing the sound I decided to buckle down and finally complete my bachelors degree. I actually decided to switch majors to computer science. It was 1989 and I came across an old edition of the Communications of the ACM from 1986 in one of the CS labs I was hanging out in between classes and I picked it up and started flipping through it and came across [Jon Bentley](http://en.wikipedia.org/wiki/Jon_Bentley)'s column called ‚ÄúProgramming Pearls‚Äù where he ask Donald Knuth to write a program using the literate programming style that Knuth has been working on to read a file of text, determine the *n* most frequently used words, and print out a sorted list of those words along with their frequencies.He also asked [Doug Mcllroy](https://en.wikipedia.org/wiki/Douglas_McIlroy) to critique it. Knuth wrote his program in WEB (his literate programming system) and was fairly long and included a custom data structure built specifically for this problem. Doug gave his critique (mostly complimentary) but then added his own solution:

tr -cs A-Za-z '\\n' | tr A-Z a-z | sort | uniq -c | sort -rn | sed ${1}q 

I had to know how this worked and who Doug Mcllroy was (I knew about [Ken Thompson](https://en.wikipedia.org/wiki/Ken_Thompson) and [Dennis Richie](https://en.wikipedia.org/wiki/Dennis_Ritchie) but why had I not heard about Doug? I soon found out that McIlroy contributed programs for [Multics](https://en.wikipedia.org/wiki/Multics) and [Unix](https://en.wikipedia.org/wiki/Unix) operating systems (such as [*diff*](https://en.wikipedia.org/wiki/Diff), [*echo*](https://en.wikipedia.org/wiki/Echo_(command)), [*tr*](https://en.wikipedia.org/wiki/Tr_(Unix)), [*join*](https://en.wikipedia.org/wiki/Join_(Unix_utility)) and *look)* but most importantly, he introduced the idea of Unix pipes. This is at the heart of the Unix Philosophy and the beginning of my love affair with Unix (first with the [VAX 6000](https://en.wikipedia.org/wiki/VAX_6000) running BSD) and then Linux in the mid 90s becoming my main desktop OS in the late 90s settling on Debian (which was my OS of choice till a few years ago when I switched to Arch.) Changed my life forever.

I discovered Linux because I subscribed to comp.os.minix and started hearing about this Unix like OS for the 386. Took me till 95 to finally get a usable OS With X-Windows working. I tried getting FreeBSD installed with no luck so Linux it is. I later tried again with FreeBSD in 97 but found I missed all the superior GNU versions of the command line apps so I soon returned to Linux with Debian in 98.
Well guess who watched Mr Robot at 12 years old!
Everyone was using Linux in college
When I managed to brick my Windows XP Install on the Pentium 4 Machine I Had as a 10 year old and didn't have the license to reinstall Windows I looked for alternatives and found Debian. 

Now I'm 20 and I've been using Linux on and Off for 8 years before finally ditching Microsoft completely 2 years ago.
1. Found out about Linux when playing Counter-Strike circa 2002. You would browse server lists and would see tux icon for Linux servers.
2. Learned more about it from an uncle circa 2005 when he helped me over the phone force an optical drive to eject when the button wasn't working (paper-clip) and then went on to rant about proprietary this and that, got me to switch to Mozilla Firefox.
3. Started working in web development in 2008 and began interacting with shared Linux hosting.
4. By 2011 I began managing a "Rackspace Cloud" VPS (migrated to Linode in 2014).
5. As a macOS user experimented with Ubuntu dual-boots on 2007-2011 MacBook Pros, a 2011 iMac, and a 2010 Mac Pro. Never used Ubuntu for anything productive though.
6. Circa 2019 I started playing with an SBC (rock64 before the rPi 4) for creating an offsite backup server running Ubuntu. Soon after, I learned about piHole and set one of those up for my home on a second rock64.  
7. As a more seasoned software engineer and FOSS user with an interest in leveling up and a discontent with Apple hardware and culture, I watched this [YouTube video](https://www.youtube.com/watch?v=5aJ9U5t9oD4) and soon after purchased an HP Z840 (to replace a 2010 Mac Pro) and installed Pop!\_OS on it. I ran my prior macOS install as a passthrough VM during a migration period.
8. By 2021 I had purchased a 14" System76 Galago Pro to replace my 15" 2015 MacBook Pro and been running Pop!\_OS daily since on both laptop and desktop.
At Uni, Slackware to run a Quakeworld server.  
We used Minix in the labs and had access to Unix servers.
My first foray was Xenix on 5 1/4 inch floppies (3) for the 286.  Next I ftp'ed BSD onto 3 1/2 inch floppies. Played with SCO Unix on my Compaq 386 installed from a 150MB tape cartridge. Next Red Hat Linux 6,7 and 8.  Daily driver was OS/2 until Fedora Core then openSUSE.  Been on Fedora since 20 I think.

Edit:
Maybe been on Fedora since 15.
A neighbor was tired and pissed from helping me with my PC and gave me a live CD for Pardus v1.0 and then installation CDs for Ubuntu 4 or 5 I don‚Äôt remember. It was around 15-17 years ago.
There was a lot of discussion about it back in 2000. The windows killer and stuff like that. :p
From what I remember, Ubuntu just showed up in the results of a Google search I did. I don't remember what it was I searched, but I was intrigued by the idea of a free operating system. So I started with Ubuntu 10.10 as my first distro. I'm still nostalgic for that free culture song they included (I think it was called Swansong?).
Guys from a french open source foundation came to my high school to make a presentation.
They gave us ubuntu CDs, I spent the night installing and tweaking, never came back ! It was something like 13 years ago.
i dont remember, really. all i remember is trying to install opensuse on my ancient pc when i was like 13. didnt get drivers working because i didnt know anything about computers, so i didnt touch linux again until i was like 20.
2011 Linux Mint 10, bought the DVD off eBay for ¬£1. Living in rural France at that time and we only had dial-up, so it was to too big to download in a reasonable time frame. Did nothing with it for a few months, until Windows Vista, I had at the time, croaked it's last breath and the PC needed a new OS. 

Tried Mint and loved it. Put it on all the PC's and laptop we had. Been using it ever since, updating to new versions as they came out.
Saw compiz effects on school and wanted to replicate it in my PC, broke my windows install and started using linux more and more until Windows had no reason to be on the disk, at some point I switched to Manjaro and uninstalled windows on the process, that was the last time I used the os named after a wall hole
I think when was scrambling on internet searching about something idk I don't remember that's how I found about linux
Literally the same path. My laptop was aging and Windows bogged it down. I had known of Linux, but never used it. Installed Mint on it and it worked great. So I put it an old desktop which revitalized that. I now have it on a newer desktop and it works great. I do have a newer laptop with Win 11, but I keep my desktop on Mint.
During Windows 95 times, I heard about a new operating system that never crashes and where no viruses exists and that is unhackable. I even somehow got it installed, but could not get X to work and with no internet to look up things, the joy was very short lived and I went back to Windows 95 quickly. However a few games even worked without X, or as I called it, worked on DOS-Linux compared to most of them that required Windows-Linux.
I have heard for a long time about some misterous Linux. Then went to a party of geeks. Some of them brought desktops with them. Our whole group was playing with a dark screen and green letters all night long. That was mid 90‚Äôs
I used to read and sometimes even buy computer magazines.  Find out about 'open source' and Linux that way. My first Linux install was a Redhat version, very early.  I once bought also a Mandrake Linux in a retail store. Cd rom and  book. The frustration of using win95 , win98, drove me to Linux.  A very big moment was when Netscape Navigator 4.0 was released.  Now you really could use Linux like on Windows.
I'm a programmer and ethical hacker, I got into Linux mainly for security and programming. Linux is just smooth.
Odin Project. It's a full stack web development course, those guys recommended me to install Linux , started with Xubuntu and never went back. I think the most fascinating thing was the terminal, which looked so beautiful when compared to windows CMD
All I remember was that I'd discovered that there's an alternative to Windows, requested a free CD to be mailed from their website.

No recollection of the *how*.
It was covered in a Popular Mechanics article around the year 2000 on how to get more out of a 486. It involved buying a special 586 chip from AMD and picking up Mandrake CDs at Staples. It worked great, but dependency hell was quite the problem and I abandoned it. I tried again in 2004 with no better luck. In 2009 I rebuilt a fried desktop as a file server and used Ubuntu. Then in 2010 I got a Windows 7 laptop and it told me my printer was too old to use, I responded by installing Ubuntu and have used Linux ever since.
i was a stupid child, fanatic of the computers and computing, i stumbled across with videos of raspberry pi, and, consequently, the so called "Linux". i first installed in the gamer pc that my fathers bought alongside windows 10 (yes i'm a zoomer lol), i started with ubuntu, and i am here, with fedora kde
It was when the lockdown first began, I have a low-spec laptop and windows runs like shit on it so a friend of mine introduced me to ubuntu and here I am now after 2 years of using kubuntu as my main OS. I love linux and I don't think I'll switch back to windows even after getting a good pc in the future.
Back in the mid 90s with various copies of Corel Linux, Slackware, red hat 5.0.  On a amd k6(?) pc we built out of parts from the likes of compusa and tiger direct.  The parents had their gateway2000.  We ran a quake and counter strike server on the Linux pc next to it (and joined as LPBs to play others who were on dialup internet!)
My friend showed me Kali Linux on a USB in high school about 7-8 years ago. He was really interested in hacking, but I was more interested in *wtf this isn't windows or mac, and it looks really cool!*

I then proceeded to get Kali Linux on my laptop at home. It's not a good beginner distro but it did teach me how to use Debian's APT package manager. when I realized this wasn't the only way to use Linux I became very excited and obsessed with trying out all the different flavors of Linux.

I remember burning at least 15 different distro's to a USB during the course of the month, trying out each one, but not really learning *what Linux was all about.*

I ended up taking a break from it until my first year of technical school (associates software development degree) where I had a class that went over the basics of how to use the shell, *ls,cd,mkdir,rm,sudo,ps,grep, even VI and vim!*

During that class even with the minimal knowledge I had gained (it really was the bare minimum basics of shell usage), I realized that there was a great amount of power to using these tools. I went home and thought, I wanna try again. So I decided to plunge into the world of Arch Linux. It was challenging, but after finally getting the install down *with the help of video tutorials*, I noticed I understood it all so much better than I used to. 

After 3-4 years of Arch, I've learned so much about the way it all works, and how it can work for me. These days I use Fedora, because despite how 'mature' Arch pretends it is, their core repositories are kind of shit for getting development libraries. The AUR is cool and all, but if you can follow build instructions on GitHub, there's really no need to have the AUR.

These days I post cool tiling window manager screenshots on r/unixporn and share shell scripts and configurations I've made, as well as daily driving Linux and being very happy with it.
Comp Sci professor made us either dualboot kubuntu or run off a live disc. I found doing work to be significantly easier on linux as opposed to Windows and all of our lab's computer ran opensuse. I still prefer it more for most things outside of gaming. I work with .NET now, so my work OS is Windows. But I use the linux subsystem over powershell whenever possible.
A friend showed me Ubuntu 11.04 and later told me I could install it myself on my PC.

I was dumbfounded to discover that I could run an OS other than Windows on my PC.
> How did you start using Linux?

I read up about Linux in the late 1990s. Still was using Windows though.

Eventually I got tired of Microsoft for the most part and I wanted to try 
different things, so I ordered CDs (I think, or perhaps DVDs; that was
already in early 2000 so I don't remember correctly, I think it was a
bunch of CD discs though). I ordered red hat, suse and debian. SuSE
had a huge book. It also was **incredibly useless**. Red Hat I managed
to install but did not like it.

Then something interesting happened. I installed debian - but the xorg
server (xfree or whatever the name was back then) did not work. So I
had to type in the commands from the handbook. And that worked.
And that made my brain go "click". I think that was in 2003 or so.

Since then Linux just made "click" in my brain. It's a completely different
way to think about computers. Aka, commandline-first, data-centric first
and so forth, UNIX/Linux pipes - that all made sense.

I think for using Linux efficiently you should know one "scripting" language.
These days I would recommend python; perhaps lua or ruby. Either way
is ok, and typically better than "use shell scripts". I ALWAYS hated shell
scripts. Still do. I NEVER understood why I should use this lousy clown
joke that is shell scripts. Even perl is better than shell scripts - and nobody
uses perl. :P

> I‚Äôve never looked back since and I‚Äôm glad I did. 

I am using Win10 too and I don't mind it. I don't love Windows and don't
hate it. Microsoft still annoys me though. All that "trusted" computing as
an attempt to steal ownership. I purchased it; I own it. End of story.
Microsoft has no meddling to try to steal my rights. (It's a similar situation
with the right to repair movement - why do THEY have to fight for
these rights? It should be part of EVERY sane constitution. But not in
a corporate-first world view apparently.)

So I think the issue is not so much about Linux per se. But more about
being able to be flexible and modular and adjust things to how YOU want
it. And there is a LOT more flexibility than in Windows. On top of that Linux
is so much faster. Just try to copy 300 .mp4 videos, it is sooo much faster
in Linux. I kind of think that the more time passes, the worse Windows
gets speed-wise.
My shitty 7 year old laptop was struggling hard to run windows 10, so I switched to Linux Lite and then to Linux Mint. 

Today I got another laptop from my sister, also old and broken, but this one runs windows, which is necessary since I'm taking a power BI course, and since Linux has compatibility with basically nothing without wasting hours of your life trying to get wine to somehow work.

Overall I'm glad I'm back at Windows, I'm more computer savvy than the average person, but Linux is just a nightmare. I'm afraid of when windows 10 is abandoned and a bunch of people like me without the money to buy a decent machine will be left to a system without protection or a system without intuition and compatibility.
1999 phatlinux, split into a few 100MB zip drive disks.
I first started with an old version of RedHat on CD that I bought at CompUSA for a few dollars.  My parents had an old PC that was collecting dust so it was the perfect machine for it. I then spent what seemed like weeks trying to get a few things working, ran into issues, had very little understanding from an additional guide that came with it, and gave up.

Fast forward a few years and I bought an old PC at a garage sale for a few dollars. I was just going to use it to tinker with and started looking at Linux again. I don't remember where but I found Ubuntu 12.04 CD's and decided to try that out. It installed, had a few basic programs, but the best part was that the ethernet port worked and I was able to use it to connect to the internet and really explore what Linux could do. 

I used that computer and the next to distro hop, kill countless /home partitions, test many terminal commands, break, reinstall, swear out loud, and eventually landed up back at **Linux Mint LMDE**. What can I say..... It's stable and just works.
Servers. From TeamSpeak to Gameservers to VPNs to SearXNG to Nextcloud etc. etc.
Had a neighbour who was hauling all kinds of interesting software from his uni. This was before the Internet was reasonably available for private individuals in my home country. One day he got us floppies with [Monkey Linux][1]. At the time I couldn't find any use for it, but it looked like something special. First distro I really used was RedHat 6. Used it for sharing internet connection with neighbours and serving a tiny stupid personal website (almost a weblog).

[1]: http://www.spsselib.hiedu.cz/monkey/docs/english.htm
Mine is a stupid person's story.
Once upon a time there is a child who is just broke Windows and can't make a boot disk for it but somehow made for kali linux and used it for almost 1 year as a casual distro. (now I am using Arch btw)
I was looking for performance Minecraft cuz windows eats up all of your ram and stumbled upon Linux, after watching a few mental outlaw vids I decided 2 try arch, that was a huge mistake during cli install I wiped my hard drive and had a bricked computer for a while.
Finally after about. A week of trying and failing to install arch I did it. Proudest moment of my life, anyway I've been a arch btw user ever since
I was frustrated with windows 3.11 in 1994 and installed Slackware 2.0 right when it was first released.  The first system I tried to install on was a 286 and I didn't get very far and wound up installing minix instead.  I finally got it installed when we got a new pentium later that year.
i found it on multiple occasions without knowing it was all linux but I figured out somewhat soon later:

- we used Ubuntu 16.xx in middle school
- my godfather got me a RPi with a Debian and a RetroPi micro SD card
- i used Linux Mint to bypass my mom trying to be controlling microshit winblows


then, later - when i figured out that those 3 are in fact Linux distros - my friend was like "yea i use Arch i love it", got me to install it and i never went back

Mint/Cinnamon were nice tho

edit: i only owned a windows (10, never a different one) device for 8 months before the Arch install happened (that was in march if 2021)
Same way I discovered FreeBSD.
Was curious so I googled for windows and Mac OS alternatives.
Just curiosity and wanting to know what‚Äôs better when.
Plus it‚Äôs fun to tinker around
I had had it with windows viruses and updates!! I wanted something fast, safe, reliable and switched to Ubuntu/Zorin 6 in 2005. It open up a whole new world for me. I'll never go back to Windows. I do keep Windows as I dual load/boot every system now. But I don't use it.
Used it in school.
My friend introduced me to it in Middle school and I decided to switch cuz I had windows 8 and it wouldn‚Äôt let me upgrade to windows 10 the normal way
Around the time of the Winblown ‚Äò95 to ‚Äò98 fiasco, walked into a software shop and ask the merchant if there was an alternative to Microsoft. He handed me a cd set of Slackware 3.4.
I think it was Leo Laporte showing off a Knoppix CD-ROM on tech tv. What year was that? I tried it out and soon we were using the CDs to rescue dead Windows machines at work.
Minecraft servers
My first encounter with Linux was 2-3 years ago at my uni. I'm studying Computer Science. We've had a course called Computer Networks. In the lab we've had old versions of NST ( Network Security Toolkit - I believe ). I would say it was quite harsh introduction to Linux, because I've been using Windows since I was 6-7 and I had 0 experience with Linux and I was completely lost. I remember once I couldn't exit vim for quite some time. Memorising commands for tests wasn't my thing either. My friend gave me a tip how to make bootable USBs using Rufus. I've created NST Live USB and tried to practice. Then I gave up. A year later me and my friends started to work on a project and wanted to gain some experience with Docker. For a solid week I've tried to run it on Windows then I gave up and started experimenting with Dual Booting ( I know VMs are also good, but I didn't want a bit laggy system). I've tried to install Ubuntu 20.04  LTS first. In the process I've managed to brick my Windows. Through a Live USB I've managed to boot up the notebook and based on tutorials I've found my Windows license key (which I didn't need in the end). After this little tinkering / investigating work I've gave it another shot, but this time I've did a proper Windows / Mint dual boot (I needed Windows for school projects and Teams). I was very pleased with it so I moved every development work to Linux. (And the only development on Windows is done when I need to check the compatibility of my code). A year later there was a small misunderstanding between me and my friend. I've been telling him I've tried Mint but for some reason he thought I was using Manjaro. He told me he had it before and was satisfied with it maybe I should give it a try. I was a bit sceptical, because I've heard about distro hopping before, and I knew it starts with the first hop. I've had an old desktop PC which often crashed on Windows 10 so I've thought why not... So I've installed the Gnome version of Manjaro and OH MY GOD. It was love at first sight. Used it for some time and after that I've removed Mint (So long partner) from my notebook and installed Manjaro on it.  Few months ago I've updated my desktop because I want to get into Machine Learning, did my usual dual boot stuff but this time had problem with the drivers. I was so pissed on NVIDIA, but I've downloaded the latest image and it worked. I was so happy and been ever since. So that's basically my short rollercoaster ride with Linux so far. :)
1998ish iirc.

My home Coherent unix on a 386 was good. Before that, Xenix, and before that OS/9 on a custom SBC my employer made. I've been using multi-user/multi-tasking OSs since one was available on an 8 bit CPU. (I even had the source for it.)

Then there was this new OS called Linux that sounded cool.

So Usenet? Seeing distros on CDs in local computer stores? Lost to history! (or maybe there are clues in my old uucp email from back then.)
For me it was a slow process of gradually discovering and using more FOSS over the years. If you're searching for DOSS online you are bound to hear about Linux. It started with Open Office about two decades ago, then Audacity eventually Blender and Krita. After a while I had heard about Linux enough times to get intrigued for real and have been happily using Manjaro daily for a year and a half. I still have a laptop that I "need" to keep Windows on, but I'm planning to change that in the near future.
It was all about matrix/hacking and be the weirdo trying new stuff to show to his friends... Ah that feeling... Eventually you turned out to be the IT guy...
Got a chromebook. Wanted to see if I could game. Installed crouton. It was cool. Installed Ubuntu to dual boot on my Thinkpad bc IT installed some malware detection malware that crippled my Windows install...
Parents' eMachines desktop running Vista bit the dust. Never used Linux before but I had recently read an article about something called "JoliOS," an Ubuntu-based distro made for netbooks with a UI similar to iPad OS. I figured if it could run on a netbook it should run on their desktop so I installed it for them.

They loved it, and they got a couple more years out of that PC. They've since used Ubuntu and ChromeOS, and never used Windows again. 

Meanwhile, I take every opportunity I can to install and configure Linux on anyone's PC who will let me. I'm a Mac user and never had a reason to use Linux myself but I'm completely obsessed. Currently have five different Linux VM's installed on my M1 pro just to enjoy using Linux whenever the mood strikes.
Bashing and trashing Windows in the late 1990s and having people tell me about this other thing called Linux.
I first heard about Linux in 1997, I think it was in one of the many computer magazines I read back then. I first ran Linux in 1999 after buying a copy of SuSE from CompUSA for $99. I kept running it until 2001 when MacOS X was released. Since OS X was now based on FreeBSD, I switched, thinking I would get the best of both worlds. I kept running MacOS X until 2006 when I switched to Ubuntu. I‚Äôve been running Ubuntu as my primary desktop OS ever since.
All the PCs in my university library run Debian.
Low ram pc that couldn't handle windows 10
So i looked for a solution online and they said try linux!
Mint was my first and i never looked back it was amazing i kept distro hopping and still doing till now
 My Tech Team in college asked me to install Ubuntu 18.04 because of ROS. 
Since then have fallen in love with the freedom with my machine, scripts and the frustration that comes alongside them xD.
It was around 1999 and I was just a kid. There was an install fest a LUG was doing. I got a copy of Red Hat installed on my box. I learned all I could but nothing worked right. Couldn't even get X running right because it didn't support my graphics card. I dropped it, went back to Windows 95/98. I kept buying CD's at the book store though. Then I got a hold of Mandrake it had these new things called Gnome & Kde. I installed it and everything kinda sorta worked.  Wine wasn't really a thing and when it was a thing, nothing worked right. I convinced my grandfather to buy xwine which later became crossover. My games mostly worked at that point so I just kept with it.
I think it goes back to 90s or maybe 00s. First I've tried various distros which came on CDs for free with magazines like PC Magazine. 

I distinctly remember installing RedHat and not being able to quit vim (yeah, true story).

I got lucky with SUSE though. Played with it for  while. However, Linux felt strange for me. Couldn't wrap may head around it. 

Then someone at work recommended Gentoo. I've followed the handbook and, lo and behold, things started to make sense to me. However, Gentoo proved to be too much for my Turion powered laptop (a full KDE install took days to compile) so, after a while, I switched to Debian and used it for many years. I've used Debian everywhere until I've stumbled upon Arch (most probably on DistroWatch) and fell in love with it's minimalist approach and KISS principles.

But Arch failed me with every damn update so, after some time, I grew tired of putting of fires, so I switched back to Debian. Years passed and the company I was working for switched us all to Apple hardware. Which is great, the hardware that is. Wasn't too happy with macOS though so now I'm back to Gentoo. My current Macbook Pro is an older Intel powered model and is working just fine with Gentoo. I can only hope that when the day comes and I will be forced to switch to Apple silicon, there will be at least one Linux distro good enough for it. Fingers crossed :D.
Not sure when I first heard of Linux. I've always lived on the internet doing internet things like downloading torrents that I got fron 4Chan, so im going to assume I heard of Linux from that, but I never tried getting into it. 

Some time in 2016, however, my MacBook had a bad crash that I had to install a new version of MacOS, but just for sone reason I decided to check out Linux and went with Linux Mint with its Cinnamon desktop, and it was awesome!!!!! 

I then tried out Ubuntu, then PopOS when it was released, then Fedora, then Manjaro, and now im on pure Arch.
I don't even remember how, but I do remember getting my hands on a shitty early netbook (800mhz!) for free and throwing Damn Small Linux on it back in 2008.

Actually, I may have discovered it via OG Xbox modding, one of the things I found out you could run on one was this distro called XDSL.
On school actually. A schoolmate used it and taught me a few things about Linux and showed me that it used less resources than windows in general.

Being a IT education and my laptop being low on ram I took the splurge. First tried Ubuntu etc, but quickly found arch. Followed the instruction on the wiki page. Learned me a few stuff as well and had a pretty decent setup going. Used it for the remainder of my education as it was way more responsive with multiple VM‚Äôs running.
My father, who was curious, took my to the local "Linux Day" in 2006. I was 12.
Now I've been using Linux as my main os for a lot of years, at least 10 I think.

My father still uses Windows: while he is fascinated by Linux and FOSS in general, he feels "too old to learn" (moreover he doesn't speak English, which doesn't help to learn in this field)
I started using Linux on a raspberry pi 3b+. After this, I decided to try it out on my desktop. I started from Ubuntu, then KDE Neon, now happy with Fedora. Also, I tinker on a VM distrohopping, trying other distros (Mint, Zorin, Elementary, Q4OS, Debian, Arch, Manjaro, MX, Cutefish, Pop!_OS, Mageia, Solus, KaOS, Artix, PC Linux OS, EndeavourOS, Alpine, etc)
I was around 11 and read my uncle's Linux books. The ones with CDs at the back, Those got me started.
I was 12. The local isp I used for dialup used Linux for hosting services. So I ended up using Slackware. I had it setup to be a dialup modem router/dhcp server and dial on demand on a spare 386 sx40.

At one point I attempted to switch my desktop over to play games using 3dfx drivers but couldn‚Äôt get it working.

It would be 20+ years until I started using it again professionally. Who woulda known.

I have way more windows experience, and I resent that fact now after troubleshooting Linux. If your hosting something and it‚Äôs supported/or has a equivalent linux product. It‚Äôs a no brainer
I was a Computer technician for a large company and a co-worker mentioned that I should check it out and the rest is as they say history. Go open source.
I was using an OS called Minix which was sort of open source but not really. You could share the OS source code with up to three people, a limitation which made it hard to distribute changed/better versions. So all community enhancements were shared as patch sets. One guy had this popular 80386 patch set (Minix targetted the 80286) that got lots of attention and was widely used, but it was still cumbersome to use and develop.

Then this other guy came along and posted to comp.os.minix that he had developed his own 386 kernel from scratch (nothing big and professional like GNU), but it seemed to scratch an itch and people started using it.

Unfortunately I could not use it right away. I was using a 68000 port of Minix and this new kernel was never going to be ported to anything other than the 386 (so said the author), so I couldn't actually run this new kernel. I saved up my dollars and bought myself a Cyrix-based 486 machine (cheaper than Intel) and I loaded Linux onto it (SLS).

I started with 8MB of RAM, but X11 was pretty sluggish so I quickly upgraded to 16MB - that was much better. And so much better than DOS/Windows 3.11 which was the other OS you could load on the machine at the time. The multi-tasking was great! I could run a mail server using UUCP to transfer email (and I did). Everything was free. Such a cool OS.
College - Redhat (6.0).Internship - Gentoo (2004.3) then Ubuntu (4.10).I was hooked since then and have run Redhat, FreeBSD, OpenSuse, and now Ubuntu since 2006 or so. Toss in RaspberryPi as well.

All of my kids have now grown up with Linux (Ubuntu).

One has dove into the deep end and has run Ubuntu, PopOS, and Kali & knows it pretty darn well (runs & compiles kernel modules). I actually challenged him to install Gentoo in exchange for getting an old laptop for free. Gentoo is what really taught me about Linux.
I was fucking tired of having to find info on those completely unreadable Microsoft pages. Unreadable because of thousands of paragraphs of unreadable text with quite small fonts.

Then I started using Linux. The answers were just a couple of lines that were actually commands fix the shit or improve my life.

This about 15 years ago.
OpenSuse on a computer course at school 20 years ago. We had our own disks that we slid in and out of the front of the computers for every class, been using linux on and off with different distributions ever since :)
 For me I always liked tinkering and I heard you could use Linux on old PCs to bring them back to life. So I went to my high school. I was 15 at the time dismantled several old PCs and took the best bits out of each and threw Puppy Linux on it. Can‚Äôt say I did much with it  but it was fun to see.

My first real earnest attempt to use Linux was in college. I used it for 6 months. Bounced off it because I couldn‚Äôt game on it. Now I am out of college full time work in IT and Linux has been my daily driver for about 2 years now.
I was a teacher in school with something weird on their laptop, asked what it was, it was Fedora. They gave me an Ubuntu live CD as well.

I never wound up installing the thing, I was too scared to brick my Windows install.

My next brush up with Linux were a few experiments with Ubuntu on a desktop. I tried to make the switch but simply didn't know enough and couldn't get it to play the games I wanted, etc.

Later I got a Mac, and learned how to use homebrew (a package manager for macOS) for some web development or some shit. From there it snowballed and the next time I got a PC I installed Linux since I could not handle an OS without a package manager anymore.
I was studying aws, came across network chuck on YouTube, and I never looked back. That guy's has charisma such a good teach he could sell water to a fish in a well.
I've known about Linux since the 90's, but didn't start using it till January of last year.
All the ftp sites had these weirdly named directories called "linux".  One day I looked inside.
When I got my first laptop about June of '20 I decided to start watching the tech Tips man since I wanted to be better antiquated with computers and one of or probably multiple of his videos were on Linux which is what introduced me to it. However I never really did anything with Linux as I had no need to and little space on my laptop for another operating system until I built my first PC this April and by that time I was informed enough about the questionability of windows and the direction that Microsoft was heading so I thankfully built my PC with Linux in mind (mostly just Radeon graphics over Nvidia) but still dual booted windows until it broke GRUB at which point I stopped using it since that was absolutely ridiculous and I had no need for windows anymore and since then Manjaro for the past 3 months and now fedora over the past few days have been wonderful and I don't see myself going back to windows other than on my laptop which is more so a youtube and messenger machine.
Some random distro review for Garuda showed up in my recommend and it blew my mind. I installed it on my laptop but after struggling with Arch I switched to Kubuntu. A few months later I installed Kubuntu on my main pc and eventually I broke it so said screw it and switched to Neon.

I got a new laptop so now that‚Äôs on Fedora Gnome and my main Pc is still Neon
Early 90s. Probably SLS or Slackware. It was a .98 kernel or such. 

Needed a proper latex distribution, and dostex just couldn't handle my thesis
I have always been aware of various Unix type operating systems. I even got some time on an early Unix. I got some exposure to Flex/os, Minix, OS9 and OS9/68K Personal Computer Unix style OSs before Linus got inspired.

In 1998 I set up a dual boot Win98/Suse Linux system. Unfortunately, Linux was too primitive and Win98 won out.

About 10 years later, my kids managed to get the family (not mine) computer completely virused. I was unable to fix it even with a total reinstall of Windows. I soon realized that  they use the machine for mostly web browsing. So I looked around for a version of Linux. I ended up with Puppy Linux because it booted from a CD and didn't need to be actually installed. At this time I was still hopeful that I could get Windows back up and running.

So basically, Puppy loads from the CD into a RAM disk. It then runs from the RAM disk. No CD needed. And when the computer is turned off, the RAM is wiped clean. The next reboot is a fresh start. Viruses will have a hard time infecting the system. I decided to just go ahead and leave the Family computer as a Puppy Linux system.

Fast forward several years and I'm working for a tech company doing validation and verification testing. My company laptop is a dual boot Windows/Linux system. Windows for running business applications and Linux for the technical stuff. But I hate rebooting between the two systems. So I start using cloud, portable and FOSS applications on Linux for the business part of my work. Pretty soon it becomes a running joke around the office. "Hey BaldyCarrotTop; When was the last time you booted Windows?" Me: "Not quite long enough!"

So when Win98 went EOL I knew that I would need to do something about my home machine. I didn't have to think too long about converting it to a full time Linux machine. I haven't looked back.
Someone in my computer class back in 99 brought a version of Red Hat in and we installed it on one of our computers. That was my first experience with Linux. 

After that, I would get copies of Ubuntu on DVD, but I never really installed them as we didn't have a spare computer. 

It wasn't until the last few years that I really considered switching over. 

I ran Kubuntu a bit back in 2018, but ended up going back to Windows. 

I decided to try it again this year on my Thinkpad P50. This computer has support for two m.2 nvme SSDs, so I put two WD SN750 Black SSDs in it.

After a little bit of distro hopping (Zorin, Mint, Ubuntu Cinnamon Remix), I gave Fedora Cinnamon a shot and I'm super happy with it.
I read about it in a hacking guide back in 97-98, it sounded interesting so I got some books about unix from the library and downloaded Slackware.
Heard about it online and from raspberry pis, so I used crouton on chromebook I had won at school and the rest is history.
Edit: This was around 2016
Stumbled onto a cool "shell replacement for Windows 98".... called LiteStep

Me: What the hell is a Shell?! Ooh that's cool, but now my computer says "Reinstall Windows", mom's going to kill me.

Discovered mIRC/efnet, learned Litestep was a port of AfterStep, running on something called Linux and BSD. Found a CorelLinux CD in the trash... 

And the rest,  as they say, is History.

20+ years later, still run Debian and Ubuntu as dailies w/ Fluxbox. ;)
I was so passionate about to learn ethical hacking and then I met this amazing operating system called Linux, the supreme Operating system where we can do the things that a normal windows can't
My story: I was reading the "alternative" computer press (not talking about windows). I was beginning to be fed up with windows 95, the final straw was the non-compatibility of Quake III team Arena, which also made my TnT2 obsolete when I switched to Windows 2000.  
I started with a SuSe, a German fork of Red Hat, then I progressed and I have since been on Debian
Some guy on Reddit, on the hoi4 sub said that Linux is cool in the comments, that's all it took to convince me. So, thank you random stranger.
I was in Small Computers School in the Marines, and one of the other students came in with a book, "RedHat Linux 5.2", with a CD in the back cover.  After I hit the fleet, I got my own copy of the same book.  And really, have never looked back.  Should have been the back half of 1998.
Late 90s. I knew of Linux just didn‚Äôt trust myself with it. I kept reading about it and learning about it though, but was too scared to delete windows and try it. Then I found zipslack and put it to use with my Zip drive on win98. I tinkered there occasionally. I got into console emulation and there was a lot of emus for Linux (plenty for windows too) but I really wanted to play these extra ‚Äúhacker-OS‚Äù emus I couldn‚Äôt on windows. I started a rom site, running on win98 (I was deeply consumed by emulation at this point), eventually geocities wasn‚Äôt cutting it. So CuteFTP became my rom site in my laptop, only I kept running out of memory and crashing and eventually I gave in and wiped Windows completely. Tried Red Hat (took a week to download on 56k!) and I didn‚Äôt like how customized it was, moved to Mandrake and it frustrated me as well. Remembered I liked that zipslack and went for a full install. Learned ftp software, Apache, loved my home of Slackware. Computer didn‚Äôt freeze anymore, my server ran just fine on my laptop running Linux. I stayed there for many years. Eventually Apple won me over with their ‚ÄúUnix-based‚Äù OS X. I still do love me some Apple hardware, but most of the Macs I‚Äôve amassed all run Linux these days. I swap between MacOS and Slackware still to this day. Right now I‚Äôm on a huge Linux kick lately. My servers are all remote now, and I‚Äôm not peddling roms anymore, but I‚Äôll be damned if Linux isn‚Äôt the most fun you can have with PC (and Mac) hardware. Linux for life & Slackware forever!
I would go around during verge collections and pickup all three old PCs. We always had to go because there was the" cable guy" who would go around destroying electronics and cutting cables.

So I had a room full of old PCs and had to do something with it. I started on freenas but that got hit by a surge. 

I used Ubuntu to rebuild the raid array because the freenas is drive was borked.
According to my install CDs from 1995. Slackware 2.3 with Kernel 1.2.8
Kid at school told us about it and I ordered some Ubuntu 6.04 cds!
I had a shitty lenovo laptop that only 32gb of EMMC and a full fat version of windows 10. Literally one update made it unusable. I installed mint and never looked back. I am using linux on all my devices now, including my phone (pinephone)
Android which is based on Linux  >  "  fdroid = open source apps " > open source community. Wow there are other operating systems other than windows for PC.  > termux in android > Linux .
Started when Windows 10 broke and wouldn't boot, burned a Manjaro KDE iso in my USB then wiped the disk, now i'm running Ubuntu.
Fedora Core 1.0 in a book.
Stupid windows 11 hardware restrictions and better performance on Linux
I started using Linux in 98-99.  First was zip slack then Redhat.  

I just always liked it more and I'm not really a gamer.
I‚Äôm old. I downloaded the original Slackware releases and compiled everything. Lots of floppy disks in the early 90s and lots of money in long distance. I was looking for a for an alternative to XENIX and AT&T SysV.
1994. Our school‚Äôs dns server got hacked. I believe it was AIX or hpux, can‚Äôt remember. My comp sci professor decided he wanted to try Linux and we got to watch him set it up and set it up as the new dns server. Later on one of the tests for our networking class was to successfully set up a Linux dns server :)

My school‚Äôs network os for file sharing and whatnot was Novell netware, and our mail server was Pegasus mail. 

Damn, lots of memories coming back. Good times.
Live in finland so been aware of Linux from the start. It got ridiculed and Linux became synonymous with geeky nerds. By me as well. Stupid kids.

Windows 10 came along, got fed up with the ads and the spyware and everything, started to experiment with Linux and eventually made the switch. One of the most liberating things I've done. Now my hobby is tinkering with old laptops and phones.
It was 2018

I was searching for alternatives of windows for my old 2005-2006 dell Inspiron when I stumbled upon Ubuntu 18.04 (Which my programming teacher tought me when setting up my raspberry pi 3B+)

It was the best experience I've ever had with Ubuntu and that laptop. I still miss Unity7 since it was my entry point to the world of Linux desktops and when Canonical switched to Gnome I was confused and frustrated with the change. But got used to it pretty quickly.

Now I am using fedora 36 with i3wm as my second OS (since I had to install windows for Photoshop and illustrator BC I am taking a graphic design cours) and I still enjoy using Linux for literally everything, from basic coding to lightweight gaming on my ThinkPad p43s with an dedicated Nvidia GPU running without any problems at all.
I started with an Atari 800 computer then moved to MS-DOS on a PC clone (amber monitor and later green screen). In 1990/91 moved to Windows 3.0 and used Windows through XP. After XP, I switched to Ubuntu and later Debian using Gnome 2.0 at home (still forced to use Windows at work). Switched to WM‚Äôs for quite a while primarily Ratpoison, Openbox, and then tiling WM‚Äôs including AwesomeWM, dwm, i3 and sway when it first came out. Then I used KDE which I still mostly use but I am spending more and more time in the terminal reading my mail and news and using Vim. Guess I‚Äôm getting old.
I read about Linux in a Popular Mechanics magazine. It was a guide on how to turn your PS3 into a personal computer. I wanted to try it, but didn't want to hurt the family computer since I was only 12, and that was our only PC.

Fast forward 9 years, I am in college. Windows 10 comes out and suddenly my Windows 8 PC stops functioning correctly after I tried upgrading. I remembered the article, and followed a youtube guide on how to get it installed on my PC. Once I got it installed, I never wanted to go back.
Shitty PC that could handle no other thing so it wasnt a choice. Fast forward a few years: got a new PC but Windows was unstable af so I switched back to Linux and never looked back.
When Ubuntu 14.08 when Unity Desktop was supported
My compsci professor took my mouse away and made me boot into redhat. 

Been linuxing ever since ü§∑üèª‚Äç‚ôÄÔ∏è

That was 22 years ago.
TL;DR: HDD in a laptop died, Linux had no problems running on an external.

Technically Android, though if we're talking about actual Linux, that's a bit of a longer story.

I grew up in a lower-middle-class family, so we could afford some luxuries such as internet, cable, and a cellphone plan, but replacement for anything electronics were hard (cheapest Samsung phones until they could no longer work was typically what we'd do). We had been given an "old" Toshiba laptop with 4 GB of RAM and an i3-2350M due to the keyboard being killed.

I was just an average kid who played video games. I ended ip walk up one morning to find that Windows would simply not boot, and I was getting the PXE boot screen.

We had no money to afford a HDD, let alone another laptop. My father granted me his external hard drive. I attempted to install Windows on it, but found performance to be lack luster, and Windows was not a fan of configuring a page file on the external either.

I ended up doing some research and discovered "Linux". I installed Ubuntu 12.04 IIRC, which was fine since I was primarily just playing Minecraft and browser games.

Over the next ~5 years I switched a couple of distros, still using the laptop and slowly upgrading / downgrading to other systems. Next was Linux Mint, and believe that was followed up by Manjaro, which I used from ~2016 to ~2018.

I believe this actually allowed me to kickstart into my career of a Linux SysAdmin.
A guy at a bookstore in Albuquerque have me an excited spiel about RedHat in ... 1996? The idea that there was something other than Windows/Mac + free was enough to start me on my way.
My neighbor had a copy of Ubuntu 8.04 on a server machine he gave me. I loved the orange. My second time was before OtherOS was removed from PS3s.
Well, it's kind of a strange story.

back in the late 1990's i had some old computer hardware that windows didn't run very well on. Someone online suggested i could try linux for it. I went with slackware, and found it to be a lot more comprehensible to me (perhaps we'd say less opaque) than windows.

Then after that i would just go with whatever was most reasonable for me at the time - linux if that made sense, windows if it made sense, etc.

As of ...2018? I'm full linux though and have no current windows installs, including for gaming etc etc.
I got annoyed by the activate windows watermark.
* When 1: In 1994
* Where 1: I saw it on a book store (Cuspide in Cordoba) 
* Why 1: wanted a cheaper alternative to windows because I could not afford it

&#x200B;

1. When 2: 2004
2. Where 2: at work after downloading the first release of Ubuntu
3. Why 2: wanted an alternative to windows because i hated it

PS: The first distro release that actually worked for me was Mandrake, everything before it was a pain to install and use. Until 2004 I was sticking to use windows, but our friendship from the 80's 90's was waning quickly.
From printed magazines and pals.

Got a 'rrrrrrrrrrr' ehem copy of some CD, then got and bough "Simtel" CD-roms and some other CD-roms with distributions (i think Slackware?) from a shop that sold them 

With a spare used hard drive and 3.5" hard drive adapter in 5.25" mount I tried. Man I failed at it a lot of times. But it was fun to try it out.

Mostly: graphics was awful to setup in 1990's and early 2000's. One wrong click and it was 'Monitor out of range' lololol

fun though
How did I found out about LInux? I think form a techie magazine in like 1997 or so when I was like 11. 

I had multiple starts with using it tough. From trying out the floppy based, can be booted from MS-DOS Monkey Linux (made by some Eastern Europian university) in 1999, to trying and using Suse for half a year in 2006, to finally finding a good place in late 2007 with Ubuntu at the time. I have been using some kind of Linux distro ever since. So it will be 15 years this October
Got hit with a w32bymer (I think that was the name) worm on Win98. Completely screwed my OS. I thought there has to be a better way. Started dual booting Win and Mandrake Linux. Eventually dropped Win all together and have never looked back. Linux user since 2000. Currently using Mint.
Windows 10 got so darn bloated that my 10 y.o. laptop was literally overheating on standby with no apps running.

Switched until I could afford thermal paste, but then never looked back

(Summer 2021)
Both my uncle and my friends brother had talked about it and I had a crappy old computer that needed an OS.
im a programmer and hear about technology all the time, and linux wasn't an exception
Got a steam deck
I started as a kid with Edubuntu. I read about Ubuntu/kubuntu, xubuntu on a old video game magazine with a really small column talking about it.

I asked my father to order one online on Canonical Website. It was free delivery! It tooks around 1 months to get it on our mailbox.

Most stuff worked well on of of the box on my laptop. Only my external hard drive full of anime was unreadable, ntfs-3g was quite unstablebat that time ^^
I was 17, in an IRC channel on dalnet. The year was 2000. I was tired of windows after‚Ä¶procuring‚Ä¶windows ME. I was mostly bored with the interface and it crashing randomly in the middle of games. Some slightly older dudes in the channel were like ‚Äújust install Slackware, we run it.‚Äù So I did, took me three or four tries to get the system running, then I installed bitchx and was like ‚ÄúDUDES I DID IT!‚Äù And one guy was like ‚Äúcool now install enlightenment it‚Äôs totally eye candy‚Äù and I hated it. I cycled through slack, mandrake, suse, red hat, played with BeOS, OpenBSD, NetBSD, then finally settled in to FreeBSD and fluxbox because the structure of a base system plus a very robust ports system made sense to me, and I didn‚Äôt want a windows work-a-like interface.

I‚Äôve now used Arch+i3 as my primary distro/working environment for nearly 8 years. Hits the right balance of a robust base system plus fairly bleeding edge packages in a powerful and simple environment, while also having the gaming potential of steam.

Tl;dr - y‚Äôall kids have it easy with your Ubuntu‚Äôs and xorgs. Most of y‚Äôall have never needed to hand edit your x11.conf or load kernel modules by hand or make sure you conf files were in good order. Back in my day we built our kernels by hand to improve performance. 

Chainloader +1 was our best friend, and freeciv was (never) good enough‚Ä¶so we dual booted windows to play age of empires with our buds running windoze at LAN parties.

Kids these days.
Well long story short my dad before he die I started in 2014
Hahaha 5 years ago, i messed up dad's laptop that he gave me to do some homework in it but i messed up something in windows registry, i wanted to take dad important work  files and a dude from my class offred me his usb stick with linux in it to boot from it and take the files i needed, i loved the penguin logo and the linux interface. Later on with my own laptop i installed my first linux on it. Now i cant imagine my laptop without a linux
At University in the late 90s the department of mathematics and computing issued a CD-ROM each semester. Partition 1 contained various materials, software and source code for coursework. Partition 2 contained Red Hat 4. I spent more time getting video card and sound drivers working and experimenting with all the different window managers then doing any of the coursework. It was glorious!
In a company internship. I used RedHat at the company and I installed Fedora on my personal laptop.
wanted to try something new in lockdown. I'm glad COVID happened.
I actually don't remember exactly how I came to use Linux,  but I think it was when I got a netbook back in college around 2008/2009 that ran a really shitty and unstable version of Windows.  My roommate back then started dual-booting Ubuntu and I'm not sure who inspired whom, but I also ran Ubuntu for a short while before distro-hopping around like a madman, getting more and in over my head and breaking things.  My biggest victory was finding a distro that was light-weight enough to let me watch movies in 1080p without stutter.  I don't remember which one it was, but it was glorious.  

Then, I got bored.  The fun part was setting it up, I had no real reason to run Linux and I found myself mostly using my Windows desktop, so I stopped using Linux altogether for a long while.  I still liked Linux for what it was and loved the idea of it, but felt like my needs (writing,  browsing, gaming) were better suited to Windows or macOS.  

A few years later I got a Raspberry Pi and tinkered away with.  Learned some basic Python and even installed Arch at one point but didn't feel I had enough of a reason to make Linux my daily driver.  

Over the years I've gotten more and more annoyed with the increasingly shameless tracking and surveillance going on in MS and Google products and that together with my starting doing the Odin Project (which requires Linux or macOS) it was the perfect storm to get me back in.  

Now I've had PopOS a my daily driver for a good month and am absolutely loving it.  I do keep Windows installed on my gaming computer as it's Windows is still way ahead of Linux in that regard and I don't have the time to troubleshoot Lutris, Proten etc.
They told me that it's unix for PC and I was handed some floppies.
A coworker of my mother's many many many years ago found out I was a computer geek and he burned her a copy of a thing called "Knoppix Linux" to give to me, and it was the coolest shit ever. Using Beryl and KDE 3 was fun - though I knew nothing and was never able to really use it fully.

My first _proper_ Linux experience came a couple years later when I first installed Ubuntu onto my laptop. That was fun.
I didnt know a thing about computers until about 2-3 years ago that I got my first personal computer with windows 7 

fast-forward a couple of months the first laptop was replaced with another one that came with windows 8.1(this is were i started to poke around with optimization guides and programs like driver booster to make that dual core laptop run a little bit faster)

Fast forward a year later switched to another computer that came with windows 10 aand it was a nightmare of compatibility, I had BSD most of the times, freezing, weird bugs with drivers, and only after a class a teacher  talked us a little about Linux

He showed us an old version of Ubuntu that still used unity, and after giving up on windows i decided to go down the rabbit hole

That was almost a year ago started with Pop_OS!, distro hoped to Manjaro, then Zorin OS, then Linux mint, then Artix and landed on EndeavourOS

Had a rough time deciding between KDE and GNOME but in the end I choosed KDE with Wayland, it was the most stable DE for my laptop  (tried other DE's like cinnamon, mate, XFCE, open box LXQT, LXCD)
my uncle installed ubuntu 18.04 in may 2018. Then i installed kubuntu 21.04 in september 2021, and then linux mint, and then i started distrohopping. Now I'm on arch :)
I moved from Knoppix to mint a few years ago and could not be happier with it.
I read about it in one of local PC magazines, back in late nineties. Didn't give it a second thought until almost a decade later when I got into open source, then I tried few live distros just to see it first hand, without any intention of using it. It wasn't until 2017 that I actually switched, and it wasn't on a whim, windows 7 was on it's way out and I decided to part ways with Microsoft.
My early 90s ISP offered shell access. :)
I really have no idea. My first linux distro was Ubuntu 16.04, and I was very young. I had probably heard how customizable and well performing linux is, so I wanted to give it a try
First truly usable system I tried was [0.12](https://gunkies.org/wiki/Linux_0.12) sometime in 1992. Linus had been putting up floppy images for a while on [comp.os.minix](https://groups.google.com/g/comp.os.minix/c/2Tm_OV64JWo), but these were just test images with partially working  systems. 0.12 was finally a complete working system. I've been using LInux in one form or another since, and finally completely dumped Windows three or four years ago.
As a developer I heavily used WSL under Windows. I realized that WSL was too limited in some cases but way better to work with, so I just switched to Linux.

Thank you Microsoft, you finally convinced me about something!
Had a guy who ran the Sun Workstation lab at the local college give me a grocery sack full of floppy disks. Installed a version of Slackware that was inside, kernel version 0.96 perhaps? Very early 1994.   


Side note: about four months later, I went back to him with a question about compiling the kernel. Asked him a question regarding modules in the kernel, and he wasn't aware that the kernel had evolved to include modular plugins. Proud day that day. LOL
14yr old me would run into webserver 500 errors from time to time and I noticed they mentioned Slackware. I was like wtf is a slackware. And it's been a journey since.
Back in 94/95 I use to chat on irc. I was trying to impress a girl who used Linux. So I got Slackware, and kinda never looked back.
My dad loved using Linux and introduced it to me when I was little. My first PC ran Ubuntu when I was 9 yr old.
Okay little late, but Idc


technically I first used linux when I was a little kid and my library had a kids Hannah Montana os installed on a kids computer with a bunch of weird learning games


But I actually learned about and really got into linux in highschool when I bought my first laptop and wanted to download programs for animation, audio, photo, and video editing, as well as the arduino ide

I had no internet, and no money, so I needed free compact software that worked entirely offline, I found a handful of programs that suited my needs, did the math, and realized it would take me hours to download them all for windows at my local library.

Then I found https://gitlab.com/giuseppetorre/bodhilinuxmedia

And it was smaller than installing all of the programs I needed separately, (at the time at least) and would run better on my computer than win7 so I dropped windows on the spot, and it was amazing, 

I mean sure it was a month before I got my wireless card to work, and there are no compatible drivers for my sd card slot, but the difference when compared to any previous experience I had with computers was amazing
Also gonna date myself, but I downloaded Slackware 1.1 or maybe 1.2 onto 20 or so 1.4MB floppy disks to install on my 386sx PC.  Around 1994 or so...   The pain of having to edit X config files to get X-Windows working in that era...
I'm on a similar boat, it was Mandrake 8.1 from a magazine, and later Ubuntu from when they send out those free CDs.
I remember these. I also remember when Ubuntu was first put in a magazine, and when the magazines tried out DVDs so they could put more than one distro out at a time.
I heard about it on Usenet around 1993 and then found an ad in a zine where someone was selling Slackware disks.
I tried and failed to download some distro in the days of old fashioned modems (no broadband - those days you needed a leased digital like to get 1mbps).

An year or two later I bought a Mandrake CD from a bookshop - it came with a manual in a nice box.
I remember when it was just SuSE before Novell, then when Novell bought them you could order free pressed CDs in the mail (way before Ubuntu did it). I got them just because of the sweet Novell logo.
Same here, I got a copy of Corel Linux with a magazine. I tried installing it but couldn't make heads or tails of it, gave up for a couple of years and then tried some kind of ancient version of SuSE, got a little further and then gave up again, and then eventually got an early version of Ubuntu up and running and have been using Linux in some form or another ever since. :)
Lol Date Myself...I started with Unix/Fortran on an IBM 360 in 1976
Foot menu FTW. :)
Hah! Almost identical to my use case. Perhaps you were in the same era? 2000 to 2005 perhaps?
Dating successful!

LOL!!!

I bought the Red Hat 5 CDs from a local computer store and built myself a networked combination file server and print server out of an old i486 PC. It wasn't until the release of Linux Mint 7, though, that I began using it for daily desktop duty.
Slackware 2.0 in 1994.  Out of frustration with Windows 3.11.  Took dozens of floppy disks.  I didn't stick with it.  I've really been using linux full time in some capacity (other than devices) since about 1999.  At that point I was using mostly redhat on salvaged hardware.  More recently I've been using debian since 2011 or so.  Most of my use is for home servers of sorts.  

My desktops are macOS mostly.
Same.
I got a live CD for OpenSUSE from a magazine around 2010 but I couldn't get it to work. I eventually did but I think in the meantime I installed Ubuntu from a different magazine CD.
Yep! Same too!
same here
Despite a great experience with a Pi 3, it took me a while to believe that Linux could be a usable desktop like Windows was... I didn't fully fathom that Linuz had competitive DEs and userspace software. 

Then I dual booted and from that day on I only ever entered Windows to use commercial CAD.
Same!
I honestly thought I was running on negative fps on windows 10.
\^ This

That's why I moved to OS/2, before discovering Linux. One of the IT admins at work was OS/2 curious, so I volunteered to test drive it. She set me up with OS/2 along with the Lotus OS/2 suite of programs. It was glorious! It was so glorious in fact, that I bought OS/2 for my machine at home.

OS/2 was da bomb and a much better Windows machine than Windows ever was!
<3

I can relate to that sooo much ...
Windows has not been crashing much since Vista. 

If crashes - there is a good chance your hardware has gone wrong. 

If Linux works on the same hardware - then linux dirvers don't fully utilize it.
You bought Linux?
He said it was "self-hosting", so I gave it a try, because I wanted to use my computer at home instead of waiting for time at the school computer lab.
OGüóøüóøüóøüóøüóø
Real OG
My dad bought a copy of SuSE 5.3 in 1998 and quickly lost interest. But I couldn't get enough of it, spent ages going through all packages from the CDs and compiling my own kernels. All without an internet connection. My proudest moment was when I finally got sound working after a few months.
That's the best way to learn. My parents were not tech-savvy so that would not have worked for me. But one other kid in school was a computer nerd. Both his parents used computers professionally so he kind of got into Linux early on.
Go Dads!

I force my kids to use it on their handmedown thinkpads. Pop!OS. 2 years later and they haven't complained. ;)
Lol my father still falls in scam advertisements and has no problem using pcs with 29399393939299 sussy software and malware
Same. We had Suse Linux 7.1 on our computer back then, dual booting with Win98SE but I liked suse much more.
Similar story for me. In college all the computers in the engineering lab were running Solaris on SPARCStation 10s, and that's what got me hooked on UNIX. After graduating I wanted to run a UNIX system at home, so I bought a Red Hat Linux 3.0.3 CD and installed it on a used Compaq Prolinea 486 PC I bought for cheap at a computer show. I continued with Red Hat Linux up through version 9, then continued on with Fedora Core 1 up through Fedora 36 today.
Based city
I didn‚Äôt use Red Hat or Fedora for decades because of that. They were in versions in the 20s when I gave them another shot. I think I entirely missed yum and went straight to dnf.
but the 2.4 kernel brought DVD support! that was huge!
Oh... I haven't thought about that magazine in forever. We used to get it at some of the bookstores here in the US. (Or maybe we still do. I don't know. I have a couple of the back issues somewhere at the house.)
I was one of those friends evangelizing poor lost souls [with evaluation Linux versions from magazines](https://imgur.com/a/QHwkIPr).
Man MUDs were great.  Awhile back I tried to find one to play but I don't think I had any luck.  There used to be so many!
Me too.  My friends talk about their Photoshop subscriptions and whether to buy the new windows. I just shake my head in GIMP & Lubuntu.
Oh, and what pushed me to try it back with Ubuntu 8? I saw a video of someone with Linux doing 3D desktop switcher and I just had to have it üòé
Is it difficult to...liberate a Chromebook?
I installed SUSE on my wife's laptop cause of vista.
Me too. I'd dabbled with Linux before, but Vista pushed me into actually using Linux seriously.
Nice one
Do you still have the "1 free incident of technical support" voucher included ?
The laptops my wife and I use both originally shipped from Dell with Vista. Today, mine runs PCLOS and hers runs Mint, and they both work better in 2022 than they would have in 2009 with Vista!
That's beautiful.
Hope your rough phase is over.  Cheers
Are you Russian?

You're dodging sanctions anyway, Fedora or other distributions may not be able to update either.
Fedora is one of the best distros available right now, bleeding edge without the hassle. I love it.
Wow! Not often you see a former Multics user kicking about anymore. Last I heard, they went the way of COBOL programmers.
> why hasn't this been open sourced?

OS/2 is actually still going, now as [ArcaOS](https://www.arcanoae.com/shop/arcaos-5-personal-edition/). It's proprietary with no free ISOs, unfortunately.
Did you ever run Minix prior to that?  It worked well but was copy writed.
For me that's the single greatest thing about Linux -- it's ability to turn "too old to work" into "runs good as new" for free.  If it breaks, not that it usually does anymore, just reinstall.
My first exposure to Unix was in 1984 on the SUN OS.  Prior to that to get portability Brian Kernigan and P.J. Plauger wrote "Software Tools" in 1976.  The book's programs are written in a C like dialect called Rat4 for Rational Fortran.  Every OS back then had a good Fortran IV compiler.  The syntax for Rat4 is C without structures. A lot of scientific software was written in Rat4.  It even worked well on IBM 370s, VAX, SUNs. 

After SUN's hit the market Rat4 died.  Software Tools writes Rat4 versions of diff, echo, join, ed, macro, ar, tr, various sorts,  cat and the Rat4 preprocessor.
> but the xorg server (xfree or whatever the name was back then) did not work

xorg.conf was the best text-based adventure game of those times :D. it even got dual monitors support. heck! I remember playing it with a friend on LAN.
Try Lubuntu
Every single floppy disk we had in the house.... recycled every "free AOL trial" floppy we had been sent with a piece of tape over the rw protect tab.  And downloaded overnight because after 9pm, connected time didn't count against you with Netcom.

Yup.... Slackware 2.0(?) with kernel 1.2.13 was my first.
This is me too.

It didn't stay installed for long either, I was a bit over my head.. but man I spent about two weeks downloading disk images after school!
1994 is definitely before my era. I would have had to use it in school already. That may have been possible but ... I don't recall anyone else in class using Linux.

Even in 2004 it was more rare.

Nowadays it's not quite as uncommon, even though it is still a niche thing.
Slackware in 1994 on a work PC for me.  Easier working with all the NIX stuff we were doing than having to use Windows 3.11 or NT telnet sessions.
You aren't alone
Me as well.  It was a magical time ;)
I only bought one boxed version of Linux.

It was secondhand Caldera OpenLinux, by a company that later became The SCO Group and decided Linux was infringing their Unix copyrights (which it turns out they didn‚Äôt own either). SCO v IBM was finally resolved in 2021. I enjoyed owning a piece of evidence.
I still have nightmares about handshake sounds. Brrrr.
Yea it may have just been suse looking back.  This was circa 2005
Lol! Corel was my first attempt.found it dumpster diving.. total failure, discovered Debian and FreeBSD. ;)
Corel Linux was really nice for the time.
Wait until Win11!!!
OS/2 was amazing! I wish it had succeeded. 

BTW: has anyone ported the OS/2 desktop to Linux?
That makes... Literally no sense. Linux drivers not utilizing your hardware doesn't magically make it stop erroring out. Typically Linux drivers do about the same thing as windows drivers (in most cases). Also the "good chance" your hardware is bad if windows shits itself is like 20% of the time (in my personal experience). Also Linux doesn't kernel panic when an app is accessing memory it  shouldn't, it just stops it from accessing the memory (a very common blue screen error).
It makes no sense since the gaming performance are still pretty much the same. Just accept that Microsoft can make mistakes too my bro.

Also, in my experience, Windows Vista was much less unstable
Windows 10 doesn't run well in non SSD. Linux does. It isn't because of a lack of utilisation. It's because of bloat.
Yeah, it was a thing back then. Internet connections were dodgy and expensive, it cost less to buy the damn thing and it also had a book.
Sort of. I didn't technically buy "Linux", I bought a box with the disks and a manual as souldrone already explained. Honestly, I'm not even sure downloading would have been an option at dial-up speeds. In '98 I would have been using a 19.2K modem. Higher speed modems were out but hideously expense and not often supported by the sending modem.
Yum was a necessity and it took them a while to figure out how to essentially steal apt-get, which was around way before yum.
Yeah imagine installing without DVDs. Or even CDs üòâ
Evaluation Linux versions?
He didn't try to persuade me to use Linux back then. The interest came from myself
Lensmoor is still up and pretty solid. He‚Äôs been running it since at least the early 90s if I recall right. 

And last I checked, if you donate one time, you get permanently protected from character deletion :)

Don‚Äôt know how many still play however.
A lot of people turn up their nose at Compiz and desktop effects nowadays.  But I kinda feel like wobbly windows and the desktop cube were *the* killer feature of desktop Linux in 2006-2009.  It turned so many heads, in my experience
Those were good times, I remember Looking Glass WM was meant to be the future, Compiz broke all the barriers, 3D acceleration started to be a thing, WiFi started working without much hassle... imma search for a time machine to go back xD
I started using ubuhtu because I liked the App Store in Ubuntu
Not the easiest, but not he hardest. Huge shout out to u/mrchromebox for it.
I think my first was Ubuntu? Can't remember if it was Ubuntu or Mint first.

Dell swore Windows was broken, Microsoft said the laptop was broken... Linux said yeah, I can run that.
Good question.  I may.  I kept the box.  I just don't know where I put it üòï
Incidentally that PC I was talking about is a Dell as well (Inspiron N411Z to be precise), has been my trusted work companion with openSUSE and Fedora until one hinge broke; now it's my dad's desktop.
Thank you kind stranger. Rough times come and go, but I stick to linux forever. It's perfect for my needs :).
> Are you Russian?

Why, yes.

> You're dodging sanctions anyway

Windows updates being blocked will likely not be part of the sanctions. It is a *potential* future decision by Microsoft (so far they have only disabled new downloads).

> Fedora or other distributions may not be able to update either

That is not how it works. 

First of all, there was already a proposition by some genius to deny access to Fedora for Russian citizens. It was shot down by Fedora council members. What Fedora can do is restrict access to Russian *contributors*, as they have done for Iranian ones(because their hands were basically tied by RedHat) should a full embargo happen, usage bans don't even enter the equation. 

Secondly, Microsoft directly controls Windows update distribution. Fedora doesn't. Anyone can set up a mirror and we already have several mirrors.

The situations are honestly not even comparable.
Ubuntu will be able to update as they are based in the UK. Fedora and Red Hat is American so they must obey US sanctions. For example you can‚Äôt use Fedora in Syria or North Korea.
never tried minix, I run unix and xenix prior to linux.
yes, while it wasn't the reason behind my decision to stay with linux, it had a lot to do with my love for foss and networking. I stick with stable distros for most my machines, where i don't have to touch anything for years so breakage isn't something i deal with, i have a laptop with more bleeding/leading edge stuff but since its not really a machine i do anything important with i don't count it.
Dennis writes a lot about the origins of Unix, portability, and the creation of C. https://www.bell-labs.com/usr/dmr/www/

I forgot all about ratfor!
What are its advantages?
Slackware 2.1 with CD file dates of January 16, 1995, for the floppies. Kernel versions are 1.0 (May 7 ,1994) and 1.1 (Fall 1994).  Lots of patch files. There is a tar ball for ilnux-1.0.9.tar.gz with a date of October 27, 1994.  I got all this from book insert.

Good times.
Slackware on Walnut Creed CDROM
Hah! I alpha and beta tested NT 4 in an ISP environment. We found and provided solutions to MS for a number of network related issues. That gained the company free licensing to everything MS made until we sold off.
Caldera came directly from the Corel Linux - which bye the bye, was a truly awesome, groundbreaking, and amazingly elegant interface.  I still mourn the sad destiny of that distro.
SCO got a raw deal. IBM‚Äôs lawyers were better than SCO. SCO stood for Santa Cruz Operation, for those that didn‚Äôt know. They were the only non-vertical SysV vendor for many years. Though, I personally prefer Solaris.
Win11 will be a distant whistle in the night as the Windows train rolls on without me. I jumped off that thing a long time ago and never regretted it for a moment.
I still have the +/- two dozen floppy disks that Warp shipped on, as well as the disks for the Lotus suit. I've been meaning to get a floppy drive to see if I can get OS/2 to run in a VM, but life keeps getting in the way.  : (
In my case nouveau silently freezes the whole PC...

well network might work but keyboard is dead locked too

The card is stated as fully reverse engineered :-)))
Windows has a hybrid kernel, so a crash of a wifi/sound/gpu driver won't crash the whole system
A couple of months ago was running win10 vm in qemu right and straight from the 7200rpm/3.5" 1TB WD Blue.  Vm (4cores/6GB-ram) was used as a web/YT browsing instance  for an android RDP client connected to living room TV. 

I can't say it worked really butter smooth and fast. But it worked with bearable boot times. Host was Ubuntu 22.04 32GB of RAM and everything was using the same HDD. 

To my surprise overall slowness was in todays limits. The whole system boot time was about 3 minutes, the host has SSD and it takes about 1.5 minute to boot Win10 with the similar VM. The most annoying thing was the QXL driver. It crashes from time to time corrupting picture, so it was necessary to close the RDP session and start it again.
yeah, i posted separately, but i started with slackware, downloading over modem to put on floppies to in turn boot from. then found mandrake CDs! and Corel at some point, i liked Corel linux. the only CD i have left is mandrake though. but i have a crap load of floppies all over the place
Around 199x and early 20xx, there were various specialized Linux magazines giving free copies of Linux distributions (checking the image linked on my comment is obvious enough). 

The point of my comment was for u/FryBoyter that mentioned his friend original box, so I linked my "evaluation version without support".
The top case on my old laptop broke a year ago, and I just bought a new old-stock one for $18 online and installed it. There are so many parts now that are cheap, why not? My wife's got upgraded from 768p to 900p on the screen.
>(because their hands were basically tied by RedHat)

I'd just like to point out that this isnt entirely correct.  This was because cryptography was at that point considered arms and munitions , and providing arms and munitions to countries on the embargoed list required a lengthy process which required approval (and likely $$) on a per instance basis, which was not plausible for a free download.
I only used Minix 1.5, I think, which was released in 1991, though I have the textbook for Minix 1.0.   See Wikipedia.  

Now Minix 3.0 runs in ALL post 2015 Intel chipsets.   (Probably the OS with the most seats in the world).
Very stable, lightweight. Everything just works!
Don't use nouveau then... The Nvidia drivers, while proprietary, do Work Most of the time.
Yeah when I heard Mandrake was an easier RedHat I was sold and had to get it. Because Red Hat needed some easier.
Ah, I see
I'm not entirely sure whether or not Fedora is accessible to download and use to countries on the embargoed list and it's not like I can get a VPN to set my location to Tehran.

But [I do know that the access was explicitly restricted to contributors](https://ahmadhaghighi.com/blog/2021/us-restricted-free-software/), down to removing every mention they ever helped with the project.

Meanwhile, in this [proposal thread](https://discussion.fedoraproject.org/t/fedora-council-tickets-ticket-390-restrict-access-to-russian/37280) one of the Fedora Council members states:

> As you may already know, we currently cannot accept contributions from (for example) contributors in Iran. Some have rightfully pointed out that this is antithetical to the spirit of open source software. We are working with Red Hat Legal to get an exception similar to what GitHub was able to achieve.

Which seems like a pretty clear-cut "our hands are tied" to me.

As for encryption, this is honestly pretty muddy, [Fedora Legal Export page](https://fedoraproject.org/wiki/Legal:Export) has the now-infamous prohibitions, but it also states in the very end:

> Fedora software in source code and binary code form are publicly available and are not subject to the EAR in accordance with ¬ß742.15(b). 

So it's all a bit muddy.
Could I run windows programs in it? The problem I have with Linux is that very few programs are crossplatform, and if I want to use my computer for work related stuff it just doesn't work because all of my work related stuff is for windows.

I could use Libreoffice as a replacement for Excel, but it would just make things weird, everyone else at work would be using Excel, maybe it'd have to use power pivot and since it's an extension I wouldn't be able to make to work.

I've tried getting PowerBI to work with wine but wine seems more like a very far fetched botch than a reliable tool.
üòÉ blob works days and days long.

But there are dangerous waters ahead - the legacy blob will never support Wayland üòØ and Vulkan of course.
What GPU do you have that you need the 340/390 branch?

470 and 515 Support wayland Out of the Box.
very old 220 :-) When Win10 passes away  it will be complete upgrade to a iGPU/APU, but not a discrete one.
Are you using a Laptop or can you Upgrade the Card?
At this moment it will be unfortunate investment in a discrete video again.

If the US suffers from the GPU mayhem you could imagine what is going on in worse worlds.  Somewhere outta there the GT210 is sold for about 50$ and GT730/1030 climbing to 60--100$
You could try to get your hands on a GTX 750ti. Being a Maxwell Card, it's still supported by the newest 515 Driver and you can get them for under 50 bucks
Well it depends on the user needs.  I am not gaming so it's not that urgent for me to buy an old card as well. I don't know if 750ti supports all the modern codecs h265&VP9&AV1, but I suspect that video decoder isn't that fresh in those cards.  So they might  lose to any fresh Celeron/Pentium in terms of daily computing and YT streaming.
You complain about only having a Card that needs the 340 and when I recommend you a semi recent Card you Shit on it for Not having the latest and greatest Decoder?


I Just wanted to Help, you know
üòÆ
You have some security enhancements, but use zen instead of hardened kernel, may I ask why?
Some people believe that hardened kernels are less secure than normal up-to-date ones because they're older. I'd guess it's that.
It's built for pentesting, not for being secure.
When pentesting, you don't want to find yourself suddenly on the receiving end...
Why bother setting up apparmor and firewall then? Those seem like great security features, would be good idea to finish up with proper kernel an soon as I doubt that pentesting platform would benefit from a miracle of increased performance
I have an Nvidia RTX 2060. Manjaro. HTC Vive. 

The audio setting doesn't work properly so you need to use the OS programs to mirror the audio.

The hardware firmware can not be updates in Linux.

Motion smoothing doesn't work on Linux.

I couldn't get The Lab, and Half life Alyx enjoyable on Linux. They stutter a lot. 
 Even the home screen stutters a lot (it's an open issue on their github). 

I can tolerate some hickups in Beat Saber. It's not too bad (but it's worse than Windows).

If I need to play smoothly (without getting motion sickness) I boot into Windows, especially if I have a guest. But since I mostly play Beat Saber, I just tolerate it and play in Linux.
https://gitlab.com/vr-on-linux/VR-on-Linux

https://www.reddit.com/r/virtualreality_linux/
I have been using a valve index for almost 2 years now entirely on Linux.  Issues  do crop up occasionally and some games are kind of janky. Most however seem fine and I mostly have no problems.

I am able to update firmware without issue. Only complaint is no passthrough camera support. 

I run amd cpu/gpu using Ubuntu.


Oh and make sure to use X, Wayland doesn't work yet.

Edit: in excited to see what comes out of https://simulavr.com
SteamVR has a few bugs and some features are completely missing on Linux - and Valve doesn't seem too interested in fixing it anymore. That said, performance is good enough and it generally does work fine; I definitely still have lots of fun with it.

So I'd recommend you to see for yourself if it's good enough for you, but keep that Windows partition around as a fallback. It's not unlikely you'll want to go back to it for VR.
Depending on if you're bit by https://github.com/ValveSoftware/SteamVR-for-Linux/issues/334 either completely unusable or alright.
I have *never* gotten VR working properly on Linux, across five distros and a GPU/CPU upgrade. I'm on a 6700 XT / 3800 X now. 

At best I've gotten into Steam Home, but I've never gotten the overlay working, and tracking was much less stable than on Windows. But I haven't even gotten that far in months, generally I either get nothing at all from the headset, the VR view pops up on my monitor instead of the headset, or it's hugely unstable and crashes immediately. I've tried a wide variety of fixes and workarounds with little to no success.

VR and anti-cheat are the two things keeping my Windows install around, and the latter is rapidly ceasing to be an issue. I think the only non-VR game I've played in the last 5 months that wouldn't work on Linux was Due Process, and they're working on it.
I had the same question when I wanted to switch. My situation is a little more tough. I own a Oculus headset, and it does not support Linux at all.

At first, what I did is switch to a dual boot system, and called it a day. Until a few days ago I figured out how to make a VM that has control over my GPU (which means near native performance). It's called r/VFIO if you're interested.
I play VR games on Linux (Vive) and it‚Äòs great! Both native & Proton.

Make sure the steam-devices package is installed (for hardware support) and that‚Äòs all! Just install SteamVR as on Windows & do your roomsetup.

Never tried Beatsabber, but I played Half-Life: Alyx, worked perfectly.
It works perfectly for me, I have a valve index
I've not had any issues running VR Proton games so far. I haven't played all that many to be honest.
It's janky at best. Beat Saber is acceptable but a lot of other games just aren't enjoyable. I also have a continuous problem with the boundaries constantly moving which is annoying. I'd stick to windows for VR right now.
VR is the same as it will ever be. Works fine if your headset has the Linux drivers. A bunch of oculus headset have community made drivers, but you mentioned you had Valve index which supports Linux well enough.
What a coincidence, earlier I decided to try my index on Linux to see how VR had come along. With Flatpak steam I was able to install steam VR, but couldn‚Äôt get any games or even the tutorial to launch (just stuck in the VR loading screen). I know some people have gotten it working but I haven‚Äôt the 2 times I‚Äôve tried
I can get vr chat to run just fine but you won't see video links so you couldn't watch movies if that's your thing
I haven‚Äôt played a lot of VR games, but those I tried ran perfectly if not even better on Linux.
In my opinion, Every Linux distribution(except debians) do a better job at everything then Windows
lol, lmao
Have been using the Valve Index, on 5700xt. For me audio was just a matter of switching output in pavucontrol, as the audio did not switch automatically.

Index firmware can be updated in linux, but base station power control does not work AFAIK. So everything is not ideal even with that hardware.

Generally AMD performance seems to be better than NVIDIA (IIRC async reprojection or something did not work on nvidia), but some games refuse to even start for me. (The lab for example, uses proton). But most games work on proton fine, and the native alyx version works great, aside from some mod incompabilites.

Also SteamVR has more bugs on linux, some affect more than others. For example last time I tested the overlay worked about 50% of the time. Restarting SteamVR usually fixed it though.
Thank you I have subscribed.
> Valve doesn't seem too interested in fixing it anymore.

Rumor mill says that Valve is currently focusing Steam Deck and then plans to make a stand-alone headset running SteamOS but that's why they are currently concentrating on non-VR work. Obviously there is no way to verify that.
Are you using gnome wayland? If so, that's why it doesn't work - gnome doesn't support the needed functionality to let programs like SteamVR take over the headset.
I did the same thing and it's awesome. My VM starts on boot and all I have to do is put the headset on my head and bam, I'm in VR. No rebooting, no starting a VM.

It's a bit of a pain to set up at first and you need to be a bit lucky with the hardware that you have. But once I got it running, I haven't encountered any problems.
I'm running SteamVR just fine with the Valve Index but I initially installed the [Steam app directly](https://cdn.akamai.steamstatic.com/client/installer/steam.deb) using its `.deb` file. My guess is that the Flatpak sandboxing is interfering. Whoever made the Flatpak file may not have tested VR functionality.
Pretty sure i read a recent proton update were this was fixed.
Except oculus headsets
I've tried it on multiple DEs/WMs, both Wayland and X11.
Oh you're starting it at boot? Interesting. What's your setup? I have a dGPU/iGPU system, and both of my monitors are connected to my dGPU, so I'm basically running a single GPU setup where if I want windows it kills X and logs me out. Kinda annoying, but imo a bit better than dual boot since you control the entire OS, and  no need to worry about Windows Boot Manager killing GRUB (or in my case once I kill my dual boot and switch to permanent Linux, EFIstub).
ALVR is making the quest quite usable.
I have 2 dGPUs and I use virt-manager for my Windows VM. I passed through a SSD, the good dGPU and an USB controller to the VM. If I want to use Windows, I have to switch my HDMI input on my monitor or just put on my VR Headset and control the PC through the Oculus software.

I don't quite understand your setup though, why do you have to kill X and log out to use Windows?
Because I'm using a single GPU setup and therefore forcefully passthrough'ing the GPU connected/used by X crashes/kills it unfortunately.
I didn't even know that was possible. I pass my GPU through as a kernel parameter. And as far as I know, if I want to use it again, I'd need to reboot my PC and remove the kernel parameter on boot. 

Are you able to regain back control of the GPU, after you forcefully passed it through to windows?
Yup! As soon as windows shuts down.
Linux Mint is already the perfect Ubuntu respin.

Unfortunately they axed the KDE edition.
Respectfully, your thumbnail sucks
I know , it could be worse though

I think that it would be better if i had bigger text and a specific area that would be opaque with a dark colored background where i would put the text
There are better ways to say it IMO.
Keep the good work. You'll find inspiration in other Youtubers to make better thumbails. All in all, what matters is the quality of the video.
The algorithm says otherwise
we need to feed the algorithm ...
>with LVM, with BTRFS as filesystem and with separated /home partition

Why don't you use btrfs subvolumes?

According to https://github.com/koalaman/shellcheck, there is still some room for improvement in the shell scripts.
Lots of problems in this bash, the first one I noticed is:

`cat /proc/mounts | grep efivar &> /dev/null`

You don‚Äôt need to use `cat` and you don‚Äôt need to use `&>`.

What you want is `grep -q efivar /proc/mounts`.

(There‚Äôs a few more instances of similar ‚Äúcat abuse‚Äù).

Your using echo for error messages, it would be nice if they were printed to STDERR.

I‚Äôm pretty sure you can check for `$BASH` being set to check the shell you‚Äôre running under.

When you `[[ ! -e ‚Äú${boot_partition}‚Äù ]]` I think there is a test for block devices, off the top of my head I think it is `-b`. I think you also used `-e` to check for the existence of a file at some point (`chroot.sh` maybe?), in which case `-f` (exists and is a file) would be better suited; likewise you can use `-d` to check that a path exists and is a directory.
> Why don't you use btrfs subvolumes?

They are used in the script, but in this way you can reinstall (or change) you distro easily, without touching your files.
I had the same setup (without a separated /home partition) on my desktop pc and weeks ago it broke after an update. It just went into dracut debug shell, i tried a lot of different solutions but in the end I had to format everything.
That was the moment when the idea of this script was born!

> According to https://github.com/koalaman/shellcheck, there is still some room for improvement in the shell scripts.

That's neat, I didn't know something like that existed, thank you!

Edit: Sorry for poor formatting, i'm from mobile!
For sure that there are lots of problems, as my first time approaching bash scripting, I'm pretty sure this is far from being something decent üòÖ  
But thank you for pointing them out, I'll take a look on them and see what I can fix!
Wow I can‚Äôt believe this is finally happening!
Is there a way to influence what CMYK values are used? Without that it hardly matters ‚Äì if all was needed was automatic conversions from RGB to CMYK then print shops would not insist on CMYK input materials.  


What GIMP really needs is a way to work on CMYK image, not just save an RGB image as CMYK. But this is a first step, I guess‚Ä¶
still waiting CMYK on inkscape tho
This is an awesome update! Glad to see this feature finally being included. :)
What is the use of this export type (minus being cool)
/r/usernamechecksout for CmykStudent
Finallyüëå
FFS, get GIMP 3 released already!
This is awesome news. But I bet there are many people who will always have that one obscure feature that is stopping them from switching
ive seen lots of people really really despise gimp as an editor, hopefully with things like this, a UI update and non-destructive editing it can finally become mainstream (or at least more liked)
Yes, currently in GIMP you can enable soft-proofing and use the CMYK color selector to pick and change CMYK values. There's a pending GSOC merge request to further improve on this (e.g. make it use the selected softproof profile via a new API rather than use the default profile set in Preferences, switch it to using babl directly, and fix a percentage glitch that occurs when a profile is set). 

Combined with now being able to properly load and export CMYK files (TIFF, JPG, and PSD currently, with other formats liked EPS and AI planned next), a late-binding CMYK workflow should be possible in 2.99.12.
Unfortunately, the world of print is dying. Too bad it took this long. It would have been useful 20yrs ago.
Printing
Some people create content for print, not digital display.
I wish GIMP has NDE, but that will has to wait until 3.2. That is not a obscure feature. A show-stopper for many actually. If GIMP has NDE by now, I would switch, and if Krita has better selection tool (foreground extraction tool, and one that I failed to implement though it's close and unable to build Krita anymore.), I would be 100% sastisfied and would forget about GIMP entirely.
Gimp is already mainstream. If it wasn't so popular you wouldn't have so many people complaining about it
How much do you think would be left to make Gimp usable for professional printing?
People aren't printing anymore ? What are they doing now ? Handing out tablets ?
I hate printers
It's mainstream in the larger sense, but on jobs pertaining to digital illustration or photography, it's not used all that much. As far as popularity goes, it was much more popular some years ago, but waned as standards changed, and GIMP is behind there, but still usable for many though there are already other options that are more favorable for many.
It depends on what "professional printing" means to you. One of my hopes for this project is that it will get people thinking "You can use Gimp for DTP/Printing now!", which will lead to more [specific feature requests](https://gitlab.gnome.org/GNOME/gimp/-/issues) to clarify what professionals really need. Right now it seems to be more general "needs CMYK!" requests, which makes sense.

If you have specific needs or requests for printing, I'd be interested in hearing them! A rough plan for the remainder of the GSOC project is:

* Add import/export support for more formats (EPS, AI, PDF)
* Create a dockable dialogue to make the proofing process more user friendly
* Fix and improve color management and color space related features, and any bugs people notice as they start using the new CMYK features

I think next steps after that would be looking into CMYK channels and spot colors.
A lot of time and money for work

And a lot of professionals to convince it is better (economically) to ditch current tools that provenly work for them and use GIMP.

It is pointless. GIMP is niche product , trailing years behind what industry uses . There are better products for image manipulation out there.
I used to work in desktop publishing and print advertising has really decreased over the years. Of course there is still printing, but not like it used to be. I wanted to use gimp back in the day, but CMYK was always an issue.
All printers are evil.
Show me on the doll where cups hurt you
What about serigraphers?
Regarding the last step (fixing and improving color management), I'm wondering what you think of how Krita implemented it from a user (not a developer) perspective?

I think it is one step ahead in the right direction by having a single "color conversion" operation that does everything (bit depth, color model & profile/colorspace selection) in the same dialog, although the whole process where you need to import ICC profiles first does have a couple of quirks.

Right now GIMP lets you convert to a different profile/colorspace and change bit depth as two separate operations, where they really should be done together -- there's a couple of corner cases where if you do them separately you end up with a lot of banding. Plus there's the weird "Linear/Perceptual gamma (sRGB)" choice which I've never quite understood : what if you have an ICC profile that specifies a linear tone curve and you select "perceptual gamma"? What happens if you have a non-sRGB tone curve?
Thanks for your great work! I see there's still a long way to go, but it seems everything's is progressing very nicely.
What a pessimistic view. That way nothing will ever be achieved. Same could have been said about other open source projects that turned out to be quite good.
I totally understand what you're saying my uncle sold his small printing business about 5 years ago because  revenue was going down.
I keep an axe near mine for surprises
That's easy, everywhere
GIMP is very old product and is stalled in its roadmap while others drive on left and right side of it forward. I have used GIMP from time to time since more than a decade, but it never catched on and just about noone i know uses it on any platform.

There are better opensource products for tasks out there - like darktable for single/group photo manipulation, krita for others, and so on.

Ever wondered why businesses stick with Adobe products? GIMP does not even work in full CMYK mode and full non-destructive mode.

Not every open source product is better than commercialy available.
Given the amount of positive feedback I've received since I started this project, I'll have to disagree that no one uses GIMP. :)

The great thing about open source is that there's a lot of different options, so everyone can use what works best for them. My hope is that this project will improve the experience for those who use GIMP, and enable them to do more with the software.
There are other options other than GIMP like Affinity, and so. GIMP is getting better, but I don't have hope in it.
If you install joe (an open source editor), it also comes with jstar, which is a wordstar clone.
Short diversion. In circa 1998¬±1 I was using this software on Linux with KDE 1.1. 

Corel had just bought into Linux as a desktop because someone had decided: Microsoft Office was the reason people used Windows. So if they could couple their office products to a Linux distro, they'd not only earn on the office products, but also on the OS.  

Corel had hired a bunch of people to put together their distro, based on KDE. I was loosely attached to KDE as a minor developer at the time, and got to see a lot of the drama. Basically, they hired a bunch of proprietary developers who had no idea how to interact with an open source community, who showed up on the mailing list with prescriptive instructions and patches that hadn't been approved by the community. From the community's perspective, it looks like Corel was trying to hijack the development process, and resistance got pretty strong. KDE has always been a meritocracy, and someone showing up with top-down vision trying to implement something without first earning the trust of the community wasn't going to fly. 

Corel Linux completely flopped. And WordPerfect for Linux died with it, more or less. StarOffice became open source as OpenOffice (and later forked to LibreOffice). But I can't help but wonder how things would have gone differently if they'd hired from the open source community instead.
What does it do?
I for one am glad to see that people take a look at compatibility with older software as well.

For one, historical reasons, second, you do sometimes come across things that only work correctly on some older software..
I still have the official native Linux version of WordPerfect 8.1 on my Fedora system. I got it years ago as part of the Corel Linux OS Deluxe Edition boxed set I bought for $10 at CompUSA(!). It [still runs](https://i.imgur.com/GueZxRJ.png), though starting in Fedora 36 I noticed I had to do this (as root) to load the old 32-bit libc libraries that WP8.1 needs:

    ldconfig -c old
That's great! I can finally read all of the high school term papers, awkward diary entries, and regrettable poetry I typed into WordPerfect 5.1 on my mom's old 286 dos machine! (Yes, I know I could before, as you can import WordPerfect files into LibreOffice pretty well and there are many ways to run WordPerfect 5.1 for DOS in emulation, but this gives me an excuse to.)
The link to the ISO file does not work.
FWIW, what's being shown off isn't a useful piece of software (to most people, at least), but instead a porting feat
As noted in these comments above, WordPerfect for Linux still runs on modern distros. Here are the full instructions, in case you are interested: 

[https://xwp8users.com/](https://xwp8users.com/)
Use [Basser Lout](https://savannah.nongnu.org/projects/lout)
How does WordPerfect compare to Lotus 1-2-3?  Which one is better?
that's pretty neat. TIL
I liked Corel Linux, if I remember correctly. I was a KDE user until about 2001.
WordPerfect is an old word processor. Probably useless to most.
This is the perfect use case for Flatpak, Appimage, and Snap.
It works, you just need to jump through a couple of links to get to it.
Lotus 1-2-3 was the #1 spreadsheet program at the same time Wordperfect was the #1 word processor, other than that they do not compare at all since they do completely different things.
WordPerfect was the dominant desktop word processor prior to Microsoft Word, which ultimately won over market share as it ran better on Windows at the time.

Before Microsoft Word took over from WordPerfect, WordPerfect had taken over from WordStar as the dominant desktop word processor.

WordPerfect continued putting out releases and is still supported today, initially bought out by Novell and then by Corel.

When using MS Word I frequently find myself just wishing I could "see the codes" as Wordperfect let you view and edit the internal markup that made up the document.
Happy Cake Day

>WordPerfect is an old word processor. Probably useless to most.

Yet it worked so well.  Fast (written in assembly language as I recall), stable, well thought out, clean screen, reveal codes, printer drivers.

Windows (and some poor decisions at SSI) killed it pretty much--guess it is still on life support out there.
What do you mean? [I do all my spreadsheeting within PowerPoint.](https://www.youtube.com/watch?v=1SNxaJlicEU)
>  "see the codes"

reveal codes
Use LaTeX
Yeah, I was thinking the same thing. I feel if you have the patience to read old books on CLI based word processors unless you really want to use a retro computer like George RR Martin. (of which I like his choice of Wordstar 4, he uses the DOS version, but there's a CP/M version and you can still build a CP/M machine in a cave with a box of scraps)


I feel if you want to use a CLI Word Processor, just learn a markup and compile to PDF.
[deleted]
Yep Latex is kind of the same idea applied to typesetting
For slightly less informal writing that still needs to look good printed or online, the various markup languages, like markdown, are useful.
Write one CSS for markdown and you‚Äôre good to go forever. If your layout gets super complex, use Scribus.
Love Scribus! I don't know CSS, so i use pandoc to print, etc my markdown. For some stuff, I use asciidoc, also handled by pandoc.
You can use `-x .git` as an argument to diff to make it ignore the .git directory.

But I'm having a hard time seeing a proper use-case for this as a specific feature. If you have two repositories of *different* projects, I can't imagine any reason to compare the contents, and if they're two repositories of the *same* project, you can connect them as secondary remotes and then just use regular `git diff`.

The problem with using regular diff (as opposed to git diff) on two repositories is that there's more than just the .git dir to worry about. You may also have object files and other temporaries lying around, making the whole comparison a study in frustration. Honestly, the fact that you're asking about *optimizing* this sort of thing makes me wonder if there isn't something inherently broken in your current workflow.

But in any case, `diff -x .git` may help for now.
[Meld](https://meldmerge.org/).

Amazing for diffing two directories, instead of just doing it file by file.
I don't know about those features specifically, but lately I've been enjoying [riff](https://github.com/walles/riff), and some other options I can think of are

- delta 
- diff-so-fancy
- colordiff
- difftastic
Did anyone ever complain about the speed of grep? I fail to see what is "modern" about that tool (ripgrep)? Ignoring the contents of .git by default seems like a bug to me, if I want that there is an option to do exactly that.
Maybe delta and exa can help you achieve sone of your interests. They are both rust tools.
`git diff` ignores files listed in `.gitignore`, why not use that?
    tree -fi dir1 | grep -v \.git > dir1.txt
    tree -fi dir2 | grep -v \.git > dir2.txt
    sdiff -s dir1 dir2
`git difftool --no-prompt --extcmd='bat --diff'`
meld is where its at
riff at first glance seems to be similar to wdiff, which I already use, but I'll check it out. I already use delta, but that if it's what I think it is, it's a pager for git? It's cool and it makes git output easier to read, but I suppose you mean something different. I already know about colordiff, but as think that might be redundant as diff does colour output nowadays.

Never head of diff-so-fancy or difftastic, so I'll check them out. Cheers.
Disclosure: I'm the author of ripgrep.

> Did anyone ever complain about the speed of grep?

Uh, yeah, all the time. :-) And the frequency of complaints depends a lot on _which_ grep you're referring to. Some greps are faster than others. And also the types of searches you're doing. For example, using GNU grep, a simple `grep -r foo ./` in a big repository is not going to use parallelism and it's likely going to search a whole bunch of crap whose results you don't care about. You can make it use parallelism (like with `parallel` or `xargs`). You can also add filters to include or exclude the files you don't want. But ripgrep can do the former automatically and also make a pretty damn good guess about the latter in the vast majority of cases. And the automatic filtering usually speeds up the search too! So it's a double win.

`ack` started this tradition decades ago. So, not exactly a new development.

> I fail to see what is "modern" about that tool (ripgrep)?

It's a communication failure. Notice that "modern" appears precisely zero times in ripgrep's README. In general, the word "modern" in this context has no precise definition. It usually refers to an aesthetic user experience that exists in contrast to tooling that has been around for decades and hasn't undergone any significant UX changes. (In the name of portability and backcompat, which are worthy goals.)

> Ignoring the contents of .git by default seems like a bug to me, if I want that there is an option to do exactly that.

You are misusing the term "bug" here. Instead of using it to refer to "unintended behavior by a program," you're using it to refer to "behavior you consider undesirable." That ripgrep does "smart" filtering by default is, indeed, a feature and a not a bug. More than that, as someone who talks to ripgrep users a lot, having that smart filtering enabled by default is one of the most important features driving the adoption of ripgrep. Similarly for `ack` and The Silver Searcher (both released long before ripgrep).

ripgrep doesn't just ignore `.git` by default. It ignores all hidden files/directories, respects your `.gitignore` and ignores binary files (like GNU grep's `-I/--binary-files=without-match` option) by default. Using `-uuu` disables all of that and causes ripgrep to search the same stuff that a `grep -r` will.

It is my view that this default is part of what makes ripgrep "modern" in the minds of most folks using "modern" as ad adjective to describe tools like ripgrep. It isn't the _only_ thing, but is definitely a big component of it. Effectively, "modern" is an imprecise way of saying, "a user experience that fits my use cases more commonly right out of the box instead of requiring me to configure it."
Do you mean delta the git pager? I use that already but don't see the relevance. Maybe you mean something else? I already use exa as an ls replacement. I add this wrapper script so that it uses the same switches as ls: https://gist.github.com/eggbean/74db77c4f6404dd1f975bd6f048b86f8
If I want to compare two different directories, as I mentioned, and I want to ignore `.git`, I'm comparing two different git repositories. How do I do that with `git diff`?
Thank you for the thorough explanation. I myself prefer the programs to do exactly what I am telling it to do, nothing else. I believe there is a use case for \`ripgrep\`, probably many users will find a use case for your program. But again, it is not "behavior I consider undesirable", it is just strange, today this is .git, in future it will be something else, grep will just continue to work. I expect to grep everything, if I want to skip something there is an option. What is next, modern \`rm -rf\` that will ask if I am sure if I want to remove pictures from my last vacation? That is just strange to me.
I'm sorry, I misread your question. 

Even then, you could just add your two checked out git repositories as remotes for a third repository in which you use `git diff`.
> I myself prefer the programs to do exactly what I am telling it to do, nothing else.

You can't take a hardline stance here. It's always one of degree. Not even grep "does what you tell it to do," for example. If there's binary data in a file, for example, grep will tell you about the match, but it won't print the match and it won't tell you about all of the matches. You have to opt into that with `-a/--text`.

> What is next, modern `rm -rf` that will ask if I am sure if I want to remove pictures from my last vacation? That is just strange to me.

That's ridiculous. ripgrep's default filtering logic isn't arbitrary. Does there exist a common way to express "remove pictures from my last vacation" that people are doing over and over again? Nope. Nope. Nope. So, _really_ bad comparison.

I had several little `grep` wrapper scripts for well over a decade before I wrote ripgrep. Now they're all gone, because ripgrep just does what those things did, but does it automatically. From what I can tell, this has been the same experience for a whole bunch of other people as well. From spring chickens to greybeards. From systems programmers, to security researchers, to UI developers.

ripgrep takes _existing_ conventions and leverages them to do what you usually want, especially when performing code search.

You know what's great though? ripgrep never was, is or will be a complete grep replacement. So you can keep using grep! See: https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md#posix4ever

Or, you can `alias rg="rg -uuu"` and get a tool that "does exactly what you tell it to do." And it will still run circles around grep because most grep's I know of don't use parallelism. They were written in an era that predated the mass availability of multi-threading and nobody has had the gumption to follow through on a patch for them. (IIRC, there have been attempts to do it for GNU grep, but they've never been merged.)
On a checkout of the `chromium` repo:

    $ time rg Openbox | wc -l
    8

    real    0.285
    user    1.567
    sys     1.649
    maxmem  83 MB
    faults  0

    $ time rg -uuu Openbox | wc -l
    8

    real    2.520
    user    2.657
    sys     2.999
    maxmem  78 MB
    faults  0

    $ time grep -r Openbox ./ | wc -l
    8

    real    5.485
    user    3.576
    sys     1.882
    maxmem  11 MB
    faults  0
Thanks, but I was wondering if there was a quicker and simpler alternative to diff and this doesn't really fit the bill. Cheers.
Thanks, sorry, I didn't mean to bash your program, and I agree, \`rm -rf\` was a bad comparison. But I still have issues with your defaults, i.e. hidden files/dirs, isn't that just a convention for apps that display files, there is nothing special about hidden files. If I am grepping, of course, I want to include them, I don't want the special option for that.

It is just too smart for me, you know exactly what it does so it works for you, but such and similar option will confuse me, I would not know what to expect. There is no mystery with grep, I am using it for 20+ years, without aliases, wrappers, just plain grep, it will work for me for another 20 years. It will not be replaced.

I wanted to ask about improving grep but you already answered that (improvements not being merged) that is bad, but it is not always easy to merge something to apps created a long time ago.
> If I am grepping, of course, I want to include them

Again, like I said, the vast majority of time, especially when people are doing code search, you _specifically_ don't want to include results from hidden directories.

> so it works for you

Not just me. It's on millions of developer workstations.

> There is no mystery with grep, I am using it for 20+ years, without aliases, wrappers, just plain grep, it will work for me for another 20 years. It will not be replaced.

Without aliases or wrappers? Now _that's_ strange. ;-) See, we've come full circle.
Maybe I am just too old. I would prefer to have all the performance improvements you did but without any special things, I certainly don't agree with defaults for "hidden" files.  Will \`rg -uuu\` do that for me?
I'm old too. And I know lots of people older than me using ripgrep. :) Lots of grey hairs.

Yes. `rg -uuu` will get you perf improvements unrelated to filtering.
Ok, so this old man will try your modern grep app :) I remember you from \`xgb\` Go library that I used, and some statements that you are not interested in Go anymore but are using Rust now.
Oh hmmm I don't think I ever said I wasn't interested in Go. :) I love Go and still use it at work. I think it's a great language.

It's just that most of my free time is spent on Rust these days. I am particularly interested in working on low level text search (regex engines). It's hard to do that optimally in Go.
I've always appreciated when typing trainers had quotes pulled from real world books, so you get apostrphies and commas and such. Helps develop that groove in your brain, I found that otherwise, I just didnt use punctuation when I'm typing real world or if i was forcing myself to, punctuation would be a stumbling block that I'd have to type much slower to accomish.

What is the autocomplete thing for for your terminal, like it fills in what would be filled in if you hit tab. Shown when you entered termtyper
Ubuntu based distro moment (mint 20.3) https://imgur.com/M0VdnVM.png
Github: [https://github.com/kraanzu/termtyper](https://github.com/kraanzu/termtyper)

Try and lemme know your thoughts <3
Emacs users be like `M-x speed-type`.
That is super cool! Would you be offended if I fork it and rewrite it in C++? I'm a bit old-fashioned.

I'm a purist that likes to avoid runtime dependencies when I can. I saw the comment from the person who couldn't run it because of mismatched Python dependency, and figured that it would be a fun project to make that conversion.

Of course, I'd give you full credit on my GitHub, and have a link to your repo from the README. And it will be a proper fork, so GitHub will insert a link at the top saying, "forked from . . ." But I fully respect that it's your creation, and I don't want to "steal your thunder," so to speak. I have a lot of projects on the backburner, so I really don't mind if you'd rather I didn't.
The AUR version gives me:


`Traceback (most recent call last):
  File "/usr/bin/termtyper", line 5, in <module>
    from termtyper.__init__ import main
  File "/usr/lib/python3.10/site-packages/termtyper/__init__.py", line 1, in <module>
    from .ui import TermTyper
  File "/usr/lib/python3.10/site-packages/termtyper/ui/__init__.py", line 1, in <module>
    from .tui import TermTyper
  File "/usr/lib/python3.10/site-packages/termtyper/ui/tui.py", line 12, in <module>
    from termtyper.ui.settings_options import MenuSlide
  File "/usr/lib/python3.10/site-packages/termtyper/ui/settings_options.py", line 11, in <module>
    from termtyper.ui.widgets import banners
  File "/usr/lib/python3.10/site-packages/termtyper/ui/widgets/__init__.py", line 2, in <module>
    from .race_hud import RaceHUD
  File "/usr/lib/python3.10/site-packages/termtyper/ui/widgets/race_hud.py", line 10, in <module>
    from ...utils import Parser
  File "/usr/lib/python3.10/site-packages/termtyper/utils/__init__.py", line 3, in <module>
    from .play_keysound import play_keysound, play_failed
  File "/usr/lib/python3.10/site-packages/termtyper/utils/play_keysound.py", line 2, in <module>
    from preferredsoundplayer import playsound
ModuleNotFoundError: No module named 'preferredsoundplayer'`

both times with python-playsound or python-playsound-git installed
Off to a good start, although some [minor bug fixes](https://i.imgur.com/9LPscDQ.png) might be needed. I'd also recommend adding the ability to quickly reset a test by hitting tab.
Really cool! Love the sound effects, the whole interface is also very nice and simple. Maybe make the font size adjustable?
looks great! reminds me a bit of gnu typist
Very nice. A terminal game. I recommend you use a more low resolution, like one with 80x25 is better (the classic screen).

I have also created some games on terminals, almost all for my powerful BlackRook server.
[removed]
Huge fan of your stuff!
Hey! love the project but for my taste, sound effect is super annoying, especially that i just finished build of quietest keyboard i could ;\]. An ability to turn off sound would be great.

On top of it, besides WPM a CPS would be useful
[deleted]
I have been looking for something like this for quite a time. Looks cool!
Kid named finger
There are both modes for numbers and punctuation that can be toggled using "ctrl+n" and "ctrl+p" respectively.
Also, yeah I am planning on adding real world passages.
The auto-complete feature is default in: the fish shell...which I am using in the video
This works for me: https://www.typelit.io/
Hey, what is your WPM average? Mine is about 90 WPM average in Typeracer and 100 WPM average in Monkeytype. And wow, your verbosity amazed me.
Damn T_T
Thanks for the link as the top website on my search engine is some other project!
Cool project! Thanks for sharing this. :)
Awesome!
thanks. you game are very cool. :)
Hey, thanks <3

> Would you be offended if I fork it and rewrite it in C++?

What? No, Go ahead. I would love it <3 
Also, Do let me know so I can try it too.
Additionally, If you have any questions ... I'm more than happy to help.
Happy coding <3
same
My bad.... I totally forgot to replace that.

But... I can't find preferredsoundplayer in git.. Currently, You'll have to install it by pip:

pip install preferredsoundplayer

Any ideas how I should tackle this otherwise?
Can you write the steps to reproduce the bug?

There is a setting to reset a test by hitting a tab. By default, it is off :)
The font size depends on your terminal ;)
Haven't tried that...will do ;)
Hey there! Thanks ;)
It's shell-color-scripts

Here: [https://gitlab.com/dwt1/shell-color-scripts](https://gitlab.com/dwt1/shell-color-scripts)

aur: https://aur.archlinux.org/packages/shell-color-scripts
Hey there! Thanks...that means a lot <3
You can turn off the sound while typing in the settings.

Whats a CPS? characters per second?
XD
https://www.keybr.com
?
Cool, thanks!
Man, it's not the best, between 50 and 60 most of the time. I kinda got there and stopped
Thanks bud ;)
<3
Thank you!! I'll be sure to let you know!
Might fork it too! Been looking for a base typing repo on which to add features involving webcam-derived view of fingers (eliminating kids‚Äô look-down reflex) and eventually some machine vision to highlight next-key and an overlay of their hands semi-transparently with next-finger highlighted.
I'm sorry

In the meantime you can install preferredsoundplayer by pip:

pip install preferredsoundplayer
I just did a test without missing any letters, and got the attached result. I tried doing that two more times and got an accuracy of >100% 2 out of 3 times (the third time it was 100.00 %). In the three instances I saw the bug, my corrected speed was also higher than my raw speed somehow.
You're welcome <3
You can stop train typing if you're ok with the speed! You don't have to reach those crazy speeds.
I'm an avid monkeytype user and I recently got into linux
Great idea!
That's strange...I'll look into it as soon as i reach home:)
Hey u/kogasapls, that was a a count issue and was fixed just now :)

Give it a go again :)
Awesome! Happy hacking :)
thx m8
Chrome has Live Captions that I can use when playing any video or podcast content. I don‚Äôt hear well, so this has been a game-changer for me.
Curious what you mean about saved passwords? Firefox has a built in manager and even sharable via an account too? Unless you literally mean the act of migrating your passwords?

For me I keep a chromium installed for when Google Meet/ Video chat software busts or Google Cloud Console fails to load certain pages.


I find besides those instances 99% of the time webpages don't load on FF is because either uBlock is preventing a """critical""" asset or Firefox's privacy modes are. The mock Google Analytics update helped lower that number - but some website don't guard against "my tracker didn't load" and that busts the page.

Also sometimes I've seen odd stuff when using container tabs likely related to cookies.

Overall, been on FF since the early 00s and never once felt the need to make a switch.

But tl;dr - sometimes Google services don't work. Chromium comes in clutch.
I use Firefox 99% of the time.  There is one site I need to use that just doesn't get past the login screen in Firefox so I use Chrome for that.

Google maps / streetview seems slightly more responsive in Chrome.
Gotta love the responses in this thread. OP asks what's preventing people from switching to FF, and half of the responses are people explaining why they *do* use FF.
I use vivaldi because I prefer the way it does tabs and other functions

Also I kind of dislike firefox because the tab bar is so huge since the redesign a while ago. But the dev tools are better
Embedded google translate for web pages. I had to move to other country because of war and most of local websites are french-only (and I don't speak french).
I also changed to Firefox recently. Everything moves faster on it, it just feels lighter. And it's stupidly customizable, you can literally do anything you want on it. I even use Fennec on my phone.
Ctrl+Shift+A (if there's an equivalent on Firefox, feel free to let me know, and no, the address bar search isn't an equivalent.)

On Fedora with a YubiKey, Firefox [constantly spams `Please enter the password for the PKCS#11 token PIV_II"](https://bugzilla.redhat.com/show_bug.cgi?id=1892137) for 2 years now.

Google services, like YouTube, calendar, and sheets, simply run smoother on Chromium-based browsers. I know that's not Mozilla's fault, but it is what it is.

I find Chrome's profile management more streamlined than Firefox's. Separate profiles are better than multi-account containers *in some cases*, particularly where it would make sense to have a separate set of extensions.

Chrome's tab grouping. I wish Firefox containers and Chrome tab groups would have a baby because Chrome's tab group management is a lot more streamlined and organized but they share cookies. If Chrome could let you segregate cookies between groups like Firefox containers, it would be perfect for me. I just use separate profiles to achieve the same thing, so the combination of tab groups + separate profiles + Chrome's profile management brings Chrome ahead for me in this regard.

Not Chrome exactly, but vertical tabs in Edge are better than any vertical tab extension I've used for Firefox, and you can hide the top tabs without CSS hacking.

Hardware video decoding on NVIDIA.

Overlay scrollbars.

Chrome's reading list.

Native google translation features are better than extensions I've tried in Firefox.

It's small, but I prefer the custom search engine interface in Chrome to Firefox.

It's a lot easier to open a set of tabs from one device on another in Chrome.

Runnable, savable javascript code snippets (not javascript bookmarklets).

My job requires an extension only available on chrome.

I have an Android phone and like the integration. I'm aware of the privacy concerns, but for me personally I've chosen to accept the trade-off.

The general experience feels smoother across the board. Again, I know it isn't Mozilla's fault when devs mostly focus on Chromium-based browsers, but it is what it is.

Feel free to let me know if there are fixes or alternatives to any of these because, despite this long list of pro-Chrome features, I'd prefer to use something more ethical.
First, not a real Firefox problem, a browser monoculture problem. Some websites only work with a chromium based browser. Not many, but enough to let me use Brave instead Firefox. 
In example, the PlayStation store. I couldn‚Äôt complete the purchase with a newly installed Firefox, but with Brave with lots of privacy add-ons. 

Secondly, while the iOS version of Firefox is usable, I prefer the more basic brave UI which I personally feel is more focused and efficient to use.
Nothing. Using Firefox on all machines and even phone. Best browser ever.
Corporate standard is G Suite for documents. Anything other than chrome will occasionally take too long to load or even hang on open. 

I spend my day in these docs, so I do that in chrome.
Better scrolling. I am used to the scrolling in chrome on linux because i always used it. Occasionally, when i open firefox for some purpose, i feel annoyed by scrolling, and no the thing i am talking about is nit related to scrolling speed which can be fixed. Also firefox seems slow to me at times.
webapps is the only thing that makes me use chrome.
Personally, I am using Vivaldi now, so not exactly chrome, but It's chromium based. I was long time Mozilla fanboy since old Mozilla suite. I've tried Firefox since it  was called phoenix.

I've switched because some ajax heavy web pages just do not work well with Firefox. I guess it's because chrome is in kind of Internet Explorer position now. Even Microsoft's browser is now chromium based.  Back in the IE times, you could say - meh some weird IE only web pages don't follow standards, so I can ignore them. Now, Google dictates the standards and Mozilla is trying hard to catch on. And most developers just don't bother to test their pages with Firefox, because now, Firefox is the odd one. Now is impossible to ignore chrome only pages.
Firefox Containers are must haves.
PWA (progressive web apps). I‚Äôm still shocked that Firefox doesn‚Äôt support this.
I switched to Firefox a while back, the only thing that annoys me is that you can't cast YouTube to a chromecast.
Recently Firefox started slowing down for me, and I started having trouble with video playback on some sites. The de-googled chromium browser is faster and I don't have any problems with video playback.
Use bitwarden for your logins and passwords
Chrome is fine for my purposes. The privacy concerns honestly don't bother me much. I'm used to Chrome. I'd rather focus on other things.
Firefox removed tab groups for no reason
the wasm performance and debugging experience is a lot better in chrome. for a while in Firefox, there was this issue where trying to run a wasm app with the console opened led to an out of memory error, but I think it was fixed.

I use Firefox 99% of the time though.
I tend to do work stuff in chrome, personal browsing in Firefox. This way you don't have to mix history, saved passwords, bookmarks.
I get ads in Gmail when I use Firefox that I don't get when I'm using Chrome
You can't use Firefox for some online meeting tools, it's really annoying. I have to fire up Chromium just for that.
Color management. It's an atrocity with Firefox ...
I used Firefox as only Browser.  Then DNS over https with fucking cloudflare was a thing, leaking internal DNS request do cf. Ignoring the Systems CA certificates,  ignoring system DNS.  And i was gone.

May have changes but they lost my trust.
Firefox doesn't allow you to change your new tab page to a local html file, even with extensions. It used to be possible, but they removed that feature.
Firefox is slower and the developer tool are worse
For me, it's the fact that Firefox on Android is awful.

I use my phone as an extension of my laptop, and need a sync feature between my mobile browser and laptop. 

I've tried to switch to Firefox, but just can't stand the mobile browser. 

I use Vivaldi and I'm super happy with it.
Vivaldi gang assemble!
There's 2 things that made me return to Chromium when I tried switching to Firefox.

The first one was hardware acceleration wasn't properly working for me in Firefox. Video hardware decoding worked, but I couldn't fix the bad performance in WebGL based stuff like Google Maps Streetview (playing Geoguessr).

The second one was the lack of screensharing in Discord in browser (I use it when I want to share stuff with audio).

I would switch back in a heartbeat if I could fix at least the first problem.
In my experience chrome is way faster
I was a long time Netscape, then Firefox user. I'm talking 90's-2000's. Firefox really screwed the pooch about the time they decided to switch plug-in models. It was Vista all over again. They put a bullet in the head of the old plugins before developers had reworked their plugins for the new model. They also completely lacked tabbed browsing and book-mark syncing. I played with Dolphin and Opera for a while when dial-up internet was all I could get, I needed the remote proxy render to get through some web pages without reading an entire fiction novel waiting for the page to load. About the same time I moved out and got DSL, Chromium and Chrome showed up on the landscape with tabbed browsing and book-mark syncing.

Firefox also had a stagnant era so bad even Pwn to Own decided to drop them from the challenges since they had no security updates or feature updates for a while, Firefox wasn't interesting or any challenge to break.

Then there was an era where Firefox decided they wanted to partner and shovel a search engine at you, I forgot which, yahoo, bing. Yea.. no. I avoided Ubuntu when they decided to do web search desktop crap and used alternative distros. 

Yea, sure Chrome/Chromium open a tab to google search, but it's actually a FUNCTIONAL search engine unlike yahoo-bing. New tab and home page isn't full of news snippits with ads inter-mixed like Edge is now.

So I stick with Chrome.

We do have Chrome, Firefox, and Opera installed at work. I do occasionally try all 4 browsers on websites when something goes wrong. I sort of use Firefox as my completely unmodded browser. No plug-ins, nothing adapted. It's pretty default. If stuff doesn't work there, there has to be a problem with the website.
Brave browser FTW
What keeps me out of the Firefox:

* No sane support of profiles. It‚Äôs crucial for me as I‚Äôm often sharing my desktop with colleagues during meetings so keeping browsing history private in a separate profile is a must for me. I already had an incident with Firefox when it had exposed sensitive info in the address bar during a meeting so I‚Äôd uninstalled it immediately after the incident. Actually firefox has profiles but it requires many hacks to get them working like in chrome.

* Worse GPU performance in virtual machines. When I‚Äôm using a browser I‚Äôm trying to use it everywhere to have same environment. For example I‚Äôm using a Linux VM for my work on a Mac host. While Chrome can achieve 60fps in VM easily on top of virtio gpu, Firefox has 30fps or worse with higher CPU usage.

* Less than year(!) ago Firefox got a back/forward mouse buttons support on macOS whereas Chrome was supporting them for a long time.

* I‚Äôm a bit locked in Apple‚Äôs ecosystem so I‚Äôm using iPhone with a Mac and Firefox on iOS is terrible (IMHO) in terms of ui and extensions support so I cannot use it as my daily browser across all devices.
I use chrome because Firefox can't import multiple-account-same-page from my chrome
Not particularly chrome but chromium based browsers (using edge, brave, and opera and kiwi and bromite). Because of site functionality and less broken events like YouTube not functioning properly and video chat sites like zoom or even ms teams. Plus edge has vertical tabs which Firefox isn't and doesn't want to implement out of the box. Edge and opera has save memory and tab suspension and tab reactivation manually which Firefox doesn't has. Plus firefox unreasonably consumes too much of data and ram usage even when compared to google chrome. Easy import and export option in chromium based but nightmare in firefox browsers. Look at password option. It doesnt even has inbuilt weak password scanning. Plus their android version (post 70) isnt up to the mark and has various issues. I dont guess r/firefox will be interested in knowing all this BS.
Need that hardware acceleration, although it is supposedly coming soon.

Also I use Brave instead of Chrome
I like that google execs and their ad partners are getting fat and happy while limiting my choices by turning my own data against me.
I'm sure I'm doing something wrong on my end, but Firefox takes around 2-5x longer to start (10-20 seconds) than Chrome/Chromium (5ish seconds max). Not sure what the issue is but I've experienced this across multiple distros at work and at home.
I've used Mozilla browsers since Netscape, but relatively recently switched to Chrome:

* Performance - Chrome is just \*way\* faster than Firefox, no way around that one (Quantum helped, but it's still nowhere near).  Especially on mobile.
* HTML/CSS/file format compatibility - Lots of little weird glitches you don't realise until you build sites yourself.  For example I had an issue where Firefox simply would not load a font for a site I built despite every single other piece of software working fine with it.
* Random breaks - SSL certificate validation failure, some big sites not working while the Firefox team sorts it out, etc.  They're infrequent, but annoying and show the gaps in quality and professionalism at Mozilla.
* Mobile version trashed - Their switch to abandon extensions in their mobile version (others than those they allow personally) really was annoying and the primary reason I used it on mobile (we had custom extensions for use at work).
* Significant internal miss-management - The ridiculously high CEO pay, lay offs despite the former (and ditching MDN staff), buying pocket, letting the above random breaks happen, etc.
* Sponsored ads in browser - I know you can turn them off, but it is a really shady turn for Mozilla.  Spouting "anti tracking/advertising" at the same time as embedding ads in their own software.

None of these things is enough in and of itself, but it was an accumulation of all these niggles and more over the course of years, and the sponsored ads was enough to tip me over the edge and abandon it permanently.

The reality is Mozilla gets such a staggering amount of its income from Google, and has done for many, many years.  There is no "supporting the little guy" with Mozilla.. they've been sucking that corporate teat hard for nearly two decades, and as soon as some of that support is threatened they go straight into ads in their software.

I wish there were better options.  A browser is such a big project though, it requires such a commitment as the specs are evolving so fast.
Vivaldi for life
I've switched back to Firefox for a few years now. Even despite the fact it pisses me off because it leaks an *insane* amount of RAM and GPU memory after being open for a while, even after closing all my tabs. In terms of video memory, it can take up like 5.5/6 GB of it, causing games to crash when I launch them.

Chrome has a problem with memory usage in general too, but at least I get close to everything back when I close all my tabs in it. But I don't use it since it's a privacy nightmare these days, and shit like sending all your browsing history to Google is under a weasle-worded preference called "Helped make the web better". Only use I've had for Chrome recently was for work, when I needed to use Google Sheets since I'd be working with documents with many thousands of "IMPORTRANGE" calls that'd cause Firefox to choke really badly.
I am a heathen and use Edge. It has really nice vertical tabs, tabs groups (even within the vertical tabs), and Collections that sync. Firefox has collections on mobile that do not sync, got rid of tab groups but had done them in a different way, and extensions can do tab groups through the side pane but not nearly as nicely.

I prefer Firefox where I can but when I am doing a project, Edge ends up being more useful.
I have 64 GB of ram and nothing else to use it .
I use Brave as my default. Brave was created by the software engineer that produced Firefox. I use Firefox on occasion and still love it!!
I log into Google and all my  bookmarks and extensions are synced in. Mainly convience.
Google translate automatic translation on chrome.
I‚Äôd be using FF if it supported HEVC but it doesn‚Äôt https://bugzilla.mozilla.org/show_bug.cgi?id=1332136 ‚Ä¶ once I noticed that video is usually a lot worse in FF I started using chrome for video, and then I gradually switched to chrome for everything.
Onshape my browser based CAD tool, but I just run it in Chromium. 

Need browser based cos not much CAD works on Linux. Yes freecad I've used it and it's OK, but frustrating at times
Its not faster than Chromium (no matter what ads say as this has been tested several times) and it does not have privacy and security out of the box like Brave. You have to add extensions and settings to get close to Brave in those regards
WebMIDI
I use brave browser. Brave browser/chromium has better video hardware acceleration on my old i5 4th gen. Sure, I can manually configure ff to use hardware acceleration. But than again another x window problem appears.
It syncs better. But then again, I cycle through Chrome, Firefox, and now trying out Brave browser cuz no matter what, none of em are perfect ü§∑üèæ‚Äç‚ôÇÔ∏è
Ff abandoned progressive web apps on desktop, some of which (ie Google chat) are a key part off my workday.

Also I've found that edge actually outperforms Firefox, chrome, and even plain chromium.
firefox colormanagement sucks on macOS, I use google workspace at work etc...
I generally use both erring to chrome though, it tends to be faster (even when spoofing UA on firefox) and more stable then firefox is for me
Chrome profiles are much easier to use

I have separate profiles for a bunch of use cases - home, work, play, dev, etc

I know FF has profiles too, they‚Äôre just much more difficult to use and change
\>>>>Chrome instead of Firefox<<<

We need to go in 2008 and later. Mozilla was a very heavy  browser.
PWA support. I used to not care but after using PWA's I find it hard to go back to not using them, especially on Linux. I know you can install extensions to Firefox that add PWA support, but it doesn't perform as well.

Firefox *used* to have PWA support that you could enable, but the feature was never completed. As a result, they have removed it from current builds with no plans to revisit it on the desktop.

As such, I am stuck with chromium-based browsers that handle PWA's for the desktop.
Nothing. If I wanted to use a chromium browser I‚Äôd use Brave.
RAM usage, mainly. I don't get Chrome's reputation of using a lot of RAM, but Firefox for me uses way more in comparison. I can open lots of tabs in Chrome and it'll handle them just well; that's not the case with Firefox.

I actually like Firefox more, though, because it feels snappier when browsing and scrolling, but I don't like having to keep an eye on memory usage if I open a few tabs.
I think it's more or less people getting used to using Chrome and don't want to change. People naturally don't like change. I would suggest using brave browser. It has every privacy protection built into the browser . So you don't need to add privacy extensions to it.
The multi-profile support in Chrome is better, particularly from a UX perspective.

Containers aren‚Äôt the same and the exiting profile switching UX is very cumbersome. If they sorted this out, I‚Äôd jump on Firefox.
It supports HW acceleration
I use firefox for almost everything. Except two:

1). Google Meet. Just works a lot smoother with more features on chrome. I'm pretty sure it's Google using nonstandard technologies again. But I'm forced to use Meet for work
2) My bank's mortgage management interface just doesn't work on firefox.
I like to use the same software on all platforms and Firefox doesn't work well with yubikeys on some platforms(like Android). 

Also most features of Google services ( meet, maps, gcp) work like crap or with reduced features. I know this is on Googles side, but it is an issue for work environments.

Besides those two things, I really like Firefox and wish I could go back to it full time
I cannot remove the connected device no matter how many times I‚Äôm trying to delete them, so when I‚Äôm sharing tabs I‚Äôm seeing lots of connected devices. Because of that I‚Äôm still on Chrome
Other way round for me. Have been a Firefox user for the past 4-5 years, then I went back to Chrome because Firefox for some reason keeps on crashing on me. Not the mobile version on iOS though.
I prefer chrome‚Äôs devtools, also it‚Äôs easier to load extensions from source.
1. Native ability to have vertical tabs that does not share the same space on the sidebar as the bookmarks.
2. A mobile browser that does not make me open every bookmark in a new tab.

Until then I will stick with Vivaldi.
Using chrome, I only need one browser. Using firefox, I need 2 browsers, as numberous people point out in this thread, they need chrome/chromium just in case.
Tab group, lol
[Mozilla already has a guide for a seamless transition to their browser](https://support.mozilla.org/en-US/kb/switching-chrome-firefox)
When someone confesses that they allow their browser's password cache to store all their important passwords, I re-think my evaluation of that person's level of intelligence.
i've been trying to stick with firefox because i generally like everything about it more, but i have this strange issue where page loading massively slows down after its been running for several hours. hopefully i'll figure it out some day :(
I use Firefox as my primary browser. However, I still keep Chromium (not Chrome) as a backup since some websites such as MS Teams work better in it.
I don't use Chrome, but I do use Brave for when I need Chromium compatibility and better Google compatibility.
I use Vivaldi, which works better for my workflow than Firefox. I can't live without them tab stacks. Not even Tree Style Tabs compares to tab-stacking on Firefox. There's also actual tab tiling, seeking in picture-in-picture views, a better and more customisable look than Firefox... yeah, it's not open-source, but compared to the other guys, they're small, so I understand.

There's also the issue with what Mozilla's management has been doing (see: the Mr. Robot extension fiasco, which judging by the Turning Red promo, Mozilla has doubled down on). Vivaldi is a breath of fresh air in comparison (FWIW, they're former Opera devs). What they don't open-source, they make up by being open to their users.

Vivaldi has a lot of support on the mainstream distros compared to other Chromium forks (aside from Void). Arch and Gentoo package it in their main repos.
Chrome is faster.
I switched from Firefox a while ago.

Better hardware video acceleration support, Chrome it's just snappier here and Reddit on Firefox is a joke(I don't intend to use Markdown Mode). I don't know about security, but I don't really care.
I use both. Chrome for professional email et Firefox for personal one.
Google translate (or any other translation option incorporated to the browser). I know there's an extension for Firefox but the extension implementation and performance isn't nearly as good as the integrated tool on Chrome.
I have used chrome and firefox both for normal use and for development purposes and here are some conclusions I can make-

1. Chrome uses more RAM than firefox.
2. Both are having their architectural and engine difference so need to have both for development purposes.
3. The web engine used by chrome and firefox are very different and sometime things only work in chrome but not on firefox.
4. Firefox is more handy and convenient to use and provide much better shortcut for navigation as I have gone through it.
5. Chrome uses built in sign in and email id so you don't have to worry about.

Hope from above observation you can make the conclusion that 
For normal user firefox can be a batter choice but for a web developer both are necessary to be used for testing purposes.
I only use Chrome at work because it is required to be signed into a company administered account.  At home I've been using FF since before Chrome even existed.  There is nothing that I have ever needed Chrome to do that Firefox couldn't also do equally well.
First, Chromium offers an arguably more secure product than Firefox. I won't go into the details with this conclusion but I'll leave that up to your research. 

Secondly, (Ungoogled) Chromium presents me with more granular inbuilt controls per-site than Firefox. I can delete cookies for just one website and even declare that certain websites should never be allowed to save cookies. I can disable javascript for a website etc. Granted that some of these things you can somewhat do with an extension, on Chrome and even on Firefox. It is nice to have them baked inside the browser so I can place less and less trust on extensions.

Oh, also because I now like PWA apps which Firefox ceased to support on desktop for some weird reason.
No support for PWAs is HUGE for me. Also some websites are not testing their stuff of anything other than chrome so, some things don't work like filling some forms, logging in, it's not a big portion of the sites I visit but a decent amount.
Honestly you shouldn‚Äôt ask that question in r/linux. 
I‚Äôm sure the share of FF users here is way greater then the Overall share of FF. All you get is a bunch of answer like ‚Äúnothing is keeping me to chrome, I use FF‚Äù
looks better, feels more modern, i already have it on all my devices, sync works everytime, every extension i could ever need + other things i can‚Äôt remember now or i just don‚Äôt realize are chrome only

i also feel that firefox is always one step behind and i like to run the latest software and technologies. 

i don‚Äôt care if google is spying on me. i do feed the YT algorithm on purpose so i get better and better recommendations. i also have ublock and adguard at the router level so there are no visible annoyances.
Nothing. I love Firefox more.
Every time I've swapped to Firefox, I've been unable to stay on it.

Sometimes my Firefox install just *breaks* and I have no idea why. I hate to tinker and add extensions, so my changes are usually minimal, but they're enough that some sites will stop working. I've never been able to figure it out.

This is the most important thing! Firefox just *breaks* in ways I've never had other browsers do. I've wasted *so much time* on Firefox that I've basically sworn off it.

I use Brave, which has a really nice UI for turning Javascript on-and-off on a per-site basis. This is essential for me. On a security standpoint, I trust Brave devs more than random extension devs, so the more features in the browser, the better.

Brave also has Chrome's *profiles* feature. I use these very often, and can be accessed entirely with keypresses. AFAIK, this is not possible on Firefox profiles, which has a far worse UX and requires mouse clicks (or pressing "tab" many times.)

For my use cases, Firefox is way too much extra effort to use.
Better tab grouping, Chromecast support, read later view is better than Pocket, performance and battery consumption on my Mac is better.
I do use Firefox but lemme explain.

Edge: their built-in PDF Viewer is better than everything I've seen (Okular, Adobe reader, Firefox, ..). You can add text, annotate and do basic editing in the browser. It's interface is waaay more simple than other software.

generally chromium browsers are faster than Firefox, you can check any benchmark on a reddit post or some blog on google search, it's a fact.
I use FF with strict privacy settings, no cookies, Noscript, uBlock and via VPN for regular browsing.

I use chromium for development and no addons except uBlock for sites I trust where I have to log in. 

The reason is, that I don't fully trust other browsers than firefox, but the privacy settings and addons slow down FF. Just a tiny little but, but still recognizable. Also routing through other countries sometimes has shitty sideeffects, like sites not displaying the correct language or government services not allowing access for foreign IP addresses.
I‚Äôm boycotting Mozilla due to their history with the Rust Foundation, otherwise I‚Äôd definitely be using Firefox.
I don‚Äôt like Mozilla
Firefox is a better alternative at the simplest level because it's not a Google program
I don't use Chrome, but I haven't consistently used Firefox since around version 3.x. It's so slow, bloated, and awkward compared to just about any other browser I've tried. It seems every release of Firefox ads more features I don't want. I used to use Firefox (instead of IE, for example) because it was "just a browser", light, fast, standards compliant. Then it slowly became a bloated monster. I don't need a developer suite with a hundred unused features, I just want a web browser.
"Close all other tabs" and "close tabs to the right" should be first-level menu options.
I don't use Chrome but I do use Brave. I still find Chromium-based browsers to blow away Firefox in terms of speed and compatibility in many cases. Firefox just performs pretty poorly on mobile devices and laptops and there's always some site or another that doesn't quite work with it.

Firefox's WebGL support is also extremely slow and stuttery in Xorg and they haven't fixed that issue for years.

With Brave, I get the speed of Chromium without Google's data collection and in many ways it beats Firefox's privacy and fingerprinting protections out of the box. Plus being able to sync your browser data between devices via a E2E sync chain without giving your email address and other personal info is just awesome.

I also think Firefox is missing the web3 boat. If you look past the crazy crypto market volatility and the NFT silliness, you start to see that being able to authenticate and pay with just your wallet key for any online service instead of giving out your personal info, CC card, etc. for each site is a game changer for digital privacy.
I like that there is extensive telemetry being output so that if I do something stupid there is someone looking over my shoulder that can give me assistance.
I use Firefox for everything except for when I‚Äôm developing. The dev tools in Chrome are just flat out better, plus I‚Äôm more familiar with them. So I only use Chrome when I‚Äôm working.
Speed, chrome loading time is way faster than Firefox
Chrome just works. Ive never had an issue with it. Not broke, didnt fix it.
Chrome does not look alien on KDE. There were official repositories for /r/openSUSE. Netflix and everything else with patent encumbered codecs just worked. I use the password manager. Android support is pretty good except missing ad block. My TV has a ChromeCast which I find handy.
When I am using internet, I am basically using most of google tools.  
Google search, Gmail, Youtube, google maps, google passwords, google contacts, google calendar.

It's just convenient. With google translate and google image search just a step away.   


Google Chrome is the first browser that made me feel speed when I started using 7 years ago. May be now the difference is negligible. I haven't just tried yet.
Mozilla executive salaries
Firefox removed the features that made me use it, and chrome based browsers like brave work better with gsuite and YouTube
The profile syncing, ff really doesn't replace it well
Got all my accounts passwords  extensions bookmarks etc synced  in chrome and its a pain in ass to transfer it in firefox so i use chrome
Only one thing: Line chat app.

I use Arch and the only way to use such app reliably is through chrome-only extension.
I use Brave
Wait! Switch?!
I've been using FF before chrome even came out.
This means you actually chose IE over FF at one time, 
Used FF and then Chrome AFTERWARDS, 
Or you're like 15 and never knew life before Chrome
Password manager shared with Android.

In other words, a Firefox Sync account cannot store Android app passwords. Google does it by default on Android and after a simple configuration step on iOS.
I only use Firefox or Brave.

Firefox isn't the fastest and it's not the best but it's a great all rounder and I trust it more than other options
FF is my main browser, I use it mainly as another way of getting a second browser on the same computer so I can keep some stuff separate. Like if there's malware that only affects the browser hopefully the FF malware won't be able to get access to my work browser and I can just use my work browser only for work stuff.

Obviously, all bets are off if the malware escapes the browser but in my mind it's a sort of failsafe to not mix personal and work browsers.
I use both but find chrome to be a smoother experience... This is noticeable when I'm moving tabs around or displaying an extension when clicking on it.

On mobile Firefox is my main browser.
Chrome on Android is much better than firefox. I use kiwi browser on my phone (chromium based) because of extension support.

As for PC front, I own a Chromebook and a desktop therefore I use Chrome for seamless integration. Moreover as webdev I need to cater the mass audience of chrome users. 

Down vote me of you want, but I think firefox has lost the browser game long ago and it's not so far in the distant future that it will become the new Internet explorer. Chrome's V8 engine is hell fast. Moreover, it's easier for developers to only target the one base (We already have  apple for making things difficult). Finally, chromium is also open source with much better support due to google's backing.
So as a developer I found many things really frustrating, some of which I can not remember right now.

First thing is the lack of a proper datetime input. It is not supported by the browser natively. See the bug ticket [here](https://bugzilla.mozilla.org/show_bug.cgi?id=datetime)

About that, many feature are lacking in Firefox compared to chromium, many of those are experimental features, like css backdrop filter.

Those are minimal but quite annoying nonetheless. I've been considering chromium for quite a while (not chrome)

Edit: fixed bug link, thx nextbern
Chrome syncing is faultless, Firefox Sync was hit garbage when I tried to use it. 

Chrome Dev tools are incredible and as a personal preference, I prefer them.
I just prefer chrome's design. Firefox tabs are so huge !
Chrome is just simply more polished on Android (running a Google Pixel). Firefox looks like shit
It's simply not as fast. Full stop.
I used to use Firefox for everything but when I switched to Brave for better security, so now I am using a ‚Äúchromium‚Äù type browser now as my default. I also use Edge because it auto-finds coupon codes during checkout.
Honestly, I'm an Android user. I'm far enough into the Google ecosystem and chrome dominated at the start that I have no reason to switch now.

I did use Firefox before Chrome existed but chrome just came in and dominated. Sandboxed tabs, a talk manager to kill misbehaving tabs and bookmarks synced to my account? It was great.

 And still is IMO. 

Chrome is king, and if you can live with the telemetry and other data collection it's a great experience.
Take back your privacy and use Brave. Webpages also load 3x faster!

P.S.
All your beloved Chrome extensions work there too.
Firefox > Chrome
There are Linux people using Chrome?
Firefox ships with adware. Brave ships with adware. Edge ships with adware. Chrome ships with adware. Opera ships with adware. Chromium does not. Therefore, I use Chromium.

Once Gnome Web has all the extensions I use working fine (ublock origin, bitwarden, xbrowsersync among several others), then I'll switch over to that.
I lost my 2FA token and they won't let me delete/recreate my sync account
I only ever start chrome when using stadia.
A side question: why nobody wrote a Firefox extension that stores passwords in the same place in a Google account as Chrome does? Or a Chrome extension that uses Firefox Sync as a place to store passwords?
I have problems with twitch streams randomly pausing the video feed in Firefox, and it does feel a bit slower in general, but I can live with those tradeoffs.
I use both regularly, more so Firefox

Chrome honestly works better with Google things. Little annoyances like the volume slider on YouTube Music - with FF it's like half a second delayed to dragging while in chrome it's immediate
I have run in to many problems with websites not fully working on FireFox, and for my job it is a pain to open chrome just for that. So I just use Chrome because it is the most used so all sites work with it and I when do dev work I test it on Chrome mostly.
Chrome way better than Firefox for me BUT I can't stop using Firefox because of a simple extension; [Simple Tab Groups](https://addons.mozilla.org/firefox/addon/simple-tab-groups/)
Chrome is for Chromecasting stuff, Firefox is my daily driver. I know I could do Chromecast with Firefox but I like the separation.
Cast to chrome cast, that‚Äôs the only thing I use my laptop for anyway.
I use Firefox usually, but switch to chrome for Google meet because it works better. Moreover, I use it also  for Netflix/prime video because it seems to me that video hardware acceleration is better on chrome.
ctrl+tab is the only reason I use firefox since ever
Lack of background effects in Google workspace meetings which we use at work. But i use it literally only for that.
I use both, in fact they're too amazing to just pick one. I just think if a malicious actor is going to exploit, he's going to pick the most popular browser. So I use Firefox for YouTube videos. Chrome for saving my passwords and all.
I use firefox dev edition as my daily driver, but the one thing I always miss is the chrome dev tools. People tell me that firefox has better dev tools, but until I see live editing of JS, I'm not convinced. I don't care about fancy CSS editing, I don't care about editing fonts, I just want to be able to edit arbitrary JS on a live running page! Honestly I'm shocked that more people don't think of this as a core feature of a web browser's development tools that's sorely missing on FF
Personally I use FF as my main browser but still keep chrome installed for browser games because i have noticably worse performance in FF
Nothing honestly I just wish the phone app was faster. The previous version where it had a different ui which supported add ons was a lot faster and smoother in terms of ui etc. but it did use up more battery and overall I like the new ui on android too. 

Still the older one felt more inline with desktop version and was faster
Multiple profile support. I know there's an extension to create containers but it's not the same as having it create multiple instances without having to click through a bunch of text menus in about:profile(s?)
The Google ecosystem and YouTube playback (video codecs / drm in general), ...and the passwords. I haven't tried the switch in a long while tho
I only use chromium for Google meets because it seems to work better there
Years ago Chrome was slim and Firefox was bloat. Then they rewrote Firefox in C/C++ and mad it slim while Chrome became more and more bloat. Stayed on Firefox since then.
To any FF users I strongly recommend the Multi Account Container extension. It takes a bit to configure and get all things rounded up but it's great for privacy and multi account usage
Do you make math videos?
I really like firefox, and use it everywhere, but i do have chrome installed. To answer the question, i use chrome when websites break in firefox.
In my case it‚Äôs because the vendor either only supports their stuff when run on Chrome for their reasons, or their stuff just won‚Äôt run on anything else period. Dealt with the same thing years ago when the in house devs only wrote for use on IE. I can only sip tea when it got retired and they had to scramble to finally rewrite their crap software.
I guess I can say I used it a long time ago when it was still called Netscape Navigator. I didn't like a new version that they released so I changed to Opera. I used Opera for maybe 10 years or so and then changed to Chrome. I see no reason to change back to Firefox. I have tried it through the years but I don't really like it.
Streaming services work out the box with Chromium. Firefox just feels outdated, like it did back in 2001. Chromecast, extensions are easy to manage. Idk just like it more overall.
I used to use Firefox for everything. I don't know why I stopped? I've been using edge for the last 2 years simply because its connected and synced through my Microsoft account and all I use is windows. Plus my work account can be connected so everything syncs between home and work super easily. 

I need to get back into Firefox again. I miss it
videos dont work on any other browser other than chrome which is the single most annoying thing ever
Ecosystem. 

Google is a bad company but their a bad company who have captured my attention and patronage across multiple platforms. 

I use it on my phone, my laptop, my work computer. Multiple web apps work best in it or are updated first for it. And it's just convenient to have them all connected.
I've always used it, and I understand its features although I was tempted to switch to opera at one point.
[removed]
GameStop wallet.. otherwise everything is Firefox. Not sure I believe the low reported market share.
I have to use chrome instead of firefox for specific virtual console applications. Firefox won't let me copy/paste commands while chrome will.
startup time is an issue I often run into with Firefox, probably because I am logged into my mozilla account and stuff gets sync'd. Apart from that there is rarely a reason to use anythign BUT Firefox. I actually donate each month ...
I used Firefox from 2003 or 2004 and onwards until people said Chrome is faster. I switched to Chrome and haven't really gone back since everything is now tied to my Google account anyway.
I run chrome on all my devices and it seemlessly syncs with no issues and rubs with 0 issues on any device. And I don't have to go through and install it and other extensions to get the functionality I want. As far as performance, is negligible between the two browsers. Some updates one the faster then the other, in the end they added close enough it doesn't matter.
Firefox is simply much slower than Chrome no matter what I try to do to rectify it
For me i use google activity which is like a time machine to see my history across all google apps, has helped in finding lot of things , some movie or book i searched many years ago, which websites i visited for that, etc so it is like time machine for me to go back and find something which i have forgotten and remember only hints
Development and new APIs. Working with files on Firefox has consistently a pain in the ass, and don't even get me started on the damn Transition API or PWAs. And the Chrome DevTools just feel much nicer than the Firefox ones (Mozilla please for the love of god add a flexbox picker). Firefox is great, but not being able to install a desktop PWA is a complete no-go for me, and I don't want to wait a couple of years to get newer APIs.
For the record I use  FF as my daily browser.
I only use firefox but a few problems i have with it compared to chrome are:

1) Some sites seem to purposely works slower/worse on firefox. some straight up dont let you use the site unless you use a chromium-based browser. I sometimes have to use a user agent addon to pretend to be chrome, and law-and-behold everything works perfectly well (almost as if web standards exist).

2) google owned websites work a bit worse on firefox.

3) No ability to translate whole pages as far as i know (maybe there is an extension for that?).
some google meet features work on chrome only  
skype not good on  firefox  
firefox freezing on my arch+nvidia always. still i cant figure it out.  
chrome handles HiDPI better than firefox    

I use firefox for containers features only. but, I am still trying to use firefox hardly.
I‚Äôve had a few sites straight up not work properly . It also sometimes causes my audio driver to commit die when watching video
I use Firefox but sometimes want to switch completely to Chromium-based browser because:  
1 - most of them seem to work well with KDE Plasma global menu out of the box  
2 - WebGL is very slow in Firefox on Linux. On websites like Sketchfab I get a lot less FPS than on Chromium-based browsers  
3 - sometimes things just don't work :/ but as mentioned by a lot of people here, it's not really Mozilla's fault, but rather Chrome being a monopoly
I was a Firefox user, before Chrome was a thing. I remember it took a long time to start the damn browser (around 40-50s), then sometimes it just crashes. I think it was because my specs was too old and couldn't handle newer version of Firefox.

Chrome, on the other hand, didn't have those problem. I switched to Chrome ever since and have no reason to look back.
I mostly use firefox now, but I did use chrome for a while. I use gmail for my personal email and have a few of their services. Chrome has some great cross platform functions like generating a qr code for a url and the ability to send urls and other information through devices that have their account registered with google. I also have a number of google home devices and and chrome has excellent control built in to the browser.

For people relying in browsers for password management, you are probably better off with sometime like bitwarden or coughing up a few bucks for something like lastpass.
I‚Äôve had troubles with memory leaks in Firefox
Certain sites i use on the regular refuse to work properly on firefox.  Specifically banking sites and a few others.   i'm not interested in having multiple browsers installed.  just give me one that works everywhere.
Use Bitwarden instead of any sort of built in password management nonsense. That way passwords aren't tied to your browser. I use a Chromium browser just because of the various security related features. https://madaidans-insecurities.github.io/firefox-chromium.html
I use chromium when I need some things to work. Had problem with ticketmaster.    
My firefox is a bit security hardened which causes some problems.    
However I love firefox and detest chrome. Chrome is bloated and being too inventful when it comes to doing things standard. Also tracking...
The only thing that don't work for me in firefox it's vaapi. Already tried everything in archwiki and don't know why doesn't work. Actually no other browser vaapi works out of the box, just chromium. I do use firefox for everything else, it's the best by far.
You almost had me, OP. Thought you were [this gent](https://www.youtube.com/c/3blue1brown) for a moment
Chromium crashes on the File dialog in my current env, so Firefox it is.
Probably an unpopular opinion, but I like how it lets me see every page I visit under Google‚Äôs My Activity page.

I already use my Google account for most stuff so using Firefox Sync would just be an inconvenience.
On the work end, I usually use Chrome. I can install Firefox but since it is a work device, I‚Äôm not one to seriously customize a device that isn‚Äôt mine and I strictly use for work purposes.

I used to be one to use Chrome whenever I use Drive/Gmail. I‚Äôve been using Firefox with Account Containers for my Google stuff on my main laptop. Chromium‚Äôs snap does not work well because the menus are illegible, just colored squares when I tried it I think? (Context: Kubuntu 20.04 on ThinkPad E595) I looked into Fedora but have WiFi issues. Debated retiring it for a Framework or a used ThinkPad from eBay instead. I am not a college student anymore and I don‚Äôt necessarily need the newest hardware right now.

At least, I think I may be more inclined to remain in Firefox.
Because everything I do is already integrated with Google. Multiple business accounts, multiple personal accounts, etc.
I disabled location in macOS, and every time a page tries to get the location, it crashes. 100% of the time.
I like Chrome's dev tools and extensions better than Firefox's.
Ecosystem I guess...
There are 2 things that Firefox handles poorly:

* Browser profiles (no container tabs are not enough)
* Webauthn for passwordless is stilll stuck on version 1.
Only time I use chrome over Firefox is when the DRM/website is causing a video to display in less than optimal quality.
On some ARM devices I notice chromium is much faster and smoother than Firefox.
Containers plugin in Firefox is what makes me have it in all of my devices
Google sheets doesn't work properly in Firefox, and the back button on my mouse goes back two screens.  Otherwise I would.
When I use a window manager, not desktop environment, I need to use a chromium-based browser because Firefox has pretty bad screen tearing in 2k/4k 60fps YouTube videos. It seems like the compositor in Firefox isn't as featureful/robust, since videos in chromium play fine in a WM or DE. I am using the proprietary Nvidia drivers so maybe it has something to do with that, but again chromium seems to be able to handle that situation.
Crome automatically translates pages in different languages so that's why I use it. I have to test a large number of websites of different corps everyday so it's a necessity.
Near full extension support on Android as Firefox is still lacking in that regard. I use FF on everything else though. Kiwi Browser on Android.
The one thing chrome (and its derivates) do better than firefox is tab grouping and management. So as my work browser, which I use to access outlook and office 365, I use Edge. For my personal stuff I use firefox.
I've relented to using Firefox but I really don't like that the bookmark manager opens in a new window instead of a tab. Also I prefer Chromium's bookmark hierarchy structure. It would also be nice to move the Ctrl+F bar to the top.

If you know of a way to do that on Firefox, I don't think I will ever go back to Brave.
I use Firefox on PC but wanting to switch back to Chrome. I use QtPass for password syncing so that part is browser-independent, but history sync is a feature I must have. Seeing that Firefox Android app is so buggy I might switch back.
I use firefox but I wish we had the great implementation of forwards/backwards gestures that chrome has
I use them both (one for work, one for personal; it's fascinating how the search engine differs over time). I think they each have their pros and cons. I use a third party password manager, though, so I don't have them split between two different browsers.
video playback runs a lot better in chromium for me. i also use it to test my code. the only thing I like better from firefox is the json view thingy
I just love everything google so ye
I don't use Chrome but I do use Vivaldi for some websites that just don't render correctly on Waterfox or Firefox (I use Waterfox since it lets me put tabs below address bar as a option rather than fiddling with CSS changes that break every other update).  I use a good number of websites that just straight up don't work or render correctly in Firefox due to not using standards or implementing the standards differently (Homebrewery is one of them for D&D-style statblock creation).

Spelltable also doesn't use my webcam's full resolution in Waterfox but it does in Vivaldi for some reason.
Firefox is my daily driver but Chrome's history browser is way better
You can export settings and passwords to a CSV file (I think) on chrome which is easily imported into firefox.  Transferring settings from one browser to any other is pretty simple these days.
I usually use Chrome for dev tools but I highly prefer edge instead for RAM matter..
Google Meet
It's installed.
Screenshare with audio on discord web
I don't like the history window of Firefox and prefer the tab in chrome
I've been using Firefox for years on windows and Mac os. I installed Ubuntu today and Firefox freezes all the time, had to switch to chromium.

:/
Speech recognition support.

For everything else, I already use Firefox.
i was using chrome only for codecs but then i installed the codecs for Firefox and now chrome serves absolutely no purpose in my life. my entire google account is basically just a spam catcher at this point
When Firefox switched to EGL (from GLX) it became much smoother to use. So that is not an issue any more for me.

But there are some usability things that I often end up running against.

From small things like how switching between tabs works to things like how smooth video playback is (basically, hardware acceleration support). And I detest "extensions" for fixing things I consider lack of foresight from developers.

So, it has improved, but for many things the usability considerations are not there yet and Chromium-based browsers are more pleasing to use. I use Firefox sometimes for some specific things, but mainly Chromium-derived browsers for daily browsing.
Couldn't figure out how to install it on my Chromebook.  Vastly prefer FF on other machines though.
I actually switched to Firefox a couple months ago because Chrome/Chromium don't support virtual keyboards and KDE's one doesn't have manual pop up support.
the force dark mode flag.
(third party extensions are shit)
I use Firefox since too many years, I'm a web developer and I'm used to Firefox's Developer Tools.   
The only thing I like about Chrome is the out-of-the-box "search with google lens" context menu option when right clicking on images (useful to get OCR text from images), but I think there's a Firefox addon to get the same functionatily.
Started with chrome, shifted to Firefox as Chrome was hogging a lot of RAM but now changed to Brave after a RAM boost-up great ad-free spotify and youtube experience.
Ability to automatically sync passwords from chrome to my android phone. Someone should make a Firefox extension that does that. Just a google password backend for Firefox.
My Firefox suddenly stop responding
Chrome ignores access keys and tabindex.  I use them all the time.  
Sure, I can use vimpirator, but I shouldn't need to install something to use standard html features.  
Firefox is the best.
Right now, nothing at all. Been maining Firefox for a long time. Nowadays I never use chromium, don't need it for anything.
My company uses Google authentication for every single thing in its corp IT stack, and uses Google Data Studio for data processing & accessing. There is literally no other way for me to work other than to use Chrome with all privacy options in its most open state, like keeping third-party cookies on. It just doesn't work on anything else. I don't know what Firefox can possibly do to counter the fact that Google's corporate offerings just refuse to work properly on browsers other than google chrome with its default settings.
On ubuntu 20.4 I had a bug where the page wouldn't load, I actually still have it, on 2 computers actually. Even after cleaning everything it's still here. Alsonvery often it says it hzs to restart. I used firefox for years, like 14 yeqrs until I had those shit bugs.

Also I'm using google services so the browser integration is very transparent on chrome.
I have to use it for this one browser extension that is vital for work. It sucks
I haven't tried Firefox
Nothing.
I'm a Firefox user and Linux and Android.
Testing my web application on the most used browser. I use Firefox for everything else.

Saved passwords I use Bitwarden. Way better than Chrome in every aspect of password management.
I like Firefox. But these forced updates are a real problem for me when I‚Äôm developing.
A Firefox bug reported and unfixed for 15 years which causes cmd-1 to zoom in instead of switching to the first tab on MacOS with Czech keyboard layout.
I use Qutebrowser. It looks awesome and works well. It beats whatever vim extension you can find for firefox. So, I just kinda use webkit not because I like webkit. Just because I like one of the frontends webkit gives me better.
Work cause I need them separate, but sometimes for some reason I cannot upload videos on YouTube on Firefox .
I almost always use firefox but i need the keep chromium around because college exams happen there. I open it once every 3 months. 

One of my friends says every other browser is infinitely better than chrome but he won't shift because all his data is on chrome
I daily drive Firefox and keep an unaltered chrome under the elbow just in case. Notably:
* I've had problems with Google meets, my company's choice. In fact my homepage in chrome is google meets.
* Live pages translations (ik, available from Google translate too). Just right click and have the content translated is super easy, especially since I changed country and often face administration or important websites in a language I don't understand.
* If a website breaks under FF, I first try private navigation, and then Chrome if it still doesn't work. If it works in neither, their website is broken. If it works only in chrome, their website is trash.
The only way I use chrome, or rather chromium, is via electron. I'd love to see a widely adapted firerox based alternative
They like having spyware built into their browser
Firefox doesn't seem to be able to translate pages.
Firefox is faster, but some sites flat out don't work on Firefox anymore (ie, my bank's website...)
For browsing, I use Firefox. For work, I use Chrome, becuase it has better developer tools.
1.	My passwords are saved on Google. 
2.	Chrome feels more lightweight. 
3.	I use web apps and chrome can create shortcuts for Whatsapp web for example.
I am speed with chrome
Work makes me use it. I‚Äôve exclusively used Firefox for my personal browser once the quantum rewrite. 

Day to day usage it‚Äôs better although there is some days maybe 2-3 days a year where Google pushes updates to YouTube or the few odd changes for Reddit that messes with Firefox. Spacebar doesn‚Äôt work for video and new Reddit has a few hangs.

Work wise some things don‚Äôt work as cleanly or work differently on Firefox. Specifically reading from serial ports and how some devs set up sizing of some pages. There‚Äôs just some different calls and they don‚Äôt play as nicely with Firefox. They don‚Äôt prioritize Firefox size it‚Äôs only two devs and one client that uses Firefox instead of chrome/edge/desktop client. At least according to tracking we have.
Sometimes I use brave (it's faster for some google sites) and it's so annoying, on ff I have buttons to launch new windows/private windows and I was genuinely surprised to find I couldn't do it in chromium like it feels like a small feature but if you're used to it there's no going back
Missing some extensions that‚Äôs not available on FF, that‚Äôs my only blocker ATM.
If you‚Äôre a web dev you probably have to have it so you can make sure your stuff will work in WebKit and V8. Feels like the majority of people are using some flavor of Chromium these days. And there will always be those few websites that don‚Äôt work on your usual browser.
Used to be performance. Now it‚Äôs only the Google eco system integration.
The ability to easily switch between different profiles with different extensions and logins and the like to keep my work and personal accounts separate.
one specific extension, mouse gestures GX
can't live without it 
I tried switching to Firefox, and many other browsers, but nothing compares and any other mouse gesture extensions aren't good enough for my use case
I have a very exotic situation. You see the thing is, i switched to Chrome 10 years ago, and in conclusion, here we are.

Seriously, i don't get excited about browsers any more. Does it do the job? Yes.
Does it transmit to my data to [evil corporation]? Yes! And I don't care anymore.
[Other browser] is faster actually and more privacy friendly. Great, I don't care, i just want my daily cat videos on my monitor.
Integration with many devices through my Google account.
Obscure reference but [mobilephone2003](https://m.youtube.com/c/mobilephone2003/videos) YouTube page is probably the reason I‚Äôve only used Firefox for over 10 years. Their customization is unparalleled to me.
I've got a Chromecast on one TV, and have Plex on my laptop.  I use Chrome to cast Plex content to the TV.

I know VLC can cast, too.  Use it if you like it, I'd rather use Plex.
I use Firefox and I will continue using it.

Its the only browser that gives you freedom and you do whatever you like with it.

Once I tried Google Chrome. I installed an addon to do something, but when it updated, google disabled my addon.
Firefox will prevail.
I suspect you've run in to sites that won't work, as with the Age of Internet Explorer.
I use hibernation for long periods of time, Firefox slowly creeps RAM and Video RAM. First I thought my monitor was broken, until I found out there was not enough Video RAM left for all the pixels. Killed Firefox, restored with all tabs open, works fine again (for some time).

Some years ago, all Video playback was stopped once any tab would have network traffic (load/reload). This drove me insane enough to switch to chromium after many years starting with Netscape Navigator. I only switched back when FF got multithreaded.

On Linux however Video rendering (Youtube) is still a real pain, so I watch again with Chromium.

Devtools are a mess. Some are better in Chrome, some are better in Firefox. Its always back and forth. For a long time, Firefox was really lacking there.
Firefox just gotta stop sucking I actually moved back to chrome after being on ff for many years. Mozilla should have never let eich go, and instead they focused on everything but making a good self sustaining browser for a very long time, hopefully someone can right the ship
I get BAT for using Brave, and that's pretty much it. I used Firefox for several months, and only had a couple small issues with a few websites.
Live broadcast to TV via chromecast device. Some multimedia site just won't work with Firefox (Live Casinos for instance).
Better selection of extensions
I mainly use Firefox but seldom have to fallback to chromium. On desktop that's for MS Teams and ability to download PDF from Google Docs. On mobile that's for better stability and no random crashes (weekly) and bugs (dead tabs etc)
Firefox is slow to adopt some new CSS features. And I‚Äôm tired of having to make workarounds for sites just for Firefox. 

Also the fact that they‚Äôre bleeding market share, but the CEO gives themselves a huge pay raise.
Tab trees and PiP videos keep me on Firefox.

What was the question again?
I do use Firefox primarily, but Google Translating entire web pages is kind of a necessity for me so I open Chrome pretty frequently
Nothing, updated my  pc and after attempting to use the chrome sync (in Linux) and noticing the lack of support I dropped it, and also Ff has improved greatly since I last used it, and the god send of simple ad blocking
I use Microsoft Teams as PWA.

The last time I checked Teams didn't support calls or meetings in Firefox, only chromium-based browsers. So that's why!
Because Firefox has a bit less tracking going on. The requests are still coming in. Firefox denies them while Chrome has a way to expedite them. So Chrome is faster.
I mainly use Firefox but the three use cases for Chrome for me are:

1) Chromecasting - it's a proprietary protocol but has major advantages over its nearest competitor Miracast.

2) The "app mode flag" e.g. `google-chrome --app=https://www.google.com` opens a browser without a URL bar making any website into an "app". Setting the same thing up with Firefox requires complex profiles with specific add-ons and about:config flags

3) Firefox by default does not support touch and scroll gestures on GNU/Linux - Chromium/Chrome does. They can be enabled with an about:config setting or load prefix - something to that effect, but it needs to be enabled by default on compatible hardware.
Firefox user, but Chrome has collapsible tab groups which are fantastic and would be pretty nice to have in FF.
I switched to Firefox and if I "need chrome" I use a chromium based browser like brave or just chromium itself
backdrop-filter: blur;
Chrome has better security
I actually use chrome because I've had syncing issues in the past with FF between my phone and browser when it comes to syncing passwords. I tried changing to Firefox multiple times but migrating my over 1k passwords, and having the fear that the sync service will somehow break again kinda keeps me away from doing it.
For me it's the fact that GFN only works on Chromium based browsers.
i use ungoogled-chromium simply because im used too it yes i have also used firefox for a while
Video uses a lot less energy under Chrome on my ancient Linux MacBook. But didn't work at all for Amazon Prime, so... To Firefox I go.

On my Windows box, HDR content requires either an app or Edge, so I switch around.
It‚Äôs right there ü§∑üèΩ‚Äç‚ôÄÔ∏è
I only have Chrome when I use Xbox Cloud of Stadia, which isn't much these days.
I gave up on Chrome over the years as I found all the software it installs and leaves running on MacOs --- very difficult to remove.  I switched to Firefox Developer's Edition and put in a lot of bookmarks.  As far as I know, Chromium does not do this on Linux, but I now find it very convenient to continue sharing bookmarks, searches, and a few other things across different OS platforms, using a Firefox account.
Give unto Google, what is google. I use Google apps on chrome, since they are tracking me anyway. The rest Firefox or detuned Brave.
Work says so
I was using Firefox as my main browser, but since I started using the browser, Visual Studio Code and IntelliJ IDEA at the same time, I switched to Google Chrome because it consumes less RAM in my laptop.

But I do intend to come back to Firefox once I put more RAM in my computer.
Chrome's out-of-the-box profile switching ... exists. I know FF has profiles buried under configuration and can be coerced into working like Chrome. Or I could just use Chrome.
i mainly use Firefox or Firefox based browser but use chromium for one task and this task is content that has widevine drm because it seems to be a higher quality on chromium but i heard that edge has a even higher one but i will never use that thing on Linux
From my perspective as a user, they are essentially the same, yet chromium has a better track record amongst security researchers. (I do not use google chrome.)
Completely stuck in the Google ecosystem. Too lazy to switch.
I use a Chromium-based browser. While it's not perfect, Firefox has slight issues with video acceleration while running on battery on my switchable graphics laptop that makes it slow.

Being able to watch a 4k video is nice.
I don't
For me it is the password manager, and close integration with my Google account/identity, which works well on both my Windows PCs, Linux PCs and Android phone.
I don't use Chrome but maybe some people don't use firefox because of the shitty UI.
I hate them both.   I use Vivaldi.   Love it.
I used to use Chrome because Firefox‚Äôs dev tools weren‚Äôt capable of inspecting websocket frames. However now it does and I also don‚Äôt like giving Google more info in me than required. 

Firefox is great.
At this point I'm just used to Chromium, and it hasn't given me a good enough reason to try switch away. Whenever I try to switch away, it's always annoying little nitpicks, like how tabs scroll when you open too many tabs, instead of getting smaller and smaller like chromium. 

From memory, I was originally attracted to Chromium over firefox 12+ years ago because it wasted less vertical space, especially on linux, where firefox took ages to implement custom UI support.
I'm already on Firefox, Google is trash in general if there is even small way not support them I'm in
Not gonna lie I am up for diversity in browsers and stuff, but chrome is faster on my low end i3.
And I like the classic menu type of chrome than the blown up recent ui changes made by ff.
Firefox has had several major security issues over the last few years.  It‚Äôs been banned outright at work, not that corporate decisions always have a relationship with reason. I guess everyone has, but seeing that most people are going the way of Chromium lately, I just don‚Äôt see any attraction to Firefox.
At home, I use Safari ‚Äî better incognito security (no cookies shared between incognito tabs), fast, and better battery life. 

At work, on Linux, I use Chrome because of App mode which allows me to have outlook web calendar and email and Devdocs open in dedicated windows that my window manager can individually reference as if they‚Äôre native apps of their own.  If chrome didn‚Äôt insist on opening links from within app mode in chrome, even if it‚Äôs not the native browser then I‚Äôd use Firefox for everything else, but it‚Äôs annoying to have some stuff in chrome and some in Firefox, so I gave up and just use Chrome.
It integrates very well with my Chromebook and phone. I can easily see all my tabs and continue/send searches easily.
FF fort whatever reason usually hard crash my desktop. Distro doesn't seem to matter.  My laptops are fine. So I use FF on them.
I find chrome to be better at memory management given similar number of tabs to manage. Firefox silently flushes the memory for tabs not used in a while and reloads pages when those tabs are opened.
I wanted to switch to Firefox, and made it my default browser on desktop and mobile for several months. But there were two things why i ended up back in Chrome:

1. The omnibox search in a website
2. The 'convert to app' functionality:

For#1: 
I like starting to type the a website, then pressing Tab and searching inside said website without loading it first. Firefox has a similar thing, but you have to do it manually for every website (and usually have to start the query with '@'), which slowed me down even after months of using FF as my default browser.

For #2: I have several websites that I wrap to make them look like native apps. Have not found a good way to do that with Firefox.
I don't. Chromium is almost unusable on linux. well, I do use it on Windows. Mainly because I have Chrome on my phone, so I like to have my stuff in sync.
I've actually been thinking about switching to Chrome from Firefox. There are three main reasons: Chrome has noticeably better performance on streaming video, especially YouTube, the decision to sandbox Firefox on Ubuntu and not allow you to install it through apt means many of my extensions don't work, and Firefox has issues with some website interfaces that I use for work.
I currently hate all browsers equally.

I go through phases where I try to switch back to Firefox and something always pulls me back to a chromium based browser (usually either compatibility or performance). I don't necessarily prefer one or the other, but recently at my job we were saved a whole headache because a user was using Chrome and it blocked the suspicious website (turned out to be malicious), if that was another browser i'm not confident it would have blocked it.  
Another thing I like at work is being able to run it as another user on the same machine, I always had mixed results with this on Firefox where as Chrome always worked perfectly for this process.

It feels as though there hasn't really been any innovating with Firefox and they have spent a whole lot of time chasing Chrome's coattails and not actually made any meaningful strides towards their primary niche, which I would say at this point most people suggest they want from Firefox is the privacy they don't get from Chrome. To that end I use Brave in addition to Chrome and Firefox, but it's still not perfect itself.

I was there for Firefox's beginning and I loved it, but it just sort of went stale for me.
This one is a genuine issue. Audio. Idk why, but when I play YouTube audio through both Chrome and Firefox, it comes through much less muddled in Chrome. It's not bad enough in Firefox that I think there's an issue, I'm just sensitive to that sort of thing and picked up on it.
I find the Cypress End to End testing framework doesn't work well with Firefox. Half way through the tests Firefox becomes unresponsive and locks up. Don't have this problem with Chrome, Edge, or the bundled Electron "browser".
It has my passwords and im too lazy to move them to a better browser lol.
I have two laptops (work and personal) and use chrome on one and Firefox on the other. They act basically the same besides occasional pop-ups warning that a site isn't going to work on Firefox (spoiler: it still does). I'd change to using Chrome on both or Firefox on both, but change is hard and while I prefer the ethos of Firefox, the two browsers feel essentially the same to me. Neither has a real edge over the other in my experience.

I also tried using safari for a few weeks, I hated the location of the tabs bar (I like it on top) and more importantly I had difficulty setting up or finding replacements for the few extensions I use
I've been using Firefox since they invented "Tabbed Browsing". Haven't touched anything else. And they've grown (mozilla) into a privacy company that supports our rights, which is awesome.

Edit: maybe they didn't invent it, but I remember them being the ones bringing it into popularity
I use firefox for power efficiency. I am using Ubuntu on a laptop
HDR support? Most of the time in still using FF tho
Absolutely nothing, but I've had issues with some hid devices --> Firefox seems to have a peculiar way to manage those in some cases like Wacom tablets, or alternative keyboard layouts that use too many/nested dead keys.

The latter made me change but certainly not for Chrome.

**--> I ended up using Brave, but I'd love to keep using Firefox**
On windows i always use Microsoft Edge for video streaming services like Netflix. 

Edge actually playbacks in full hd. All other browsers are 720p.
FF for most everything except:

* a telehealth provider that blocks FF
* for some reason FF with gnome doesn't have a "always on the visible workspace" option when I right click on the window, and I need that sometimes
WebGL performance
The history tab. Its cursed in Firefox.
Chromecast, recording and sharing of singular tabs are 2 of the things preventing me from switching
sites just look *different* in chromium. I think it's the font rendering? Not entirely sure. Chromium also tends to be better with supporting newer css features correctly, I recently had a bug to fix across multiple sites where Firefox didn't understand a grid layout that chromium and safari had no problem rendering (safari instead had an issue with a text node in a flex box...)
On my RasPi 4B I now use Chrome for watching fullscreen video, e.g. Youtube, as Firefox recently started dropping lots of frames (only when fullscreen) for no apparent reason.
For me is the way Bookmarks and Tabs work.
Chrome has better webgl performance for my use cases (CAD on integrated graphics), plus the integration with Google services is super helpful (especially with user profiles). I also find the developer tools to be a bit easier to use and more intuitive out of the box. Also, Firefox is unusably slow on my phone and I prefer keeping everything synced across devices.
There‚Äôs this one extension I need that automates a huge chunk of my work, it‚Äôs not available on Firefox.
I just like it better
It is faster in my experience, and the Google Account saves are perfect. And I like the general (default) design better
I keep trying to switch the Firefox but then defaulting back to Chromium browsers eventually. Chromium just "feels" smoother and faster and honestly I prefer how it looks over Firefox.

You also mentioned that Firefox "does everything Chrome does almost as well...". I like using the browser that will do the best job most of the time, that unfortunately has been Google Chrome for me.
I am a Firefox user but I love how consistent and polished Chrome (or especially Edge) feels, and AFAIK this is a reason for many people to stay on Chromium-based browsers. For example, do you know how to remove saved email from memory in FF? You have to open a form, select one you need to delete via keyboard arrows and hit Delete.

Of course there are workarounds like userChrome.css (one of my most favourite Firefox feature), but can you imagine regular user writing or even installing one?

I think FF devs need to make a global UX survey campaign and then make 1‚Äì2 updates dedicated just to this problem, without big UI changes(!!!!!). 

**‚†Ä**

And that's it, mostly. There's a few other small features, but the vast majority of regular users didn't ever used any of them, so nothing to care about.

(oh, and I also love how smooth scrolling feels in Edge, not sure if it's a Chromium feature though)
I switched back to chrome when some of my daily extensions did not work.
Most websites are optimized for chromium browsers and I spend a lot of time on the web so it just makes sense for me to use a chromium browser (brave) to browse, otherwise I'd be using librewolf or icecat anytime I can
Youtube is faster on Chrome. I feel it.
On my computer, FF needs ages to load up any sites. Can‚Äôt figure out why, I have tried almost everything. 
I used to be FF fanboi, but not anymore. 


I use Vivaldi BTW.
Well, Chromecast is such a thing. Heard about a hack to use streaming a tab or desktop also with Firefox...
I use Chrome because it works with plain ALSA, and Firefox removed support. In the past, I always used Firefox though.
How much better V8 is. Spidermonkey is cool and dandy (even mongodb uses it in it's shell for the request/response middleware and i think it's awesome) but it really needs to step up. Also PWA's are a no brainer when someone asks "i need an application thad needs to run everywhere" and you don't need peculiar OS capabilities, and firefox (desktop) is the only one that doesn't fully support. I Still use firefox for my dev stuff (both dev and esr versions) to check if everything is fine when i develop stuff thoe
YouTube with a touchscreen on Linux Firefox is infuriating. That's the only use case where I prefer chrome.
My wife refuses to use anything other than Chrome and truthfully in my case it always works well for everything I need.
Unbearably slow on twitch
javascript runs faster in chrome but I dont use chrome/chromium because It gives me mouse lag on games on windows too
There is literally nothing Chrome does that Firefox does not.
What's holding me back?Having to switch to chrome in order to be able to switch back to Firefox.

Haven't used chrome in years. Sure I have it installed on some of the systems and in recent years it was mostly Chromium that got installed for those edge cases where I needed it and Firefox didn't work.  But these are few and very far in between.

I never felt the need to stop using Firefox and that so-called difference in speed is mostly theoretical.  Yes you can benchmark it, but IRL there is not much that one experiences of it.  
And ad-blocking really works on Firefox. The best speed boost you can get on today's internet; getting rid of all the unnecessary crap by not loading it in the first place. Makes even the slowest browser fly compared to one that is bogged down in the ads and privacy stealing javascript.
Why I went back to Chrome after trying Firefox:

* Works better on more sites (e.g. some are only tested on Chrome and are more buggy on Firefox; e.g. easyaccess anime)
* Reddit (other than opening images) faster on Chrome than Firefox
* Precise scrolling on Chrome, not on Firefox (on my Kubuntu 22.04 anyways)
* More extensions in Chrome webstore than Firefox store (e.g. Mortality - Death Clock - New Tab, which doesn't seem to be available for Firefox)
* Bookmarks are easier to navigate (Firefox's bookmarks settings are a mess)
* Easier to toggle bookmark bar on site (Ctrl + Shift + B)

Overall Chrome just seems slightly faster and simpler to use for me than Firefox's cluttered UI.
I tried switching from Chrome to Firefox, I couldn't hack it. It's a lot slower, and is just clunky. I've switched to Vivaldi and couldn't be happier.
As a company, their support for deplatforming and third parties deciding what I should see.

As a product, I don't remember the last time they shipped something I was excited about. They deprecated keyword search sync via bookmarks, for example.

Brave and Vivaldi are more direct in their support of user control and routinely ship good features.

I just like the competitors' attitude and products better. If it was between stock Chrome and Firefox, I'd use Firefox.
Why not both? But vanilla chrome is better to use for webapps, FF has more familiarity as daily driver with us old schoolers.
I followed Netscape Navigator to Firefox.

When Chrome came out, it was a joke.

Then it didn't have the extensions I need.

Then it used literally four times the RAM.

Then I still don't care.

Nothing has ever^‚Ä† made me use Chrome.

^(‚Ä†) there was that small stint of time between using Windows FF+Silverlight with WINE and using FF with Chrome User Agent for Netflix.
Which poses the question...what makes you use Firefox instead of Librewolf?
Nothing, because I've been using Firefox as my main browser for the last 5 years. Firefox has more power than any other browser I've used before. Some websites drag on chrome, like YouTube and reddit, but they run smooth on Firefox. I have all of my passwords on bitwarden. Firefox is my main browser on android, too. I'll continue using it until Mozilla kicks me away. Lol
Google, because they intentionally make things work poorly on Firefox
 Both look like crap. Only Opera üòä
What holds you back from using anything BUT Firefox would be my question.
What's Chrome?  :P

I'm old school, been using firefox for many years, using Debian for many years.  Other browsers come and go.  Chrome is just the latest.

Some day, if firefox ever stops doing what I need, then I'll look around.  Until then...
Have you ever opened up you task manager with chrome running? Chrome eats up ALL available memory with hundreds of duplicate instances.  I will have hundreds of tabs open on Firefox and it barely uses 2g of ram, along with all my extensions.  I'm a Firefox user all the way
Firefox's losses of powerful addons, easy customization, and power user features over the years has convinced me to use Chrome a lot more than I used to.  Firefox keeps trying to be more like Chrome, and since Chrome is a better Chrome than Firefox, I have less and less reason to use Firefox.  The only reason I still use Firefox at all is for its better privacy features.
The only reason I'm using Firefox is a combination of Firefox Sync, which lets me sync my mobile and desktop devices, with the adblock in the mobile browser. If I could sync Bromite with Chromium, (and I mean fuly sync, open tabs and all) with Chromium, or mobile Chrome supported extensions, I would be using Chromium or Chrome.
I use Palemoon and Vivaldi.

I use Palemoon because it is essentially Firefox before it went to shit. I use Vivaldi because it the only browser that caters so much to the power user, offering all sorts of customisability and features just because it can (previously only Opera on Presto really did this).

I don‚Äôt use Firefox because, frankly, it doesn‚Äôt suit my needs. Why use a browser that fucks with my workflow when it decides on a design change or otherwise breaks addons I‚Äôm using? Every time I fire up Firefox it is blatantly clear it isn‚Äôt as performant or compatible as Chrome (Firefox users are in denial over this). It is pretty much a Chrome clone at this point, so why would I use it over actual Chrome? Had it still kept is previous design and didn‚Äôt fuck with the addon system it would have a strong argument for usage, but since it decided to piss on those what‚Äôs the point?

Who the fuck is Firefox trying to target? It isn‚Äôt the fastest or most compatible browser so isn‚Äôt appealing to the casual or the person just wanting to get things down. It has gimped its addon ecosystem to the point where Chrome is its match, so it isn‚Äôt appealing to the tinkers or power users. It doesn‚Äôt come with a stack of in-built features or customisability on the level of Vivaldi so I don‚Äôt care about it.

Mozilla took a direction that moved away from when Firefox was truly great to chasing the dream of being a Chrome clone. Becoming a Chrome clone may not have been the intention but, to me, that‚Äôs what the end result is. I don‚Äôt see the appeal and I certainly am not willing to risk having another fucked workflow when it changes course again in the design and addon fields. Even if Firefox became good tomorrow I still couldn‚Äôt bring myself to trust. Until it builds up a track record of not doing the same dumbassery it currently is I daren‚Äôt touch it. It has, in a sense, burned its bridges with me and holds nothing appealing for me.
Chrome‚Äôs performance has gone to shit in the past years. High memory and CPU. You know it‚Äôs bad when Edge and Firefox do better. Brave, tested recently had the lowest resource usage.
I actually use Edge, which I found out this last year is infinitely better than Chrome.
Because a majority of web developers use Chrome, develop for it, and it just works. There is no need to switch back to Firefox. If for some reason you believe Firefox has less tracking or more privacy or whatever, you are just fooling yourself. In reality the browser should be the least of your concerns if that is your goal.
Even though Firefox has always been the best browser, I find I have to use Chrome as my default browser, because I have a GOOGLE ACCOUNT. I use Gmail- and all the other Google apps, because I want to have it all linked up to each other...
I don't know how to easily transfer all my bookmarks and passwords in a way that isn't super tedious
I changed years ago because Firefox started eating all my memory and running crazy slow.

I can't say that, at the time, I recall it being a site-compatibility problem, although who knows now?

Since then when I've tried it has always looked weird because I had changed to a hi DPI screen.  So I can never be bothered to carry on.

The above may not still be true (so please, Reddit, don't take it as a prompt to tell me how wrong I am), but those were my reasons, maybe that helps Firefox somehow. Inertia is huge in software choice.  It took a lot to make me move away from Firefox, Mozilla really screwed the pooch during some vital years.

I've also read enough negatives about the Mozilla Foundation that I'm not really sure it wouldn't be out of the frying pan into the fire.  I don't think they really care about my privacy any more than Google do.
I mostly used Chrome for development (I like their dev tools a bit better, and more users use Chrome) but still tried to support Firefox because I believed in them.

Until they felt the need to write a blog post advocating for centralized deplatforming. When someone spreads misinformation about me and tells me they will try to kick me off the internet without even hearing me out, I tend to take that seriously.
Firefox used to not have features in the browser: "Use that addons / extensions / etc."

Also they broke these extension all the time. So I would only install minor releases and wait for the addons to work and not have FF execute undesirable javashit because noscriot was broken today and next month.

Vivaldi does include some of the important features and it does not break extensions every week. (I do not have a reason to try to switch back)
I need chromium for school, and Firefox does not use chromium
I...don't know. I actually don't have a reason, just habit I guess? I don't even make heavy use of any extensions really. I really should move to Firefox. Maybe I'll do that this afternoon after work.
Is Live Captions only in English, or does it support other languages?
That is really amazing! Is the feature chrome exclusive or is it also available on chromium?
Yes! Absolute game changer.
Unrelated but my OnePlus phone also has live captions that show up when something is playing. I don't know if other OSes have that built in but I figured I'd let you know and it might come in handy to you .-.
I don't have the hearing issue, but I find the live transcribe function on Android Roms to be more accurate that the closed captions on a lot of videos.
Google purposefully breaks a lot of standards and introduces bugs that have to be worked upon by the mozilla team which also contribute heavily to their websites not loading properly or working as intended sometimes. This is not a conspiracy theory and I read about this from a tweet by a mozilla dev
>Overall, been on FF since the early 00s and never once felt the need to make a switch.

Yeah what does OP mean "switch to Firefox from Chrome?" I've been on Firefox since before chrome existed. :D
As a laptop user, the only thing missing from Firefox is the haptic capabilities, like being able to swipe sideways to go back and forth in pages. But also this doesn't seem to be possible at all in Linux anyway, which is a bummer. (Also hoping someone sees this and knows how to implement it)

A couple years ago chromium had many add-ons and extensions, but the things that interest me have been getting built into browsers (like dark modes or pdf viewers for example), or the extensions have been getting released on Firefox. I've been using Brave until recently, but it works badly with selenium and tools of that sort, so I'm back to Firefox
I had a lot of passwords saved in  my chrome account, and the password autocompletion was very handy. And switching to another browser meant losing that ability which had been holding back me from switching for a long time. Several months ago i finally decided to switch, and started to use Bitwarden for passwords, which is amazing software. I also keep chromium, just in case something doesn't work.
I haven‚Äôt tested this in Linux so I don‚Äôt know how applicable it is.

In Windows, the way that Firefox stores passwords is fundamentally insecure unless the disk is encrypted. You can copy the %appdata%/Mozilla folder over to another machine, have access to all passwords, and will even stay authenticated to the Mozilla account to receive new passwords as they are updated.
Because Google intentionally did this to make you use Chrome
what about openstreetmap?
Linux subs are almost always Firefox circlejerks when it comes to browser related posts/questions.
Well, I think you will find that in the Linux world, Firefox is much more popular than it is with the general population. And this is being asked in the Linux reddit.
Reading comprehension failure
Yeah cause not that many Linux users seem to use Chrome?
Mozilla recently added this [embedded translation functionality](https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/) as an add-on. It works well considering that the translation is done entirely locally.
Aaah. Yes, I forgot to mention that.  That's the only thing that Firefox doesn't do. I believe you can do that with an extension, but using Chrome in that case is more convenient.
Ohh yes I really miss that in firefox
Since you mention that you like that Firefox is so customizable: did you know you can change its user interface completely? And I mean completely completely. It‚Äôs just an CSS file and there is a community that is sharing  those files on the internet of course.

https://www.howtogeek.com/334716/how-to-customize-firefoxs-user-interface-with-userchrome.css/

Ironically the css file is called userChrome.css
For me it feels like chrome is way faster, at least on reddit or imgur.
> I also changed to Firefox recently. Everything moves faster on it, it just feels lighter

Because you havent installed a ton of addons etc. 

Stock chrome is also really fast
>I find Chrome's profile management more streamlined than Firefox's. Separate profiles are better than multi-account containers

>Chrome's tab grouping. I wish Firefox containers and Chrome tab groups would have a baby because Chrome's tab group management is a lot more streamlined and organized but they share cookies. If Chrome could let you segregate cookies between groups like Firefox containers, it would be perfect for me. I just use separate profiles to achieve the same thing, so the combination of tab groups + separate profiles + Chrome's profile management brings Chrome ahead for me in this regard.

These two things i need badly in firefox, only thing stopping me getting completely off of chromium. I hope somehow mozilla dev consider this.
> I wish Firefox containers and Chrome tab groups would have a baby because Chrome's tab group management is a lot more streamlined and organized but they share cookies.

Older Firefox had an addon called TreeSize and another for managing cookies whose name currently escapes me. TreeSize still exists but just seems buggier and slower than it used to be before the addon system overhaul. Combined with the cookie manager addon it might have done what you were looking for. I used to use it all the time to sign into websites with multiple accounts at the same time. This addon was also a causality of the addon system overhaul.
> Hardware video decoding on NVIDIA.

On Chrome? Not NVIDIA, but I've been trying to make it work on Chrome or Edge with an AMD card and I gave up, even after following the whole Arch wiki information. 

Firefox on the other hand just worked on my GNOME + Wayland environment. I wish Edge worked, I prefer it over Firefox (I also love its vertical tabs), but the lack of hardware video decoding kills it for me.
> no, the address bar search isn't an equivalent

Why, I did a quick test, and they seem to do exactly the same thing.
[deleted]
PWA's also. I love having apps act as if they were desktop, especially with apps that are otherwise electron. Spotify and Discord work great as PWA's in my experience and I've really missed it in switching to Firefox
iOS only allow webkit (safari) based browser. The only thing you get by using Brave or Firefox is setting sync and a different UI, but the engine is the same as Safari‚Äôs in all ways.
Most websites that say they only work end up working fine on Firefox for me.
> and even phone

I must say that FF on my phone is awesome. It has the top bar at the bottom. Why didn't anyone else think of that? So usable...
I use it on all my machines except my phone, unfortunately. Let‚Äôs hope Europe will force some kind of browser choice on iOS as well!
The best thing about FF on your phone is you can install an adblocker for YouTube and also if you switch to the desktop version of the site when watching a YT vid you can lock your screen without it interrupting the video. It also allows you to make the video into a mini player when you minimize and switch apps to have it playing in the corner. FF makes YT premium redundant.
Had to be an Arch user‚Äôs answer. 

‚ÄúHey guys, what makes you use Chrome?‚Äù ‚ÄúWell I don‚Äôt, I use Firefox, and Arch btw‚Äù
This. Been using it when it still was Nestcape Navigator üòÇ Been using Safari a while because bundled with macOS, and still sometimes do. However, Firefox is on all my machines, Windows, macOS, Linux, Tablets etc. And my preferred browser of all time.
The battery drain of Firefox on android is insane. It drains more battery than my 120hz screen! How is that possible? I'm thinking about switching to some other browsers but i just can't because of the extension support...
I use Fire Fox focus and it‚Äôs great.
Do you mean progressive web apps that you can launch as if they were desktop applications?

Because that's the only feature of Chromium that I use, too.
FirefoxPWA somewhat fills the gap. It's an extension that allows PWA installations.
This is the big one for me too. I use PWA's all the time.
There is a program developed by Linux Mint called [webapp-manager](https://github.com/linuxmint/webapp-manager). It is a bit limited but sometimes it will do the job.
> Now is impossible to ignore chrome only pages.

I wouldn't say that. I think what you're saying is true, but mostly very exaggerated. It's very, very rare for me to find a site that doesn't work with Firefox. In fact, I can't really think of any off the top of my head.
Multi-account containers and Temporary containers are awesome, can't live without them anymore
‚òùÔ∏è
One of the features that I wish Brave Browser had.
two question i have:

1. I'm using container, but what i liked in chrome is to club few tab together which helped in being organised and keep a lot of tab open. it would have been helpful if tab in same container could get clubbed together.

2. Chromium based browser also let's me keep miltiple profile in browser which again helps in being orgainsed.
I have tried a lot but is there any of having multiple profile in firefox.

Thanks
They actually worked on it for a while but then actively decided to stop the development.

https://bugzilla.mozilla.org/show_bug.cgi?id=1682593
Exactly.  This is so frustrating.  I switched to chromium based browser ecosystem recently only for this reason üò• but I love firefox
Is there a Mozilla developer here that can kindly explain why there isn't an effort to support chromecasting in Firefox either natively or via an extension. Not just on youtube but other sites as well. Is there a patent on this being held by google or something? This seems like it should just be an obvious addition to an otherwise great web browser.
I don‚Äôt even know what that is lol
However in firefox you can see them in a floating window, even on top of a game,(fs, borderless window). Last time i checked Chrome didn't knew this trick.
I have all kinds of issues with casting to a Chromecast anyway. I looked at the reviews for the google home app and a lot of people say the same thing.
/u/1Blue3Brown, please try the troubleshooting steps on the Firefox [wiki](/r/firefox/wiki/support/troubleshooting) and let us know if you need more help.

It helps if you can tell us what didn't work - like "safe mode didn't work, neither did a refresh or a new profile."

That will help us prevent back and forth and let us help you better.
That's the password manager  I'm using. It's an amazing piece of software.
When the makers of Encryptr tell you they are getting out of the password manager business but recommend a completely different company's product due to just plain being objectively better, I consider it worth a look.  And they weren't lying either, Bitwarden is awesome.
I was having memory issues with the official bitwarden server implementation (specifically with the MS SQL Server container... It felt like a memory leak but may have been some periodic reindexing call or something... Either way it was eating all 8gbs of memory in my server) so I swapped to [this implementation](https://github.com/dani-garcia/vaultwarden). Way lighter on resources, uses all of the same extensions and apps. If you run into issues it's worth a look.
Any way to get Firefox to stop asking to save passwords?
For me it's absolutely laziness. I know there are huge suites of "transfer tools" to bring all my content over to chrome, but I simply don't feel like it...
You can reinstall easily [https://addons.mozilla.org/en-US/firefox/addon/panorama-tab-groups/](https://addons.mozilla.org/en-US/firefox/addon/panorama-tab-groups/)

Still using my tab groups like it's 2016
Try changing your user agent (with an extension)
It's why I use Brave, some websites just don't work well in Firefox and need a Chromium based browser. I have a hardened Firefox for when I'm browsing non login websites, but everything I login to I'll use Brave.
Because Google intentionally did this to make you use Chrome
I've found that most stuff that doesn't work on Firefox works in Librewolf.
[deleted]
Could you expand on this please?  I don't the issue you're raising.
I don't know that better Maps performance is because of hardware acceleration
Goggle account and bookmarks sync were the killer features of the Chrome.

The second bullet was  the video playback. It ate almost everything from Flash to something new. Whereas Mozilla/Firefox always complained about missing video plugins.
I find Firefox Multi Account Containers to replace and be far superior to profiles.

Instead of having to log in / logout of different profiles for different environments,  I have each environment sandboxed into their own color coded container tabs.

While I can have all the environments open at the same time,  they are completely separated and don't share cookies, memory, or any information with each other.
For the first one, Firefox does have good support for profiles, but for some reason does not surface it. You can get it with an easy extension: https://addons.mozilla.org/en-US/firefox/addon/profile-switcher/
> Less than year(!) ago Firefox got a back/forward mouse buttons support on macOS whereas Chrome was supporting them for a long time.

That was a bug that took them awhile to fix. Before the bug they've had support since well before Firefox was Firefox.
Firefox has supported multiple profiles since like 2003. You can even password protect them. You do need to make a launcher but that's a 3 minute task.
It does support profiles. I use them all the time.
> No sane support of profiles. It‚Äôs crucial for me as I‚Äôm often sharing my desktop with colleagues during meetings so keeping browsing history private in a separate profile is a must for me. I already had an incident with Firefox when it had exposed sensitive info in the address bar during a meeting so I‚Äôd uninstalled it immediately after the incident. Actually firefox has profiles but it requires many hacks to get them working like in chrome.

It isn't really a hack to use the UI in `about:profiles` to launch profiles. :/ You could also just use Firefox Developer edition for separate stuff.
>	The reality is Mozilla gets such a staggering amount of its income from Google, and has done for many, many years.  There is no ‚Äúsupporting the little guy‚Äù with Mozilla.. they‚Äôve been sucking that corporate teat hard for nearly two decades, and as soon as some of that support is threatened they go straight into ads in their software.

It‚Äôs almost like developing a browser is very expensive. How exactly should Mozilla find itself if you don‚Äôt like ads or getting money from search engines?
Unsure why downvoted, but it's all true. You just can't say certain things out loud, I guess.
> HTML/CSS/file format compatibility - Lots of little weird glitches you don't realise until you build sites yourself. For example I had an issue where Firefox simply would not load a font for a site I built despite every single other piece of software working fine with it.

Chrome isn't the standard, how do you know you weren't doing it wrong but was accepted by Chrome? 

>Random breaks - SSL certificate validation failure, some big sites not working while the Firefox team sorts it out, etc. They're infrequent, but annoying and show the gaps in quality and professionalism at Mozilla.

Pretty sure Google has botched SSL cert renewals as well. That kind of stuff happens, unfortunately. 

>The reality is Mozilla gets such a staggering amount of its income from Google, and has done for many, many years. There is no "supporting the little guy" with Mozilla.. they've been sucking that corporate teat hard for nearly two decades, and as soon as some of that support is threatened they go straight into ads in their software.

Man, I guess the solution is to go straight to Evilcorp - like, what are you even saying? "I heard some bad people donated money to a cause I care about, so I'm just going to support the bad people instead!"
I hope you are using your 3 chome tabs allowance wisely
Chrome doesn't support HEVC either though... https://caniuse.com/hevc
See https://accounts.firefox.com/settings#connected-services
Chrome isn't bug free, you know.
As soon as the sites will log you off trice during one use, users will store the password and I can't blame them - I do it on medium-important things, too. Also unimportant forum accounts.

My important passwords are in gpg files.
>I don't know about security, but I don't really care.

That's the spirit.

If you have nothing to hide, you have nothing to fear. /s
> Better hardware video acceleration support

Same for me. I use [FoundryVTT](https://foundryvtt.com/) every week. I get a solid 60fps in Chromium out of the box. I get <40fps in Firefox. Until Firefox fixes that (and removes their out of the box adware), I'm sticking with Chromium.
I saw benchmarks, but myself never felt that Firefox is slower.
Give Firefox 103 a shot, it comes out on July 26 and should fix hardware accelerated video decoding on Linux
> Better hardware video acceleration support

Pretty sure there *isn't* support, so that's a very odd thing to say.
By webkit you mean *web engine*, Safari uses WebKit.

All browsers handle logins for decades.
> Sometimes my Firefox install just breaks and I have no idea why.

There are hundreds of millions of people using Firefox and they likely don't have many of those issues, so it is likely that you are unlucky. I can understand being gun-shy about this given you experience, but sadly all software has bugs.
https://www.ghacks.net/2022/07/16/light-pdf-editing-is-coming-to-firefox/
> Edge: their built-in PDF Viewer is better than everything I've seen (Okular, Adobe reader, Firefox, ..). You can add text, annotate and do basic editing in the browser. It's interface is waaay more simple than other software.

Sad how quickly people sell out open source for a tiny bit of functionality (yes, I realize some people don't care about that, but this is a Linux forum, so I expect that a lot of people will).
[deleted]
That was my situation))
I imported history/bookmarks to Firefox, and started to use Bitwarden for passwords. It was painful at first, but the passwords were transferred rather quickly and suddenly the sun was shining again)
Have you considered using the Windows app on Bottles? It works great, bar some issues with the lack of Wine webcam support (it can even take a screenshot if you're on X11).
Or could have used Safari, or Opera, or Camino, or Konqueror, or Epiphany, or any other non-IE browser.
> Moreover, it's easier for developers to only target the one base (We already have apple for making things difficult).

Wouldn't it have been great if the web was reliant on IE6?
> First thing is the lack of a proper datetime input. It is not supported by the browser natively. See the bug ticket here [here](https://bugzilla.mozilla.org/show_bug.cgi?id=137367).

Are you sure you don't mean https://bugzilla.mozilla.org/show_bug.cgi?id=datetime ?
Try compact UI: https://support.mozilla.org/kb/compact-mode-workaround-firefox
> Honestly, I'm an Android user. I'm far enough into the Google ecosystem and chrome dominated at the start that I have no reason to switch now.

Extensions on mobile.
Use a new email?

PS: there is no other way to handle this from a security perspective!
I imagine Mozilla wouldn't trust Google with storing users' passwords,¬π and Google doesn't want someone else to store users' passwords because Google wants as much user data as it can get its hands on. For Google, user data is money.

¬π This would also make plenty of users to either stop using this functionality or dump FF.
> I when do dev work I test it on Chrome mostly.

You are just perpetuating the very same problem!

You can instead (at least) report sites to https://webcompat.com
Your submission was automatically removed because you linked to the mobile version of a website using Google AMP. Please post the original article, generally this is done by removing amp in the URL.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/linux) if you have any questions or concerns.*
ü¶Å
Those sound like bugs - have you reproducing in a fresh profile?
Note qutebrowser uses QtWebEngine by default, which is based on Chromium. You *can* still use it with QtWebKit currently, but that's based on a 2016 WebKit, so I wouldn't recommend it.
Huh? Brave has the same options FF has as far as I can tell.
> Whenever I try to switch away, it's always annoying little nitpicks, like how tabs scroll when you open too many tabs, instead of getting smaller and smaller like chromium.

I feel like that is better in Firefox - why would you want your tabs to be less accessible?
Try compact UI: https://support.mozilla.org/kb/compact-mode-workaround-firefox
Even Chrome? I'd think they would be supported at least as well as Edge, if not better.
> for some reason FF with gnome doesn't have a "always on the visible workspace" option when I right click on the window, and I need that sometimes

Enable the titlebar.
Would this telehealth provider work if you switched your user-agent string?
Yes, font rendering is incorrect in Chromium on Windows and Linux.
> I keep trying to switch the Firefox but then defaulting back to Chromium browsers eventually. Chromium just "feels" smoother and faster and honestly I prefer how it looks over Firefox.

Try continuing to open tabs in Firefox and get used to how it just keeps working. Chromium *will* crap out far before Firefox will.
Ironic flair given that Vivaldi is closed source.
I have it on my PC, but I don't really know what the exact benefits of using Librewolf are.
>what makes you use Firefox instead of Librewolf?

Browsers come and go all the time. The ones that still haven't died are badly supported and/or eventually sell out, like Brave now having in-browser ads.

Firefox, on the other hand, has been here for quite a while and has a positive reputation when it comes to security and privacy.

I hadn't heard of Librewolf before, but it appears to be Firefox with some tweaks. Installing uBlock Origin, changing search engine, or disabling pocket is just default stuff I do when I make a new FF installation. Personally, I'd rather be the one doing this knowing the browser I'm using is and will continue to be well supported and has a track history of being on the right side of things.
I am capable of following instructions on how to change about:config values and also think that some of the librewolf measures are excessive
Powerful add-ons are still available: https://webextensions-experiments.readthedocs.io
Takes less than 5 minutes to figure this out.
>  I don't think they really care about my privacy any more than Google do.

I mean, their sync is encrypted end to end and Google's isn't, so... it is kind of obvious.
Let us know how you get on, and join us over at /r/firefox!
I‚Äôm not sure. If you click the three dots in the upper rh corner, click Settings, then click Advanced on the left side of the screen, then Accessibility, the option for captions comes up. There might be a way to get other languages‚ÄîI know there is in the captions for Google Meet.
There are support for other languages, and I‚Äôve used them, not sure if they rolled it out yet for general availability.
Big 'ole blob of ML model weights.

https://github.com/biemster/gasr

https://hackaday.io/project/164399-android-offline-speech-recognition-natively-on-pc
I don‚Äôt know. I know it‚Äôs available on Android, Chrome, and Google Meet.
Galaxy phoned have it too
Wait one plus has its own OS?!
My "favorite" was when they made the blurred background in Google Meet work on Firefox by mistake for an entire week, while still claiming that Firefox doesn't support the necessary APIs.
Does that require web developers to use new technology or does it break existing websites?
can you share the source or the author of the tweet?
Embrace. Extend. Extinguish.
One of the big reason why I reduced my use of google products as much as I could, I essentially only use youtube now.
I've been on Firefox since before it was called Firefox. ;-)
Yeah, I remember all the fuzz around when Google released Chrome, many of my college friends were all excited just because it was a Google product... I never liked it, I kept using Firefox then and will keep using it I'm sure
I could have sworn Firefox offers to import saved passwords.
You can export them from chrome and import them to FF. Takes like 2 minutes total. I use a password manager though. Bitwarden is open source and it‚Äôs value is waaaaay above the 10 usd it costs per year
I moved to keepassxc after finding out that chrome passwords are supposedly not handled very securely on client devices.    
Sure, I guess having to manage the database file is meh, but it has an extension that works just fine and is pretty close to just using the built-in.
I switched to Firefox and exported/imported my passwords just fine.
what about you use chrome user agent?
> Linux subs are ~~almost~~ always ~~Firefox~~ circlejerks

Fixed it :P

>Linux subs are almost always ~~Firefox~~ circlejerks ~~when it comes to browser related posts/questions.~~

Arch/Gentoo > Mint/Ubuntu

apt > snap

cli > gui

twm > de

vim > ide
I found extension that can translate selected text (by opening google translate tab) but not a whole web page
I think I've heard recently that they are working on something like that. But it should even work locally to not send a copy of the text to a third party.

It seems like it's not shipped built-in, but as an extension: https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/
Not really that ironic, the parts around the actual website pane have been called the Chrome since well over a decade before Google Chrome existed.
As a Firefox fanboy, they both feel the same out of the box for me. For some reason, when I get all the extensions I use, Firefox just slows down a bit while Chrome stays snappy. Now, I'm not saying this is because of the browsers, it might be because of the extensions, all I know is, this has been my personal experience with the two.

That being said, I don't care. Slower or not, I'll still use Firefox.
New reddit? Old reddit loads so fast I can't tell the difference. I think youtube does feel a little faster though.
I agree, for a tab-hogger like me chromium handles it a lot better than Firefox which starts to slow down on my machine after about 20 tabs
According to most benchmarks Chrome is faster a bit
Yes, I've got it working with `--enable-features=VaapiVideoDecoder,VaapiVideoEncoder` in ~/.config/chrome-flags.conf, confirmed on chrome://gpu. I still experience issues with Wayland on NVIDIA in certain applications, so I'm still on X11. I think there are some additional flags you need to enable in chrome-flags.conf on Wayland.

Edit: Edge doesn't use chrome-flags.conf, but I just verified that it works on Edge too when you pass the flags in the terminal.
See [here](https://www.reddit.com/r/linux/comments/w15cze/what_makes_you_use_chrome_instead_of_firefox/igmrbg4/).
[Here is an image that illustrates the difference with the same tabs open and same tabs recently closed](https://i.imgur.com/qlg0RNP.png)

Notably, on Chrome:

- I can see which tabs are playing media
- I can see and search recently closed tabs
- I can close tabs from here
- Tabs are sorted by most recent activity rather than in the order they were opened
- I can see more tabs at once while searching
- I can see *all my tabs* in a vertical overview
- I don't have to type an extra key and `%` is an awkward key to press
- You actually have to type `%` then space, or you will search google for `%example_tab_search` (illustrated by the top Firefox picture) unless you navigate with the mouse or cursor keys.

EDIT: Oh, I almost forgot, [in Firefox you can't search across containers](https://i.imgur.com/7SYXaMb.png). Why would I ever want it to work like that?
Discord works better on Wayland as a PWA for screen sharing.
I know that, missed a ‚ÄûUI‚Äú in my reply.
Yikes, the illusion of choice is all they give you on your own phone. Rotten Apples.
Good for you?!
Most Android browsers have that option now.
iOS Safari has been doing that for about a year now
As a lefty, I hate it so much. Thankfully there is an option to put it back on top.
I think Opera mobile had that as well. Really handy.
They got that off the SailfishOS Browser! That's very well designed for one-handed use.
‚ÄúWhy didn‚Äôt anyone else think of that‚Äù rofl
The Windows Phone internet browser had it 7 years ago...
Firefox and Firefox-based browsers have additional security vulnerabilities on Android, so you may want to keep that in mind.
Wait what? You can't run other browsers on ios?
Pro tip: Assuming you're looking for FOSS alternatives, there's a great YouTube client called NewPipe that allows all of that as well and has no ads.
Same here, except that i use mobile YT website.
Background playback works on Brave too.
How the hell do use it on tablets? Do you mean iPads? The Android version is unusable on tablets tbh.
Hmm, it seems alright for me. But i do cap refresh rate to 60Hz. S21 FE.
FirefoxPWA somewhat fills the gap. It's an extension that allows PWA installations.
Yes!!!
Firefox customized and locked down for researching and stuff. Chromium for stupid company thingies.
 I didn't mean that pages do not work, but are annoyingly buggy on Firefox and OK on Vivaldi. Maybe "chrome only" is not correct term, since I am not a native English speaker. But maybe they are "Chrome first"?
Is there a difference between temporary containers and using private browsing mode? I've always used private mode for browsing in a temporary session
What are those?
Same! It‚Äôs awesome
Can someone explain the practical applications you could use those add-ons for, and the difference between the two. I can understand multi-account containers, so I have a work GMAIL and a personal GMAIL< I can stay logged into both using multi-account containers, right? What else? And when would you use temporary containers?
1. https://addons.mozilla.org/en-US/firefox/addon/simple-tab-groups/

2. https://addons.mozilla.org/en-US/firefox/addon/profile-switcher/
Because Google intentionally did this to make you use Chrome
Remember https://hacks.mozilla.org/2014/09/matchstick-brings-firefox-os-to-your-hdtv-be-the-first-to-get-a-developer-stick/
There is an extension and bridge for Chromecast on Firefox. https://github.com/hensm/fx_cast
Not a dev, but Google won't let them. Like, Google won't tell anyone how to do it so only they can so people use Chrome. That's the end of it. Google is like the mafia with the W3C, they have to do as Alphabet says or else‚Ä¶
Why doesn't Google add it to Firefox? It is their proprietary device that needs special handling, after all.
It is a closed protocol
Chrome also has floating video popups.
Then my work here is done
What about KeePass? It comes highly praised and with a steep learning curve I guess :)
Yeah there's a switch somewhere in the preferences
I went through this other day for Teams actually! It at least made calling work, and you can hear everyone and send video... Buuut you can't see anyone else's video. Microsoft is also in the Chromium ecosystem with Edge now, so I guess it's just yet another Chrome-specific feature that isn't part of Web standards.
You're getting downvoted for stating the facts.  The Google Voice website, for example, regularly gets hung in Firefox (possibly not a coincidence) and works fine in Chromium-based browsers.
i don't want to use another Chrome based browser
I wasn't talking about any google product; but I will need to test it with a modified user agent.
That doesn't really make a lot of sense, given that Librewolf is just a set of configuration settings for Firefox (that are more likely to break things).
>Not sure why you're being downvoted

Reddit and someone saying something against the favorite thing ;)

&#x200B;

> I see no point in using firefox honestly.

Me neither - at least not when graphical work is involved. Otherwise it's a great browser honestly.
Don't know why the other comment was deleted. Had 2 screenshots which shown the issue perfectly well. 

Firefox isn't using "normal" color profiles but rather an own or false one. This results in pictures being rendered in often a very warm look rather than what they should look like. Especially a criterion for exclusion for graphical work.
Honestly, I'm not sure. From what I could gather it seemed that WebGL-based stuff had performance issues in my case so I assume it was related to hardware acceleration.
IIRC 2D Canvas rendering is CPU only for Firefox on Linux. You can see this by going to the "HTML 5 fishbowl" on Azure and comparing it to any Chromium based browser.

There was a time whem Cairo actually was accelerated with XRender. But with the transition to 3D compositing they abandoned that backend because it didn't work well with it. And now with Wayland taking over reenabling that would be more difficult as well.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In case you do not know: Firefox has had bookmark syncing for a long time now.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'I sort of use Firefox as my completely unmodded browser. No plug-ins, nothing adapted. It's pretty default. If stuff doesn't work there, there has to be a problem with the website.' Yes - in a sense. Yet many websites have been designed to accommodate Chromium-based browsers only; and, at least arguably, some of Google's services work poorly with Firefox _by design_. On those latter two points, see elsewhere in this thread.
Containers are designed to keep sites separated but you still using the same browser with bookmarks/history. My concern is to keep bookmarks/history separated but yet synced across devices. This can be achieved only with profiles. Yep, FF has them but they a) well hidden and not a part of regular ui b) you can‚Äôt run several profiles at once. That‚Äôs what I meant.
This plugin is only a profile switcher but due to Firefox design you can use only one profile at the time. To break this you need a workaround.
Yes, that‚Äôs true. I meant profiles never were a first-class citizen so you can‚Äôt:

* Feature is well hidden so you can‚Äôt switch/open profiles using regular interface.
* Run several profiles simultaneously.
* No profile support on smartphones.

To workaround this you need do some hacks (like a ¬´launcher¬ª you mentioned) except the last one.
They're getting downvoted because they're wrong, not because it's true.

Otherwise:

* Firefox HTML/CSS compliance is second best in the biz. They're behind on a few fringe features Chrome implements. When I see someone saying a font won't load and it's a firefox issue, then they're taking advantage of a Chrome specific quirk and it probably isn't HTML 5 compliant.
* Random breaks are because the web is written for Chrome. It sucks, but that is not Firefox's fault.
* The mobile version lost access to every extension and only has a few now, so let's switch to mobile Chrome which has never had extensions
* If there's a company who is bad with management, it would be Google. Their CEO:worker pay ratio is insane, and they have a history of trashing products after a few years
* Google Chrome is built around sponsored results, it's literally Google's business model

In some benchmarks Chrome performs better, but I use Chrome every day for work (I have to) and do not find much of a difference. More stutters when closing Chrome tabs is the biggest difference I notice tbh
Chrome isn't really much worse than Firefox when it comes to memory management
Huh, I guess video looks better in chrome for a different reason. Thanks for correcting me!
File a bug? https://firefox-source-docs.mozilla.org/performance/reporting_a_performance_problem.html
That might not be unanimous, but I feel a pretty noticeable difference here.
Okay, this might be pretty niche, but I ran the Surface Linux kernel on my Surface Go. Firefox was a giant battery hog while Ungoogled Chromium wasn‚Äôt. Firefox lagged during animations while Chromium didn‚Äôt. Firefox (even with the h264ify extension) wasn‚Äôt able to do 1080p full screen reliably while Chromium did.
IME Chrome actually feels slower to me.  Sometimes when I click on a tab in Chrome there is this millisecond delay before the click registers and the tab switches.  I never had that with Firefox.
Right! I am aware they are addressing video acceleration and sandboxing issues, but it's not just that, and now I'm really enjoying Chrome. hehe
Yes! Thanks for correction
The thing being that my use case always causes me to run into bugs on Firefox which I don't run into on a Chrome browser.

Don't get me wrong, ideologically, I want Firefox to reign supreme.
thanks!
The thing is, I wouldn't be using it if I had to edit 1 pdf document a week or a month. But when you're a student and your whole job is to sit in front of a laptop and read and work on pdf documents (=lectures), you wouldn't care if it's open-source or closed-source.
Neither. I'm just a guy from the old internet, where providers didn't stalk the community and try to make money out of every little shit website or service.
The official Wine DB says the latest desktop version is broken as hell.
Fixed it, thanks
And fixed markdown too
I didn't know this extension existed ! I think I will stuck with Chrome tho, but if someday I switch to Firefox, I'll use this !
I didn't know this extension existed ! I think I will stuck with Chrome tho, but if someday I switch to Firefox, I'll use this !
They could assume that whoever owns the email owns the Firefox account, and let the person delete the account.

Deleting the account doesn't expose any personal data.
Please reread what I am asking for - a third-party extension, not something built-in.
The mouse button thing is definitely a bug. The Google sheets problem is working as intended by Google. They fuck it up on purpose for other browsers.
Ah thanks. Also, aren‚Äôt you the person who maintains qute? If so, awesome piece of software. Keep up the good work
can you customize the toolbar area to whatever you want on brave? (not talking about custom css)
It's totally a personal preference thing.   
But for me, smaller tabs are way more accessible. If I have to scroll, then it takes time to find the tab that I want. Or worse, I might forget about a tab all together.

[This is what my screen typically looks like](https://i.imgur.com/5utVPkO.png), I can see all my tabs at a glance, and the fact that they have collapsed down into just favicons is not a hindrance to me. I remember tabs with a combination of favicon and location.
Chromium actually gives you the choice nowadays - there are flags to enable tab scrolling and you can adjust the degree to which tabs shrink before they start to scroll too. Tried it on Brave and haven't found a reason to turn them off.

The only real issue is that it kinda enables me to hoard two gazillion tabs. But I get the best of both worlds with scrolling and collapsible tab groups.
https://help.netflix.com/en/node/23931

Look under "Netflix features" Edge and windows apps are the only ones playing over 720p.
Why would I do that? Why would I force someone else's browsing methods onto myself. I use what's best for my use case and so should you. If you prefer how Firefox handles tab management by all means use Firefox. I only every have a max of 5-6 tabs open at any time so I don't have any problems with Chrome.

If your point is that opening 100 tabs is easier in Firefox than Chrome then again it doesn't really influence my decision. I love Firefox, but I find Chrome works better for me on the sites that matter the most to me (YT, Netflix, Plex, Reddit).
It's more privacy focused than Firefox. A fair few of FF settings are disabled.
Librewolf is a fork of Firefox so it will be around as long as Firefox I suppose. 

And it's not really FF with tweaks, more FF with a lot of stuff turned off.

But if you are comfortable with making your own changes to FF do that, some people aren't though so an otb solution suits them.

I also have ungoogled-chromium and hardened-firefox installed which are both good but required some setting up. And after I'd done that....I carried on using Librewolf anyway. It just suits me. And I have tor-browser as well if I ever need that.
> The ones that still haven't died are badly supported and/or eventually sell out, like Brave now having in-browser ads.

Well, that was actually the entire point of Brave - it is an ad network built into the browser.
Last I tried was years ago so I tried again. It wasn't intuitive at all and required a lot more menu hunting then needed.

It isn't nearly as straightforward as it could be I had to extract html and csv files instead of just using the built in tools.
  
Yeah, I got it done but I can see why a lot of other people just didn't bother as it requires more effort than such a simple task merits

The way I see it, if it requires a google search to figure something this basic out, something's wrong

Mozilla ought to make the process simpler
I installed it and moved everything over. Works well.
Wtf
https://www.zdnet.com/article/former-mozilla-exec-google-has-sabotaged-firefox-for-years/

Here‚Äôs an article
Bingo. Been saying for some time, they are Microsoft now, and worse. Everyone tells me, even here, to shut up because they are ‚Äúopen‚Äù and they love the GPL. They absolutely don‚Äôt, they use the Open Handset Alliance to monopolize the ‚Äúopen‚Äù platform called android, where we‚Äôre told they should be praised. Incorrect, kernel mainline be dammed, to say it‚Äôs android, you must install their apps. Apple is closed source, a walled garden you scream at me not realizing google is just abusing you to take out a competitor. 

The irony? Web is great, look at all these **applications** running on it. In JavaScript, why Microsoft had to crush Netscape, fear. We‚Äôre now free! Except what made JS more viable, fast? Their engine, their browser. 

It‚Äôs just market control, same as Microsoft - who actually contributes money and code to open source in comparison. No company is altruistic, spare me. Numbers show a difference. All three companies leverage open source, we all benefit from those tools, that code, but one has become a clear lying grifter. 

No one, sadly, seems to care.
EEE
they even dropped the don't be evil mentality
eMbRaCe ExTEnD ExTinGUisH üòÇü§£üòÇüòÇüòÇü§ëü§ëü§†ü§†ü§¢ü§Æü§Æü§Æü§ßü§íüí©üëøüòàüòπüòπüôâüôäüéäüôàüôàü§ïüò¨üò¨üò≥üò≥üò≥üò≥üò≤üòØüòØü§®ü§®üßêüßêüßêüò°ü§¨ü§¨üò§üò§üò§ü´°
On android you can access youtube (and subscribe without a Google account!) through NewPipe. It's not a perfect solution by any means, but it works great and obfuscates just a little more of my data from Google :)
Netscape Navigator?
Me2.  Started with Mosaic then Netscape, but of course I was using Archie/Gopher/Telnet late 80's/early 90's.   Who remembers MUDs?

BBS before that too I guess..
I miss Thunderbird.
Phu, 

In my day's there was no Chrome or Firefox  
One had to download Mosaic and fork it to get Netscape.    
What is that newfangled Firefox thing you all talk about?
As a kid, I was excited when Chrome first came out because it worked so much better than Internet Explorer. Having better speed, tabs, and extensions was so nice. The public schools reinforced usage of Chrome as well. I just wish that I would have known about Firefox then, because if I would have, I would have switched. By the time I got around to trying Firefox, version 56 was out. Since switching to Firefox, I've witnessed it decline and become too much like Chrome. The only change that I like from the past five years is being able to block automatic videos and audio. Every other change has been disappointment after disappointment, or it's been a change that I feel indifferent about.
It does...... it has since version 80 I believe.... You can export them as well.
I‚Äôve used KeePass for a while, but personally I find Bitwarden to be easier to use across devices.

KeePass was really useful to start SSH and login to a server with a simple shortcut, but since I don‚Äôt need to do that for my private pw‚Äôs and like to keep everything synced Bitwarden became my main pwmanager.
Well, there was LastPass for a while, but they done f‚Äôed up.
Same here, I like that KeePass doesn't make me login to some centralized Internet site, like other password managers do.

Local password file.  Perfectly secure.  Works for me.
Oh yeah it is highly recommended to NEVER use the password "manager" within browsers. Bitwarden is great and currently what I am using. Debating on making the jump over to KeePassXC though.
One of my seinor developers told me how he was hacking people's passwords saved on chrome. Chrome used to save passwords without any encryption. After knowing that, no sane human being would save his passwords there)
No they implement functionality that isn't part of the web standards into Chrome, then they make their websites use these new features of Chrome. Then they use this as leverage to force the web standards organizations to codify the feature they implemented, or to just hurt their competitor browsers

Even if Firefox implements it, if it's not a standard & the only things using this are Google products, Google can just change their implementation of the spec in Chrome and in their websites and now Firefox is going to be completely broken on Google's sites

Google is happy to abuse their browser monopoly
True.
I'd imagine it probably exists, google exposes these kinds of things in their web APIs, theres no inherent reason it couldn't be made.
If you paste an URL into google translate it will give you a translate URL for the webpage with a header where you can revert to original or change the translate language.
How well does it translate? Can you share your experience?
very weird it's not even featured on the website...even when they have a fucking page on translating addons
I tried it but unfortunately it supports only few languages and French is not among of them. Also translation quality is worse than Google Translate.
TIL!
Same here. I have to use Chrome at work (or had to) and always found Chrome a bit snappier with all the same extensions compared to Firefox, although I still solely use Firefox everywhere else.

Really, the only reason I haven't switched away from Chrome at my job is that Chrome's tab grouping is so damn nice and simple. Makes organizing projects really nice. The extensions I've seen for Firefox just aren't as convenient.
Use NewPipe, it's blazingly fast

It's a FOSS YouTube app on the F-Droid store
Last time I tried Chrome it took much more memory in my use case (which is often ~100 tabs :P)
And everybody hated it at launch iirc.
What bothers you about it as a lefty? I‚Äôm one as well but I have no problems with it.
Interesting, what makes the bottom navigation bar unfriendly to left-handed users?
why would being left-handed matter to the vertical placement of the bar?
As a lefty, this comment makes no sense. Apple‚Äôs lack of support for moving.the.period.though.is.enough.to drive me nuts
Pretty sure that is one of the arguments Apple is using to keep alternative browser engines off of iOS. Pretty amusing to see people parroting this stuff in a Linux forum.
You can, but internally they all have to use the system WebKit.
Android Version works great on both of my fire tablets.
It's not because of 120hz tho.

It seems like it's because of extensions (uBlock, Privacy Badger, SponsorBlock, Google Search Fixer) but i can't live without them...
You have a bit more control with its settings, like for example you can let it keep temp tabs you visit in history, or not
They're Firefox extensions

https://addons.mozilla.org/en-US/firefox/addon/multi-account-containers/

https://addons.mozilla.org/en-US/firefox/addon/temporary-containers/
Yeah, that's so cruel ü•∫
what would be a better way to do it? Standardizing it first would probably take years
That does make sense, chromium supports Chromecasting yet is fully open source, surely they could figure out how its implemented via their open source code right?
I've seen this [scenario](https://i.imgur.com/oiayJWO.jpeg) before... Also using bitwarden, it seems I didn't have to do any work here.

Edit: It's open source and you can self host it. Just a few nice things to consider.
Online vs offline. Pick your poison
It'd be great if there were evidence it isn't a coincidence. Sounds kinda iffy on legal grounds.
Yeah me neither.
It‚Äôs not the user agent detected. It‚Äôs proprietary software stuff
Yeah you're probably talking about MS Teams, there's an extension that fixes it https://addons.mozilla.org/en-US/firefox/addon/teams-phone-fix/
Maybe it has a builtin user agent switcher, idk.
Oh really?  Thanks, I was not aware of this.
You can run multiple profiles. Otherwise you are right it's not the most user facing.
>Firefox HTML/CSS compliance is second best in the biz. They're behind on a few fringe features Chrome implements. When I see someone saying a font won't load and it's a firefox issue, then they're taking advantage of a Chrome specific quirk and it probably isn't HTML 5 compliant.

That's not true, there are lots of little quirks with the way that Firefox renders that is non-standard.  I've worked with Chrome / Firefox / Safari / IE / etc. daily for over a decade, all have there little oddities - Safari is hands-down the worst, but Firefox is noticeably behind Chrome.

The site with the font issue is [toychester.com](https://toychester.com), the ARCO font used for the feature headings - take a look for yourself.  All the others work, but not that one in Firefox despite it working everywhere else I've used the font.

>Random breaks are because the web is written for Chrome. It sucks, but that is not Firefox's fault.

No, you're misunderstanding what these are.  The random breaks are the Mozilla organisation making cock-ups which they then hot-patch later.

They had one that broke all addons because they didn't renew their certificate that signed them.  They had another that blocked some major sites because their certificate processing messed up.  Build-specific issues with the Firefox browser.

>The mobile version lost access to every extension and only has a few now, so let's switch to mobile Chrome which has never had extensions

We had multiple add-ons we used at work, which was what was so great about Firefox mobile - the performance wasn't the best, but it was open and flexible!  So we didn't move from Firefox to Chrome, we had to create our own apps using WebView (effectively Chrome) instead - and now we don't use Firefox on mobile.

>If there's a company who is bad with management, it would be Google. Their CEO:worker pay ratio is insane, and they have a history of trashing products after a few years

My point isn't that Mozilla is better/worse than Google, it's that they're the same.  So there's no point supporting Mozilla as some kind of "plucky underdog" or bastion of community development - they're just another big corp burning through hundreds of millions with little to nothing to show for it.

>Google Chrome is built around sponsored results, it's literally Google's business model

They don't advertise within their browser UI itself.  That's a big step for Mozilla to take.  For me that is a step very much in the wrong direction and one I won't support in any software I use - it's the halmark of "dodgy shareware".

>In some benchmarks Chrome performs better, but I use Chrome every day for work (I have to) and do not find much of a difference.

It is night and day for SPA's and complex sites, Chrome is leagues ahead - Firefox isn't even in the top 5 for performance on most benchmarks.
Yeah I guess it was a common dumb joke. That said if you delve into about:config, you can tune a few things in firefox to reduce memory use (at the expense of performance and in some cases security). Not sure to what extent you can do the same on Chrome.
Might be related to this bug if the video's downscaled. https://bugzilla.mozilla.org/show_bug.cgi?id=1371999
Chrome feels snappier to me as well. That's why I stick with it over Firefox.
Fedora now seems to have VA-API for Firefox, so it's probably worth another try if you were running Fedora on that device (looking at your flair): https://mastransky.wordpress.com/2022/06/08/firefox-with-va-api-for-brave-fedorans/
Well, if you ever want to troubleshoot in the future, you can try /r/firefox -- we'd love to help you get to the bottom of it.
It doesn't get easier in the workplace to stick to your values.
> Deleting the account doesn't expose any personal data.

It doesn't, but it can be a real hassle for someone whose accounts have been compromised.
I did. It does seem you were talking about a third-party extension and not Google or Mozilla. My bad.
Because third-party extensions, like Bitwarden,  rely on their own storage scheme,  this allows you to use the same extension on multiple browsers, on multiple platforms and have the same information available in all of them.
Yup, that's me - thank you! :)
It's mostly just stock Chromium UI with a different theme wise - they have an inhouse sidebar for bookmarks and opening sites quickly, but that's pretty much it. The project's more or less entirely about privacy, independent web platforms like their search engine and ad agency / tipping service, and some general crypto things here and there.

If you want UI customization in Chromium land, Vivaldi is where it's at. Similarily user control and privacy oriented like Brave, but they focus more on the browser as a tool - *tons* of UI customization and they even have a mail client, calendar and feed reader built in.
Sure - try adding another 200 tabs in those tab strips to see how that ends up working. ;)

I can't be bothered dealing with that and Chrome feels very inferior to me in this regard.

But yeah, I don't look for tabs in the tab strip generally. I use the tab search and I use the address bar like a search for my open tabs, which is far more accessible than trying to find tabs visually - and if I am looking visually, seeing more of the tab helps there too.
Only a decade or more late and the browser itself can't handle massive tab loads. 

Glad to see Chromium playing with the idea at least, though.
It isn't about being easier, it is an area where Firefox is simply better, and where you may be able to enjoy those gains if you prefer to. 

>I love Firefox, but I find Chrome works better for me on the sites that matter the most to me (YT, Netflix, Plex, Reddit).

Frankly, it doesn't sound like you love Firefox. If you did, you would instead lobby those sites to work better in Firefox - and you would use Firefox to help make that argument for support via your own "vote".
Also breaks more pages as a consequence.
> And it's not really FF with tweaks, more FF with a lot of stuff turned off.

...Isn't that Firefox with tweaks? Is there anything that it does that can't be done in Firefox?
>But if you are comfortable with making your own changes to FF do that, some people aren't though so an otb solution suits them.

I do understand this could be useful for those not comfortable with making these changes themselves or not having someone to do that for them.
üëç
[E E E   eEeE](https://www.youtube.com/watch?v=nANdDIDQ2rc)
Companies are good, actually, and never act maliciously or nefariously in their self interest.javascript:void(0)
Yeah, I'll probably switch to new pipe once Vanced stops working.
Yes! And then Phoenix and Firebird, before the project settled on its present name.
Post-Netscape, it was briefly "Phoenix" (versions 0.1 - 0.5) and "Firebird" (0.6, 0.7) before finally "Firefox" in 0.8.

https://en.wikipedia.org/wiki/Firefox_early_version_history

Around the beginning of 2004 is when I started using it, someone told me about this new browser called Firebird, but a week or so later when I bothered to do something with that information, it'd been released as Firefox and I had trouble finding reference to it under the old name.
I was running a BBS and MUDs were the coolest thing back then.
I ran a BBS, and I was a MUD wizard. Yes, I'm old. (I still have my USR Courier modems.)
But Thunderbird still exists, I'm using it (not happily, but it's the least bad for me)
Chrome was fairly clean, fun and performant compared to the browser I used before it, Opera. I have used firefox but Chrome was something new for me. Every time I tried using firefox, I didn't know what's the point apart from convenience since that's the browser that came with the OS. After using it for a while, I now like its environment better than shady things google has embedded into chrome and the chromium project in the past years. Even though when it comes to performance, I doubt stock firefox is superior.
And thanks the European Union you can export your passwords from chrome, too.
And for the people who want the comfort of syncing on multiple devices, there is Bitwarden.

>highly recommended to NEVER use the password "manager" within browsers

why
My passwords are saved in Firefox and Chrome...
My favorite is that [a Chrome security tech lead was just kinda like "Lock your computer lol" ](https://news.ycombinator.com/item?id=6165708)
That's nasty yo
is firefox even a chrome competitor? As far as I see only MS Edge is competition. Why they would hurt firefox?
I agree.
For me, a Spanish-speaker who most of the time translates English into Spanish, but it should work in other languages each other, this extension is almost perfect. 
Github: https://github.com/FilipePS/Traduzir-paginas-web
Firefox addon's page: https://addons.mozilla.org/firefox/addon/traduzir-paginas-web/
It translates the page in real time and other neat and useful features. But the bad is it constantly stops translating the page and forces to reload the page or refresh the cache on the extension's options
I haven't tried it... yet :)
I ended up switching to FF from Chrome because of Multi Account Containers extension.
I use tab tree Extension on FF. 
Can‚Äôt say it‚Äôs very organized, but that‚Äôs just me
I found that Firefox's adblockers speed up page loads more reliably than the Chrome based options, and so I have found Firefox faster on most (but not all) websites.
Try tab stash. Changed my workflow completely.
All those reasons apply to me, but there is also something else, Brave like syncing.

&#x200B;

Edit: and multiple instances on mobile
I was talking about desktop firefox. Specifically, this screen:

https://i.imgur.com/9J2ITv9.png

I think it's a few miliseconds slower on firefox but it's not terrible at all.

(Thanks though)
Chrome is infamous for that, although as I write this comment Chromium is using  722MB of RAM with \~40 tabs open, personally I'll prefer that over lag anyday but on my lower spec machines like old laptops I'll run LibreWolf or Firefox since Gecko does feel snappier on lower end hardware
Well that was for different reasons. Their initial redesign of safari that predicated this was absolutely abhorrent. But they changed it a lot before release
Can't hit the tabs/settings buttons comfortably with one hand.

I am guessing this depends on how wide your phone is.
I'm a lefty and not bothered by the bottom bar. Quite like it in fact.
The bar contains buttons that are then in a position that is harder to press using the phone with one hand.

It's just less ergonomic.
It is a fact, and me mentioning has nothing to do with iOS. You can check with any Android privacy community.

Firefox does not have a webview on Android, so both the Gecko engine and the system (usually chrome) webview are used together, resulting in a larger attack surface. In addition, a site-isolation feature is still in development, making it more vulnerable to other attacks.

Here is a quote from GrapheneOS https://grapheneos.org/usage#web-browsing

"Avoid Gecko-based browsers like Firefox as they're currently much more vulnerable to exploitation and inherently add a huge amount of attack surface. Gecko doesn't have a WebView implementation (GeckoView is not a WebView implementation), so it has to be used alongside the Chromium-based WebView rather than instead of Chromium, which means having the remote attack surface of two separate browser engines instead of only one. Firefox / Gecko also bypass or cripple a fair bit of the upstream and GrapheneOS hardening work for apps. Worst of all, Firefox does not have internal sandboxing on Android. This is despite the fact that Chromium semantic sandbox layer on Android is implemented via the OS isolatedProcess feature, which is a very easy to use boolean property for app service processes to provide strong isolation with only the ability to communicate with the app running them via the standard service API. Even in the desktop version, Firefox's sandbox is still substantially weaker (especially on Linux) and lacks full support for isolating sites from each other rather than only containing content as a whole. The sandbox has been gradually improving on the desktop but it isn't happening for their Android browser yet."
I honestly can‚Äôt stand Android (but haven‚Äôt used one in at least 4 years‚Ä¶). So I am only speaking of the iOS variant of Firefox
Do you only use those tablets via the touch screen or do you attach a keyboard to them? Firefox is pretty good as long as you don't use it for purposes that go beyond media consumption and pair with a keyboard. There are a ton of bugs and limitations.

PS: I cherish whoever downvoted me before.
Oh yeah, running that many extensions might affect battery life. I only use uBlock right now.
Than don‚Äôt use it, you‚Äôre teaching them their tactics works, you need to treat those corporations like toddlers. If they start holding their breath, ignore them completely. I stopped using Youtube, Google (the search engine), moved away from android and deleted my Google account last year, because I can‚Äôt agree how they handle things.

Did the same to Facebook. I feel like my life and mental state is better for it.
What everyone else does? Use the user agent to determine the browser and write your apps so they work in all major browsers. 

From /u/adrianvovk
>No they implement functionality that isn't part of the web standards into Chrome, then they make their websites use these new features of Chrome. Then they use this as leverage to force the web standards organizations to codify the feature they implemented, or to just hurt their competitor browsers
>
>Even if Firefox implements it, if it's not a standard & the only things using this are Google products, Google can just change their implementation of the spec in Chrome and in their websites and now Firefox is going to be completely broken on Google's sites
>
>Google is happy to abuse their browser monopoly
Dlna
It might be an absolute maintenance nightmare.  IE Firefox figures it out, the next day Chrome fucks up the API specifically to block FF.

It also might be and under-the-table deal where FF doesn't implement this feature and Google keeps funding them?

Honestly IDK.  This feature seems to be too big for no one to want to implement it.
lesspass is both ;)
1. Implement new behavior 
2. Document it online 
3. Blame other browsers as outdated
4. The internet is stupid and agrees, Safari and Firefox are at fault
I was talking about another pile of trash which is called "Starleaf"
> That's not true, there are lots of little quirks with the way that Firefox renders that is non-standard. 

You are just saying that - Mozilla has actually been on record trying to untangle the mess of bad specs in standards to try to fix the situation. What is actually buggy or wrong? 

>The site with the font issue is toychester.com, the ARCO font used for the feature headings - take a look for yourself. All the others work, but not that one in Firefox despite it working everywhere else I've used the font.

Looks fine here. Maybe it is something on your install? 

>They don't advertise within their browser UI itself. That's a big step for Mozilla to take. For me that is a step very much in the wrong direction and one I won't support in any software I use - it's the halmark of "dodgy shareware".

What do you think Chrome is *for*? It is an investment in disintermediating other browsers to provide an onramp to Google services, and to create competitive advantages to those services via owning that onramp. Just because they are playing chess while you are quibbling about checkers doesn't mean that the whole thing is based around ads - Google ads.
Chrome doesn‚Äôt display ads, instead it sends all your requests to google associated with a unique ID. Check the Ungoogled Chromium project and the list of things they remove from Chromium (not Chrome), it‚Äôs insane.

Meanwhile, Firefox display ads that can be easily removed and that do not track you so they can get some revenues from other sources than Google, a thing you are asking them to do but criticise when they attempt to do it. Same story with Pocket, Mozilla VPN and so on which are all attempts to diversify their income source.

Instead you switch to Chrome to make sure Google has even more users, more market share, and more power, and will complain when they will be in total hegemony‚Ä¶ That‚Äôs hypocritical at least.
woah that's cool thanks!
I just open another window. There have been times were I get up to 5-7 windows with that many (about 80) tabs open. I'm not saying you have to like it, just that it works for me and I like it.

Though I also treat the tabs getting full as a signal to start killing tabs.
I use Firefox at work, Chrome just works better for me so that's what I choose to use at home.

I understand your point about voting with your choices, it's why I use Linux, but I'm just answering the question that OP asked. Most people use Chrome because it's familiar and more performant in general use cases. I look past the failings of Linux because I personally believe in the project and the community. I don't hold the same reservations about Mozilla though and so don't choose to use what I "feel" is an inferior product.
Define "breaks more pages"?
Not really, but if you adjusted the settings so they were the same as Librewolf then it wouldn't be Firefox. And how many people are going to sit there and change all of the necessary settings?

Especially when a lot still use google (or BING!) as their search engine!
Yep. I downloaded it for the first time when it was called Phoenix. I have used it ever since.
I was using it already when it wasn't even named Netscape Navigator. It was Mosaic and it was available on the Amiga as well.
Phoenix really was a breath of fresh air in an era of bloated browers.
back then did we have 6 afaik render engines Gecko, Webkit, Khtml, Preston, Blink and MSHTML(or what the IE engine was called)
Lol, I used to be able to read as fast as the modems could download at 2400 baud but it was a bit of a struggle. Once they got past that -  no way...
What!!! I thought they dumped it back in like 2010? I shall now go investigate.
I thought you could always export them?
KeePass can sync on multiple devices, if I want, it's an option.
Before I begin I will say that using a browser for your password management is better than just using the same password for everything - but it is a browser. They're main function isn't to safeguard your passwords. It is to give you a pleasant browsing experience. I am not saying that Google and Mozilla do not take the security of their customers seriously - but it's not their sole mission. Compare that to a company like Bitwarden, who I have really only heard great things about and enjoy using myself, that is their one and only goal. I do not know this for 100% certainty but I would go out on a limb and say that a company like Bitwarden is putting more resources into their password manager than Google or Mozilla are with their little add on manager. Or you could go with something like KeePass where you host your own passwords locally. I guess you could get into a debate over which one of those are more secure as well. But that would be comparing yourself and your knowledge of keeping your system secure against the people over at a company like Bitwarden. And it isn't all about security either (obviously that is the biggest reason) but browser pw managers aren't really as powerful as other standalone password managers. As far as I know you are not able to make your own complicated password via chrome/firefox (could be wrong but that is what I have seen)  you can't even change the length or create random user names like you can on other standalone password managers. 

And of course at the end of the day you have to trust a company that almost certainly only looks at you as a dollar sign, but which company do you trust? It's really the lesser of two evils in my opinion. But I am for sure not going to store all of my passwords on Googles servers. Unless you're self hosting and then you have to trust that you know enough to keep your system secure.  

But in reality this is really all just my own opinion.
I mean it‚Äôs better than nothing. And hey I‚Äôm by no means a cyber security expert but, in my opinion, I recommend using a standalone password manager. I‚Äôm sure there are pros and cons to both but if anybody asked me I would recommend not using the browser integrated pw managers.
Is this the case today, 9 years later?
Firefox and WebKit are the two competitors for Chrome. Everything else (brave, edge, etc) are all forks of chromium
Edge is based on chromium, it's basically just a skin and some addons
Safari and Firefox are really the only competitors to Chrome as their engines aren't Chrome's blink
Microsoft Edge is chromium based. I think Safari would be a stronger competitor than Firefox.
> is firefox even a chrome competitor?

Uh yeah.
why people dislike me? because I told Firefox isn't chrome competitor?
Containers are pretty great, although it's not really the same as tab grouping. My projects tend to require logins to the same systems, like Jira, so segregating sessions actually makes it a bit tougher, unfortunately.
I use containers together with Container Color Toolbar, do handy!

https://addons.mozilla.org/en-US/firefox/addon/container-color-toolbar/
yeah ff is a little slower, but only with sync, extensions, and the rest on for me.
But how does putting it on top fix that? Seems like it‚Äôs further away from your hand.
How so, it seem the same to me. Though I use my right, when I use my left there isn't much of a difference whether it's on the top of the bottom.
> It is a fact, and me mentioning has nothing to do with iOS.

Do you know what an analogy is? 

>Firefox does not have a webview on Android

Frankly, I am not certain that it is possible for Mozilla to rectify this, as Android may simply assume a Blink based renderer here. 

In any case, the argument of multiple engines is exactly the same as in macOS, Windows or Linux, as apps can embed whatever libraries they want to render web content. The reason people don't make such facile claims on those OSes is because they recognize that security often has trade-offs, which is not an argument that single minded security folks appreciate.
My reply was to the person saying it was unusable on Android tablets. Personally, I can't stand ios, but everyone likes things different. No judgment from me here. I know lots of people like ios, I just don't like it.
I only use them via touch. If I need anything more than touch I'm just going to use a laptop or desktop instead.
You seem to gain control over your privacy & life fully. I am also trying slowly to move on to alternatives. Have moved from google search very early though but didn't find any good alternative of youtube
so you use iOS now? or...
Doesn't quite apply to Chromecast, though. How does it detect that you even have one nearby for that cast icon to work? And do you want to show everything you're playing to every website that hits that API? So you have the browser handle a lot of this -- it's the thing that scans the network for nearby Chromecasts.

So they'd have to make the cast protocol itself a standard, which, for some reason, they don't want to do.
> Use the user agent to determine the browser

That's actually something you *shouldn't* do but everyone *does*.
I am not talking about google making their apps not work in Firefox, but them implementing new feautures to chrome. It would take years to standardize stuff like streaming api or usb access before implementing them.
what's Dlna?
Well, firefox still supports video popup. Maybe it's possible to add option to pipe that video into *something* and make that something into chromecasting program. That would be at least easier to mantain.
Who cares about fault? I just need it to work. I don't care why it doesn't work.

My boss won't let me stop working because my preferred browser needs an update.
> Though I also treat the tabs getting full as a signal to start killing tabs.

That is a way that Google's decision has influenced you to use the product the way they want you to use it - I don't like being bullied, personally. ;)
Personally, while I enjoy Linux, I think the web is an even more precious resource (and holds the future of computing in many ways) - losing the web to Google (or Microsoft before it) would be a horrible consequence, and one that seems to grow more and more likely as the web grows more and more complex. 

If you believe in the web and you "love Firefox", you owe it to yourself to develop some "belief" in the project - because right now, that love seems quite hollow.
More pages will have issues in Librewolf than in Firefox.
> Not really, but if you adjusted the settings so they were the same as Librewolf then it wouldn't be Firefox.

How so?
Trident.

But Blink wasn't forked from Webkit until the 2010s, so call it five.
Mozilla no longer develops it.
Browser was called Firebird, Thunderbird is mail client :)
eh, better than nothing. the type of people that use built in browser password managers will likely not use dedicated passwd managers and will use the same password for everything
https://lock.cmpxchg8b.com/passmgrs.html

There are pros and cons to both.
[See for yourself](chrome://settings/passwords). Reddit won't let me link it, but the url is `chrome://settings/passwords`.  Chrome still doesn't require authentication for that section of the settings page.
        
      
          
Actually, as it turns out, that's changed. I remember being able  to previously view passwords without authentication.
And technically Chromium's rendering engine, Blink, is a fork of WebKit.
But Google force notification to install Chrome only in Edge
Because you're wrong.

It is a competitor.  Not a viable, long term competitor at the rate things are going , but most definitely a competitor from Google's viewpoint.  Their goal is, and they've said this, complete browser domination.   Anyone that even has a single user is a competitor to them.
Oooh just installed it,  that's handy,  thanks!
We need answers
I guess it depends on the size and shape of your phone.
I didn't say that Firefox should never be used, or even never be used on Android.

I only said to keep it in mind. The fact remains that compared to other browsers such as Vanadium and Brave, Firefox is more insecure on Android.
Yeah, somehow I tabbed the wrong reply in the thread.
This is the thing: Mozilla is (again) not prepared for a change happening in the market right now. Android tablets such as those from Samsung are becoming more popular among students that need a browser that works with both touch and keyboard. 

I'm personally missing features such as a tab bar, sticky desktop mode and per process site isolation that every Chromium browser out there has.
Unfortunately there isn't really any alternatives to YouTube. Replies will probably tell you about Peertube or LBRY/Odyssey, but they just don't have the content to be viable. There's Invidious, but it still uses YouTube, just with a privacy-respecting front-end. I've resorted to staying on YouTube, but staying logged out and using uBlock Origin of course.
Nokia 3110. probably.
Unfortunately that was the only reasonable alternative.
You mean so all websites work the same on all browsers? I get that would be ideal but how do you get browsers to develop then?
Yeah, fine add features but then don't make sure your apps work like crap on other browsers.
You do you. But yes those inclined should be educated about what influence Google has upon technology, society, etc.
> Who cares about fault? I just need it to work. I don't care why it doesn't work.

Congratulations on never progressing past being a petulant child.

I guess you don't care whose fault it is that things don't automatically work in Linux either.
If I was using firefox (with it's default scrolling tabs behaviour) then it would be bulling me into a mere 20 tabs per window.

The 80-100 that Chromium allows me before breaking my workflow is vastly superior.
Sorry to be so dumb but can you give me an example of a page that breaks in Librewolf?
Because it'd be Librewolf.
I have been brought high and low with just 2 replies.
Who develops it then? Wikipedia says it's still Mozilla.
I don't remember Firebird. I used Firefox and Thunderbird all during the early 2000s before Mozilla stopped supporting Thunderbird.
It was the same for all browsers back in the day
See https://www.reddit.com/r/firefox/comments/vi2bvm/whats_the_state_of_security_on_android/idhnpbx/
Depends on what you understand in *development*. W3C got the standard building upon a standard building upon a standard (repeat) even more bloated than it has to be. Result: No one can implement a whole web-engine from scratch.
>I guess you don't care whose fault it is that things don't automatically work in Linux either.

Not when determining why I don't use Linux as my desktop at work (this being more in-line with the original question).

I have a single application that won't work in Linux/wine. I have to use this application. I've posted on the "please add Linux support" thread, I've talked to our sales rep and sales engineer. Beyond that my hands are tied.

I know who is "at fault" for it not working, it just doesn't matter. I'll not die on the sword because a third party doesn't support my preferred OS.

At home I don't buy software that doesn't work on Linux/Wine. Even then fault doesn't matter. It doesn't work, so I don't buy it/use it.

I also don't care whose fault it is when Linux software doesn't work in windows.

>Congratulations on never progressing past being a petulant child.

Do you find these personal attacks to be helpful in some way? They just seem like an unnecessary provocation that interferes with open discussions.
> If I was using firefox (with it's default scrolling tabs behaviour) then it would be bulling me into a mere 20 tabs per window.

Well, like you said - it is personal, but I don't see how a favicon really helps (alone) to provide enough context to what is in the tab (if you are visually looking for tabs). But like I mentioned, I rarely visually look for tabs anyway, as tab search is far more efficient for me.
Here's a basic one that many people will see over and over again: https://www.reddit.com/r/LibreWolf/comments/vo67qr/how_can_i_fix_this/
No, it would be Firefox.
No, it is very much still a thing being developed.  It's just not Mozilla anymore.
From Mozzillas FAQ:

>Thunderbird is an independent, community driven project. Therefore its paid staff, budget and fundraising are entirely managed and overseen by the Thunderbird Council, which is elected by the Thunderbird Community. 

.

> (Mozilla Corporation no longer develop Thunderbird. But Mozilla still supports Thunderbird by hosting many of the Thunderbird resources.)
Mozilla beat Google with an optional master password.
the majority of that comment is referring to desktop specifically?
So massive companies who are owners of content and distribution get to determine that you have to use their distribution? Sounds a bit like what we are also dealing with in media these days.
>     Congratulations on never progressing past being a petulant child.
> 
> Do you find these personal attacks to be helpful in some way? They just seem like an unnecessary provocation that interferes with open discussions.

Frankly, it was an observation of your attitude, not meant to be a personal attack. I suppose in hindsight, I should have depersonalized it further - but I'll admit to being annoyed. 

>I know who is "at fault" for it not working, it just doesn't matter.

But it clearly does - it just means that in your specific case, your "hands are tied". That is an entirely different thing than what you said in your initial post, and this could have been prevented by making that statement instead. 

I just don't understand how you simply don't care about who is at fault, and it really came off as petulant.
Search based navigation never really clicks for me.

As for the lack of context provided by favicons; I'm not really doing a visual search, it's more of a spacial recall. The favicons are just guiding/anchoring that spacial recall.

If I was designing my own navigation system (and I've been tempted), I would probably replace the favicons with thumbnails. Even if the thumbnails ended up the size of the favicons. I'd also make it zoomable, and probably a in more of a 2d grid than a linear row.
And it's answered on the same page.

By the way don't use whatsapp, it's dirty. Almost as bad as reddit.
The new 102 version is quite a good bump. Supports CardDAV address books and Matrix chat. Really quite cool.

I still use it because I don't use webmail and it's really still the best email client. They just don't seem to be a thing anymore.
But was it encrypted in the beginning?
Yeah, that is fair.
I don't use WhatsApp, it is just an example.
I'm not sure that matters that much. If I'm walking away from a workstation for 2 minutes, I'd be more interested in just blocking access via the ui.
So it doesn't work on something that you don't use?
Would have mattered to me. It's not only the possibility of a human, but more a virus or whatever.
The same issue happens on eBay, which I do use. Doesn't matter though - the web is huge and everything isn't about me.
You can either release the work into the Public Domain or you can use one of the Creative Commons licenses. They were made for things like this, you can pick yours [with their wizard](https://creativecommons.org/choose/).

Personally I'd go for public domain unless there was a lot of highly custom work done, at which point I'd choose [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) meaning people are free to use and modify, but my name has to stay in the thank yous :-)
No-one cares about your dotfiles. MIT.
I think you're overthinking this a lot.
> I mean, they were written by me, but they're essentially just a set of options being passed to their respective programs.

But the decision and creativity in choosing those parameters for those options was yours. To take this logic to an extreme, most programs are just a set of options being passed to their respective compilers or interpreters.

> It doesn't quite feel fair to the developers of Alacritty, for instance, for me to say "Alacritty I want you to use this hex for the color blue" and then turn around and distribute that as if it were my own work.

What right of the Alacritty developers do you feel you are violating by doing this?

And how *else* would you distribute it as, if not as your own work? You certainly cannot honestly describe it as being written by the Alacritty developers. In fact, most people would not like having something not written by them to be attributed to them.
How about the [unlicense](https://unlicense.org/)?
The obvious choices are CC0, Zero-clause BSD, or Unlicence.

For things like dotfiles, I personally prefer the latter. Dotfiles are a collection of trivial things, which are barely copyrightable in the first place. I probably sourced a large fraction from other places, and I have no interest in retaining rights for any of it. Who cares if someone else steals it or claims credit for it? The Unlicence makes it clear that I genuinely don't care what happens to it - as long as you don't try to bother me be about things like warranty.
I don't think "dotfiles" are licensable at all?
I would choose the licence that puts the fewest restrictions. MIT should be fine. GPL seem overkill for dotfiles. You could also go for some creative commons but in that case I recommend super-simple ones.
I simply don't bother.  Mine is just configurations, maybe some nifty scripts that I've got no problems with people using.
Rule of thumb:

License text should be shorter than the code. Unlicense is pretty short.
My favorite licenses are GPLv3 and WTFPL.
i dont think it matters, i have dotfiles on my repo and i didnt care much about what license im using
https://en.wikipedia.org/wiki/WTFPL
Doesn't need to be complicated for just dotfiles. You could use any of the common ones.

 - GPL:

V2 makes it so any changes in use should be given back to the original author, this is what the Linux kernel is licensed under. v3 makes it so you cannot redistribute the code in any way that encroaches on user freedom, but for dotfiles this doesn't typically matter.

 - MIT, various BSD & Apache licenses:

they're a good catch all in the sense that they are open source licenses, but not explicitly libre/free licenses. These are called permissive licenses in contrast to 'copy-left' style licenses such as the GNU GPL licenses.

I personally really like (and use) the [BSD 2-clause license](https://opensource.org/licenses/BSD-2-Clause)
I guess that makes sense. I'm not sure I could explain exactly what right of theirs I'm violating, it just feels sort of... wrong, for some reason. The best way I can describe it is that it feels like I'm creating an extremely low-effort fork of their project. Maybe I'm just overthinking it.

So should I just treat my dots as my own work, and license them the same way I would normally (while respecting the licenses of the programs themselves, of course)?
Unlicense is an invalid license in most EU countries. I'd Recommend bsd-0 or cc instead.
Depends what‚Äôs in there.  I have a 1300-line Emacs configuration with bunch of functions defined there.  That‚Äôs definitely copyrightable.  My shell config is around 500-lines total and it contains various functions as well which are copyrightable as well.
I actually think CC0 is better than MIT for this purpose. CC0 is just a waiver of all rights the relevant jurisdiction will allow you to waive, or basically just public domain even in jurisdictions that don't have or don't allow dedications to the public domain.
What are the main pros and cons when deciding between GPLv2 and GPLv3?
> V2 makes it so any changes in use should be given back to the original author

I'm sorry, what? That's not what GPL2 is about.
> The best way I can describe it is that it feels like I'm creating an extremely low-effort fork of their project. Maybe I'm just overthinking it.

I think you are. In using your dots, someone _still_ has to comply with the licenses used by the relevant software. You're not violating anyone's license and you're not allowing anyone else to either.

CC0 'em and call it a day. Most (maybe not all) of your dotfiles would probably be found to be trivial and therefore uncopyrightable if it ever came to a legal contest anyway... but it won't.
> So should I just treat my dots as my own work,

Well that sounds like MIT style yes? Kind of just attribution and the no-warranty disclaimer. End of story.

Creative commons is a bit similar.
GPLv3 has patent protections akin to Apache and is also compatible with Apache (which GPLv2 isn‚Äôt).  It also has clauses against locking hardware, the so-called [anti-tivoization](https://en.wikipedia.org/wiki/Tivoization) clauses.  Still, probably best to use ‚Äòor later‚Äô anyway.  I wouldn‚Äôt consider using GPLv2 for anything new.
GPLv3 puts a few more restrictions in place, as counterplay by RMS for software-as-a-service (and then not be required to open source it), and I think a few more in terms of patents. But I think the GPLv2 is fine too.

The question is: does it fit to dotfiles? I am not sure. I think dotfiles should just be as close to "public domain" as possible. Not every country allows for public domain use; I think in Germany you can not omit it, by default it will always be attributed to a specific owner even without any attribution. Don't quote me on that though, I am reading up more on english-centric laws than german ones these days. :P
To be honest i don't know either but you can google it.
Linus Torvalds said during an interview that he was almost tricked into using a the GLP3, which he was unhappy about do it's stronger copyleft rules, he said "the important thing about GPLV2 to me was that I get the changes back, I don't care what you do with it, but give me the changes so I can use them."

I'm sorry, is he speaking of a secondary license that I am unaware of?
You are wrong. That's not how the license works.
Ok? How does that change the legal matters GPL2 is concerned with?
Im not sure what you're on about, if you feel like I'm wrong then just elaborate on why or make your own reply, why waste time trying to poke and prod at what I said if you hold all the answers? Are you trying to get open source license knowledge clout? GPLV2 makes it so you can copy the work and and redistribute it under your license, much like other permissive licenses, but it has a clause for not slapping a proprietary license on the derivative work. When I made the reply it was not my intent to write the license verbatim, but just give a simple summary...

They're asking about a dotfiles repo, and I hardly would assume anyone is going to fork his dotfiles and try to license it in a proprietary way. If you assume otherwise, please share with the class sir!
Get help, you need it. I'm not even sure how to reply to that stream of consciousness.
Dude, reply with what I'm wrong about and why, if you can read it's not hard to make sense of
I re-typeset Issue #2 of /u/kjodle's *the codex" [zine](https://old.reddit.com/r/linux/comments/vxlfmm/still_having_fun_learning_latex_so_here_is_issue/) in troff, just out of
curiosity to see how hard it would be to convert from LaTeX to troff. It
turned out to be not too hard, just mostly tedious. For years I had typeset
many small documents in troff, but nothing as big (40 pages) as this. Along the
way I learned a bunch of useful stuff about troff.

For those unfamiliar with it, troff is the old UNIX typesetting system created
at Bell Labs by Brian Kernighan and other UNIX luminaries in the 1970s. Linux
uses [groff](https://www.gnu.org/software/groff/), the GNU implementation of
troff. Troff produces PostScript output, so to compile the document, besides
groff you'll need ghostscript (for converting from PostScript to PDF) and the
PSUtils suite (specifically psselect, for rearranging pages in a PostScript
file). In Fedora the required packages are: groff, groff-base, ghostscript,
psutils.

Unlike LaTeX, troff is single-pass only. So while LaTeX might need multiple runs
to get things like references and the table of contents resolved, troff works a
bit differently. In particular, the table of contents can only be printed at the
end of the document. You then need some "post-processor" script to move the
table of contents where you want it. That's where psselect came in handy.

Another difference is with image files. Groff can only use EPS files. So I had
to convert the original PNG inages to EPS. I used Inkscape to convert all the
PNG images except the PDF Chain screenshot&mdash;I used netpbm-progs for that,
since the Inkscape conversion was too blurry. That ended up increasing the PDF
size from 289kB to 563kB, but the improved image quality was worth it.

I used the `me` set of macros, created by UNIX legend Eric Allman (the creator
of sendmail). Typically the `ms` macros are used for creating long structued
documents, but I chose `me` because it simplified mimicking the custom
headers and footers in the original LaTeX version. The `me` macros have been
maligned for many years, but I found them useful. I did discover some bugs,
though, and despite coming up with workarounds I'll report the bugs.

I also used the traditional troff preprocessors for drawing pictures (`pic`),
creating tables (`tbl`) and typesetting math (`eqn`). One issue I couldn't work
around was `pic` always centering figures. So to get the right-aligned house
picture on page 39 I cheated: I used an EPS image of the house (created by
TikZ!). However, I did use `pic` to draw the house on page 40.

Groff has no way that I've seen to wrap text around an image, but I came up
with a poor man's imitation: after placing the image use `.sp` commands with
negative numbers to move subsequent text upward, with `.br` commands to cut the
lines short so they don't overlap the image. Of course this gives a ragged right
or ragged left alignment, but it's the best approximation of wrapped text I
could think of. Maybe someone has a better idea.

Finally, I typeset the cover in troff, using `pic` for the background color.
I couldn't get rid of a 0.25" white margin at the top of the cover, until I used
`.vs 0`. To make things easier I put the cover at the end, then used psselect to
move it to the front. The `compile.sh` script handles all that and creates the
PDF.

I corrected a few typos from the original (e.g. changed "Candian" to "Canadian")
and changed "Typeset in LaTeX" to "Typeset in troff", but otherwise left the
text unchanged.
I think "the codex" is going to be a Rosetta Stone of markdown. Kinda like how the Lord's Prayer was used to understand grammar in languages. People would look at translations, transliterations of every word in the sentence to get a feel for the grammar.
> Along the way I learned a bunch of useful stuff about troff.

I miss people like you on online Linux communities.
Truly great stuff.  Curiosity is the best reason‚Ä¶ and it  takes you down the deepest rabbit holes.
I have two OG Pines here that I haven't done much with since the OSes all seemed buggy. Does "stable" mean it's all working?
Pleasantly surprised to see my crappy dinosaur era obscure phone getting support. It would be absolutely top notch if the camera is working, but it's fun to play around regardless.
That horrible logo though
Stable means it won't break things when updating and won't change as much as edge will. Not stable as in "everything will work out of the box and it's a finished product".
Hey,
So they have been made a lot improvements lately. 
Apparently there is a community firmware for the modem which increases the reliability of receiving/sending text messages and phone calls. 
A lot of news articles suggest that general user experience and battery lifetime also increased noticeably.
As far as I can comprehend the Pinephone is still not quite daily driver material for most people but there are folks that are pleased with the experience the Pinephone offers.
Definitely worth a try.
PS: Since you have two Pinephones are you interested in selling one of them?
Thanks. I'll try it.

I'm not interested in selling one tbh, one is a misprint BraveHeart with the Pine logo on the camera upside down, and the other is a 3gig ram half-a-pro.
The main reason people tend to dislike Ubuntu recently is because of snaps.

Previously, there was some dislike related to Amazon tie ins.

To my knowledge Ubuntu has _never_ received criticism for being too easy, at least not criticism that was taken seriously.
When Ubuntu started, it was the pioneer in being "easy". They really tried to make a transition from (or dual boot from) windows as smooth as possible. And at the time, that was unique. I remembered trying Ubuntu when it just came out, and was totally wowed by the experience, compared with my prior experience of setting up RHEL.

Nowadays, however, when it comes to personal computers, most general purpose distros are equally easy to set up, so Ubuntu has lost its unique selling point on that.

Ubuntu is often disliked today due to snap. They are trying to push Snap, but a lot of people simply don't like how it works. Combined with its prior abandonment of Unity, it feels like Canonical is struggling very hard to find its own selling point, but their effort is not hitting the customer's pain point.
Ubuntu was easy.  Then it became painfully slow and pushy.  Not my style....anymore.
I think sometimes people just search how to hate the most mainstream thing around... snaps are today's reason. Also I think Ubuntu's focus turned away from desktop experience. Once it was the most usable distro out of box. By long shot. They decided to innovate. To change stuff. That got them attention, love and hate. They decided to do their own things, ignoring others. That brought them mostly hate and pain. They found out that when they are ignoring others, other will ignore them. So Ubuntu was alone in pursuing their ambitious goals. This clash left some people bitter. Snap is going very much same direction again. So it doesn't help. The desktop leader is now Fedora, but they don't have as privileged position once Ubuntu had. Today there many distro spinoffs trying to deal with desktops, eye candy and whatever. Ubuntu is not leader anymore, but still it's former tech dominance is what keeps it having decent market share. For some Ubuntu being mainstream and it's bitter past and the fact it doesn't care that much about desktop anymore is just good mix to hate it
The initial ease of installation and working "out of the box" is also why it's notoriously hard  to change config files without breaking something. I remember a while back a case where a partition was mounted encrypted, but the partition wasn't listed in either fstab or cryptab. And many many other cases where deep down there are secret patches that make things work, as long as you don't stray the path.

Compare to (cough cough) Arch, where the initial config might be a pain, but everything always behaves as expected, and you can easily track down the how and the why.
Ubuntu used to be a great distro which made simple to install and run Linux: they were assembling and fine tuning tools available to everyone in a perfect way.

Then they got the "not invented here syndrome" and started going alone with poorly designed and afterwards abandoned tools (examples: upstart, mir, unity) instead of joining the community efforts on similar tools.

Recently they shifted the focus to the remunerative server systems business and reduced the effort on community management/ marketing - I remember when they were emailing for free a CD to your doorstep.

In addition to that there's the band wagoning effect: Ubuntu used to be the content creators most cherished distro till Arch came along for a few years  and now Fedora is up.

That's why they are loosing traction on the community, but they are  definitively improving their business.

My use case is similar to yours: you possibly can be happier on Debian these days with a stable system and flatpaks/appimages for some specific tools.
Nah, it's because Canonical has a history of going down the custom route often forcing it on their users.

Eg. snaps, Upstart, Mir, Ubuntu/One store, ppa/launchpad, search bar collecting info from you then selling it to Amazon, etc.

The one that really bugged me was pushing Unity as the default desktop before it was ready.

In many ways they have done a lot for Linux but in many ways they differ quite a lot from the open source community/culture (i.e. suffer from NIH mentality).
I'm still holding a grudge for selling users search data.
I personally dislike Ubuntu cause it's always, always had been giving me a ton of problems.
More than any other distro.
i installed it on my doughter laptop as a replacement for deepin distro and there were many problems with hardware / software setup.  the performance was very bad.  after many tryies to find distro wich will be less problematic we ended with archlinux.
First of all, I would like to say, I could not care less what distro you like to use; everyone has their preference. However, I hold a grudge against Ubuntu because, and this could just be the Arch user in me talking, it's slow and bloated (one reason I switched from Windows). Canonical also has the terrible habit of "reinventing the wheel" to solve a problem even though a good and commonly-used solution already exists. And almost every time, they eventually switch to that solution anyway. And while you may not care about the back-end of your system, I do. I enjoy digging around my computer, making rices, booting up VMs, changing stuff and recompiling, etc...

I think Ubuntu is a great OS, and many people (including me) ran it as their first Linux distro. I just think for my personal needs, it was better to move on to another OS once I had more experience with Linux.
Its hated because of snap. Try launching firefox and go have some coffee, or even watch a movie.
Ubuntu is just a fork of Debian Testing branch,but it is less stable than the Debian Testing branch.

Weird desicions by Canonical to use snaps by default instead of deb packages,weird default GNOME customization,borked PPA's,there are a bunch of Ubuntu forks out there that do Ubuntu's job better and more stable than Ubuntu itself does like Linux Mint,PoPOS,KDE Neon,Kubuntu and others.

Atm Ubuntu feels less polished than its forks or even Debian Testing.

Just because Linus Torvalds uses/used Fedora does not mean that every other Linux user should use Fedora,Linux is about choice if you want to use Fedora/Debian/Arch Linux use Debian/Arch Linux/Fedora if you want to use Ubuntu and its forks-use Ubuntu and its forks,if you want to use Slackware or Gentoo or Void Linux or Solus or PCLinux use these distros,it is all about choice.
Linus Torvalds has used RHEL/Fedora for much longer than since 2016.

Use whatever floats your boat.

I started with Ubuntu in 2015 but got soon fed up with every repo-hassle and every single of them making the update process slower and slower. The Amazon act was the final nail to that coffin and never looked back.
In the 3 months I used ubuntu, I became ubuntu recovery expert. Because something would break every week. (This was around 2009, and my system only had an integrated graphics card, which wasn't fully supported by drivers.) Then, I switched to debian which was pretty much the same experience because of even older and more broken drivers. 

Then I switched to arch, got the latest intel driver stack, latest mesa & set up the system the way I liked and never looked back.

**Ubuntu is easy if you behave the way it expects you to. Arch is easy if you want your system to behave the way you want it to.**
Why does this suddenly jump into the third-person?

How much did Canonical pay for this post?

lolol jk(sorta)
There is less flex with Ubuntu. That's why people dislike it. You don't see anybody with a htop desktop Screenshot running Ubuntu. Not cool enough
Congratulations on defeating the idea nobody advanced of Ubuntu being too easy.
there are a lot of reasons to hate Ubuntu. Fedora is just as easy, if not easier than fun to and it doesn't get nearly the amount of hate that it does.

The biggest reason I hate Ubuntu, is because it feels like it goes out of its way to get into your way, everything feels like a hassle to me in Ubuntu in a way that neither fedora or arch does.
It's hated because it's 1% Linux and 99% Politics.
No, it's becasue Canonical has done and still does shady and bad things like snaps, selling your data to Amazon, using GNOME and the fact that upgrades suck because point release.
Honestly I am personally put off by orange and purple color scheme.  I think this is one of the major things holding the distro back, as insignificant as it might appear.  I think they need a more professional or clean looking desktop option out of the box and you would see people stick with it longer.

Of course, I know you can change everything in it, but newbies don't and nobody wants to look at an ugly color mess.
In my opinion, anyone who hates software instead of simply using what suits them should seek medical care.

Apart from that, it doesn't matter if someone hates Ubuntu. If you want to use Ubuntu because you like it, use it. Just as I use micro instead of vim, for example.
I have used Linux for over 20 years and have used it on desktop as a daily driver for 10 years. I use Ubuntu because I just want to get on with things
Ubuntu's CEO is a dick.
Not hated, however I don't like it. You have a huge desktop and you can't do shit there, just stare a wallpaper.

Xubuntu is more functional for me.
I love Ubuntu on my servers. 

But for everyday work and personal usage, I just don't like how Ubuntu's repos are outdated most of the time.
I put Fedora on my laptop recently and it turned out to be just as easy as Ubuntu to get going. My guess is that after Ubuntu lead the way, all others who wanted to have any desktop market share at all were forced to compete in first install time user-friendliness.

And thank god for that. I remember having to set up X11 configs and install whole bunch of garbage, and add my user account manually to about 10 different groups just to get basic desktoppy things like sound working. What a pain Linux used to be.
"Too easy?" I don't think so XD
So easy that if you want to have more software available, or more updated, you will need to add an additional repository (Flathub) but you will also have to install the Flatpak engine. The only way to do this is using the Terminal. And then the novice user will end up with two software stores at the same time (increasing RAM usage). Easy.
I am not sure what is easier in maintaining bloated Ubuntu desktop or flatpaked Fedora. 

But in terms of installation process Fedora based distros a little bit harder to install and figure out disk partitioning.
I think it's not really 'hated', but people refrain from recommending it because there are better alternatives. Sure, it gets the job done, but if there's something better (at no cost), why not recommend that?


As for it being easy, I can only speak from personal experience here (which is obviously contradictory to that of many people), it seems a bit too complicated. My first distro was endeavouros, and the update process was simple enough: if the packages get an update, you update, nuff said. With ubuntu I had to research an hour trying to figure out how the versioning and backports and PPAs work (ironically if you look at my post history you will see a post asking how updates work on endeavour).
Hate is a strong word. Move away from and not use because it doesn't fulfil its promise, due to upgrades hosing the system? Yeah that. This was back v10 though, just after the brown desktop era(is it still golden brown these days?)
Ubuntu WAS hated because it was too easy. When it first came on the scene it drastically lowered the bar of entry into Linux desktops, and a lot of the old guard took exception to some of the new riff-raff. They got over it though. And yeah, now snaps are the main reason to hate Ubuntu today. Snaps are Ubuntu trying to be Red Hat. And not even doing as well as Red Hat at being Red Hat. But at least they're helping to prove why a community model works better in many cases (not all but many).
There are people in the Linux community who are "snobs" and don't like any distro that is easy to install and which caters to people with limited knowledge of computing. These tend to be people who are running distros like Arch, Gentoo, and sometimes Slackware. (Not all people running these are like that, but many are.)

Ubuntu, which was the ultimate in easy-to-use Linux when it started out, was an obvious target of that crowd. For them, yes, Ubuntu is "bad" because it is "too easy." But that is not the only reason people don't like it. Some people started disliking it many years ago, when they switched from GNOME to their own Unity desktop (only to switch back later on). Some don't like it because they adopted the systemd init system. Others don't like it because they view its parent company, Canonical, as ham-handed and acting at times like the Microsoft of old. And some just don't like Snap, their self-contained software distribution system in competition with Flatpak and AppImage. Or some combination thereof...

Personally, I am not a fan of Canonical, but I'm not going to savage Ubuntu. It has played, and still does play, a huge positive role in pushing Linux forward. Indirectly, through Mint, it got my wife to convert from Windows. It doesn't cause me any problems and I see no need to hate on Ubuntu. Those who dislike Ubuntu, or Canonical, can choose something else.
Snaps make things easy for a developer but at the cost of eating up more resources, it's bloat that will eventually come back to bite the users. All the things people switched to Linux for is put to the bin with stuff like this, as a normal user with recent h/w they won't notice it at first maybe but those with older h/w, definitely.
A lot of the other distros work with "upstream" project to solve problems, aint no upstream of snap store as as far as I can see, its propriatary.
In the end very few people are interested in installing an after market OS. They want to treat the computer like a set top box, software and hardware integrated.

And more recent developments in system design, ending the practice of including OS and software install media in favor of recovery partition etc, push this notion even further.

Note how even Google is primarily targeting the business world with their "ChromeOS flex" push.

While RH has found a niche in the business world, Canonical seems to have found one in webdev.

I can't say i care for either, as money seem to scramble priorities. The unix wars have turned into distro wars, as each one that's backed by a commercial entity try to own the platform via EEE.

And frankly i think Ubuntu is the lesser evil here. They try their own thing, in their own distro, but do not foist their way on the ecosystem.

Nah, the real issue is RH's duel with Oracle, after the latter forked RHEL, as it has put RH into a war mentality that has made them throw their weight around more.
Exactly. Ubuntu just goes out of it‚Äôs way to be a pain sometimes if you want things configured different from the default, while other distros usually keep things straightforward.
For a very long time I would distrohop on the same encrypted LUKS partition, because I would keep my /home. Ubuntu and maybe Debian are only systems that didn‚Äôt (and I think still don‚Äôt) allow for unlocking the disk and reformatting specific partitions, it would look like it was doing that but would actually recreate the LUKS partition instead of reusing it and blow away all the data. I had to manually create the target and force cryptsetup into the init among other steps.
People always say this but some of those precedes RedHat versions. 

Snap, Upstart, and Mir came before it's counterpart. Canonical just always lose to RedHat. Probably because RedHat is a lot larger and has a bigger and better team behind it and comes up with arguably better product.
Every single Ubuntu Installation I had over the years destroyed itself at one point (At least on Desktops and Laptops) but the more distros I try the more I get the feeling it might just be gnome falling apart
I've never had any problems with Ubuntu on my hardware, from 12.04 to 20.04, I never had issues.
Yes, I agree with you on all that you said, use the Linux distro of your choice, it's all about choice, but at the same time, don't shame users who choices to use Ubuntu due to ease of use, I've been criticized for using Ubuntu and was told I'm not really a Linux user if I use Ubuntu.
>Ubuntu is just a fork of Debian Testing branch,but it is less stable than the Debian Testing branch.

Ubuntu is based on Debian Sid, not Testing. It is also much more stable than both of them.
EndeavourOS is even easier if you hate command line installers and dealing with NVIDIA.
Historically, there was plenty of flex using ubuntu.
I agree yeah, the orange and purple color scheme is ugly, but I don't mind personally because, as you said, I can change that, I always change the color scheme to a dark theme to save my eyes.
Here here! I was scrolling for this! Orange and purple, plus gnome 3‚Ä¶ it‚Äôs just not oriented around how I use a desktop.
The new version includes multiple color schemes in the appearance setting, and doesn‚Äôt have much purple. I was pleasantly surprised.

I remember when Ubuntu used to be brown, back when they had naked people in the wallpapers.
I find the color scheme gorgeous
How dare you suggest people use their operating system for the stuff they do on it, rather than for the operating system itself?!
That is true of Flatpak and appimage too but they don't receive the same level of dislike.
They also take things like updates out of the admin's hands.

Snap bricked my pi cluster by filling up the partition on all 10 nodes on an autoupdate.  Ok, so it wasn't actually bricked, but I would have had to boot to rescue mode and clear out a bunch of stuff manually, and I wanted to keep them all under automated control.  So I restarted.

And it wasn't an end-user software.  It was micro-k8s.
I honestly don't know what you mean by "aint no upstream of snap store". Canonical is the upstream of snap store, since they are the sole developing team behind it. The server is not open sourced, but to be honest, most servers aren't. 

The biggest problem of snap store is that snapd hardcodes their own snap store, and provides no way to add a custom store, unless recompiling.
Publishers put their stuff on the snap store, so in that sense upstream is involved.
Also, RH employ many of the principle devs of various "independent" projects. And they in turn use Fedora as their playground, before RH skim the more stable bits to create the next RHEL release.
Different experience for me though, my Ubuntu installation never really had problems, aside from hardware issues like HDD breaking due to bad sectors, but other than that, no problems, Ubuntu just install and works out of the box for me.
I had an old laptop where older Ubuntu releases had working function keys for the brightness, etc. controls, then a later release broke that and it never worked again. I submitted a bug report on it, and about ten years later I got an email asking if this was still a bug.
>I've been criticized for using Ubuntu and was told I'm not really a Linux user if I use Ubuntu.

who says that lmao

or rather, how old are they? 14?
Probably, people saying that Ubuntu is "not real Linux" are referring to Ubuntu's controversial decisions (like snaps) that make Ubuntu (in their opinion) worse and less "in the Linux way" than other distros. AFAIK also ChromeOS is technically a Linux distro, but I wouldn't consider it at the same level of Debian, Fedora or Ubuntu itself.
>I've been criticized for using Ubuntu and was told I'm not really a Linux user if I use Ubuntu.

Probably you should not get  easily offended by random  young people  too much for using a distribution that you like in your case Ubuntu.

Why does it bother you what someone else on the internet thinks of your distro of choice and if it works fine for you? 

You can always listen to constructive feedback and proper criticisms, just to make sure everything works a expected. 

Just use the right tool for the right job,if Ubuntu works for you,then use that distro.

There is a plethora of distributions out there Debian/Arch Linux/Fedora/Ubuntu/Gentoo and their forks-all of them are great and also have their upsides and downsides. Just use  the best distribution that best fits your needs without listening to random people,do your own research,RTFM/man pages and ask if any issues arise on the support forums of that particular distribution.
For sure it's just my opinion and taste, but I believe most people don't like purple / orange desktop statistically.
Snap initial startup performance is significantly worse than the others. If they performed as well (and had done things like proper theming early on) I think there would be less criticism.

There will always be some because they are centralized.
For me personally it's because they're trying so hard to force it upon people. Quietly replacing Apt packages with Snaps and not even keeping the option to download them normally is a step too far.
Never used them, do they clutter up the mounts too? I've been running Ubuntu as a secondary for years before they decided to force this snap thing on me and at some point I found myself spending way too much time removing all this from my fresh Ubuntu install (I test a lot and reinstall the secondary often) and decided to switch to Debian.

Debian's DE integration is far from perfect, but at least it doesn't get in my way. I'm eventually planning on moving to something like dwm once I've finished learning C and read [TLPI](https://man7.org/tlpi/).
My biggest problem is verification of snaps. How am I to be really sure that some snap application (or Flatpak or AppImage for that matter) has not put some backdoor or something malicious inside? Normal repositories are checked by maintainers. 

Do Snaps, Flatpaks or AppImages get the same amount of resources for security verification?
I mean the snap store itself.. not the stuff on the store.
The fact that Ubuntu is so popular shows that you are not alone with that experience :P
Bad sectors are 90% of the time not a hardware issue,its a filesystem(ext4/NTFS/btrfs) issue,just use `sudo shred -f -v /dev/sdX(depending on your partitioning,different for nvme)`  on the HDD/SSD takes time but fixes most of the stuff.
I never really used the functions keys of my laptop to be honest, I've always used the GUI for that.
Some of those things are true (initial startup time especially). But that is not what the other commenter is complaining about ("bloat" / size) which does apply to the others.
I also don't like this transition (or at least the way it has played out) for desktop but I think the idea that it is quiet or for some ulterior motives is a bit of a myth. Ubuntu actually talks about snap on their website and elsewhere quite a lot, and has there reasons for going down that road. I do personally feel that it would be better as an option not a forced default, but I understand from the developer's point of view how Snap on desktop makes sense for a few reasons. Ubuntu's other (non-desktop) products use snap and those customers are generally happy or neutral about it. Aligning desktop with this distribution model is more efficient in terms of developer time and resources I would think, and desktop Ubuntu is a free unmonetozed product.
Flatpaks use process namespacing, so there aren‚Äôt and extra mount points visible.  

Iirc Appimage also creates mountpoints but they‚Äôre ephemeral.
I believe it is open source too, at https://github.com/snapcore/snapcraft and https://github.com/canonical-web-and-design/snapcraft.io
My experience is that my system randomly freezes and according to GNOME Disks, my HDD's bad sectors are increasing, so using the information given to me, I replaced my HDD with a SSD.
The GUI didn't always work either. It was bizarre waking the screen up from sleep sometimes. Never happened with Windows.
The final size is exactly the same. You either need those shared resources already installed on your system, or as a runtime. 

I pretty much like Flatpak and use it a lot. For example, I don't need to worry about old browser on Debian with firefox flatpak, or install 3rd party repositories on openSUSE for codecs, I simply use again firefox flatpak and mpv. 

Also I can install bottles and steam from flatpak without polluting my system with i386 libs. This helps a lot leaving system snapshots small too.

The final install size at the end is mostly the same as if I installed them with deb/rpm. I even have an Ubuntu machine and use both snaps and flatpaks. Firefox recently received an update and now the start time on my rig is few milliseconds. I understand that this might not be the case on older machines, but nevertheless, older is older, whatever the tech.

Calling it bloat is a bad choice of words because you need those libs like it or not, being as separated packages or inside the container. There is a tendency now to call it bloat... For example, recently an Arch user complained on openSUSE subredit about tumbleweed being bloated because installed too many packages. Fun fact, you can pass a --no-recommend flag to zypper and it solve the problem he was complaining about, but the most funny thing is that pacman doesn't have that option and if you compare both installations, the result was that arch installed like a shitload of language packages you like or not, resulting on a higher disk space installation than on openSUSE tumbleweed. But see, openSUSE is bloat.
They could have done all that without removing the possibility to install the regular apt version.

That alone puts me off Snap. I want to get away from the forced "We know best for you" bullshit of Windows.
> My experience is that my system randomly freezes and according to GNOME   
Disks, my HDD's bad sectors are increasing, so using the information   
given to me, I replaced my HDD with a SSD.

Just because a program on the software layer tells you that something is bad like a bad disk sector,does not make it so on the hardware layer,these bad sectors are related to the filesystem or filesystems (NTFS/btrfs/ext4/etc) that you use to partition the HDD/SSD,not the hardware itself.Every time you copy/paste/delete a file/folder a process occurs on the side of the filesystem replacing older data with newer data,there is no permanent deletion,etc.

So to cut short there are bad sectors caused by hardware layer failures which are very very rare and there are bad sectors caused by the software layer which are very very common,in this case the filesystem,these software layer issues are repairable just by using the commands I provided in the comment above.

Also here is the more simple version of how to distinguish hard and soft bad sectors on storage devices:

https://www.howtogeek.com/173463/bad-sectors-explained-why-hard-drives-get-bad-sectors-and-what-you-can-do-about-it/

Hope it helps you in the long run not to buy a new SSD/HDD every time you hit a "bad sector" error on the software layer.

PS: Freezing of the whole OS can be caused by many other issues not just SSD/HDD,it can be the PSU/RAM/GPU/Motherboard on the hardware layer or simply and more commonly these are errors on the software layer,even configuration of the DE and its components itself,lack of drivers or improperly installed drivers with missing dependencies all can be the issue causing the freezing of ones OS.
Different experience for me, I just use the GUI, I lower or increase the brightness with my mouse scroller.
You can safely disregard Arch users complaining about other distros being bloated.

In fact you can disregard any complaint of bloat whatsoever unless it is about Windows (even then it's overblown most of the time) or the user is trying to use a laptop made in 2010.
But removing the deb packages from the repos is literally the point.

Browsers are massive beasts and afaik (not a dev or a packager), it is a highly nontrivial task to even compile eg. Chromium.

Moving browsers to snap is because so that Canonical does not have to maintain 4563345 versions of the complicated browser softwares and keep them up to date as well (you don't keep browsers static even on lts releases).

They need to maintain one version only, and every Ubuntu user from now on (regardless of which version they are on) will access that version.

Also the Firefox snap is maintained by Mozilla itself, which is further less maintainance burden and the people who make the software get to maintain it (which in this case is very much a good thing).

If they also kept a deb version of Chromium and Firefox in the repos, it would completely defeat the purpose.

I also don't see how this is a "we know better than you" attitude, literally every distribution makes choices on behalf of the user, eg. on Fedora you install rpm Firefox that has been packaged by the Fedora packagers, and if you want to use Firefox from a different source, you need to jump through some minor hoops.

If you don't like the packaging method used on Ubuntu, you can install the flstpak version, the tarball version, or straight from a ppa.

I would argue the Ubuntu situation is actually much better than what is on Debian stable, where the browser is an un or barely maintained version of Firefox esr and if you want a safe and up to date browser on Debian, you also have to install from a third party source.

At least on Ubuntu the default browser experience is safe and reliable, the cold start time be damned (i speak from personal experience btw, i am not a huge snap fan at any rate but I use the snap Firefox on Ubuntu and the biggest issue I had is that I needed to edit a text file to get jupyter notebook to work with it).
I haven't used Ubuntu since 20.04, but I don't know that they did actually disallow installing via apt, i think they just stopped using their own resources to maintain the .deb versions. Are they standing in the way of installing the .deb version maintained/hosted by someone else or direct from the browser developer?

If not, then they are just allocating their limited resources how they see fit, which is their legitimate prerogative even if it does frustrate some end users.
The software is usually getting its information from SMART on the drive controller. If it is getting increasing bad sectors it usually does mean there is a hardware issue.
I should buy a new CPU fan and re-open that bug report.
I pretty much agree, in the Linux world I feel the people loudly complaining about "bloat" (those who use that specific word) fall into one of two groups. The majority are (1) Linux newbies, coming from Windows, Android or another commercial platform where bloat (in the sense of pre-installed commercial crap that is there for some other reason than to benefit the user) is a real thing, or (2) the Linux cultural elitist/gatekeeper types that call anything that they don't subjectively like/care about bloat. There are good reasons to want lighter or more minimalist distros for some people, personalities, and use cases but generally speaking i take the statements of the people throwing around the B word with a grain of salt.
No, you can install browsers from whatever source you like. They simply stopped including browsers (well Firefox and Chromium, Epiphany is still available :p) in the repositories as Debian packages.
Oh you absolutely can still install it by apt still if you add a ppa or something.  
I was just referring to their own repos.
>The software is usually getting its information from SMART on the drive controller. If it is getting increasing bad sectors it usually does mean there is a hardware issue.

[https://en.wikipedia.org/wiki/S.M.A.R.T](https://en.wikipedia.org/wiki/S.M.A.R.T).

See the section about SMART accuracy. Not in all cases also SMART's accuracy is a bit debatable when it comes to consumer grade SSD's/HDD's usage.

Assuming that the user does not have a 200TB HDD/SSD server storage type of device with a prolonged usage of 10-15 years and just a basic 500GB-2TB HDD/SSD on a consumer grade laptop or workstation,in 90% it is the file system error that is reported as bad blocks,especially this occurs on Windows with NTFS,you can encounter it on Linux too,but the cases are more rare,you can easily fix by just booting up Linux Live USB and running:

`sudo shred -f -v /dev/sdX`

`sudo wipefs -a /dev/sdX`

You also need to wait for the shred to finish,it takes a long time(a few hours depending on storage space).
Agreed and unless you're on a miniscule disk, calling installed software bloat is irrational. If it's running and it's unneeded, it's bloat, but when you're calling dependencies such as fonts bloat even though it'll have no impact on your system is like people getting annoyed at there being too much oxygen in the air cause they aren't breathing it now.
It is a frustration for sure if you don't want to use the snap version. But I do feel like that is their prerogative to do, since it is their resources being used to maintain a free product, and from the development and maintenance standpoint the snap/Flatpak model offers some substantial advantages and efficiencies.
I have a similar problem with my second hard drive, a 3TB WD Purple.

The symptoms are:

* It started about last october, after I rsynced a bunch of huge files from my primary drive to the WD Purple. Checking those files at the target location scared me with IO errors.

* File gets written without problems, and I can access it as long as in the cache, but when it clears out (or I restart my system), the files could get read slowly, or not at all. It doesn't bog down the whole system, only the application I do the checking (like a hash check in a terminal). The larger the file, the more probable it is for it to happen.

* The S.M.A.R.T status shows increased number of read error rate, and there are pending bad sectors (usually 5, but sometimes 8), but the uncorrectable sector count is still zero. Last time when it was upped to 8, the system also made the partition read-only, but that only happened one time.

* Everything I've saved on that drive can be read without too much problems, only the newer stuff is what can be iffy. Sometimes older stuff also gets problems, can't read certain older files but those issues get magically fixed by themselves.

* I also have to mention that this drive is a drive made for surveillance purposes (I bought it out of mistake), and has a [firmware issue with write caching](https://www.reddit.com/r/linux/comments/c59nry/btrfs_vs_write_caching_firmware_bugs_tldr_some/) (I checked the drive firmware and  it matches so it could be an issue). I disabled write caching since that tread until these problems appeared.

* Tried to use the `badblocks` command, but it was bogged down with the first sector after two hours

My question is: should I try the shread/wipefs method you suggesting to the other user (after backing up everything on that drive), or it's maybe a hardware issue for me? also can those commands can be used on partitions (only `/dev/sdX2` have these problems, a smaller  `/dev/sdX1` doesn't have these problems, `badblocks` could even finish that)?
>that tread until these problems appeared.  
>  
>Tried to use the badblocks command, but it was bogged down with the first sector after two hours

You can backup the data and use the methods I mentioned in the previous comments,just be aware that `shred` takes very huge amounts of time depending on the size 3TB will probably take half a day or more to finish,but it needs to finish the shred process.
Thanks.

So basically `shred` overwrites data and `wipefs` gets rid of the old filesystem "magic strings" as the manpage put it?

One last question, it is better if I do this with the partition unmounted and from a live stick like Ubuntu installer?
> One last question, it is better if I do this with the partition unmounted and from a live stick like Ubuntu installer?

Correct the whole disk should be backed up if you need the data and unmounted.You can do it either from your existing Linux install if it is not the main driver or from any Live USB Debian/Arch/Mint/MX Linux/Manjaro/Ubuntu/PoPOS Live ISO.
`if [ -f "$HOME/grabby.conf" ]; then printf ""; else`

No need to do an empty printf, simply do a `if not exists` 

`if [[ !  -f "$HOME/grabby.conf" ]]; then ...`

Since you're having fun with shell scripts check this out: https://mywiki.wooledge.org/BashPitfalls

Good job again!
what terminal emulator is this? I like the Firefox like tabs :)
[https://github.com/sannfdev/grabby](https://github.com/sannfdev/grabby)

Edit: I've had people ask, so I'ma just say it: This isn't just a repo for you to download from; feel free to make any changes and push them.
\> uses vim

\> always moves and deletes by single characters

No hate intended, it just seems kinda funny to me that you use Vim as a "dumb" editor in this demo when it's its weakest part
I see .cargo, hello fellow rustacean
I do feel smth's not right here:

[Image](https://imgur.com/a/DWjfAoQ)
What do you use as your desktop environment / compositor? It looks so beautiful
God damn that looks good and modern.
Note: This was just a personal project that I thought I‚Äôd share, so if it breaks for you, my b. Just lmk on GitHub on Reddit and I‚Äôll try to fix it asap
What about neofetch?
Thanks and thanks! That was actually debug code that I forgot to remove.
The pacman package is ‚Äúcutefish-terminal‚Äù. It comes with the cutefish de
Would it be possible to add an example or template config file? I don't see any.
Ah yes, another man of culture. Hello
https://imgur.com/a/DWjfAoQ
Just a tad lmao. You can fix it just by adding a space before each thingy in the box

Edit: Oh crap; a lot more is broken than I originally saw. I‚Äôll try to fix it soon lmao
I use cutefish DE. The terminal and file manager come with it
Thanks lol
What about it
Wups, nevermind then.
Is this cutefish ui generally? I'm into Linux, but I don't get how the UI swapping stuff works. This looks really really nice, would like to try it out tbh.
Literally never heard of cutefish, this is news to me
Running the command/file will automatically create one if it's not present.
What distro is this? I use arch btw
Yeah it says fish is /bin/bash. That's what I was trying to point out. I also did have to install wmctrl
Okay, I pulled the changes from GitHub and added your script to my fish config:

[Image](https://imgur.com/a/nyllU8Z)
Yep. Cutefish isn't that customizable, so this is the default. I don't mind the lack of customization, though, it looks pretty clean out of the box.
Yeah it‚Äôs an Ubuntu-based distro, but I just use their desktop environment
Got it. Perhaps I didn't look closely enough. 

It's still important to give an idea of what can be customized/configured.
I'm an Arch user (btw) too
/r/cringe
Sadly there seem to be no cutefish programs in my distro's repositories. I use Linux Mint. But it looks as though I could get the source code from [here](https://github.com/cutefishos/terminal) and compile it.
Nice. Cutefish has their own gui library, so if you just use the terminal and not the DE, idk if the blur and transparency will work. But I have no clue
A long time ago I used to use Expect to screen scrape (using a 3270 emulator) mainframes, it worked well and was a lot faster than the commercial product we were using at the time.
Only on Linux can you use a 32 year old piece of software and be *that* happy about it \^\^
I discovered this program last week when I was trying to process ```npm``` output in a pipeline. It has a dumb output format and there's no option to disable colors and emit line endings, so I used ```unbuffer```
expect is really neat! Can be used to automate things that would otherwise be sort of a pain to automate.
You know something is good when people still use, copy, or port it 3 decades later. It used to mainly be used  with tcl (tool command language). Does anybody still use that?
I actually ran into an issue with Ansible using the full python version of this (pexpect). Funny enough, RHEL 7 doesn't even have pexpect in its repos despite a surprising amount of ansible relying on it...
Definitely going into my programming toolbox. I even made a separate php script to clean up the input args prior to sending, then parse the output before returning the response to the client.
Did you lodge a bug for that ? I'm pretty sure that Red Hat heavily supports ansible on rhel 7.
You would think, considering they acquired ansible and completely reworked RH Satellite to use it. No, unfortunately I changed jobs before I could submit the bug report. Never went back to see if they allow anonymous reports.
RH Eng usually gets many anonymous reports due to the type of customers that use it.  It might be too late now... rhel7 is in maintenance mode now and is 3 releases old now.  They are pretty good about making things work, that are supposed to work.

If there are bugs like that, its best to ask in in the first few years of the product.
This looks great! I have been looking for an alternative to Albert. Are there plans to also do file-finder within Findex?
Currently I'm a bit confused how and where files should appear. I'll think about that after my exam ends(it'll start at Sept 15).
Check out albert. They do it prefectly. https://albertlauncher.github.io/
Thanks, I've checked that out. It's quite nice. I'll try to implement that feature :D
Man those RK3588 boards are nice. 

16 gigs and 4 A74 cores and 4 A55 cores sounds like a very nice little desktop setup. 

I need to get my hands on one of these things. I was always curious about having two types of CPUs on a single system, but never went into a deep dive over them.  Like is it possible to disable the A74 cores sometimes?
They seem quite expensive tho, 750‚Ç¨ at AliExpress
Frankly, I wouldn't buy any ARM-center desktop or embedded board that cost more than $100. RISCV at least has novelty and is interesting in being actually open, whereas x86 is actually good for desktop usecases.

Maybe if at some point I needed to do some serious server stuff I'd consider it, but even then...
The Rock 5b should retail at around $200-$220 for the 16GB model.
I wouldnt even buy a Raspberry Pi right now for 100‚Ç¨~

2018~Atom boards cost about 50-60‚Ç¨ right now and are as fast or faster than the Pi 4, while being cheaper+more open.
>in being actually open

RISC-V is open for manufacturers, not consumers.
You'll never have an architecture open to consumers until we all get 3D PCB printers.
Thanks to this submission Flatpaks are no longer overrated, but simply "rated."
There's also the fact that apps work everywhere the same, be it good or wrong. So bugs are generally easier to reproduce, and quicker to fix
The problem is maintaining the different packages for all the different OSes takes time for the package maintainers, and it's often done by volunteers. Flatpaks (and snaps) reduce the amount of effort it takes to maintain everything. I agree that system packages are better, but ultimately beggars can't be choosers, so I'll deal with a Flatpak if that's what the maintainer wants to do
honestly i use flatpak as a last resort. but considering its main competitor is snap... flatpak certainly does a much better job id say. idk if they're "overrated", i think most people in the community take it for what it's worth
My experience has been like this: When apps are aware of Flatpak, they work perfectly. When apps aren't aware of Flatpak, we get some issues, but they mostly work OK.

I believe it's in the best interest of the community to make more apps aware of Flatpak, because then the experience will be the same as outside of Flatpak with all the security/updatability/distribution/universality benefits that come with it.

Flatpak isn't hype-worthy because of the experience it has now with apps that don't know about it. Flatpak is hype-worthy because it makes app development on Linux much more accessible, which means we're gonna have a lot more high quality apps. This is already happening with programs like GNOME Circle
In the current ecosystem, flatpaks do have certain drawbacks compared to normal packages from a user perspective, especially since both types of apps dont mix well with each other. Flatpaks truly shine when paired with an immutable base OS. The reason Android is so bulletproof in terms of stability is because it mounts large parts of the base os read-only and wraps all applications in a robust sandbox with fine-grained permission control. macOS and even Windows are beginning to do the same.

Pairing Flatpak with a read-only /usr partition allows us to replicate this model on the Linux Desktop. This will all but eliminate major breakages through updates, as it replaces updating single packages with atomically replacing the entire OS with a tested snapshot. (as well as reverting if anything breaks) Centralizing app distribution will also significantly reduce the amount of duplicated effort each distro has to do in order to package said apps for its own system (and often multiple versions of it). This becomes especially relevant as with increasing complexity, distros such as Debian are starting to have serious issues with keeping up.

TL;DR: While Flatpaks in their current form aren't perfect, they provide an essential building block for the Linux desktop of tomorrow.
>modding doesn‚Äôt work

I doubt this one, because I have a bunch of mods on my GTA:SA installation. But even if it is true for whatever game you're trying to mod, it's probably an issue specific to the game or you're just doing something wrong.
I prefer Flatpak simply because it is dead simple to sandbox applications with flatseal. Being able to disable x11 and other permissions for apps is a killer feature you don't hear a lot of people praising flatpaks for. 

For all the user apps I have installed, I always check if the flatpak is available first before installing the native package.
Flatpaks are great and getting better all the time.

I use the steam flatpak, and the only difference after they fixed the pressure vessel bug is this: the path at which games are installed in the file system is annoying, and the steam overlay actually works in civ vi when running the flatpak version
Many people use them as a second choice or when they're imposed by the distro itself, like elementary OS for example (don't mean imposed in a bad way, btw). I personally use my distro's repos first, AUR second (I know they can break, or cause issues, or can be unsafe...etc), and flatpaks third when I absolutely have to.
How does flatpak help in not stealing data by discord? Is it because flatpaks are containerized or something?
Sure flatpaks minimize what discord can get with telemetry but it doesn't stop it. Not sure why you think it will stop them from using your data.

Note they are not STEALING your data. You allow them when you create accounts and accept TOS.
I learned the hard way that VS Code flat pack/ Snap locks the app to a fake file system which cannot find Dotnet installed in the default location. Took weeks to find this issue.
My take on Flatpak is pretty simple: If an app is available as a Flatpak I always try it first and only if there's something which doesn't work I'll use the package from the distribution repositories instead (if available).

Out of the ten most frequently used apps on my system six are Flatpaks, so I consider them not at all overrated for myself and I'm really glad this technology exist.
I don't know if I'd be fully committed to Linux **without** Flatpak. Before using it, I've ran into dependency hell issues which got so bad they made me reinstall biannually or so, which is kinda exhausting.

I use Fedora Silverblue now and have an extremely stable experience with up-to-date apps; I'm really happy with it. Apps are universal, auto-update (via GNOME Software, toggleable), and don't break. I can run "bleeding edge" stuff without worry. 

For example, I'm thinking of trying the GNOME 43 Alpha soon on my workstation, and if something goes wrong, I can always rollback my base system without data loss. I also use the amazing Wine manager app [Bottles](https://usebottles.com/), which uses a git version of libadwaita so new, even **Arch** [doesn't package it yet](https://github.com/bottlesdevs/Bottles/issues/1747#issuecomment-1179693297).

Also, I'm interested in making apps for Linux, but there's no way I'd consider supporting 15 different distros where my apps are packaged/patched (in unknown ways) by third parties. That sounds pretty painful. Flatpak saves me there, too.

I encourage people here to run more Flatpaks with an open mind and think of the possibilities for a better Linux desktop in the future. IMO the things I outlined above are a more desirable model than what's standard now; there's a reason why the new SteamOS is using a comparable software stack.
The biggest issues I have with flatpaks at the moment is that not all apps are interoperable (Scribus, for instance, can't open an image editor or a browser instace regardless if they're native packages or flatpaks) and the fact that updates tend to require more bandwidth - and my internet speed is rather low. Also, the way they integrate the Nvidia driver into the sandbox is not ideal. 

These may sound like major issues, but in actuality, they aren't that big of a deal.
Not sure what issues you are facing with Steam and Flatpak. Works fine for me.
Why does that make them overrated? They have their use like anything else.
Yup, I just dealt with steam flatpak tonight. It took hours to figure it out. Apparently if you don‚Äôt have your secondary drives already mounted with permissions already set, before you install and open the steam flatpak, then you can never use that drive, even if you run the file system user override for the package. Uninstalling and reinstalling doesn‚Äôt work if done through the shop since it retains settings somehow. I uninstalled and installed through apt and it immediately worked.
If it works smoothly for you, it is underrated.

If you have issue with it, it is overrated.

Everyone has an opinion.
I think they're pretty good. I still only use them when I can't find a system package, though.
I'm a pretty simple user as far as packages are concerned - i look for rpm packages first and foremost, if those packages are outdated (or indeed nonexistent), only then do i look at a flatpak. Practically speaking, i find it more convenient to just use my package manager instead of flatpaks.
As someone who distributes applications on Flatpak, I can say that it requires some special treatment that can be a headache at times, but it is a nice option for users as it allows supporting many other Linux distributions I otherwise would not have the time to package for. (So without it they would be stuck building from source.)
I use them exclusively for things like Slack, Zoom, Discord, etc, propriety software that I need to use but I don't really want on my system. It just is cleaner and a little more private way to install those compared to traditional packages.
Discord still saves and uses all data you send through their service I don‚Äôt know what you are talking about. Have they ever being caught harvesting data from the computer itself?
That's right, but you have to take the following into account:

1. Flatpak is still being developed and a lot of things are to be done, but a lot of people started using it already because it is good enough for many use cases if not most use cases (at least for average Linux users).
2. Not all Linux packages are aware of the sandboxed environment they are in, and that introduces issues too.
I dont think flatpacks should be the first choice for downloadig like fedora recommends, but the last one. It makes the developers life a lot easier and some niche apps will work as a  flatpak only on somebdistros
I like appimages. :)
Old timer here. Been using Linux for 20 years. Flatpaks are amazing and I can't imagine going back.
my problem with flatpak is the disgusting names in the cli
really is horrible
I see them as a reasonable solution for software not available in my distro. Don‚Äôt love them, but no particular issue with them either.

I also think AppImages are a very neat solution in certain circumstances, I don‚Äôt have any loaded though.
My biggest complaint about Flatpaks is that it just feels like it's not well thought out, and being developed without any consultation with developers or users.

The classic example being Discord, the default configuration of Discord after install from Flathub prevents you from accessing any files in the home directory, so you if you want to drop a photo or file or whatever into a conversation with someone, you either have to move the file into Downloads first, or use Flatseal to disable the sandboxing. Both of which is stupid. It also breaks the functionality which allows Discord to display to your friends the current application or game you're in.

Unpopular opinion, ***I know***, but, it's not Discord Inc's fault.

Just taking all the popular apps out there and shoving them into a Flatpak, with tight restrictions on permissions, without consideration for what that sandboxing will break, is the problem here.

Discord Inc never designed their application to run in a Flatpak, and I doubt they were even asked. Someone just decided to take the Discord app, bundle it into a Flatpak and distribute it on Flathub. So it's not even an official version of that software, which you'd only know by reading the tiny fine print at the bottom of the Discord Flatpak's description on Flathub.

So anything wrong with the functionality, is ***entirely*** on the maintainer.

Flatpak should be some kind of opt-in API that apps can talk to, permissions should be something apps can request explicitly while running, aka 'The Android Model'. Which is what it feels like Flatpak is trying to emulate, but doing so in a very backwards manner. But instead the approach just seems to be to turn everything into a Flatpak, let anything that will break, break, and punt all the frustration of figuring out solutions to the problems onto developers and users.

I've been using a few Flatpaks here and there when it's the only option, but lately I find myself just opening Flatseal after every Flatpak I install and immediately enabling most of the permissions just to avoid any issues. Which is just a really crap UX.
I have no issues with flatpak and for most cases where I would need to add a 3rd party repo, I prefer to use flathub if the app is available on there.

I have a feeling it's like PulseAudio, it works great, but people try to config things by themselves, have no idea what they're doing and they break it, and then complain that it doesn't work.
I use flatpak if the program has too many dependency which might lead to conflict of packages. For now I only have bottles installed from flatpak and the experience is good.
I personally just use the packages of the distro, and if I need anything not included or updated more regularly, I use AppImages. What do I need Flatpacks for again?
I'd love it if flatpak followed xdg desktop standards and could see my ~/.config and ~/.local folders and not live in .var
And that kind of sandboxing gets really annoying when it breaks some of vscode‚Äôs basic features or means that discord can only upload files from three specific paths.

Not to mention that flatpaks are *much* larger.  I don‚Äôt remember the exact numbers, but the flatpak for telegram is *huge* compared to the rpm.
It's better to think about flatpaks not in terms of user convenience, but of developer convenience. Flatpaks becoming more standard means there will be a higher chance of most programs having officially supported Linux versions, which is ultimately good for users.
Is this not the general consensus?
Agree, but for a different reason. Linux is just one of many open source Unix-like operating systems. Software should be developed and distributed in as OS-agnostic way as possible. Flatpak is okay, but it makes me sad to see open source projects narrowly focus on it (or other Linux-only technologies).
Even if the Flatpak is more up to date than the distro's repo, I'd still prefer it over Flatpaks or Snaps.

Less "weirdness" in native apps IMO. I always some see some weird behaviour from either Snaps or Flatpaks. Although, some snaps/flatpaks are better than some.
In my eyes, Flatpaks are trying to become everything. They're not just a way of distributing applications, but also a way of packaging them, sandboxing them and some even advocate to replace all system packages with them.

It's much like with systemd - it's okay now, but it's replacing everything. It's much more than an init system. Flatpak breaks Unix philosophy by trying to do many things at an acceptable level than doing one thing perfectly.

My real issues, however, stem from an increases supply chain attack risk. If, say, Fedora's or openSUSE's repositories get compromised and all of the packages are henceforth replaced with malware, the only users who will suffer will be those that use the distributions in question. Not the entire Linux ecosystem.

With Flathub, users of essentially every single distribution, as long as they have a Flathub application installed, can have their setups compromised. The compromised Flathub server (the security of which is still not entirely clear) will distribute malware to every single distribution through a popular Flatpak like Discord. The sandbox features will simply be disabled by the attackers and you will have one of the biggest attacks on Linux desktop in history.
Unpopular Opinion? What? Are you sure?
I generally agree.  I don't think the entire Linux ecosystem should switch to Flatpaks, but I think they're great for complicated builds, proprietary software, redundant builds, and even *building entire distros around*.  I think some appliance distros like Silverblue would be great for people that can live in that environment, but I also think we should also still have native packages.

The Steam problem is a personal issue you'd need to learn to resolve, and doesn't inform the greater point, however.
I‚Äôd rather install a standard package and use Firejail or similar for sandboxing, personally. Flatpak and snap drive me nuts.
Flatpaks also lacks control over app daemons unlike in snap or native packages using systemd
I just hate how it'll download a million Nvidia supported card format environments and take longer than the actual app updates.
I use Guix system which some packages are difficult to find / not available. Flatpak is really nice for apps that I need to have for work or for hobby projects (Arduino studio for example).
I totally agree, flatpaks have their uses but system packages are always gonna win.  Snaps are just plain useless.
The standard approach to computer stuff should help here: it depends

Package is in the repos? I'll install that over any other option. When the choice is between using a PPA, compiling from source, or flatpak, 9/10 times I'm choosing the flatpak. There are edge cases of course like you mentioned, but the vast majority of things I'm using in flatpaks work perfectly. This is doubly true for my Steam Deck where almost all additional packages I've installed are flatpaks.
tbh, i dont think ive ever used a flatpak so i cant really say there overrated
Flatpak is just docker for GUI apps, change my mind ‚Ä¶
At the moment it is not unpopular opinion - you are just noticing people start to use and adhere to flatpak. But many people (most?) still dislike.

What we do not know yet is how much flatpak will make it. I believe it will be huge because i see mostly advantages. The disadvantages

\- yet another format -> already enough distro support it so it is already less a burn. This will even increase and we will see more & more distro with only core package and not bothering with app packages

\- disk size -> somewhat ennoying for some cases but will not be a real deal for desktop

\- ram -> only if you use many runtimes. I doubt this will be a real issue

\- security limit -> flatpak has tangible improvement to offer , so unless someone comes with a superb sandbox system there is no reason to be a real issue

\- bugs -> there are no bugs that cannot be fixed. I mean i am not aware of fundanmental design issue in flatpak causing bugs.

&#x200B;

likely flatpak are the future
If you want to mod, then the deb would be better, but for the average desktop user who wants to install Chromium, OnlyOffice, maybe install KDE apps on a Gnome system, etc and have them just work Flatpak is alot better as they always have the latest version and can uninstall programs without leaving configs/dependancies
I haven't had to use a Flatpak on Arch at all. Pretty much everything is either in the official repos or the AUR.
Have you tried snaps?





/s
Flatpak has already won. Just look at Flathub statistics from 2020 and 2021. Honestly, I'm really glad that GNOME is pushing the flatpak standard, it's incredibly easy to make your own with Gnome Builder (p.s. check out that sexy 43.alpha release) and GTK apps tend to be flatpak aware.
I have to disagree, Flatpak'ed Steam plain works better for me. I also like how (most...) apps also just dump their own settings and the sort onto their own folders, so just `flatpak uninstall --delete-data` wipes them clean from my system.

The real problem I have with them is inconsistency with system theming (I have to do it every time I reinstall and I've already forgotten what I did) and apps that do not use portals. They force you to allow access to entire drives or your entire system which defeats the whole purpose of sandboxing.

Oh and development software/IDE/SDK flatpaks are hell. Don't do that to yourselves.
I think if you title your post "unpopular opinion" and then its rating goes into triple digits you should be permanently banned from reddit. Seriously, at this point it's on par with "upvote if.." epidemic
I honestly don't understand why deb/rpm's aren't good enough in the modern Linux sense.

Snaps are too fucking slow to load, even after the first load.

Flatpaks give me nothing I don't already have (prove me wrong?).

Anything I really want is in deb form (btw I run Ubuntu). So why would I even care? (not trying to @ you OP, just discussing the topic is all <3 )
I'd rather not use Linux than having to deal with Flatpak. As long as it's not standardized in every distro it doesn't matter so I hope it stays like this.
Is this unpopular? At least with me (and in my "social bubble") this opinion is very popular.
> it breaks things

Like what, for example? I've been using Steam Flatpak for over a year and would like to know what problems.

> modding doesn‚Äôt work,

I'm trying to understand how you came to the conclusion that Flatpak can affect this. I admit I don't use many mods, but when I did (GTA) it worked without a problem.

> should be only used in certain cases.

It's funny to read this when other OSs seek this direction and more and more developers adopt it.
Flatpak has made application development on Linux much more accessible.

Slowly it has taken its place even with the noisy minority that still continues to share misinformation about it.

Flatpak isn't overrated, it's underrated.
In order:

* Source (GitHub/GitLab)
* System Packages
* Flatpaks
* Web Apps
* \[1 billion steps down\]
* Snaps
Afaik steam os revolves on flatpaks
> discord not stealing your data 

Hahaha, thanks, that made my day. 

Seriously now: no, flatpacks have nothing to do with security. Of any kind. Do not run applications that you do not trust or would
not run without flatpack on your machine. 

There is no security.
Slower than native.
Are flatpaks better than debs?
He has a discord server
I think this is pretty much the consensus view.

Everyone prefers system packages, no matter what package manager your distro uses, but if you don't have one available, then you look for a Flatpak.
i just don't get why we're installing a standardized os on top of all of our nonstandardized oss...  can't all of the nonstandardized oss just standardize themselves?  it's not much of an os if you can't take an executable offa here and copy it to there...  If the whole idea of linux is to share, why won't my executable run on any os (for a given cpu)?  I'd put that on debian\* vs. fedora\*.  i'm stayin in the debian\* camp.  How about instead of every app developer coding to a standard os that has to be installed on top of the nonstandard os, we have the nonstandard oss rewrite to run on the standard os.  wouldn't that make more sense?
Whats the alternative? They work better then snap and appimage. Flatpak solves a problem we have had in linux for years and that's a unified way to packing apps across multiple distributions. Developers are no longer going to bust there butts to make binary for ever flavor of linux out there.
This is an unpopular opinion? I thought everyone hated flatpaks.

>But when you start using flatpaks for other apps such as steam, it breaks things, modding doesn‚Äôt work, etc. flatpaks are great but should be only used in certain cases.

Absolutely. I think it's not just proprietary apps, i also like them for things with a lot of dependencies or small self-contained apps. 
 What's really bad are IDEs, the sandboxing breaks a lot.

>Imo system packages are much superior.

They can be, but i wouldn't generalize it. With certain apps i had a lot less trouble to get them as a flatpak.
 

Also what's important to remember is that packaging distribution packages takes time and effort. If your distro has a lot of maintainers that package exactly those things that you need, not broken, and not outdated, good for you. But because of the maintainer workload, a lot of distro packages (especially less popular older tools) are not great.
ok bud
Tried to install Audacity with Flatpak on my Arch machine. It didn't even run. The AppImage version works as it should.
Wait ... Does modding works on system packages? I thought it works only on windows
Isn't flatpak the future? Distros like Fedora Silver Blue seem like the way forward for Linux to reliably capture desktop market share.
Could not disagree more. Firstly, Flatpak give developers one platform to target, which works on ALL Linux systems (even immutable root FS distros). Secondly, Flatpak is a boon on distros like Mint, Debian, RHEL and Ubuntu LTS. The software in those repositories can be very out of date, and, conversely, newer libraries on some distros can cause some programs to segfault. 

In my experience, I can always be sure that a Flatpak will work, because its dependencies are bundled together (but runtimes are still shared, and the Flatpak environment is deduplicated using hashes and other clever tricks). There is no dependency hell with Flatpaks, and no failed updates. I use Flatpaks for every app excepts IDEs, where there are still a few issues here and there that need workarounds. (Nix packages?)

In my opinion, the base package manager should only be used for kernels, drivers and desktop environments. Anything that requires deep integration and can‚Äôt be sandboxed.
Fwiw I use steam in a flatpak and as long as you add an external place for it to install games (I use another drive), modding can be pretty easy
The only issue with flatpak is permission. It's weird. And need many manual intervention. 

I would hope the permission system would be easy as android.
>But when you start using flatpaks for other apps such as steam, it breaks things, modding doesn‚Äôt work, etc.

Welp, I have been gaming on my computer with Steam Flatpak for basically a year now (since they fixed Proton running on Flatpak), way better than the repo version and way easier to install other Proton versions like GE.

Also, same with Firefox, Flatpak version works better for me as I don't get weird bugs on extension pop-ups and other stuff. I basically go Flatpak first and if there's a bug, or I don't like something, I use the repo/rpm version.
You have very much simplified flatpak to certain apps. But I still wanna know why modding doesn‚Äôt work for you on steam? I‚Äôve been using the steam flatpak and everything I used to do works on the flatpak.
Average user seldom needs applications outside the repos of distribution, even if he installs a flatpak or two, does it make the system secure/private? I don't think . 
I am not opposed to flatpaks, I've used them and they are better than snap ,appimage , but the real benefit of flatpaks will be experienced after we move towards silverblue, microos type distros of future which are similar to Android and iOS ., They can be updated on the fly and won't get messed up or apps won't have access to the core os . We are still 3-4 yrs away from this kind of distros to become new normal.  The thing about phone is not the problem of Os but the giant corporate houses watching closely everything we do, cause they can. So privacy is actually highly compromised already, everybody cannot become Anonymous and hide behind proxies , it is not possible, it is not practical, it is not advisable.
I just want my pc to catch up to my phone. Which pains me to say.
It there any perfomance difference beween flatpak and native packages?
Check out fedora silver blue, flatpaks are pretty much necessary there for a good experience
It's really not *that* unpopular of an opinion, as I'm sure you've already seen, and you're only saying "unpopular opinion" to bait engagement and pull people to your side of the issue.

I'm of the mind that Steam-as-Flatpak is the future we should be working toward for Steam on non-SteamOS distros. That is the version I use exclusively on my computer, with mods on some games included. But it does have some warts that presently have to be papered over. For one, it doesn't use the file picker portal at all, so you can't select library directories without poking holes in the container. That needs to be solved upstream, and then the Flathub package will end up with a much better out-of-the-box experience.
Truly, just invest time and effort into guix or nix to make it more user approachable. All these things like flatpack, snap or appimage are just poor mans way of solving the same problem.
I think the only reason people care about flatpak is because of snaps. They do pretty close to the same thing. One was overrated, one was underrated, and both should sit somewhere in the middle. Not a bad idea, either one, but neither is the future of Linux program distribution.
Could be unpopular... but a reason, if not te main, for me, to switch from Apple OSX to Linux was the inconsistency of their much-touted "unix core" when they base their overbloated system on the money you can spend for their RAM, drives and CPUs to support duplicate, triplicate, or quadruple libraries. And on various architectures. 

This is impractical if you spend ‚Ç¨9000 on a workstation mega-AI-hipermegaquantical PC (SuperSaien oc), but totally absurd on a normal-person-PC. "Apps" are 80% junk packages, and that's not very ‚ÄúUnix‚Äù. Im sorry, but was not my decision, When I born Unix was here, yet.
I mean, Flapak doesn't auto-update.

Also, on Steam all you have to do to mess with the games is go into .var/app/com.valvesoftware.Steam and there is everything you would normally have in your home.

There are many things were Flatpak has problems and/or doesn't work as well as a system package, those two are not. Also ideally the sandboxing would ask for permissions on the fly, but I don't know if that's even possible.

Nonetheless, in all honesty, Flatpak is probably enough for most users that simply want GUI programs.
I wish that flatpak could be integrated into dnf. I don't like managing the system through multiple package managers.
Very popular opinion
Thanks to this comment, I no longer feel overwhelmed when talking about flatpacks, now I'm simply whelmed.
Honestly what software out there is flatpak only? If flatpak is the only option I just use different software.
How does flatpak do a better job?
While I believe that Flatpak is a step in the right direction to catch up with the progress of other popular OSes regarding application sandboxing (especially Android and iOS), it doesn't come even remotely close yet and shouldn't be advertised as a sandbox right now.

Read more here: https://github.com/flatpak/flatpak/issues/4983
Flatpak support only makes Flatpak easier to use. It doesn't make the apps better. It just adds useless complexity to the app, and then you have to do it again to achieve Snap compatibility. And to make things worse, high Flatpak/Snap adoption is a self-reinforcing cycle. If it happens, then *everybody* has to have Flatpak or Snap installed on their system.
So like fedora silverblue?
>Centralizing app distribution will also significantly reduce the amount of duplicated effort each distro has to do in order to package said apps for its own system 

Fedora with their version of flathub: am I a joke to you?
As a user, not dev, il am partiuclary pissed when apps force me to find sp√©cial folders or simply can't let me access to my davs folders due to security concerns. As a user perspective, it's not a feature, it's a bug.
I would hate it if a desktop OS enforced a read only partition module. Even on Android that's far too restrictive for unofficial apks you might want to run and customize, so people "root" the OS. While I do think sandboxing is good, limiting the end user is not.
It'll always be tomorrow.

You don't need flatpacks, snaps, or anything else of the kind to make the OS portions of the OS read-only, you just need some disciplined package maintenance.

Flatpacks and snaps work against disciplined package maintenance.
Can't wait to see the desktop of tomorrow... containerized electron based de maybe even...
>. . . it replaces updating single packages with atomically replacing the entire OS with a tested snapshot. (as well as reverting if anything breaks)

openSUSE Tumbleweed already does all of that using [Zypper](https://en.opensuse.org/SDB:Upgrade_Tumbleweed#Situation), [openQA](https://openqa.opensuse.org/) and [Snapper](https://en.opensuse.org/SDB:BTRFS#Default_Subvolumes) respectively.
I've modded all the Sonic games available, Yakuza 0-6, GTA 3-5, FF7R, and The Sims 3 (Luris Flatpak), so idk what OP is talking about.
Modding absolutely does work if you know what you are doing.

I've modded FF7R to fix the stuttering issue in Steam Flatpak.
I‚Äôve had issues with stardew valley modding.
Edit: idk why I am downvoted. This was a legitimate issue. Smapi wouldnt open the terminal so I could see if more needed updating.
And you have GTA installed via flatpak? A wine prefix isn't a sandbox let alone a flatpak.
That's because flatpak was made by and for folks who care about things like disabling X11, when what most users really want is just a system that

* solves Dependency Hel, and,
* can run exclusively in the userspace

For most people full sandboxing is more trouble than it's worth, especially when your video editor is rendering outputs by default to its internal file system and not, you know, `~/Videos` (looking at you, old flatpak of kdenlive üò°).

I, too, will go with flatpaks over native packages, largely for the portability, but I inevitably spend a large amount of time post-install reconfiguring everything in `flatseal`.

At the end of the day I just really wish there were an alternative that worked more like `conda` (aside from, well, [`conda`](https://docs.conda.io/en/latest/)).
Is the AUR more unsafe than flatpaks? Installing flatpaks feels like it gives you little feedback - you dont have the nice rating numbers and comments, and you dont get to see where the source is coming from directly. Flatpak does have sandboxing, but afaik the packager can opt out of a lot of it. How do I know that the flatpackager didnt insert something malicious in between me and the dev? With the AUR I can at least see where the source is coming from.
Discord can't see your files (depending on what permissions you give it), it can't see your open processes, etc. It can still spy on your other X windows, but that's a limitation of X11.
Yes, I chuckled a bit when OP mentioned that. Discord probably mines more data from your messages than from your OS information. Not to mention how snoopy Discord is on network data; I doubt you‚Äôd even be that private unless you use a VPN.
OP is making things up, Flatpak wouldn't protect you from that.
Yes, they are sandboxed so discord cannot take system data. Discord is basically just spyware.
iirc flatpak vscode has a welcome message which tells you exactly this
Say you're a programmer who doesn't read documentation without saying you're a programmer who doesn't read documentation. hahahaha

Jokes aside, in the first run of VS Code you have all this information. You didn't need to do a lot of searching.
My strategy is the opposite. Flatpak is my last resort. If something doesn't work or break with the app in the repository, or an old stable version doesn't have a feature I want, I use the Flatpak. My computer has exactly 0 Flatpaks installed.
This seems like a good way to go about it. Win-win imo
I share the same opinion. Silverblue is a wonder and a great example, I agree with everything you said. As a developer, I share the same opinion about how painful it is to maintain multiple packages. Anyway, I agree with absolutely everything you said, even about Bottles.
Flatpaks are used by me on POP in pretty much the same exact scenarios you've discussed for the same exact reasons.  

I don't know what the OP is  talking about re: modding, I can mod almost anything without too much hassle, though, I did need to do some flatseal work to convince steam it was okay to use my spinning rust drive for game storage.

>I've ran into dependency hell issues which got so bad they made me reinstall biannually or so, which is kinda exhausting.

As someone who has run Linux for 10 years on various distros, including many of them on Arch‚Äîwhich is known for its bleeding edge packages causing breakages to your system on occasion‚Äîand makes it a morning routine to always update all my packages to the latest as soon as they are available, I can't possibly imagine what you did on such a routine basis that you completely messed up your system to the point that you had to reinstall the whole OS biannually. You're either very careless with your system, or you're being intentionally histrionic and intellectually dishonest.
My main obstacle (strictly user POV) with Flatpaks is always the same. My internet is shit where I'm at. I'm still stuck on ADSL+ speeds with shitty ISPs. The first Flatpak download is always 1+GB despite the fact that, allegedly, my system comes with a base runtime already installed. The App I want is already available some other convenient way that's just 3MB download. Guess what I'm going to choose? I don't want the uber bleeding edge, I don't need the uber bleeding edge. Quite the contrary, I've encountered software that break features, introduce bugs and break systems quite often. I prefer when the software is stable and doesn't get in the way of productivity. Why would I download an entire new system to run on top of my already properly installed and functional system?

With de-duplication issues and irresponsible developers who refuse to update their libraries or use sane development principles, it's just trading one set of problems for a different one.
> Also, I'm interested in making apps for Linux, but there's no way I'd consider supporting 15 different distros where my apps are packaged/patched (in unknown ways) by third parties. That sounds pretty painful. Flatpak saves me there, too.

As an app developer, that's [not really your job](https://drewdevault.com/2019/12/09/Developers-shouldnt-distribute.html), it's the job of distro maintainers. Assuming of course that your app is free and open source, if it's a for-profit thing then I guess you should package it where it's profitable to do so.

The number 15 is also very inflated, if you're doing this then you probably don't need to worry about anything but Ubuntu or Debian, at most Fedora or RHEL as well. Even as an arch user, there's absolutely no need to package it for arch because the community will probably provide better packaging than you unless you're also an arch user and will be dog-fooding your own AUR-package.

Just look at Discord for example. Hugely popular app. You have two alternatives for linux download: `.deb` and `.tar.gz`. Similar for spotify, snap and `.deb` for "debian/ubuntu". Steam - only deb as well.

As for libadawaita and gnome 43 alpha, I don't think alpha releases really count. If you're using an alpha release you're signing up for potential troubles, that's quite literally what alpha releases are for.
A flatpak retains its settings by storing its data @ ~/.var/app/$FLATPAK_ID - uninstalling a Flatpak doesn't remove the folder it makes. But if you delete that remaining folder, the flatpak won't retain its settings/data anymore
Steam with flatpak is overall just a terrible experience, and I hope no one has to go through that pain again.
Ditto and infact I have rarely tried them. But they are godsend in some niche cases when either the .deb package won't work or isn't available (which is quite rare for Debian and Ubuntu). For example VS Code, VLC, OBS, Browser, Postman, peazip, steam, Inkscape etc. all have native packages.
Mhm, appimages are interesting. I think that they are more useful as installers rather than actual apps to use. (Not that I like installers)
It's comments like this that make me feel like I'm missing something. Been using Linux since ~2004, so not as long as you but close. I haven't found flatpaks to be  especially beneficial. I think I have discord and signal installed via flatpak mostly because that's how pop suggested I install from the store.

 I had issues with steam and package manager one worked without issue. 

Don't get me wrong. I like the idea of them and I do think devs will prefer them which means better software for us, but in my day to day use flatpaks just don't have anything drawing me in.

What am I missing?
Nowadays you can use the (parts of the) title of the app instead of it's ID and it will figure it out or ask you on case of ambiguities.
Yeah
> my problem with flatpak is the disgusting names in the cli really is horrible

Its necessary for name collisions though. Just alias stuff that you call from the command line often.

`abbr -a -U nvim "flatpak run io.neovim.nvim`
So one of the first things I've done / do is go in and simlink them from the exports/bin directory and their long names to /usr/local/bin (or even ~/.bin/) and some shortform name.  

Take a look at /var/lib/flatpaks/exports/bin
Flatpak programs are GUI-based, who opens GUI apps through the CLI?
AppImages are convenient for sure, but they come with all the same drawbacks of Windows .exe files. I don't really see why you'd want to use such an insecure format unless you can't get the software through your package manager or flatpak.
AppImages have a major issue in that they will always be in at least one of these states:

- Temporary
- Outdated/Insecure
- Bloated
> I also think AppImages are a very neat solution in certain circumstances, I don‚Äôt have any loaded though.

AppImages don't do anything to make apps portable, unlike Flatpak and Snap.
Have you tried this recently? I just launched discord to try uploading a file. I'm able to select files from pretty much anywhere. The only override i have for flatpaks is allowing access to `~/.themes`

I can't be sure, but I think the file picker may be using portals.
Appimage is not guaranteed to work out of the box on many distro. At least flatpak is reliable than appimage in this regard. Try viva studio appimage in recent Fedora or Arch and see if that works out of the box.
Same. If I need the absolute latest version of an app, I'll look for a flatpak or AppImage, otherwise I'm content using my distro's repo.
Yeah I guess you are right. It‚Äôs weird because consumers agree with me but YouTubers and such think flatpaks are amazing and the future.
Firejail is a bad sandboxing tool and has a long history of sandbox escapes and privilege escalations. It is a SUID binary and is overly complex while being written in an unsafe language.

[https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=firejail](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=firejail)

https://seclists.org/oss-sec/2017/q1/25

Bubblewrap doesn't require being SUID (unless user namespaces are restricted), and is very minimal, which makes it much easier to verify for soundness.
Anything that requires manual intervention is a worse default.
Firejail's sandbox is way worse than flatpak's
Anything that requires manual intervention is a worse default.
This, 100%.
You're not wrong, but flatpak had the great advantage of having update able and sharable runtimes separate from the apps. Docker containers have to be completely rebuilt if something in the lower layers changes or updates.
I use Fedora so it‚Äôs RPM based not DEB
I thought it was a unpopular opinion, i was wrong!
Some reasons Deb/rpm alone is not enough for modern Linux distro
---------
* I don't want to nuke my desktop by installing a 3rd party gui app.
* I want to avoid dependency hell at all cost. Linux distros have improved upon this problem but it still can happen.
* I want to use updated version of an app, but my distro doesn't provide that. I don't want to build from source.
* I want to use different version of same app. It is easier to do with flatpak than deb or rpm. If there is a commit of gimp 2.10.6 or may be even 2.8.xx in flathub repo, I can potentially use that version along with gimp 3.0 beta in the same system. How can I do it with Deb or rpm package? Building from source in newer distro?
* It is easier to test alpha and beta software with flatpak. 
* Flatpak apps work across almost all the distro. So if I change my distro, I don't have to redownload all the apps again.
I think it is fair to say, that from user side there is not so much to gain than from the developer side imo. Flatpak came out of the GNOME development to ease up distribution of GNOME apps and free up distributors working on packages.

Another motivation is that the form an important foundation for immutable distros like Fedora Silverblue.
There's a lot of money pushing it. It'll probably become as standard as Systemd.
Doubting the order of the first two
This is the only correct view on using Flatpaks to achieve "security".
You may be thinking of Snaps. I haven't really heard any complaints regarding Flatpak packages' speed.
Debs are just system packages for Debian based distros.
Better how? If it's a well made Deb that exists and is up to date and compatible with your system, it might work better than a poorly integrated flatpak. But that's not always going to be the case .
For distro-agnostic software, yes.

Otherwise, it depends on the case.
Who does
Nix solved the same problem since 2003.
There are quite a few comments in this thread from people who hold that Flatpaks are the best invention since the wheel. According to them, installing software directly is nearly impossible to do safely and reliably, and any problem with Flatpak is either not a big deal, or it's an outright lie.
What? No?
> Flatpak give developers one platform to target, which works on ALL Linux systems (even immutable root FS distros).

True only for certain definitions of "work". IMO, if I can't call it directly from the command line, and I can't open any file the kernel would allow me to open, then it doesn't "work".  If it's a non-GUI program and I can't seamlessly use it in a shell script, it's broken. If it's a system daemon, I expect to find its configuration files in /etc, where I should be able to edit them if I'm logged in as root (I reject the Ubuntu idiom of configuring `sudo` to allow general root access from my regular user account). The solution isn't to move all my data to ~/Downloads, create helper scripts, or to change my behavior in other ways. I'm not changing my behavior to accommodate Flatpak's design decisions.
On a fundamental level, no - performance of any app in any app format principally relies on how it was compiled
Use an alias to make it a single command. At that point it's less effort to update at least. Otherwise, I'm pretty sure the Gnome Software store will update both (If you're using Gnome).
\*flatpak
Bottles is IIRC.
>If flatpak is the only option I just use different software

Why?
from experience - and, considering the posts ive seen about snap, many other's experience - flatpak generally seems to work and be a lot faster than snap
While I agree that Flatpak isn't a perfect sandbox, I can't agree with the premise of that issue. Flatpak is a sandboxing tool (it provides the functionality, after all), and it's existence as one is leading to better sandboxing in a way that is attainable.

Security is always traded off for convenience. The whole "we should scrap the entire app distribution ecosystem and redo it securely" thing would be so highly inconvenient for users to the point where it cannot happen in Linux. At the moment you can't install and use the software you need to do your job, you couldn't give less of a shit about security; that's just how it is and I've seen it happen time and time again. Computers are tools for most people and if they get in the way it's bad. Even in the state Flatpak is now, where apps can punch bug holes through the sandbox, is causing posts like the one we're commenting under. I think it's clear that a sandboxing platform that prevents the user from running the software they need is not going to succeed

For example: what would happen if apps didn't have permission to allow x11 or PulseAudio into the sandbox? Those apps would not be able to display anything to the (still massive amount) of Linux users that use x11, and they wouldn't be able to output any sound. I ask you what would be blamed in this situation? Flatpak most certainly. The apps not so much; it's not the apps that are broken, but the platform that imposed restrictions after the fact that broke them. In this scenario, nobody will use Flatpak, and instead the apps will continue to be used completely unsandboxed. A strongly-enforced sandbox will end up doing nothing to improve Linux's sandboxing situation. Maybe for some very niche distros it might work, but it will not be mainstream.

Flatpak, I think, took an intelligent approach to this problem. It is secure by default, but allows the decades of legacy software that cannot function in this secure environment to opt out of the security. We're trading off security for the convenience of an existing software library (so, at worst, it's a net neutral for the ecosystem). This allows more apps to run as a Flatpak, which Flatpak can leverage to push for stricter sandboxing. The more apps run as Flatpaks, the more pressure there is from the users of those apps to stop opting out of the security (positive for the ecosystem). The more users are using Flatpak, the more pressure there is for Linux distributions and desktop environments to provide sandbox-safe APIs (portals) so that more apps can function securely without punching holes through the sandbox (bigger positive for the ecosystem). Flatpak *has* to start out as a framework for sandboxing that allows some restrictions to be bypassed, because this allows apps to grow into the sandbox along with the rest of the platform

The fact is that this model is *working*. A couple of years ago, Flatpak apps needed massive holes in the sandbox for basic things to function (like reading settings, talking to your display server, playing audio, etc). Nowadays, portals have improved enough and apps are becoming Flatpak aware enough that many of those glaring sandbox holes have closed up, and will continue to do so. Permissions that were once required for most apps to function are now considered "dangerous" by your app store. Desktop environments are growing the number of portals they provide, and the amount of functionality apps can rely on without punching holes into the sandbox. The platform as a whole is improving its security model (replacing X11 with Wayland, replacing Pulse with Pipewire) and this is allowing even more holes to be closed up. Immutable distros are starting to appear because Flatpak can run most of the apps users are using, and so the distro can get away with not managing packages. Immutable distros will allow secure boot and other such key security measures to creep into our ecosystem. In the coming years, I expect more of Flatpak's permissions to become unused by most apps, so then the distro can treat more of these sandbox escapes as "dangerous" and warn the user about them. I expect that desktop environments will slowly warn the user more and more aggressively about dangerous sandbox escapes, slowly raising the pressure for apps to get in line and make their sandbox more secure. This trend will continue.

My point is it's impossible to flip a switch and make sandboxing happen. I like to compare it to Apple with their Arm chips. What would happen to the M1 computers if Apple didn't include Rosetta in them? Even looking past the issue of "no apps run on the platform", how would apps even _get_ onto the platform? With Rosetta, you can use your existing X86 tooling and binaries to bootstrap yourself into the Arm ecosystem. Without Rosetta, you're cross compiling blind (I know there's ways around this; my point is it's time consuming and inconvenient to do so, and developers might not find it worthwhile). The same applies to Flatpak: if a sandbox can only be completely off or completely on, it is so much harder for apps to justify the massive amount of work necessary to support Flatpak. But if apps can just disable parts of the sandbox to easily get into the ecosystem, they will then succumb to pressure and start enabling more and more sandboxing piecemeal, and overall this will lead to good sandboxing on Linux.
Tbf. this depends strongly on your understanding of the term "sandbox". Sanbox in the sense, it is completely locked down within the system: no. Sandboxed in teh sense it does not mess with your installation: yes.

Also wouldn't be containers, where I can completely lock down ports etc. the better option to provide a "real" sandbox? The goal of Flatpaks is to cause as little overhead as possible and I doubt this won't work with a completely secured sandbox as the app should be able to have access to the system to make it feel as native as possible. I am not really sure that this was the mission goal in the first place. Could b wrong though ...
you are mentionning an issue which status is closed ( "invalid")

and which you opened yourself
I think you mixed up different technologies. X11 makes apps spy on each other, sure. But what does that have to do with flatpak? This is a separate problem and it‚Äôs being dealt with by wayland. And file system permissions are something the maintainers of the flatpak choose not the flatpak project itself. Most flatpaks don‚Äôt have file system permissions, but the ones that do are advertised in GNOME software as can access file system. So everything is advertised properly. And besides, soon enough everything will be using portals and all of this will be in the past.
I don't like that Flatpak apps can grant themselves permissions upon installation. It feels a bit pointless at times to even have permissions. Of course an user can change them after installation and block off everything they'd like, but as Linux is slowly going mainstream thanks to devices like Steam Deck, more new user appear, and they will probably not know about permissions at first. A bad actor can just make an app, grant all permissions and just break out of the sandbox. Wouldn't it make more sense to be more like Android and ask the user for permissions?

I guess all of this doesn't matter as much if you use curated sources like Flathub, but the permission system still feels incomplete to me. I'd be glad if someone could explain to me why this system works like it is right now.

>Flatpak support only makes Flatpak easier to use

Flatpak support makes the Flatpak sandbox function as a sandbox in the first place, not just "easier to use"

>It just adds useless complexity to the app

Being able to remove gaping sandbox holes from an app's sandboxing is far from useless

>you have to do it again to achieve Snap compatibility

As far as I know, Snap is using the same portals as Flatpak is, but don't quote me on that. So no, you just support Flatpak and snap support should come for free(ish).

But that doesn't even matter because Snap is irrelevant. Only one distro is pushing it, and Flatpak runs perfectly well on that distro anyway. So just target Flatpak and be done with it

>And to make things worse, high Flatpak/Snap adoption is a self-reinforcing cycle

That is, quite literally, the point. That is the point of the comment you're replying to as well (that more Flatpak adoption = more pressure for apps to support Flatpak, and why that's a good thing)

>If it happens, then *everybody* has to have Flatpak or Snap installed on their system.

And this is bad because? Imagine the horror of... the same app working on basically any distro. Truly terrible
Pretty much, yes. Although we may even go further than that and make directories like /etc and /var completely dynamic. This concept is called the [stateless system](https://0pointer.de/blog/projects/stateless.html).
Or Ubuntu Core (with snaps)
Isn't Fedora overriding the invert DNS namespace with their own, untrusted, version of apps away from the control of the actual developers?
If I recall correctly, elementary OS also has its own flatpak repo
That will be a thing of the past in 37.
From what I know (which may be wrong) it‚Äôs not their own version it‚Äôs just got any non-free flatpak‚Äôs hidden by default.
Snaps literally got a ton of hate for having a centralised app store
I really love the Fedora project. But when it comes to Flatpaks it seems that it generates a division among developers, some bring arguments that make me doubt if they care about credibility as a developer.

I think it's normal to have other repositories, it's healthy. But not for repeat work.
GNOME has the nightly repo where they have their apps, Elementary has a repo for their apps, Flathub for whoever wants to put their apps and Flathub beta the same. But what does everyone have to do with it? There is no redundancy between them, they only distribute what they propose to distribute.
This business of repackaging and creating a repo just to filter doesn't make any sense. One of the first things I do when installing Fedora is remove the Flatpak repositories from them and add from Flathub.
Silverblue still has a writable /etc, and you can overlay packages on top of the immutable filesystem.
Limiting the user is sysadmin's bread and butter. Ultimately, flatpaks is targeting enterprise that don't use Windows for their workstations, flatpak+silverblue that is. And it does a MAGNIFICENT job at that. An update gone wrong? Use the prior snapshot. A user needs X software? Sure, but in his own sandbox. Need to deploy an image out for X financial profile workstations? You almost made me break a sweat. 

You may or may not be right and have your reasoning, but their endgame is not hobbyist domestic users.
today's root is all about overlays on top of the RO system. remounting /system as RW is basically dead. all hail magisk
The future of Linux is Wine in Bottles ;)
> If you know what you are doing

is the key there. I prefer flatpak over dpkg / apt whenever I have a choice, but the first two things I always do are:

1. flatseal to give the flatpak a ton more permissions (mostly file and system executables)
1. symlink the the flatpak's data directories up the yuzu

From there you can put your data files, custom configs and mods in the places you're actually expecting... most of the time, at least.
FWIW, stardew valley modding specifically doesn't work, because it requires some more dependencies iirc which aren't in the flatpak.  
There's an open issue about it on the steam flatpak repo.
What sort of issues? Maybe someone can help
Huh? I have GTA:SA installed on Flatpak'd Steam. I then downgraded the game back to v1.0 and just modded it.
It's funny. I wouldn't even look at flatpak twice were it not for the sandbox features it offers courtesy of bubblewrap. 

Even though Flatpak is mature enough, some developers have not fully ported their apps to the platform. It can be annoying for the user when the first time they want to access output files from a location they are used to, only to find nothing sometimes or even scanty documentation on this "bug". Basically translates to poor testing/ communication from the flatpak package maintainers.
How are ya installing Flatpaks? I see ratings and comments for them in GNOME Software on Fedora 36.

Also, there should be links on the Flathub site and in the aforementioned software store where you can view the Flatpak app's repo via its submission to Flathub's Github.

Yeah, the AUR is definitely more unsafe than Flatpaks, as Flathub has a team of maintainers (similar to distro package maintainers) which check packages, while the AUR doesn't.
Like u/cangria said, flathub has a team that checks stuff. Also, I know that gnome software has info on every package you install. So does discover. AUR is less safe than flatpaks because it is just random people making packages and posting them. Arch clearly states that they don't maintain AUR and it could be risky. Kind of like "use at your own risk".
VPN doesn't make you private
They are kinda sandboxed if correctly configured and if the sandboxed application does not poke too hard against ist sandbox. (Which Discord probably does not)
Proof? Would be interested in specific articles, etc. I mean, its not unlikely, but still...
Not really seeing anything unusual from their TOS:

https://discord.com/privacy#the-information-we-collect

Is there something specific in there that stands out?
and it tells you how to use system terminal.
I think the benefits of Flatpak apps are usually worth it to prioritize them over system packages:

* faster updates (I can use a stable distribution and still get access to the latest app versions)
* reliable and easy back rolling to previous versions without ABI conflicts
* sandboxing for free which is also easily configurable, like I see no reason why steam should be able to access my private documents, ssh keys, etc. in the first place
* less pollution through system dependencies (I see no reason why I should install half of KDE, including some of its services like baloo, just because I use an app like kdenlive every now and then)
I don't want to install random crypto app on system I want to try and uninstall 5 min later. And to have all the app clutter on /home, /home/.config. And left junk dependency on system.

flatpak makes it much much easier.
Thank you!
I mean when you think about it that is not a downside of flatpak but instead its advantage; you basically give permissions to apps and decide the security measures yourself by the virtue of simple toggles of containerization which is imo better than apps deciding themselves without asking you first when you install it
Yeah. Honestly, I also feel like applying Linux gaming tweaks is even easier with the Bottles Flatpak, because I can just toggle stuff off and on really easily. It's batteries included.

Way easier than when I used to use Lutris as a system package; I had a lot of trouble getting Mangohud and stuff to turn on.

Yup, the Flatpak permissions thing is a minor annoyance that happens on a couple of unofficial apps. It doesn't usually come up and the situation will get better over time; really not a good reason to avoid Flatpaks IMO with all their benefits.
What did you do with flatseal? I just encountered the same issues tonight. I set the file system overrides through bash and that didn‚Äôt work. I eventually found flatseal and tried that, but it already showed the permissions I already set through bash. Ultimately I uninstalled and reinstalled through apt. I read somewhere that if the drive wasn‚Äôt mounted when you first installed the flatpak, then you can never use it, and that is pretty much the experience I had. I hadn‚Äôt set up the fstab rules before installing the flatpak and even after I set them and the file system permissions, steam couldn‚Äôt see those drives.
My bad on that phrasing - I'm referring to my 'I have to reinstall' rate, which was biannual on one computer or another. As for reasoning, it was like half dependency hell, half other stuff.

Dependency hell when I ran some Arch-based systems; I've also lost systems to partitioning issues and a time where all non-flatpak apps stopped working because of a disk space issue. I've also had a Linux system run like a champ for a long time. I've just been using Linux for a couple years though, and I had a greater ratio of problems as I started out.

Anyway, stuff happens. Please don't take it as an opportunity to pass judgment on my technical ability; IMO it's more productive to discuss how Linux can provide paths where one can shoot themselves in the foot less easily (at least, those kinds of discussions are my interest).
This is ridiculous. Of course users, especially ones that are newer to linux run in to these kinds of problems. Especially with the vast amount of terrible advice and guides that are outdated or misinformed. One example might be if a user adds a bad apt repo that starts causing errors with updating. This can be confusing to someone that has never seen it. Or what about following a guide to install games from heroic launcher/lutris and the game doesnt launch. The user checks the logs and it says something about a missing library. Newer users might think they messed something up.

Why do you have to be so aggressive towards this person and their experience?
> The first Flatpak download is always 1+GB despite the fact that, allegedly, my system comes with a base runtime already installed.

Nah, [the way it works](https://theevilskeleton.gitlab.io/2022/05/16/response-to-flatpak-is-not-the-future.html#size) is the first few Flatpak downloads are always big, as it pulls in a lot of popular runtimes. Then things are shared and deduplicate as you download more Flatpaks. This system works best and works well if you use Flatpaks first and foremost so they can deduplicate with each other; my downloads are really small.

It'll definitely be rough for bad internet to download new runtimes if you're pulling a lot of older or unmaintained or out-there apps, but it's fairly similar to pulling a lot of new dependencies (without the system breakage worries).

> I've encountered software that break features, introduce bugs and break systems quite often. I prefer when the software is stable and doesn't get in the way of productivity. Why would I download an entire new system to run on top of my already properly installed and functional system?

Well, Flatpaks on Flathub provide the latest stable release by default, so that works well then! I'm just saying I can personally use the bleeding edge without worry of my desktop breaking down and I find that really cool. I think the default should be the latest stable releases. And either way, Flatpaks won't break or introduce system-level bugs or anything, in contrast to system packages.
Yeah, but then I'd still be supporting a ton of different configurations of Ubuntu and Debian, and facing bug reports from people regarding downstream packaging issues. I'd much rather people run my app in an environment that I know will work for them; I'm worried about the issues downstream patches can introduce.

Well, Bottles is working beautifully with libadwaita git and as for GNOME 43, we'll see! A lot of people on Silverblue were enjoying the GNOME 42 beta a month early.
> it's the job of distro maintainers.

And that's what scares me the most. The number of bug reports that are caused by bad packaging. The package exists and the upstream supporting it are different things.
>As an app developer, that's   
>  
>not really your job  
>  
>, it's the job of distro maintainers. Assuming of course that your app is free and open source, if it's a for-profit thing then I guess you should package it where it's profitable to do so.

Well, the creator of Flatpak, however, brought up a good point here: You still free up distributors who than can focus on the main system. Main motivation of Flatpak was to package Gnome apps so the distributors can focus on the integration of the Gnome base which is already enough work.

Especially if one builds complex apps with a lot of dependencies Flatpak can really help ease up things in dependency hell. Considering distributing apps with over distros with very different life cycles (e.g Ubuntu vs Fedora) containerization is a gods end for devs and distributors.
Distro maintainers should focus more on the base image, core libraries and core apps needed for stable, functional and secured OS. They don't need to waste time packaging hundreds and thousands of 3rd party GUI apps/software.
With that knowledge at hand, you can also clean up orphan/obsolete data from removed apps and runtimes by running:
    
    flatpak remove --unused --delete-data
Oh that‚Äôs good to know! Thank you. Will FLATPAK_ID be something with the package name in it? If not, how do I find out the ID of a specific package?
It's funny you say they are more useful as installers as the whole concept of AppImages is that they aren't installed at all. Download and run, that's it.
lots of stuff you might want to essentially plug in with scripting or configuration isn't actually going to use or consider that shell alias.
Flatpak can run anything. It just provides nice integration for freedesktop standards.
A lot of people do! 

It's way more convenient when you're opening files, especially with zsh or fish. It's typing one character and pressing tab 3 times to open that file rather than opening your file manager, slowly navigating to the file, and then right clicking to use the right program.
> Outdated/Insecure

this can happen with old/ unmaintained  flatpaks 

>Bloated

most of teh time their like at most 50mb bigger than a deb file , at 50mb increase do you really care?
I don't know what "guaranteed" would mean in this case. You're always depending on the work of developers who you have to trust. Sometimes software doesn't work as intended. That is the case with Windows, with Linux, be it the distro of my choice, software packaged for it or other software. AppImage has worked for me so far. The software you mention I don't know about nor have a need for.

AppImages are as easy and uncomplicated to use as in 1) download, 2) make executable and  use. No installation needed. Being just one file, I can manage them easily. There are apps in the distro that are small and easy to install that make their administration (updates, integration into the os, etc.) even more easily and  those apps are completely optional. And you can have different versions of a program running concurrently with different AppImages. It's great for testing beta versions and what not. 

If I understand correctly, flatpaks work with a runtime that you have to install first. That's basically having a platform inside a platform (the distro I already installed). For my use case, that's unneeded complexity.
Most of those youtubers aren't developers or even expert users. The are just users like yourself plus a massive ego and a decent camera. They are the last people you should acquire your opinion from.
Flatpaks are the future.

You're like the people using X11 in 2010 or sysvinit in 2015 - it might not be quite there yet, but it's the future so you should be making plans for how you'll use it in the future instead of complaining on reddit.
Can just edit .desktop files. Snap is a buggy piece of garbage.
The same applies there
1. I don't nuke my desktop by installing a 3rd party GUI app, I haven't ever had that happen.
2. Dependency hell hasn't happened to me in years.
3. PPAs/repos or just downloading the deb/rpm directly means you can get the latest versions. You don't have to exclusively rely on the distro repos.
4. Build from source lol? What is this 1998?
5. I don't test alpha/beta code on my main systems, so different scope.
6. I also don't change my distro on my main systems with any regularity, so that doesn't give me value.

Flatpaks may work for you in your bespoke scenarios, but for the majority of Linux users, it doesn't provide real value. The scenarios you described are generally not scenarios the majority of Desktop Linux would be in or care about.

I'm not saying you're bad for liking Flatpak, you do you. But these examples are exceptions, not the norm. Flatpaks (and snaps) are trying to be the norm, but their value to most scenarios is almost nothing vs deb/rpm.
None of this really has value to me, since the majority of gnome in my distro is already served from my package manager (deb), and any extensions I install go through the gnome extension ecosystem.

Also, RPMs seem relevant to anything Fedora, so... uhh...??
I said compared to native, not snaps.
I consider  flatpak as a way to help us move towards nix style distros. Nix itself is too big of jump to make all at once for folks who maintain the distros as they are now
Flatpak is designed for apps, not shell scripts. In any case, you can do everything you mention by setting the relevant permissions.
Not with snap it seems?
Thanks to this correction I no longer mistype the name, now I simply type it.
Do you think installing 6 flatpaks at the same time can give me abs?
Flathub aur appimage and distro packages

https://docs.usebottles.com/getting-started/installation
Hating
My personal experience is the opposite, strangely.  The first time you start a snap, there's usually a 3-5 second delay, opening it after that has no delay.  I've tried a few flatpaks and ran into bugs, but that's probably because most flatpaks tend not to be maintained by the authors.
Definitely not faster than snap but it works, to be honest I rather just do installs from the terminal or app imagine
> Security at the expense of usability comes at the expense of security.

- https://security.stackexchange.com/a/6116
Counterexample: ChromeOS arguably has the best sandboxing in the market and it is extremely easy for users. Same can be said of modern smartphones (android and probably iOS too).
Nobody that cares about security would want to touch X11 and PulseAudio.

The fact that many distros still use them shows that most distros do not really care about their user's privacy and security.
I could have copy-pasted it, but the result is the same.
Right, but it's a valid issue. Users will see he tag line on the flatpack homepage and think "Flatpacks are magically more secure".

But they aren't in most cases.

It's like buying a 6-inch thick steel door "because it will make you more secure", then hiding the key under the mat.
>And file system permissions are something the maintainers of the flatpak choose not the flatpak project itself.

That is one of the major problems, the app decides its own permissions and can just access your bashrc, SSH keys and whatever other sensitive data in your home folder, it just needs to declare filesystem=home and Flatpak will grant it access to your home folder without user interaction.

That is not a sandbox.
That the plan with portals. Portals allow to that. But that needs the apps to use it in code which needs porting. Portals also have fully cover all needed use cases yet. Once that transition is done, most flatpak permissions will become obsolete or for rare niche cases and then installation could give warning prompts or could be blocked from places like flathub.
Software stores like gnome software display a warning label if dangerous permissions are required.
>I don't like that Flatpak apps can grant themselves permissions upon installation. It feels a bit pointless at times to even have permissions.

This is an advantage of the Snap Store. Developers can choose permissions for the app they are snapping but they have to [get approval](https://snapcraft.io/docs/auto-connection-mechanism) from Snap admins for super privileged permissions or even to enable by default privileged permissions. And because there is only one Snap Store, this security handling is mandatory.

You could distribute an app outside the Snap Store but you'd have to convince people to install it with the --dangerous command line option and it won't automatically be updated.
> Being able to remove gaping sandbox holes from an app's sandboxing is far from useless
> 

If sandboxing is not what the app was designed for, then any problems with it are the responsibility of whoever is attempting to sandbox the app.

>free(ish).

So not free at all, then.

>That is, quite literally, the point. That is the point of the comment you're replying to as well (that more Flatpak adoption = more pressure for apps to support Flatpak, and why that's a good thing)

Flatpak is not good for any of *my* use cases. If nobody adopts Flatpak, that means I can run my system without it.
Lennart was right
How that article doesn't mention [Clear Linux](https://docs.01.org/clearlinux/latest/guides/clear/stateless.html) is beyond me.
Elementary is dead
Not really. It's still:

DNF > Fedora Flatpaks > Flathub's Flatpaks

So whenever they packaged something, it will be preferred over the Flathub version (just like Firefox for example). On 37, they're only gonna add the Flathub repo by default for proprietary software they couldn't package, but they're still maintaining and preferring their flatpaks.
I think you missed the parent's point. Fedora has its own flatpaks, composed of only Fedora packages, and that is not going away. Thus, flatpak/flathub doesn't reduce duplication of effort for Fedora.
[deleted]
I think on Fedora 35 it was their own, because I remember flathub and Fedora flatpak repos showing separately.
In the next release they will use normal Flathub.
Not a chance. For Firefox for example, their version was a couple of day outdated after each Firefox update from the official flathub
Isn't the issue more that you have centralized app store that you can't change, you can't add sources and so on, whereas with flatpak Flathub is the biggest and best, but you can use whatever sources you want?
I hate snaps for their poor performance and generally being a worse version of flatpaks that Canonical are weirdly commited to replacing traditional packages with.
The issue people have with the snap store is the proprietary back end.
Some People must have something to hate so they can live their lives
You mean hate for having a proprietary app store
Yeah, is there another snap store,, anywhere ?
Sure but it gets really messy when you try to change things like the DE.
I think this is the fundamental divide in flatpak - its very good for enterprise support, redhat's bread and butter. But it's much less useful and more annoying for the random hobbyists that use desktop linux. I still find flatpak useful now and again but overall the user experience for linux "power users" is abyssmal, even if the ux is good for people who just want to get their work done on their work machines. (For example, the entire cli experience of flatpak seems like an afterthought - which is the very opposite of how linux usually works)
Could be. I see the current trend is to waste as much resources as possible to do as little as possible. That's why we now have fancy electron based terminal emulators that fit nicely into flatpaks/snaps/docker(?)...

Flatpaks may work nicely for proprietary software, but i don't like the tendency of moving absolutely everything into sandboxed environments with added unneeded overhead.
Get out :)
Check I edited the comment
The point was that GTA isn't part of the flatpak ecosystem. Saying flatpak allows mods isn't technically true since GTA (nor its mods) isn't a flatpak.

Steam on flatpak also isn't truly sandboxed. This is why it can run games outside of itself. Wine isn't steam either.

Flatpak also doesn't control/police what executables other executables call either. It's either allowed filesystem access or not.
Ah, that makes sense. I use the command line to search and install flatpaks. With AUR packages it automatically tells me the vote tallies and shows me the packagebuilds, but flatpak doesn't yet.

They should consider adding that info on the flatpak cli installation! Also I thought flatpak was a free for all? I've seen so many packages there that seem to be broken.
How would it not? It doesn‚Äôt make you anonymous, but at the very least it would obfuscate what IP you‚Äôre connecting from.

Like for example if you traveled from your home to your work, but had a VPN on, then Discord has no knowledge of you changing networks (and thus locations), and only sees you as having disconnected and reconnected.
If people are afraid of discord stealing data why not just run it in your browser? That's going to have better, forced sandboxing while still keeping it integrated with system.
My main concern is that one of the typical responses to fix the lack of theme integration are tutorials to give Flatpaks free reign of the system. Which kind of defeats the purpose.
`flatpak remote-info -m flathub com.discordapp.Discord`

Spits out a bunch of info but the relevant bit is the `[Context]` section

    shared=network;ipc;
    sockets=x11;pulseaudio;
    devices=all;
    filesystems=xdg-download;xdg-pictures:ro;xdg-videos:ro;

The above describes what it has access to within its sandbox.

You can also override them with `flatpak override` say because you don't like it has access to all devices in order to access a webcam or remove its access to any part of your home directory even if the above says it only has read/write to your Downloads folder and read-only access to your Pictures and Videos folders.
[deleted]
I can't ref myself, but with a whitelist FW, discord tries to do a bunch of sketchy nonsense. They try to make a bunch of connections to raw IP addresses on weird ports. There are no domain names associated and approving one or two addresses will then create more attempted connections to unnamed IPs. I quickly said eff this bs with a project that wanted me to join their discord.

Discord is undocumented proprietary hackerware IMO, but I expect all internet businesses to act like brick and mortar or they are sketchy criminals.
Look it up, I‚Äôve heard it before.
This basically says they log any action whatsoever you take on their platform, as much system info as they have access to, and potentially information from other sites or apps if there is some connection to discord. 

That said, I think the idea that flatpak does anything to make discord less invasive is nonsense. 

Also, I am personally more concerned with their strong stance against e2ee and their refusal to delete user data on request or even allow users to mass delete their own messages.
None of those have ever been a problem to me. Flatpaks have their uses, but they suck in their current iteration.
> I don't want to install random crypto app on system

Flatpaks won't stop that from happening. If you're stupid enough to install an app with a crypto miner malware, it's your fault.
Yeah; at the same time though, I'm an advocate for a more low maintenance and casual user-friendly Linux experience, so a good compromise would be a permission request portal that lets people easily give access
Oh totally.. and I half suspect I made it harder than I needed too. Probably could have done some symlink games and had it work better
>better than apps deciding themselves without asking you

This is exactly what flatpaks do though...

You can change them after the fact (very easy with flatseal), but they don't prompt you for their default permissions.

I understand the aim is to introduce some sort of permissions portal system, however.
I just set the file permissions to include the mountpoint that I had already chowned to my user / groups.  And it just worked after that.
I'm down just at least give me the option to still shoot my self in the foot
Except this person is clearly not some new user stumbling as they learn. They said they do this twice a year, as if non-immutable OS's are somehow this exhausting inherently fragile configuration that only last a few months before they fall apart and need to be reinstalled. They are evangelizing Silverblue and Flatpak and are clearly intentionally depicting non-exotic configurations as more fragile than they are, and these new ways of doing things are ushering in some new age of stability.

The Linux desktop has existed for literal decades; if it were as fragile as that, it would not have gotten nearly as popular as it has.

My point is, being excited about new novel technologies is totally fine, but trying to portray the "old way" (i.e. the way that 99% of other distros still do things to this day) as some broken paradigm that everyone just begrudgingly deals with is kind of absurd. As in all things, there are pros and cons to every way of doing things.
Yeah, I know the way it works and to me the cons don't outweigh the pros. They have their use case, but I have no use for them. They should go with corporate sales, they'll gobble up Flatpaks as candy. Hell, even Canonical has managed to sell some Snaps. For the desktop Linux ecosystem, they offer zero advantages to the average user. At least not without a deep technical rewrite.

Flatpaks could be fixed to work really well with little to no downside, but that would require rewriting 95% of Flatpaks on flathub.
For support you'd be looking at 4 Ubuntu and 2 Debian versions if you wanted to ensure complete support instead of just latest Stable/LTS.

And the rates of change there aren't dramatic.

If you're packaging old versions of libraries with your application to minimize breakage from updates, you're also packaging security holes.

Better to just build static links in that case.
That's why you don't waste time with gnome base and just go kde and plasma
If there's a need for a piece of software in their software distribution, adding that is very much part of the job description. There are also different kinds of distro maintainers, not everyone is qualified to be working on security stuff.

"*They* should focus on X instead of Y" is almost never a reasonable thing in Linux. It's not a corporation which can micromanage it's resources, focuses are more often than not determined via consensus on mailing lists.

Also, packaging isn't exactly rocket science unless it's something like Chrome. [This](https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=element-desktop-nightly-bin) is what a full package looks like on the AUR. If it's packaged it's packaged.
True!
No problem and yea, it'll be something like the package name! For example, [Bottles](https://usebottles.com/) is com.usebottles.bottles

A lot of GNOME apps are like org.gnome.somenamehere

App IDs are searchable via the 'Manifest' part of app pages on the [Flathub site](https://beta.flathub.org/), as well!
They are basically just exe files. I‚Äôve seen an installer that is an appimage.
Fair enough, but the scripting is basically the same as an alias in this case. You type your script once and you are done with it.
That's true. You can run python, or any CLI program in flatpak, but isn't it mainly focused on GUI apps? At least that's what the official website seems to say
If you're using tab for autocomplete, then flatpak names shouldn't be an issue, right?
Those may or may not happen with flatpaks, but there will always be one of the above issues with every AppImage.

They are far bigger because if they do not package the necessary dependencies, they won't work on many devices. The fewer dependencies packaged, the shorter the AppImage is expected to stay compatible.
I've recently made an app for Linux and flatpak made things really easy in terms of distribution and updates. I don't think there's any cons for developers besides figuring out permissions for the sandbox (not really an issue for most apps)
Flatpaks are the future the same way that next year is the year of Linux Desktop.

> You're like the people using X11 in 2010

Go tell that to NVIDIA.
What sort  of user in 2010 would need to get ready for wayland lol? Like maybe a user right now in 2022 should consider switching, but any user in 2010 could comfortably user x11 for another decade. 

When it comes to things like flatpak and wayland and sysvinit, users and distros will switch when it becomes overwhelmingly better and more convenient. That hit the point with sysvinit a while ago. Wayland is getting there but people still face some issues. And flatpak is somewhat struggling because many users find it inconvenient.

People don't need to and should not adapt to software, engineering and design are not mantras that need to be prostelytized. Software should work for people, both the developers and the users. I think wayland is the future but there is no need to yell at users to prepare for the death of x11 - if wayland is better software they'll use it once it becomes convenient. I'm not sure about flatpak, we'll see. I have my own complaints but we'll see if the engineering principles stand firm.
I am not making any determination on snap. Never used it's never plan to use it.

(I do use and prefer flatpak though).

Just suggesting that whilst firejail may be brilliant (I havent used it), if everyone needs to manually set it up, it's not a good default choice.

Now if there was a distribution that did it by default (and did it well) that would be a different thing.

Manually editing desktop files is also not a good default. It requires a certain amount of knowledge and research to do it well.
Ik
1. You may not have done that. But I faced that in Arch years ago. Recently, a big youtuber faced similar things while installing steam. That too from distro's official repo. Not a good impression if you want more people to run desktop Linux.

4. What if I am using older version of distro and ppa doesn't cover that? Shoud I change the distro? Main author also don't provide Deb or rpm but snap or just source. PPA may also cause headache during major version upgrade. It depends on which package you are using your PPA for.

4. And yet, Arch users building packages through AUR. AUR is fancier way to build package for Arch Linux.

 I also had to build a package in Fedora few months ago. Qpwgraph (an patchbay for pipewire). It didn't provide any Fedora or Ubuntu package. I had to use it  to route  my audio. Thankfully, the dev published a flatpak version later. There is another similar tool called Pulsemeeter (kinda like voicemeteer banana). It only provided AUR recipe. No Deb and rpm at all. 1998 or 2022, you still may have to build from source. You may tell me use jack or pulseaudio related tools. But I have better out of the experience with pipewire tools.
>Also, RPMs seem relevant to anything Fedora, so... uhh...??

[Here a read what the Fedora devs say](https://blogs.gnome.org/uraeus/2021/09/24/fedora-workstation-our-vision-for-linux-desktop/)
Which is why I said you may be thinking of the wrong one. I've heard many complaints about Snaps' speed but none about Flatpak's. But if you say Flatpaks are slower that's obviously possible, though not enough of a difference for most people to care much.
It applies to snaps' performance when they actually load .. but definitely not to the boot times. The latter is a whole other mess haha
Thanks to this comment I am no longer displeased by their mistype, now I am simply pleased.
AppImage supposedly doesn't work, and the Flatpak is the most supported version.
The devs have stated that they only want user to use the flatpak version and that they're considering dropping their AUR build.

[https://usebottles.com/blog/an-open-letter](https://usebottles.com/blog/an-open-letter)
>How does flatpak do a better job?

flatpak has a few advantages.

* Higher security due to sandboxing
* No dependency hell
* Flexibility with the security model should you need to tailor it
* A centralised repository to find apps

flatpak and snap have the same aim. Where snap falls short in comparison:

* snap doesn't allow you to tailor the security model
* snap takes a long time to start the first time after booting your machine (not thereafter), although Canonical is actively working on fixing this problem

There are some instances where a standard repository installation is preferred, but by and large, flatpak and, depending on your needs, snap both work well.

What are the disadvantages? On a badly low-spec computer, snap and flatpak might be a bit too resource-intensive.

>The first time you start a snap, there's usually a 3-5 second delay

When Ubuntu 22.04 came out, the initial load time was stupidly long. Canonical has been actively working on it, so it's sort-of reasonable now, and we can expect it to improve further.
It just sums it up.
I wouldn't say those are counterexamples. Those platforms started with their sandboxing & app distribution models, Linux is trying to introduce one after the fact. It's a massive difference. Those platforms did not have decades of app development that users would expect to be able to continue making use of. Apps for those operating systems would have to be created from scratch anyways
Contrapoint;

Apps frequently violate the sandboxing, or pester the users to violate the sandboxing. If users are conditioned to accept that every single app is going to ask for some permission for something, the act of asking for permissions no longer jumps out at them, and they're less sensitive to the information conveyed.

This is why UAC on windows is entirely and utterly useless; it acts more as an inconvenience to the user, than a security feature.
You're missing my point. Legacy software depends on these technologies to function at all. People don't care about security if it severely gets in the way of work getting done.

And anyway, distros are trying their best to move away from X11 and Pulse. Pipewire exists and more distros are using it as a default, and Wayland is moving forward steadily. All this takes time
> The fact that many distros still use them shows that most distros do not really care about their user's privacy and security.

Which distros? I'm struggling to think of a major distro that doesn't offer Wayland.
That's a little hyperbolic and you know it. There's "caring about security", and then there's "sacrificing all usability for security", and the latter is what you get if you stop shipping Xorg and only support Wayland. Plasma has [a list of Wayland showstoppers,](https://community.kde.org/Plasma/Wayland_Showstoppers) and until this list is empty or gone, and until GNOME can say they also no longer have any showstoppers (don't know where they keep theirs at), we can't just drop Xorg.
>Right, but it's a valid issue. Users will see he tag line on the flatpack homepage and think "Flatpacks are magically more secure".

Could you point me to where on the flatpak homepage you see an issue? I can't find anything about sandboxing.
>It's like buying a 6-inch thick steel door "because it will make you more secure", then hiding the key under the mat.

More like leaving the key in plain sight and telling visitors/apps to not touch it, the moment you turn your eyes away, you're screwed.
The app doesn't. The maintainer does. And if your read the rest of what I said, you would know that GNOME software warns you about that. So you're main issue about "false advertising" isn't there anymore as the software center takes care of it.
Portals are optional, the app can still declare filesystem=home in its manifest and get unfettered access to your files.
Flatpak itself can not make any assumptions which frontend the user uses to install their apps.

As I highlighted in the issue, Discover and possibly more GUI frontends do not display the permissions.

The idea of implicitly granting permissions by installing a supposedly sandboxed app is an ass-backwards way to implement a sandbox.
I dont think I like that approach at all, reminds me too much of Apple where App developers are at mercy of the store admins to get rejected for reasons that have nothing to do with security.
Thanks for linking this. I had no idea clear linux did that, very interesting.
[doesn't look very dead](https://blog.elementary.io/updates-for-june-2022/)
commenting from elementary os. Best looking and best user friendly distribution
37 as in the next Fedora release, the current one is Fedora Linux 36.
It got rejected by FESCO right ?
yeah. I was trying to reply to u/pokiman_lover 's comment saying that Linux needs a centralised app store.
The intended way to do that is to use a different base, such as Kinoite. Package overlaying is only meant to be used for small modifications.
"Immutable" here doesn't mean you can't change the whole, it just means you can't modify it willy-nilly.

Installing a new DE means swapping out the system as a whole with a new system wehre the DE is installed. (In case of Silverblue, using a reboot). This system is also not mutable at runtime.
hobbyist linux user uses window manager. Ofcourse they can fix the permission. all you have to do add location in flatseal.
I think I like that trend towards sandboxing, imho it is long overdue to finally retire the MS Dos security model.
Not sure if it helps but there are ways to run the terminal of a specific sandbox, so for example you can run up a shell from the sandbox of steam, which I‚Äôm pretty sure will allow you to do anything you want to. 

``` flatpak run --command=sh <com.Whatever.flatpakid> ```


I‚Äôm on mobile so forgive me if my formatting is terrible
What you mean by 'GTA isn't part of the Flatpak ecosystem'? Even if you run Windows games through Flatpak'd Steam, they *are* being sandboxed. Flatpak'd Steam only has access to some XDG folders of the host system, but that's pretty much it, every other host folder is invisible to Steam and its games.

>Steam on flatpak also isn't truly sandboxed. **This is why it can run games outside of itself**.

This is totally incorrect, please don't spread misinformation. Subprocesses *CANNOT* bypass the Flatpak sandbox. If they do, that's a security bug.

>Flatpak also doesn't control/police what executables other executables call either

Of course it does. Like I said, the sandbox policy is also applied to subprocesses. If you happen to run a Windows ransomware through Flatpak'd Steam, the virus will only be able to encrypt stuff in `~/.var/app/com.valvesoftware.Steam` and,  unfortunately, a couple of other XDG folders (unless you manually remove their permission), but any other file on your system will be completely safe.
Actually, flatpak *does* limit what executables are called, or even seen from the host os. For example, if I install neovim from flathub, it cannot see my host system's golang installation.

As for GTA and wine/proton/whatever other sandbox tools for games, you absolutely can completely limit steam to no host folders at all, and instead force it to create, move, delete, etc. anything it wants in ~/.var/app/com.valvesoftware.Steam

This is actually how I have Steam setup, with the help of flatseal of course.
Ah yea, makes sense! It'd definitely be cool if the CLI had that feature.

Not a free for all because of the moderation, but mispackaging can definitely happen and should be reported on the app's Flathub repo
That‚Äôs what vpn ad companies want you to think. vpns proxy your traffic to a server that‚Äôs located somewhere else. It doesn‚Äôt make you somehow anonymous because all websites will still know it‚Äôs you and all your activity will be the same except for the ip. If you want actual anonymity, use the tor browser.

Another thing that vpn ad companies try to convince you is that it prevents your isp from spying on you. They always leave out that most of internet traffic is encrypted anyways so they can‚Äôt get meaningful data from you. As for the websites you visit, you‚Äôll be handing the responsibility of that data from your isp to the vpn. That‚Äôs it. It‚Äôs still there and they definitely store it because there are countless stories of authorities forcing vpn companies to hand over data about their customers.
VPN is a very big step towards anonymity, especially if you use a double VPN. Of course no level of obfuscation will hide your identity when using a service that you already gave all of your information which is attached to your user account, like discord.
Last time I used flatpak, I used [flatseal](https://github.com/tchx84/Flatseal) to set the permissions.

It has a nice GUI for setting permissions.
I mean that sounds sketchy but it's probably for their game detection stuff, so it can show when you're running a game.
Wow okay... Didn't knew that. Thank you for the heads up
> That said, I think the idea that flatpak does anything to make discord less invasive is nonsense.

Well, if setup properly, it can prevent discord from reading other running processes. This breaks "rich presence" but I don't understand why anyone would want this privacy nightmare in the first place. To think that some people actively develop plugins for it is mind boggling.

You don't need flatpak for this though since the same can be achieved with other tools (like apparmor and even os config).
> This basically says they log any action whatsoever you take on their platform, as much system info as they have access to, and potentially information from other sites or apps if there is some connection to discord.

And they can't do that if you run it as a flatpak or something?
Okay thanks. I tried that, but it didn‚Äôt work. I guess because I hadn‚Äôt mounted the drive before I installed the flatpak. Someone else just told me how to clear out the stored settings for a flatpak though, so I guess next time I can try again. For now though I‚Äôll just keep using tbe apt package since I already installed it and got it working.
Yeah ofc, FOSS means those kinds of options and distros will always be available! I just want better defaults for people who are like me.
Well, I already gave some examples that are pretty convenient to me, like avoiding breakages. Though re: the Flatpak technical rewrite you were talking about; the project could use more suggestions and contributions!

Definitely encourage you to think more about what the future of the Linux desktop could look like in general; that's what stuff like Flatpak represents to me - finding new software stacks that could work well for Linux and striving to adapt it to fit everyone's needs, it's really cool imo! I just love seeing growth and changes in Linux space
It makes no sense. Flatpak is freedesktop and KDE has started packaging its apps in Flatpak
And if that isn't enough, it wouldn't be difficult to create a script that runs the flatpak and place that script in the path.
It's actually basically the opposite of the same. You need a script per flatpak and you may need to deal with another level of quoting in complex cases.
No, because I can never remember the first letter of the org name!
NVIDIA is actually an excellent example of the point I'm making. Becuase they dragged their feet on Wayland and KMS, they are now way behind and are scrambling to catch up to the rest of the desktop and their driver is rather buggy currently.
X11 by now is so bad that I regularly fix functionality that was broken a few years ago with nobody bothering to fix it.

People who haven't switched by now are laggards who have a significantly worse experience than others - usually without noticing, because they don't know what they are missing.

The same will be true with flatpak.
There is no universally "good" default. Developers always have to struggle with the fundamental problem of functionality versus usability, and that always facilitates a product that is "good enough" for the "average" user, and an incentive to configure for the more tech-savvy.

There's also the fact that technical implementations of certain tools, even if they marginally improve security by default, are only "good" if the standards of the developers who implement them align perfectly with your own. If they don't, you've gotta manually configure anyway.

This is Linux, if you want easy by default the full 360, you're in the wrong corner of the net. Sure, distros need a baseline level of usability, but users also need a baseline level of comfort with reading documentation and following instructions, which is really all you need to configure anything. I've used Firejail for quite some time, and installation is fairly straightfoward. Using it correctly can require some reading to get the most out of what you want in certain situations, but such is life. Capability comes from depth, not simplicity. Not everything can be braindead.
> Just suggesting that whilst firejail may be brilliant (I havent used it), if everyone needs to manually set it up, it's not a good default choice.
> 
> Now if there was a distribution that did it by default (and did it well) that would be a different thing.

I use OpenSUSE Tumbleweed and in my case setting it up was writing "sudo zypper in firejail" into my terminal.
Or if you want to argue that the terminal isn't good enough you can install it from YaST software management.

Of course you need to edit it's profiles if you are using something uncommon* but it works well for the programs it knows of.

* For example it by-default doesn't allow Steam access to ~/.local *which is a good thing from a security standpoint* but it also causes problems with programs which use ~/.local/share to store their savefiles like Xenonauts
Nothing beats native in speed, it is not a point needs to be proven,
If anybody really cares about privacy or data security, then first of all ,they should leave their phones .
Thanks to this reply I no longer understand their displeasure, now I just simply stand.
I don't think flatpaks have any significant overhead apart from higher RAM consumption.
Snap provides an option for sandboxing, which imo is more versatile.
Also people use chromeOS and smartphones in a very different way than they use laptops and desktops. ChromeOS runs mostly browser-based apps and smartphones generally only have a few programs the user is directly interacting with. And even with their usability restrictions, things like copying and pasting images or sharing and managing files between various apps is still really a nightmare on android, ime.

Can you imagine if managing the file structure on desktop linux was as annoying as managing it on android? Nobody would use it anymore! If I wanted android I'd use android! Laptops and smartphones are both useful tools but they have very different use cases and those use cases shape the security and design decisions!
Of course it is a massive difference.That is the point. Flatpak is trying to enforce sandboxing on the app level and that is a mistake. Better than nothing but not by much. Proper sandboxing should be enforced on the OS level. When done properly, it should not impact usability significantly or even at all. Of course those are different platforms than Linux in many ways, but I just mean to use them to show that real sandboxing does not hurt usability like the comment I replied to suggested. I would argue that we can learn from them to build towards a better security model without losing the spirit of Linux.
A malicious program, both on Windows and Linux, can ruin someones day (or in some cases life) just with normal user permissions. Root / Admin permissions are just the cherry on top.
Apps rarely read the app data of other apps and don't have any way on android to request that directly. They do frequently bypass file access sandboxing but this is due to a limitation in android itself that has an obvious fix. (Recently android introduced media only file perms and very recently GrapheneOS introduced scoped storage where the user can (and is encouraged to) specify exactly which folders and files each app should have access to.) These operating systems also restrict user installed apps from accessing root in any way, in stark contrast to Linux. In Linux, you definitely want the user to be able to allow that, but you should not make it super easy to do, and certainly not suggested, as it is very often a bad idea. 

You'll never completely stop the user from allowing some invasive permissions, and you probably shouldn't, but I think we can take lessons from these operating systems and do better in Linux. Especially nowadays as Linux desktop grows in popularity, I suspect we will need to confront vulnerabilities in the desktop security model a lot more than before.
Mint, Pop!\_OS, various spins of distros that come with DEs that have no Wayland session at all.
Both Android and ChromiumOS don't trade usability for security.

&#x200B;

>we can't just drop Xorg.

https://www.phoronix.com/scan.php?page=news\_item&px=X.Org-Maintenance-Mode-Quickly

>Once we are done with \[their Wayland improvements\] we expect X.org to go into hard maintenance mode fairly quickly. The reality is that X.org is basically maintained by us and thus once we stop paying attention to it there is unlikely to be any major new releases coming out and there might even be some bitrot setting in over time. We will keep an eye on it as we will want to ensure X.org stays supportable until the end of the RHEL8 lifecycle at a minimum, but let this be a friendly notice for everyone who rely the work we do maintaining the Linux graphics stack, get onto Wayland, that is where the future is.
I vaguely remember them mentioning sandboxing in the past. Maybe they have cleaned up their act a bit.

But [they still talk about sandboxing](https://docs.flatpak.org/en/latest/basic-concepts.html#sandboxes), and they even state "By default, the application can only access the contents of its sandbox. Access to user files, network, graphics sockets, subsystems on the bus and devices have to be explicitly granted."

This sounds a lot like the "app store" model, where a user gets a chance to reject an application's request for location. But it doesn't mean that at all: It means "granted **(in the packaging)**"  -- which the user may not notice, nor even understand. (Like in the example of X11 permission.)
>Flatpak is a system for building, distributing, and running sandboxed desktop applications on Linux.

https://github.com/flatpak/flatpak/blob/main/README.md
Read the third paragraph in the issue I posted.

Literally every other mainstream OS handles this much better than Flatpak right now.

All of the OSes I listed require **explicit** user consent before granting access to sensitive permissions for their sandboxed apps.

The app developer should not be the one with the final say, the user should be. That's the whole point of an application sandbox.
What version of Discover did you test? [It shows the permissions since 5.25](https://pointieststick.com/2022/05/06/this-week-in-kde-new-features-and-many-bugfixes-for-plasma-5-25/).

It‚Äôs a fair point but ultimately I don‚Äôt think there‚Äôs a good alternative strategy to avoid this. Flatpak cannot centrally control what permissions apps have, and the CLI already informs of permissions apps declare. Flatpak also cannot control what GUI app stores do or not do.

The long term plan is to move everything to things like Wayland which is a sensible permission to give by default, unlike X11.

One possible improvement could be to include some helper code in libflatpak so app stores can get a permissions report without duplicating "permission judging" logic, but realistically gnome, kde and others will want to present information their own way, and judge apps differently.
It's not like Apple. The decision is made in the open. Here's a [simple example](https://forum.snapcraft.io/t/auto-connection-request-camera-for-cheese/30158) for a Snap I created.
They had a conflict in the team and development became even slower than before.
Yes, it is very friendly when you need FULLY reinstall to update)
Oh. Looks like delayed until F38.
Centralized app store and centralized repos are different things though.
And what if there is no base with the software I want?
Yes, I know. However the point of immutable systems is that they are not changed. If I am gonna change it anyway, what was the point in the first place?  
Before anybody says you can easily go back to a working system if it breaks: you can do that with snapshots as well.

On classical distros, I install the DE and then select it in the login screen. Which is kinda easy to do. But not willy-nilly as it needs root priviledges.

On immutable distros, I don't even know. Either mount as read/write and then change stuff or use rpm-ostree to put packages on top of packages. Honestly after half an hour of trying to install XFCE on silverblue I just gave up as there is 0 documentation on how to do that and the internet wasn't much help either. And the idea of having to redo it whenever there is an update wasn't great either.
Sure you can get flatpak to work fine, it's just about the UX of it. Cli is not inherently easier or harder than doing things by gui, it's just a different paradigm. And in that paradigm you can have good or bad UX, just like you can have good or bad UX with a gui
thanks for trying to help but it doesnt work like that. the mods worked well enough but it not opening the terminal led to more work than what was needed, so I decided to just switch to the package version of steam, literally fixed everything.
If you dont run X11 I guess. But here we talk about really sophisticated malware that hopefully does not exist yet or if it does, is highly targeted and not used against random gamers.
Only if it's configured that way and many of the closed source applications "need" host access.
Did you actually *read* my comment? I already addressed almost everything you just said, and I even explicitly said ‚ÄúIt doesn‚Äôt make you anonymous‚Äù in like the second sentence. 

You still fail to answer my question as to how it‚Äôs not at the very least better to not let the service see what IP you are connecting from, as then they won‚Äôt know when you‚Äôve changed networks (and this locations).

Then again, you probably won‚Äôt read any of this comment before responding anyway‚Ä¶
[deleted]
I like rich presence because I like showing off the music I'm listening to or the game I'm playing. It's just a nice little feature, that's why
The user has little control over how the flatpak sandbox works. Regardless, I agree with the broader perspective that it is a losing battle with an app like discord. The majority of the concerns lie within the app context itself.
No the flatpak sandbox does very little if anything about this.
> That said, I think the idea that flatpak does anything to make discord less invasive is nonsense. 

?
Bubblewrap already split from Flatpak to be their own thing. Like I said, a containerized app distribution scheme has uses, but Flatpak is the worse implementation of it, second only to Snaps.
That is exactly how I run flatpaked neovim. I have a script that parses and separates flags/arguments that should go to flatpak, and flags/arguments that go to neovim. So I can run `nv --share=network -- ~/.config/fish/functions/neovim-flatpak.fish` and that gets launched as: `flatpak run --share=network io.neovim.nvim ~/.config/fish/functions/neovim-flatpak.fish`. Works great.
Maybe at that point you need something else than a flatpak then! What have you been building that this has come up?
> ~~currently~~

Always. FTFY. Nvidia has always considered Linux a second class citizen not worth of time or attention. They're not behind, they're exactly where they want which is they don't care at all.
What am I genuinely missing by staying on xorg? I tried switching for a bit but I use wm_class with xdotool to mange pwas on i3 and last time I tried sway I couldn't find a good way of doing it. That's not really a deal breaker, but it means that I cannot simply just copy and paste my i3 setup into sway. I assume that functionality will come to wlroots/sway at some point, if it already hasn't - but for now i3 on xorg seems to work fine for me?

I understand it if you have some fancy multi refresh rate setup, but what do I actually gain from using a wayland compositor vs an x11 DE that would justify the migration cost and having to wait for some new features to fill some niche xorg use cases? Sure, when I build a new computer in 5 years or so moving to wayland would make sense, but why should I migrate now? It also seems completely unfair to call the people haven't switched laggards. Iirc only around ~20% of linux desktop users use wayland right now!
My issue wasnt that firejail isnt installed by default.

I may be talking from ignorance here, but my understanding is that once installed, you cant just click an application from the overview and it will run in a firejail: You either need to invoke firejail when running the app from the command line or edit the .desktop files to mention firejail.

Firejail would be a good option to the end user if that latter configuration was either not needed or done by default.

That doesnt mean it isnt useful or powerful.

(And since I am not a user of it, I may be lacking information and it may entirely be possible to use it as the default action of launching a program... if so that would be very useful)
That's a fair point regarding the speed and privacy/security. But when it comes to just dropping your phone, how practical is that really for most people?
Flatpaks are native. What you're saying is that the sandboxing may have some overhead (that for sure is negligible). Apart from that, they may have a higher ram consumption due to not using the shared system libraries, but their own ones, so there might be higher RAM consumption, but that's it.
Nani stando??? Thanks to this comment, my life is no longer a jojo reference, it is simply a jojo ference.
The overheads of snap and flatpak include extra disk space, RAM and CPU. On a pretty old computer or newer, this is insignificant. On a badly low-spec computer (we're talking seriously old and under-specced), it might be significant.

For normal use, I would agree with you.
Could you clarify, please? Unless the dev provides the "classic" option, I've been told that you're stuck with whatever restrictions the dev has chosen.
Why not the best of both worlds. The strong security model from Android, with some tweaks for Desktop use, and a File manager as part of the OS that is not sandboxed and has access to all files. Or a third party file manager that you can give full file system permissions, like you can on rooted Android devices.

>Better than nothing but not by much. Proper sandboxing should be enforced on the OS level

What do you mean by this? What is the "OS" level, and how would it be enforcing sandboxing? For that matter, what do you mean by flatpak enforcing sandboxing at an "app level"?

>When done properly, it should not impact usability significantly or even at all

How would that work? How does an app that isn't aware of the fact that it can't just start reading from the webcam and instead needs to ask for permission first continue working properly? Same goes for everything else we're limiting (filesystem access, dbus access, display servers, audio servers, etc etc)

>real sandboxing does not hurt usability like the comment I replied to suggested.

You missed the point of my comment.

Complete, airtight sandboxing doesn't hurt usability *only when the software running inside of it is aware that it is running in a sandbox*. Otherwise it *will* hurt usability. In your counterexamples, *all* software was sandbox-aware from the start, but in Linux that isn't the case and that is why our sandbox needs the ability to be a bit looser for software that isn't aware it's being sandboxed

Flatpak's model is absolutely essential because in the beginning, 0 software was aware of the sandbox and all of it wouldn't work properly. Over time, a good chunk of software is becoming aware of the sandbox and a lot of the holes have been closed up. And this will continue to improve. If we tried to just turn on full sandboxing all the way immediately, no apps would work, so no users would use it, and thus we'd end up with no sandboxing on Linux at all.

>I would argue that we can learn from them to build towards a better security model without losing the spirit of Linux.

That is, in my eyes, *exactly* what Flatpak is doing. And the key words are "build towards". Flatpak's end goal is air tight security, but to get there we need to do what we're doing now.
A sandbox is usually pretty useless against an intentionally malicious app. What it is good for is apps that must interact with potentially malicious code, like a web browser.
>Apps rarely read the app data of other apps and don't have any way on android to request that directly. They do frequently bypass file access sandboxing but this is due to a limitation in android itself that has an obvious fix.

"an obvious fix" is no fix at all until widely implemented.

>These operating systems also restrict user installed apps from accessing root in any way, in stark contrast to Linux.

Root privilege escalation is not unheard of on Android, and root level escalation is not even necessary for a lot of the major damage that can be rendered unto a target.

>In Linux, you definitely want the user to be able to allow that, but you should not make it super easy to do, and certainly not suggested, as it is very often a bad idea.

Cool. Go install Linux without using root and get back to me. "certainly not suggested" is the worst statement I've ever heard. You should never blindly suggest anything. The biggest vulnerability to any system is a naive and/or ignorant user.

>You'll never completely stop the user from allowing some invasive permissions, and you probably shouldn't, but I think we can take lessons from these operating systems and do better in Linux.

I don't disagree with the idea that we should learn lessons from these operating systems, but I certainly disagree with the lesson that was suggested to be taken away. That is, that sandbox apps that must constantly beg for permissions is a good and proper approach to security. It results in users who do not pay attention to the permissions that their apps use or ask for.
Android and ChromiumOS were built from the ground up to do that; there were no prior applications to support because these platforms were entirely new. Linux is not nearly so lucky. To some people, Wayland has been too slow to adopt. I actually *sort of* agree; Wayland has been "on the horizon" for at least a decade; but the folks designing Wayland's protocols sort of blew their own feet off in being restrictive in their design--again, for *security's sake*--and forced a lot of people to replicate a lot of work to get back functionality that was not accounted for or is outright hindered.

Putting Xorg into maintenance mode doesn't mean dropping it. Distros literally cannot afford to do it. No matter how much we want otherwise, it will be years before proprietary applications get on the ball and support Wayland, if *ever*. The options are to let people use proprietary applications that they need for their work on a free and comfortable desktop with an outdated and less secure graphics stack, or give them the middle finger and force them onto Windows. The lesser evil here is obvious. Harm reduction is important. Too many people in FOSS want silver bullets and perfect solutions. It is **actually good** to settle for doing the best you can in the absence of perfect solutions, and it is **actually bad** to do nothing to spite the fact that a perfect solution doesn't exist.
Oh, you're talking about xdg portals not flatpak. This makes it so that apps (not just flatpaks) have a secure way of interacting with your system, but it's up to the app developer to utilize them.
It's a flawed approach to push responsibility of reviewing permissions to the user.

People could ignore it, or forget about it. It's not fail-safe to implicitly grant sensitive permissions, at all.

As a user, I should have the confidence that an app cannot access sensitive files unless I explicitly allowed it. But that's not what Flatpak does.

When I install an app, it could e.g. (an IDE) say "I need access to your work folder" the app doesn't know where the work folder is, so the user is presented with a dialog where they choose which folder it is allowed to access.

The current approach of

App: "Hey, I want access to the entirety of the user's home folder."

Flatpak: "Ok."

means the user is always one slipup away from accidentally installing a malicious app and giving it access to e.g. your SSH keys.

Flatpak's permission system puts full trust into the app, making security optional.
Source?
well. haven't update my laptop for 2 years.
nope
No they aren't.
How? Lol
Immutable OSs are aimed at standard configurations. If you want complete control over what system level software gets installed, use a regular distro.

Also, note that installing WMs on Silverblue/Kinoite tends to work well, as you can reuse the software from GNOME/KDE.
>  Before anybody says you can easily go back to a working system if it breaks: you can do that with snapshots as well.

You can actually implement an immutable system using filesystem snapshots; SuSe did something like that IIRC.

> On classical distros, I install the DE and then select it in the login screen. Which is kinda easy to do. But not willy-nilly as it needs root priviledges.

I assumed root privileges.

The difference is that you can't modify the *running system* in part. You can only decide what the next running system will be, as a whole.

> Honestly after half an hour of trying to install XFCE on silverblue I just gave up as there is 0 documentation on how to do that and the internet wasn't much help either. And the idea of having to redo it whenever there is an update wasn't great either.

Give NixOS a try. It doesn't have these issues.
Oh sorry, I misread your post, but you‚Äôre right on that regard. If you‚Äôre changing locations, vpns do make it so that services don‚Äôt know that.
Do you have any details on this workaround that was used, just asking out of my own personal curiosity?
Most of the concerns are within the app context itself, not the app reading app data of other apps.
I dont really remember exactly when it changed, but there was a time around 10 years ago iirc that the Nvidia driver was way better than Amd, and games often had issues on Amd or required Nvidia to run at all.
For me the reason I would like to switch to Wayland is that right now, I must disable my Laptop screen if I want adaptive sync on my mainscreen. The second reason X11 is right now one of the biggest roadblock to a secure sandbox that actually defends against malicious software.
You're missing bugfree and performant code. Sure, stuff is gonna work somehow in some fallback x11 config on most apps, but no developer is gonna spend much time fixing things for those old setups. It's annoying because X11 and it doesn't affect the majority of users.

You won't notice the performance issues because your stuff isn't getting that much slower, you'll just never notice how much faster Wayland is. And the bugs will be annoying, but software is always buggy, right? And yeah, the glitches can be annoying, but it's been like that forever and you learned to deal with it.

Edit: [And right here is a post](/r/linux/comments/w15h56/is_x11_slower_than_windows/) showing that very same thing.
> I may be talking from ignorance here, but my understanding is that once installed, you cant just click an application from the overview and it will run in a firejail: You either need to invoke firejail when running the app from the command line or edit the .desktop files to mention firejail.

I was talking about clicking an Icon and it running in firejail.

It just worked out of the box when I installed it.

I was actually surprised when it happened since I was only planning on using it to sandbox specific applications not all of them.
They can't, that's what I am saying.
men you are lost in an infinite loop. Certainly this bug is flatpak fault
The versatility is on the dev side of things, for obvious reasons.
Flatpaks rely on apps to define conditions for their sandbox. These should ideally be enforced by the operating system and under the control of the user. Android is a good example. 

I do see the naunce you are conveying regarding apps being aware of their sandboxing. I also think flatpak is a big step forward from traditional package managers, despite its shortcomings. Sadly, the way it is right now, calling it a sandbox can be a bit misleading to the user. 
If scoped storage could be worked into flatpak and under user control that would be one concrete improvement. 

Linux does have a long way to go and I don't pretend it would be practical to get there immediately. But to cite one example, Windows used to be a security nightmare and they are now making big moves to fix this.
How do you suppose? A proper sandbox will greatly restrict what a malicious app can do.
>Distros literally cannot afford to do it.

Eventually, distros won't be able to afford shipping Xorg either.

&#x200B;

>The options are to let people use proprietary applications that they need for their work on a free and comfortable desktop with an outdated and less secure graphics stack, or give them the middle finger and force them onto Windows.

Applications that speak X11 will continue to work for many years, thanks to XWayland, but Xorg will no longer be used as the primary display server.

&#x200B;

>It is actually good to settle for doing the best you can in the absence of perfect solutions, and it is actually bad to do nothing to spite the fact that a perfect solution doesn't exist.

In the case of security, I believe honesty and trust is much more important.

>giving the impression of security when there is no actual security is far worse than having no security at all. It's a matter of expectations: if people don't expect to be able to lock their screens, they'll log out. But if they expect to be able to lock their screens and it doesn't actually work, then they're screwed.

https://www.jwz.org/xscreensaver/toolkits.html
It's currently still up to the app developer to declare filesystem=home/host and network in their manifest, Flatpak will grant them access to that without user interaction and they can then escalate to root by manipulating your bashrc, exfiltrate your SSH keys via the network, etc.

That is not a meaningful sandbox, and it shouldn't be promoted as such.

An application sandbox should never, **never**, **NEVER** trust the app/developer to decide for themself what sensitive data/permissions they can access on the user's system.
>	As a user, I should have the confidence that an app cannot access sensitive files unless I explicitly allowed it.

This is a use case covered by portals, e.g. the user chooses what files are accessed through the portal file picker. But it takes time to implement in apps and toolkits.
https://cassidyjames.com/blog/farewell-elementary
[Yes](https://old.reddit.com/r/linux/comments/w0ochl/unpopular_opinion_flatpaks_are_overrated/igjks7u/)
Centralized app store: In this case it would be flatpak. Flatpak allows 3rd party repos. Most distros actually don't even have flathub added by default. This would be like if every distro decided to adopt apt, dnf, etc as the default. Centralized in this case meaning it's the standard.

Centralized repos: This would be like if every distro switched to Snaps. Which only allows Canonical's repos. No 3rd party repos allowed.

"Ubuntu/GNOME Dev" acting like he doesn't know the difference between flatpak and snaps.
[Link](https://old.reddit.com/r/linux/comments/w0ochl/unpopular_opinion_flatpaks_are_overrated/igjks7u/)
Poettering is behind the "immutable OS" push, so it's pretty likely that all distros will be pressured to become immutable, just like they were pressured to adopt Systemd.
Big grief with NixOS is lack of selinux.  Has that situation improved ?
No, no, I got that but they made a question that was already answered (IMO).
So why not do what you can, instead of doing nothing because you can't do it all?
The funniest part of this joke is that if this was snap, it would have taken much longer for the thread to start.
Thanks to this comment I am no longer lost in an infinite loop, now I am just lost in an finite loop.
Yes, indeed. That is a problem for users. For example, I can't use the snap version of gedit, because it doesn't allow me to edit system files, e.g. `/etc/fstab`.
>Flatpaks rely on apps to define conditions for their sandbox. These should ideally be enforced by the operating system and under the control of the user. Android is a good example. 

Yes, Flatpak relies on apps to define the default confines of their sandbox. However, the sandbox *is* enforced by the operating system, and the sandbox *is* under total control of the user.

You, as the user, could revoke any permission an app has. Sure, it'll probably break the hell out of the app, but that's on you now. The user can do whatever, even if it breaks apps, and Flatpak gives you the tools to do it. Flatpak does a much better job of this than Android.

You can do it ephemerally for one invocation of an app (on the `flatpak run` command line), or permanently using `flatpak override` (or a GUI app like Flatseal). You can also punch more holes into the sandbox as the user (if you need Steam to access an external drive the sandbox won't allow it to access, for instance). You have absolute total control here as the user. That detail seems to have been lost in this discussion 

>If scoped storage could be worked into flatpak and under user control that would be one concrete improvement. 

Something very similar to scoped storage does exist in Flatpak and is under user control. This has been the case for years now

Even on Android, apps aren't beholden to scoped storage. An app can target an API version < 30 and it'll magically it'll be allowed to have the old behavior (where it can ask for the external storage permission at runtime and get full access to the equivalent of ~). Alternatively, an app can target an even older API version and that permission needs to be granted at install time (installing the app = giving it permission to look at ~; this is exactly how Flatpak works now for apps that aren't sandbox aware and need file access). Except now the play store yells at the user that this is unsafe. Which is exactly what will happen with Flatpak in a couple of years
The key reason is that a proper sandbox usually normalizes constant prompts for permissions/access, meaning that simple social engineering attacks such as "Do I have permission to rm -r -f anything?" can be surprisingly effective.

There's also the additional reason that inter-app communication is in and of itself a security vulnerability.

Then you always have the issue of perfection. How do you ensure the sandbox is always perfect? Well you effectively can't.
You're right, but it's not promoted this way, and I don't know why you're still hung on that. Open GNOME software right now and see GIMP for example. I think they still set home permissions, so look as what the software center says. It'll tell you clear as day that it won't be fully sandboxed.
And can still be side-stepped by declaring filesystem=home.

As long as apps can escape the sandbox without user consent, it's not a real sandbox.

At the very least make it explicit.
Wow. Strange. Why was Dani so adamant they leave?
>Centralized app store: In this case it would be flatpak. Flatpak allows 3rd party repos.

This is the definition of what a decentralized repo/store is.
That's the stupidest thing I've read today.

No one pressured distros to adopt systemd. It was adopted by its own merit.

The idea that Poettering is an omnipotent force in the Linux userland is simply ridiculous.
No MAC in NixOS yet.

It likely won't ever get selinux because its' not compatible with Nix' model. AppArmor has been considered but not implemented to my knowledge.
oh lol I see now
I will look into the first three paragraphs you wrote, as I don't have enough knowledge off hand to address these, and it's possible I'm ignorant of some flatpak functionality. Thanks for the direction. If you have any references you'd like to share, I'll look them over when I get the time. Otherwise, I'll look it up myself at some point soon. 

What you say about android SDK version and legacy permissions is true, and it is being phased out by google.
A sandbox should never allow direct inter app communication. A well designed sandbox should also employ scoped storage which inherently limits what files the app can access. The latter can be overridden by the user so it is important to make scoped storage easy for the user. The former cannot be overridden by the user without completely breaking the sandbox.
What if you don't use Gnome Software or the CLI?

It also only informs you and the user needs to look at it themselves.

Consent must not be optional.
Systemd was pushed by Poettering's employer, Red Hat. It was promoted in other distros by developers who had taken jobs at Red Hat. So no, Systemd was not adopted on its own merit.
Ive heard this, but I dont understand why, do you have any good reference points on why.

>If you have any references you'd like to share, I'll look them over when I get the time

Sure. Look into Flatseal and the `flatpak override` command


>What you say about android SDK version and legacy permissions is true, and it is being phased out by google.

Sure it's being "phased out" or discouraged, but Google can't just hard pull the plug without breaking apps. Flatpak is in the same situation

Of course, maybe many years after the fact the only apps using the legacy permission are malicious or not used by many people. Then the permission can be cracked down on. I envision the same happening with Flatpak apps in the coming years
>A sandbox should never allow direct inter app communication.

Any such sandbox would be practically impossible. How do you do massively simple tasks like basic window management? I mean, you need to communicate at the very least window location, orientation, and screenspace. You usually want to communicate more, like DPI and colorspace.

Then we get into the issue of libraries, APIs, and that assorted mess.

Eventually you have to poke so many holes through the sandbox it's not secure enough, or it's duplicated so much functionality from the host system it's effectively a highly specific VM.
‚Ä¶how else would you install the app then? And besides if you think the user needs to explicitly check a checkbox saying ‚Äúthis app can access this and that‚Äù then take the issue to GNOME software developers not flatpak. That‚Äôs like saying a company that makes locks is bad because some people leave their keys on the lock.
> So no, Systemd was not adopted on its own merit.

https://bbs.archlinux.org/viewtopic.php?pid=1149530#p1149530
I'm not very knowledgable about selinux but what I do know is that the Nix store doesn't allow xattrs anywhere. You can't store selinux contexts in it.

You could make a script to apply a set of contexts to other parts of the system at runtime but that's... messy .
>‚Ä¶how else would you install the app then?

On Linux, there is no good application sandbox at all.

&#x200B;

>And besides if you think the user needs to explicitly check a checkbox saying ‚Äúthis app can access this and that‚Äù then take the issue to GNOME software developers not flatpak.

Filesystem=home/host should not be granted to untrusted apps, ever. The user should be asked at install time which files and folders the user wants the app to access.

There are too many sensitive dotfiles and dotfolders, as such coarse home access makes the sandbox moot.

&#x200B;

>That‚Äôs like saying a company that makes locks is bad because some people leave their keys on the lock.

More like it takes locks from strangers and sells them to customers, what could possibly go wrong.
https://unixsheikh.com/articles/the-real-motivation-behind-systemd.html
Ah, that makes sense.   My "hope" was that each of the nix-shell sessions would be similar to how selinux wraps containers.
What a bunch of nonsense. Reads like a conspiracy theory, too. ("Facts", lmao.)
What‚Äôs this about a 5 inch Linux pocket pc? 

I sure miss the PDA form factor sometimes

Edit: what the fuck is going on here? They‚Äôre using NextThingCo hardware? Didn‚Äôt they go out of business a few years ago? 

https://popcorncomputer.com

‚ÄúEven Stuxnet could not infect our system.‚Äù 

üòÇüòÇüòÇ

Edit2: I still can‚Äôt figure out what‚Äôs going on here‚Ä¶pictures of NextThing SOCs‚Ä¶but quad core in the specs? 

Halp
doesn‚Äôt rust bootstrap itself? What does gcc rust mean? Compile rust using gcc?
Rust in GCC actually makes a lot of sense. I mean there are parts of the Linux Kernel are written in Rust.
Is the popcorn thing blobless? Looks like a Nokia N900.
Ok
I don't know where they are getting the hardware but I can hear my bank balance complaining already because I'm thinking this will be fun.
I can't imagine a need for the pocket linux PC. I already have a raspberry pi, and if I need more power, my NUC is 4.5" square by 2" high. I prefer an actual keyboard to mini versions like the one they offer on that. I'm better off tossing my NUC in a backpack alongside my mech keyboard.
Yes, the goal is to be able to compile future Linux kernels with Rust code using only gcc
There are no parts of Linux kernel written in Rust yet. It is "coming" to Linux.

And whole reason of sudden influx of work on Rust in gcc is to be able to use with Linux because reliance on LLVM was one of the arguments against Rust.
Rust might be coming to the kernel, but only for drivers in 5.20
Yup, I‚Äôll probably buy one if it‚Äôs not total BS. 

I am currently very happy with my dev term‚Ä¶but smaller is smaller!
Okay, that's just you though.
I thought they were already some drivers in Rust. Good to know.
I've not really seen a device like this so it's just piqued my interest for no reason other than that.
Please, that thing can't even compete with a Pi4 at twice the price. They're $400. It couldn't compete at $200 because it doesn't even have the specs of a high-end Pi4. I can build my own, even WITH a mini keyboard and have money left over.

If I REALLY wanted to save even more money, I could just pick up a Pi400 because it's got 4GB RAM, and is still faster than that pocket pc, pair it with a 12" monitor and still be just as well off.

That doesn't even begin to factor in the degree of support for the Pi vs that thing.

It's overpriced and underpowered, anyone who spends 5 minutes comparing the specs can see that.

I guess I should offer them congratulations on fooling you though.
There are but they're out of tree.

Which means you either have to install them seperately as kmods or compile a custom kernel with them.
Same‚Ä¶at least not for this price. It has some OQO vibes.
Your comparison is not fair, you took the expansive model that also include LORA and gnss.  
Take the base model, 300$. You are paying per monitor, battery, keyboard and portable form factor as big as the raspberry board itself, so definitely if you want to DIY you need to design and manufacture a LOT of complex PBC.  
Of course per 300$ you can get a cheap Ryzen laptop, that will eat both raspberry and this one; this is a small limited product, of course it is relatively expansive.
Okay, but that's just like your opinion though...
No wireless. Less space than a Nomad. Lame.
I'll wait for an eBay deal I think but I think I'd have more fun with an old HP Journda 690 as I have a feeling it would be more challenging.
Please tell me you're joking?

They could have built the thing out of a Pi and had a better supported board, still put their version of Linux on it because there's plenty to choose from... and could have lowered the price point of the device.

I don't know where you were going with the Ryzen comment though, you're always going to get better performance with CISC over ARM chips. You're comparing apples to oranges, there.

I'm still comparing ARM boards with my argument.
The Pocket PC costs $400. It has a 1.2Ghz processor speed and 2GB of RAM.

All I have to do to beat that machine in both price and power is pick up a Pi400. It has a 1.5Ghz processor and 4GB RAM. That costs me $100. I can get a 12" monitor for no more than $80 on Amazon right now. A 500GB SSD is going to cost me about $65 with a good adapter.

It's not an opinion.

I'm not talking smack.

The Pocket PC is overpriced, and under powered.

This is a FACT.
Who lied to you? I boot my Pi4 from an SSD, and they come with wireless.
Haha oh boy. Good luck! I don‚Äôt think any of those went above 133mhz. 

There used to be a distribution of Linux for pocket PCs called Familiar but I don‚Äôt think it‚Äôs around anymore. I used to run it on my iPaq. 

The Sharp Zaurus was as close as we got.

Edit: https://en.m.wikipedia.org/wiki/Familiar_Linux

I have fond memories of waiting hours to send the base image over RS232 :)
> They could have built the thing out of a Pi

Are you sure?  
The PI 4 module is too big, the compute module take out only a USB2.0 and there is no support for USB PD, plus they would still have to develop a carrier board to fit all the required connectivity, battery handling, screen power delivery, and whatnot.

> better supported board

Agree you have better support for mainline Chip (not sure about the GPU driver tho! PI's one started to be developed relatively recently) but here you are buying a product with multiple components, and all have to work together.  
They seems to have a decent blog where they go trough what they decided to use and why, surely there you can find more correct information than my guess

> could have lowered the price point of the device

there are full board like the CHIP with a GR8 that you can buy TODAY for 16$. The PI4 compute module is, at launch, 40$ (and good luck finding it now!)  
The high price of such small run is the engineering time, and they seems to have chosen that chip as they already used it in the past, so they chosen the safe path to push a product.  

> I don't know where you were going with the Ryzen comment

I am saying that if you judge this product by raw power, there are better solution than the PI4 too.

> you're always going to get better performance with CISC over ARM chips

are you? watt per computation pretty sure the ARM wins. And Apple's chip are competitive on the raw performance too..

> You're comparing apples to oranges, there.

You started comparing a finished product compact with display, keyboard and connectivity, to a developer board. I would say the example with the laptop actually fit better!
Okay, but that's just like your opinion dude, my guy, my friend, my long lost brother.
> Who lied to you? I boot my Pi4 from an SSD, and they come with wireless.

[I don't think you get the reference my man.](https://slashdot.org/story/01/10/23/1816257/apple-releases-ipod)
Back when I was newbie to Linux I managed to get Gentoo to install on it so I kind of want to go back and see what I could so now I'm more experienced.

I remember that Pocket PC though and from faulty memory I think they were highly regarded in the community.

As for RS232 though you should come work with me for the day as we still use them in networking for certain equipment.
You people are idiots. Done with this sub.
They pitch this thing as a hacking deck. Good luck hacking with a blackberry, even at your best you can't keep up with a decent typist. All you'll ever be able to run efficiently would be preconfigured scripts. This thing is a conversation piece at best. At worst it's a complete joke for people with more money than sense.
This is very cool! Will it be possible to use it with LXQt, which uses Openbox?
Long live to wlroots
Is that the mate-panel in the new screenshot in readme? How is it possible to make it look like that on wayland?
Looks great. But what about the name? When I read it, I read "lab toilet"!!!!
LXQt IIRC has a bunch of components that don't work properly under Wayland, and the devs don't seem particularly interested in making the required changes (as it is, a lot of their functionality is tied to X11-specific things that just don't work at all under Wayland. That's what I gathered the last time I popped into their Wayland thread on github.
That's wlpanel by one of the labwc's develeoper (consolatis), it's still on private development. I'm using it my self, just contact Consolatis.
Perfect name for shitty composer... I will show myself door.
There is a project called LWQt which ports LXQt apps into wayland, i think they are working with upstream.
Ah that's nice, thanks!
[removed]
Most of the changes to get LWQt working were made in [Mutter](https://build.opensuse.org/package/show/home:Mark-4158:branches:GNOME:Factory/mutter), [KWindowSystem](https://build.opensuse.org/package/show/home:Mark-4158:branches:KDE:Frameworks5/kwindowsystem) and [QtWayland](https://build.opensuse.org/package/show/home:Mark-4158:branches:KDE:Qt:5.15/libqt5-qtwayland). The [revisions to LXQt components](https://github.com/Mark-4158?tab=repositories) are less drastic, and limited to LXQt Session and Panel, PCManFM-Qt and adding `LWQt;` to the `OnlyShowIn=` line of the `.desktop` files for its notification daemon and PolicyKit handler.
libutthurt? :)
You don't even try it and call it shitty, what a loser you are. Go back to your mom's basement !
> Surely there must be a way to have hardware acceleration without using all of our GPU memory ? I am glad you asked because there is a solution that we will explore in my next blog post. 

Well... that was underwhelming.
Short summary/submission staement might be nice...
I just don't really know what people use RPI's for other than niche projects. I have a few and find myself wondering why I even bought them.
....And is sorely needed. 

TL;DR: GPU acceleration is in a bad state on Pi 1-3s, alternatives to the current solution have problems too. Turn in for my next blog post where I actually, IDK, maybe... tell you what my plans are for fixing this are or something...
PiHole, unbound, repositories, package sources, Ansible. These are just a few examples of what I use Raspberry Pi for on the LAN.
I'm very happy using mine as media center OSMC, Raspberry Pi3 B+, excellent for 1080p
It's not any better on pi4. I love the pi computers for projects and small servers but I can't understand how people use them as desktops for...anything. The display is so laggy and the screen tearing is absolutely unreal.
The new Vala site looks great!
Gnome has just been killing it
I really like TWiG! At least for me it's a great way to follow what is happening in the Gnome ecosystem. I found quite a few cool apps/features because of it during the last year. Good luck to the author in keeping up the awesome work for the community in years to come!
Great issue this week!
Up Mexico
Is Gnome still relevant?
Of course it is. It is one of the two most influential desktop environments and the default on multiple distros.

What makes you think it isn't?
Of course. I'd wager it's probably still the most used DE out there, and it serves as the base for several others.

The pessimist in me suspects, however, that your question wasn't an honest one, but rather yet another one of those childish "lol gnome shit, use a TWM or KDE Plasma like a real man" comments that are all to common on Reddit.

If that's the case, then come on, mate. Just let people use what they want to use and stop with the negativity.
More than ever I would say
Gnome is on fire
Usually enterprises will offer you a remote machine with either SuSE or RedHat where you can connect to debug an issue with the application. The desktop is Gnome and you cannot change that. So the whole experience is pretty bad, but considering you only use a compiler, a debugger and some prety basic preinstalled tools is ok.  

But on a personal desktop how hard can it be to change the desktop to Xfce for example? My feeling is that Gnome is for tablets and touch sensitive screens.
Just look at their post history, it's easy to see where the hate originates
Remotely is one way I admittedly haven't used GNOME so I can't speak to the experience there.

But GNOME is great on totally ordinary PCs, because of it's strong focus on keyboard shortcuts for everything and touchpad gestures. Especially the touchpad gestures work great on laptops. I haven't tried it on touchscreens, but I imagine you're right that it works well there as too.

And yes, it's generally pretty easy to install another DE and use that instead. But even without any numbers to back it up I'm still pretty sure that nobody else comes close to GNOME and KDE in usage. And that's fine, they're both great DEs. XFCE is good as well, but personally I prefer both GNOME and KDE over it.
Gnome works both touch screen and normal one. Touchpad gesture is 1:1 and has a decent keyboard shotcuts out of the box.
If you just want keyboard shortcuts why not use a tiling WM?
Want keyboard shortcuts != Want to throw mouse out window
I wish KDE Connect could implement this, so good
# STEPS

* To do this, install `scrcpy`
* Enable USB Debugging and connect your android device via USB.
* Make sure your phone is on the same network as your PC.
* Run `adb tcpip 5555`
* Now, run `adb connect <your android's ip address>:5555`
* Now you can disconnect the device from USB and whenever you want, just run `scrcpy` and enjoy!
* You don't have to connect your phone via USB ever again. Just run the second `adb` command followed by `scrcpy`.
* You can put these commands in a script and assign a shortcut to it!
Wow it even shares the clipboard. I can just hit the copy button on 2FA codes on the phone and they'll get put in the PC clipboard automatically. This is awesome :D
We use SCRCPY and sndcpy to watch TV on our computer without having wifi, it's pretty great honestly
I wish I could do this with my iPhone. ‚òπÔ∏è Even though I probably wouldn't use it for anything, just like screen mirroring. üòÖ
Do you know QtScrcpy?

https://github.com/barry-ran/QtScrcpy
Does the second command leave an always open port on the device?
Great! I love it
So this allow to any user of samenetwork access to phone screen?
Cool project but I would recommend a degoogled ROM such as e/os or Lineage os with MicroG
What's the use case for this?
wait scrcpy works with a redmi device with input on the stock rom?
"One click"
Goes ahead and lists 8 steps üòÇ
Still thank you, though.
Damn. Kinda wish I had an android now. Wonder if it‚Äôd work with iPhone.
There exists autoadb, which allows you to start scrcpy whenever you plug-in your phone.

So, I have a mobile phone cradle on my desk. Plugging in my phone, scrcpy gets started and the phone screen blanks (setting in scrcpy itself). That is all I need to almost seemlessly integrate the phone in my workflow.

Bonus: I use a Bluetooth headset, which connects to my computer, phone and office phone. Allowing to take calls from all three sources. Scrcpy allows me to interact with phone calls without touching the phone. 

ExtraBonus: There exists a companion app for scrcpy called sndcpy, which allows to redirect the sound to the PC. This can become handy sometimes, if you need the sound being played on bigger speakers. 

https://github.com/rom1v/autoadb
https://github.com/rom1v/sndcpy
 Scrpy is an awesome tool! I hope to see KDE implement a similar feature soon in KDE Connect in the near future. :)
Sorry if it's off-topic, but are those green checkmarks in the folders from Google Drive uploading? I want to use that when I switch to Linux and I'd want to know how it worked.
Anybody know the program or the script used on this video, if it is written on the description or something like this then sorry, didn't see it.
Is there a way to reverse this? In as nice of a way that is.
Yeah, no. I tried the using the tcpip one in the past, it's laggy and it often requires re-pairing. If you still want to use the wireless adb method, then I recommend just using [qtscrcpy](https://aur.archlinux.org/packages/qtscrcpy) for a much handier GUI (aka exhibit 1 of around a dozen for why I cannot live without AUR).
Easier to just use DroidVNC without needing root and messy USB debug, FOSS
https://github.com/bk138/droidVNC-NG
This seems like kde connect, I use it daily, very useful.
Does anyone know why KDE Connect hasnt integrated scrcpy?
Yeah, exactly one click
> Enable USB Tethering



99% sure you mean USB debugging
There is also a GUI version available on flathub site.
> you dont have to connect your phone via USB ever again

Not entirely true, when you restart your phone the tcpip gets reset and you have to do it again.
Isn't it limited in time ? If I recall correctly adb forbids the mirroring after a while. But I might be wrong
To add to this, the man page for this has all sorts of hints `man scrcpy`.


`scrcpy -K` will emulate the keyboard.

`scrcpy --window-borderless` will remove your theme boader, move with alt click, and close from taskbar.

`scrcpy --window-x 1080 --window-y  0 --window-height 1056` if you have a 24px panel (XFCE4) this will place the phone on the right of your screen.


Put it all together and put it in a shortcut, or better yet, in your keyboard shortcuts.
Does it also share sound output too?
KDE Connect does this as well, just running in the background.

Can be a bit of a bitch disabling every powersave feature to stop Android killing it though.
If you use Authy as your MFA TOTP solution there is a desktop application which is multi-platform. I know this defeats the purpose of MFA, just letting people know it's an option.
if you need 2fa totp's on your linux machine i can recommend pass-totp works like a charm with a simple cli
Shouldn't it be possible to just use USB tethering for that? When I needed Internet on my PC but only had internet on my phone, I just connected them via USB and enabled USB tethering. My PC just used it as if it were a regular Ethernet connection. 

I'm just curious, cuz scrcpy and sndcpy doesn't seem necessary for this use case for me.
Try uxplay
On modern Android systems (at least since 11, maybe earlier) when someone tries to connect with ADB you'll get a prompt asking you to allow it.
Should still be protected by ADB I hope.
You can skip the network parameter and keep the USB cable connected, like this:

    adb devices -l
    scrcpy --serial ABCDE123456

The first command is to list the connected devices, from the results you will look for the serial number, which then you will use on the second command. This is how I do it, I don't use the network option. Every afternoon I plug the computer and phone to start charging, then launch scrcpy (a script file) and keep using my phone through the computer. It's very very useful. Consider this: I'm using scrcpy right now on Windows, so check the documentation before trying the commands I showed, because the syntax could be a little bit different.
Unfortunately, yes.
Gaming, messaging, 2FA, phone-as-virtual-desktop
Yes, it does :)  
Just make sure to enable "USB Debugging" *AND* "USB Debugging (Security settings)".
That sounds awesome!
> Bonus: I use a Bluetooth headset, which connects to my computer, phone and office phone. Allowing to take calls from all three sources. Scrcpy allows me to interact with phone calls without touching the phone.

May I know the name of that headset?
You can have [google Drive in KDE](https://www.omgubuntu.co.uk/2017/05/kde-google-drive-integration-in-plasma) as network drive if you need it
No they're from MEGAsync.
It's paid software, and there may be other free alternatives, but I've had great luck using [Insync](https://www.insynchq.com/) for Google Drive. Sometimes it goes on sale, I think I paid less than $10 for my license.
# STEPS

* To do this, install `scrcpy`
* Enable USB Debugging and connect your android device via USB.
* Make sure your phone is on the same network as your PC.
* Run `adb tcpip 5555`
* Now, run `adb connect <your android's ip address>:5555`
* Now you can disconnect the device from USB and whenever you want, just run `scrcpy` and enjoy!
* You don't have to connect your phone via USB ever again. Just run the second `adb` command followed by `scrcpy`.
* You can put these commands in a script and assign a shortcut to it!
 You mean like mirroring PC screen to a phone?
scrcpy doesn't need root on either end, and its *much* faster than VNC.
I heard that they were working on their own solution at some point. Could ask them at r/kde
You can just run scrcpy with the phone plugged in and USB debugging enabled, without the whole putting the phone into the network adb mode. That's just if you want to run it wirelessly.
This is just the setup. After this, you can do this by just pressing the keyboard shortcut.
Oh, my bad. Thanks for the correction!
If it is limited it‚Äôs a generous one. I did this recently with an old phone, dug through every last file and old text convo for like 3-4 hours without issue. This was wired though.
My wife runs scrcpy for a day or two at a time without stopping.
Can it be used without KDE though?
No longer works with recent Android versions sadly, background access to clipboard got killed off by Google so now you need root or a custom ROM to do clipboard sync.
Sadly never managed to do this on Android 8. No matter what i do kdeconnect is killed until i unlock my device
Yeah I would definitely recommend against that if you care about security. Especially on something like your email you should always have another factor that is physically separate from the device you're logging into. An app on your phone will prevent malware attacks, while a FIDO key additionally helps to mitigate phishing as well.
See my reply to another comment that recommended a 2FA tool for the PC. There's no point in using 2FA if you're gonna keep it in your PC.
I got it working! UxPlay just works, like magic! Thanks for letting me know! https://i.imgur.com/LgCzbxi.jpg
You are awesome
The prompt also contains the PC's fingerprint. I think this was there since Android 5 or 6 maybe even earlier
Seems to be fine for a udev rule.
Then it should really be a major disclaimer at the top of your post imo - it can be a major security issue
Wait doesn't ADB require you manually approve any connecting computer's RSA fingerprint?
for my redmi note 10 sadly still on stock rom does it not work to input and now after the android 12 update did it seem to brick that even tho i have both active and had to wait like a minute because of the banner that just annoy you
Don't know the exact model but it is from the Jabra Evolve series. They are pricey but they work really well.
Thank you very much, I will search a way to use that script with a GUI so it will be easier for "lazy" people like me to do it. üòä
Throw capitalism away? No‚Ä¶ I don‚Äôt think will
But what if I reboot my phone? As far as I'm aware, `adb tcpip` doesn't persist across reboots.

And what if my phone's IP changes? This happens all the time on DHCP networks. It's not one step if every week I have to grab a new IP.

Also, you shouldn't be running ADB wifi constantly. Sure, it requires you to manually accept all first-time connections but it's easy to do so accidently. Also, it's another exploitable daemon not running in a sandbox.
Unless the phone's IP changes. Might be time to configure it statically through the router.
Or, if you want to, by having an NFC tag reader connected to your machine, reading whenever your phone gets placed in a dock for example :)
My bad, I was mistaking this with the adb shell screenrecord command
All KDE software are separate to Plasma. You can use them on anything you want.

And if you're on GNOME or ZorinOS, there's GSConnect and Zorin Connect which are forks of it made for their respective DEs.
Yes. It's distro and DE independant. I use it with Manjaro Cinammon.
and how do i bypass that auto-clipboard-sync? Since my phone is rooted. I don't mind tho opening my notification center and press "send clipboard"
Oh how wonderful, yet more "For your security!!!!" changes screwing over the users.
You mean it won't make the initial connection while locked, or none of the features work at all while it's locked?

The former is an Android limitation they can't bypass.
Of course there is a point in using it on your pc. It doesnt matter if you use a phone or pc as your 2fa tool. A second factor is a second factor. There is only a difference if your phone has a dedicated security chip to store the 2fa totp secret on.

A hardware device can be stolen as well. And if you have concerns about maleware etc then i would be more concerned about your general browsing behavior.
I had no idea this was possible and would certainly come in handy
This is only mirroring right? No screen control?
Oh yes, my bad.
Thanks!
Single Click*
Oh no
I was so excited when I got this working on my android.  Then I rebooted and lost connection.  No problem, I'll just run the script again.  It works!  Then just randomly, lost connection.  Ran the script again.  Couldn't connect.  Rebooted phone and computer.  It works.  an hour later, lost connection for no reason.  Ran the script again.  Couldn't connect.  I gave up in frustration.

It was amazing when it worked, but it just wouldn't stay working.  Still have not found a better solution.  KDE Connect was a total failure.
You can use the name of the phone instead of IP address. Use `.local` domain if it can't be resolved from just the phone's 'host name'.
Randomly assigned MAC breaks this.
uh, i can configure static ip in my android phone since 2018 or something, it's there by default
It's been a while and I think the solution was Riru-ClioboardWhitelist. I found an [article about it](https://szclsya.me/posts/android/fix-clipboard-android-10/) and the corresponding [GitHub repo](https://github.com/Kr328/Riru-ClipboardWhitelist) but don't currently have root on my phone and never used it so I can't attest as to its efficacy. It does look still maintained with a commit from May.
It makes sort of sense, I'm sure they don't want apps like Facebook and TikTok spying on the average user.

It just severely lacks an advanced users toggle to selectively enable these kinds of features on-demand for apps that really needs them.
Yeah, when it's locked and sleep mode kicks in it says "no devices paired" so it doesn't see the phone. As soon as i unlock the phone, it makes the connection and all notifications start coming in. I tried excluding it from the power optimizations pertaining to sleep mode but it doesn't help.
Yep, no screen control.
I might have to add that the Jabra headsets are not recognized as softphones under Linux but as Audio-devices.
The only drawback, if I am in a meeting on the PC and I get a call (or any accustic notification) via mobile or phone, the base station of the Jabra headset will switch over to the new source. Canceling the call it takes a few seconds on which I might miss what is going on in the online meeting.

As a softphone, that would not happen, as ist would signal that it is occupied, however, it seems there is no way to add the headset as a softphone under Linux.

A small annoying detail but I can live with it. For important online meetings, I switch off Bluetooth on my phone.
To they do Airlink for the quest with adb?
Yes, same experience here, it's rock solid over USB but it really isn't great over WiFi.

The most optimal possible solution would be a version of scrcpy that launched over USB/ADB Wifi/Shizuku but keeps running in the background even after the cable is unplugged.

It would communicate with the computer over UDP instead of ADB WiFi.
Good point. As a fix, on AOSP/Android 12 this can be disabled per network (Use device MAC instead of randomized MAC).
Hostname lookup on DHCP networks is a thing tho
I like to manage all devices through DHCP. It's just my preference.
How??
Thanks haha! Finally i can nostalgic back to my Android Pie experience with KDE Connect

Edit: btw you don't have to add the app to whitelist through command line, it'll automatically install the clipboard app to configure it
Try QtScrcpy:

https://github.com/barry-ran/QtScrcpy
Phone just for updated to 12 a few days ago, ^yay!
Wifi > Setting> Edit Connection > change ur IP, also you could apply your own dns
Thanks.  Worth trying but the issue isn't with scrcpy it's with the adb connection.  Once connected, scrcpy works fantastically.  It's the whole connection part that won't stick so using another mirroring tool is likely to make little difference.  I appreciate the suggestion though.  I did not know about that one,
Thanks!
It's possible that it's the application that's slow, not X.
X11 is windowing server API, Windows is an OS.  There are graphical drivers, Xorg configurations, compositors, window managers, openGL implementations, etc that can all have an impact.  Make sure you're actually measuring the right things.  It's likely possibly to make X a lot faster than Windows' graphical system for some things if you tune/configure it correctly.
I haven't noticed any discernable speed difference between X11, Wayland, or Windows. That said, the last two do a far better job avoiding tearing

Unix sockets are fast, I doubt this truly touches the network

This could be an nvidia-ism
This looks more like a matter of API design.

Windows has dedicated, deeply-integrated modal dialog APIs. Typically one calls CreateDialog from inside the triggering actions's message handler so subsequent keyboard messages get routed to the dialog, *well before* it shows up. Windows can take minutes to render the dialog and your Enter press will still be registered correctly.

I'm not familiar with X API, but I think on X modality is an optional feature provided by the window manager. So there is always a chance of screwing up the Enter press regardless of speed. It's kind of a necessary sacrifice to enable 3rd party window managers / compositors. But yes, I find it annoying too.
X itself is pretty fast. The problem is likely somewhere else. For example i remember certain apps starting slow, when missing some Dbus services. There is likely something similar. Or just bug in KeePass
It's always instant for me (keepassxc) on Tumbleweed with x11 and Plasma, so it's probably not x11 or keepassxc that's the issue?

>Wayland faster than X11? I don't want to use it, because my entire reason for using X11 at the moment is because I want to make a compositor, not an entire desktop environment, but I'm curious anyway.

Isn't .... doesn't Wayland need only a compositor?
Your problem is Alpine, not X11. Musl libc is extremely slow and unoptimized compared to glibc.
Did you test this on a glibc based distro?
Not only possible. It is slower.
> (Bonus: is Wayland faster than X11? I don't want to use it, because my entire reason for using X11 at the moment is because I want to make a compositor, not an entire desktop environment, but I'm curious anyway.)

Why not test that? Just install a small Wayland compositor like Sway, and try the app in that environment.
i use xfce in a very cheap vivobook and is quick+responsive
Quite the opposite. Windows has had full desktop composition since Windows Vista, and all performance metrics are under this system. On the other side, Linux benchmarks usually disable composition to stay somewhat competitive. Even casual gamers disable it because it's so bad.
Check if you're using GPU acceleration, and if it's done by a real GPU or emulation (llvmpipe).
You mean X11 vs "Desktop Window Manager" or DWM - what is called compositing window manager in MS Windows ?
I believe this is a legit question. I've also come to realize that MS Windows OS and MacOS seem to be much faster. They overall offer a fast, responsive and remarkably intuitive experience. Wayland, to certain extent, appears to address this matter. For this reason, I personally prefer Wayland against X11. I have also come much closer to GNOME 42 due to the fact that it goes seemingly better with Wayland. I think by the time Linux reaches many more users, things will get better.
Just trod off to another community :( everything Wind0ze is slower than Gnu/Linux...
To find the answer you ought to read all the 40 years long story of X11.

From the user's perspective today - X11 is slow because its development has stalled and no one wants to work on X11 (C) your lovely RedHat Inc.

Don't forget that MS might have analytics who do testing of competing products and technologies.  But concerning X11 it became obvious that their path hit the dead end well back to the times of the project WinNT Hydra and Sun Ray terminals.  MS wanted their own OpenGL and graphics foundation and accidentally did it much better than other products. Being later chased by Apple with its OSX.
>Maybe the fact that I only have graphics acceleration on my iGPU, and not my Nvidia GPU?

If you'd see how well X and Wayland run on an intel/amd iGPU, you'd take those words right back, lol.
Qt is a monolithic high-level dynamically-dispatched mess, but it performs just fine on Windows (KeePassXC uses the same backend for all platforms), so I don't know why it'd be that much slower on Linux.

Just curious, are there any X11 windowing "benchmarks"? Like, "click a button and time how long it takes for this window to open"?
I've been a long time reader of these X11 vs Windows disputes. üòÅ

And I have accumulated some conclusions from those disputes and arguments. 

Mostly X11 and Win and others... have the same tech underneath due to the nature of modern computer graphics, it only matters how each other call their graphical primitives and windowing management and of course how the vendor reflects basic tech in its own product. 

So that MS Windows, Apple macOS and Google Android are the leading winners.
I'm running on integrated graphics so I doubt it has anything to do with nvidia. Plus nvidia shouldn't affect the speed of keyboard input
> Windows can take minutes to render the dialog and your Enter press will still be registered correctly.

Now this is interesting! Where can I find some more details on these API quirks of Windows?
I believe it. I've noticed some apps starting faster in x.org than wayland.
> Isn't .... doesn't Wayland need only a compositor?

In Wayland, what people call the "compositor" is typically also the server, implementing every protocol extension needed and all window management features (usually assisted by wlroots or another Wayland framework). This means you can't mix and match window managers, compositors, and servers like you can on X11, and you also can't make "just" a compositor.

Is there a Wayland server/WM/etc with a swappable compositor? I haven't seen one yet.
Admittedly no. Does glibc make a significant difference in this regard? I'm actually working on building a (G)[KISS](https://kisslinux.org)-based installation for the express purpose of replacing Alpine so I can run proprietary apps, but it's probably going to take weeks before it's ready.
Good idea actually. I didn't know Wayland was so easy to install. Will it just pick up on Mesa and stuff automatically?
Pretty sure Windows has its own version of unredirection though. I haven't felt any of the frame latency issues that you usually find when running games within a composited environment.
Real Mesa GPU acceleration on the integrated GPU.

Obviously not using lavapipe lmao.
Yes, that's what I mean. I don't run Windows 10 with compositing off :)
Probably it's the same now. GTK/QT draw directly to their canvases. Probably not much difference from Windows.
Distro checks out
Still Jobs&Co did not choose X11 for their Next. That reflects the state of x11 in the end of 1980s...
For sure, picom is really good at vsync even on my iGPU, as long as I don't turn on the background blurring.
> Qt is a monolithic high-level dynamically-typed mess, but it performs just fine on Windows (KeePassXC uses the same backend for all platforms), so I don't know why it'd be that much slower on Linux.

Well, it's a different piece of software. It could be doing different things.

I'm not saying whether or not this _is_ the case. I'm just saying that your assumption that it must be X is just that: an assumption.

> Just curious, are there any X11 windowing "benchmarks"? Like, "click a button and time how long it takes for this window to open"?

The `x11perf` program can be used to benchmark various low-level X operations. It's probably not the kind of benchmark you're looking for though.
> Qt is a monolithic high-level dynamically-typed mess

Qt is written in C++. It isn't dynamically-typed by any means.
Dev here who had created Qt5 graphics backend port for one of broadcom‚Äôs chips with proprietary graphic stack for IPTV set-top boxes.

Actually Qt5 internally could be a highly optimized in graphic regards by offloading a lot of work to gpu. This is in responsibility of particular Qt‚Äôs backend. Even if no gpu is available it could (I‚Äôm talking about Qt5) efficiently utilize SIMD/NEON CPU instructions to perform many graphic operations.

On another hand (I‚Äôm not a big expert here) X11 was designed back in 70-80th when mainframes were a trend so X11 is mainly a network graphics stack. Nowadays X11 tries to keep all this networking stuff for backward capability meantime tries to workaround it (for performance reasons) whenever it‚Äôs possible for modern hardware. This indeed causes performance losses and that‚Äôs why unhappy devs had created Wayland which doesn‚Äôt inherit all that stuff.

Considering said above, Qt‚Äôs X11 backend could be struggling of achieving best possible performance by taking into account gpu, what capabilities provided by drivers, window compositor, etc. Every piece of this chain could be a bottleneck. Needless to say that this could be varying from distro to distro since each distro is playing around with bunch of flags/options.

Windows as a proprietary O$ is benefiting from highly optimized stack by dictating a new API rules for app devs or/and graphic card manufacturers while maintaining backward capabilities with older software. This allows Qt and other devs achieve great optimizations there. Meanwhile Linux apps are having too many options to consider so they‚Äôre trying just to support some reasonable subset on the top of Linux‚Äôs distro graphic stack iceberg. This is where Linux strongest side becomes it‚Äôs weak side.
> Mostly X11 and Win and others... have the same tech underneath due to the nature of modern computer graphics, it only matters how each other call their graphical primitives and windowing management and of course how the vendor reflects basic tech in its own product. 

So you mean, other than the operating system, the kernel subsystem, the graphic subsystem and the GPU driver as well as the graphical API, then it's all the same?

What's left?
That's fair, yea if integrated is running the show then it's not a problem. I've had a heck of a time ensuring which is actually active with those dual setup systems 

It wouldn't necessarily impact the keyboard input but it could delay the feedback loop (rendering)
Check picom and picom's bugtrackers against your integrated graphics.

The delay describe sounds akin to my AMD (R9 280/380 series) and compton (picom is a fork of compton IIRC). At the end, it was just a bug.

> Plus nvidia shouldn't affect the speed of keyboard input

But your issue isn't about that. It's about an *untimely display of window*. Which a (possibly) buggy compositor talking to (possibly) buggy drivers will most likely show.

This is before diving headfirst into the whole issue of your libc (alpine), which is good for specific goals (ie security) but lack a whole bunch of tricks under the hood to be *fast*.

Those two would be the tip of the iceberg for me, but it may have tons of layers where the issue is. And worst, combining those layers may make things worst.

ie : my hardware as a specific set of hardware issues that combo well together to make sure using a compositor (but Wayland and Arcan work fine) suck absolute ass. In addition to a specific bug in compton back when I tried it.
Just try some Win32 API yourself and read relevant Microsoft blogs. I remember this behavior because my code went unresponsive a lot but I never had to worry about wrongly-registered keys. It could be different now though. And my memory could have failed me.
But it also has downsides. Such as when the application becomes unresponsive, the entire window does

Or simply dragging the title bar suspends the application. So while you're dragging a window, your program freezes
They have different model to create windows and to fill them. X applications can let X server to do all the drawing, which can lead to better latency in certain cases. Also X has been here for very long time so some apps might just have better code for dealing with it. Or they can not support Wayland protocol, so XWayland has to be started for them and act as middle man, which slows some stuff down... There are reasons. Also there are reason why some apps should be more performant on Wayland. And it also depends on Wayland compositor implementation... It's mess
You can build on top of Wlroots library. Nobody implements the whole Wayland when making a Wayland display manager. Unless you like pain?
>Is there a Wayland server/WM/etc with a swappable compositor?

I have an Nvidia card; haven't paid any attention, really.
It's worth a try. Musl can be really slow, apparently.
Boot up a live USB on a machine with say 8gb of ram or more and you can prove it easily after it's fully booted and things are cached
Yes. Unless you have Nvidia, in which case the setup is more complicated last I checked.
Just install gnome or kde. They work on Wayland by default üëç
If you get an older card, pre-X1xx/6000, and install Windows XP WDM driver under a modern OS, you see a miracle. An insane performance/latency boost, as they aren't compatible with desktop composition.

Thing is, WDDM has the latency and performance penalty of composition, and it's still that good with it. People just forgot that raw performance could even be better.
llvmpipe is part of Mesa and is often active if there's any problem with your gpu driver. You can tell for sure by checking the output of glxinfo.
allrighty...

I do have dual boot on machines but honestly, both are so fast that i do not notice much difference while using it.

It comes up for full screen 4k videos or anything that uses gpu extensively - then i can notice some tearing of content with Linux, but that could be that i use TV as monitor.... Nothing of that under DWM as just about (except legacy apps) has by default hardware accelerated graphics.

Head to Phoronix test suite site, check there - they must have done testing of those in past years.

but DWM is actice component of largest desktop OS for business and home, very under continuing development and optimisation, while X11 well is old as hell and even when I started graphic *nix it was "it is such system hat noone wants to touch it with ten foot steel pole"
Yeah that tanks performance on my 4800mq, but on my 12600k it keep blazing at full speed with blur on. I even have a video playing as my wallpaper at 60fps all the time and it's like okay sure ¬Ø\\\_(„ÉÑ)\_/¬Ø
True, I don't know just how much code gets swapped out between Windows and Linux. The difference could be any amount.
Everything is a heap-allocated object using runtime reflection and QML muddies it up even more.
Only one piece of hardware is left - GPU. 

It is the same for X11, Win,  and used to be for mac/OSX. 

And the GPU is only labeled to support OpenGL/DirectX
I'll make sure to test without a compositor soon(tm). For now I'm kinda burned out with Linux. Spending days and days trying different kernel configurations trying to get my trackpad to work is exhausting. Especially when the default Alpine 5.15 kernel detects it flawlessly, so I know it should work.
I already mentioned wlroots in my comment :). My issue is not with implementing an entire display manager (ty for the correct terminology). My issue is with "choosing" a display manager implementation when I want to make a compositor that can be used with *anything* (like picom).

X11 gives me this separation, but Wayland doesn't.
>Nobody implements the whole Wayland when making a Wayland display manager.

Actually big desktops like GNOME or KDE do that. Their implementation of Wayland doesn't use any library like wlroots, it's written from scratch.
Wayland seems to work pretty well with my 1660 to mobile on gnome
My glibc based install still can't compile KeePassXC because of qt5's stupid linguist tools, which I can't figure out how to build from source. But as soon as I can figure out how to make a package for those, I'll be able to do the test
Alpine can't run Nvidia anyway, so this isn't a problem. (the proprietary drivers won't run with MUSL)
Plasma in most distros does not use Wayland by default yet, except on Fedora.
    name of display: :0.0
    display: :0  screen: 0
    direct rendering: Yes
    Extended renderer info (GLX_MESA_query_renderer):
        Vendor: Intel (0x8086)
        Device: Mesa Intel(R) UHD Graphics (CML GT2) (0x9bc4)
        Version: 21.3.8
        Accelerated: yes
        Video memory: 3072MB
        Unified memory: yes
        Preferred profile: core (0x1)
        Max core profile version: 4.6
        Max compat profile version: 4.6
        Max GLES1 profile version: 1.1
        Max GLES[23] profile version: 3.2
    OpenGL vendor string: Intel
    OpenGL renderer string: Mesa Intel(R) UHD Graphics (CML GT2)
    OpenGL core profile version string: 4.6 (Core Profile) Mesa 21.3.8 (git-53b2b224dc)
    OpenGL core profile shading language version string: 4.60
    OpenGL core profile context flags: (none)
    OpenGL core profile profile mask: core profile
    
    OpenGL version string: 4.6 (Compatibility Profile) Mesa 21.3.8 (git-53b2b224dc)
    OpenGL shading language version string: 4.60
    OpenGL context flags: (none)
    OpenGL profile mask: compatibility profile
    
    OpenGL ES profile version string: OpenGL ES 3.2 Mesa 21.3.8 (git-53b2b224dc)
    OpenGL ES profile shading language version string: OpenGL ES GLSL ES 3.20

:)
> Everything is a heap-allocated object using runtime reflection

In my opinion that doesn't make it what most people would call "dynamically-typed". Types in C++, even with Qt, are checked at compile-time.

> and QML muddies it up even more.

KeePassXC does not use QML.
I don't know get what your point is. The apis are very different
The thing with X11 is that such "distinction" between components is coming from the fragmented mess of X11 ecosystem, for example how whole 3D acceleration and composition is implemented by essentially building on top of an antiquated protocol. [alongside other architectural decisions I mean] 

Wayland is a lot more integrated, it cuts down on layers and provides a faster, safer and simpler system for displaying stuff, essentially same as what everyone else except BSDs and Linux has been doing. (Including Android, even Android has better stack than X11)

Downside to such an architecture is that it moves towards canonicalization, it results in massive Wayland display managers doing a whole lot of work. But the pros of such approach outweighs the con of lost flexibility.
For them it was easier to build on what they had, also mutter is older than Wlroots is it not? It was also mich better than that for a while
Just use a binary?
Try getting it via Nix (or Guix (if (like you (parens))).

(In the case of Nix, not from `pkgsMusl` or `pkgsStatic` obviously.)
I see on Debian there's an specific package but shouldn't be hard to try. The only problem I have is unable to share full screen on Chrome...
> KeePassXC does not use QML.

Didn't say it did. :)

> Types are checked at compile-time.

Sorry, I tend to confuse dynamically typed and dynamically dispatched. They're both equally undesirable imho. I'll edit.
> The thing with X11 is that such "distinction" between components is coming from the fragmented mess of X11 ecosystem, for example how whole 3D acceleration and composition is implemented by essentially building on top of an antiquated protocol. 

I'll admit that X11 has a lot of annoying, weird corners, like fonts. But until a Wayland display manager comes out that can be customized the same way Fluxbox and X11 can, I don't think I'll be switching.
I guess so. Worth mentioning is that there is KWinFT project that has wlroots backend.
Nope :)
> (or Guix (if (like you (parens)))

No kink shaming.

Those are mighty fine parens.
You need to have Pipewire set up and then flip a flag in Chrome's experimental setting. Go to `chrome://flags` and look for "pipewire". There would be only one result AFAIK and it says something with webrtc and switch that to "Enabled". Restart and you should be good to go.
So dynamically dispatched means dynamic types at runtime?
I doubt it‚Äôll ever happen, half the point of Wayland was merging these disparate layers together. Enjoy X11!
There ARE minimal Wayland environments like Sway and stuff, it's not entirely a lost cause
Oh, nice. Will take a look, thanks
I don't know what "dynamic rooms" means, but dynamically dispatched basically means each object has to keep track of which class it is, so that the right method implementations can be found. Two pointers of indirection IIRC.

It definitely has its uses but this is sort of a "death by a thousand papercuts" situation.
Lol, thanks :)
Sway is tiling and doesn't support custom compositors/shaders :(
Typing on mobile, I meant dynamic *types*
Thanks for the answer.
> The idea is to get a protocol which replaces mDNS (local service discovery), SSH (interactive textual shell), X11/VNC/RDP (interactive graphical shell), RTSP (streaming multimedia), HTTP (networked application retrieval and state synchronisation) and a few other lesser knowns, and we are nearly there feature wise.

Soooooo 9P.
I've seen numerous posts about Arcan over the years and to this day, I still don't understand exactly what it is.

Seems like it really needs some practical use-cases for it to get attention.
Difficult to get the idea, just a new O.S.
I'm still not sure whether this guy is a genius or just crazy.
[Yes and no](https://arcan-fe.com/2017/04/17/one-night-in-rio-vacation-photos-from-plan9/)
No, this sits at a higher level.
I haven't seen any particular sign of madness (in contrast to, say, Terry Davis of TempleOS fame) from the Arcan developer. On the contrary,  he's extremely lucid in his analysis. I highly recommend his [12 principles for a diverging desktop](https://www.divergent-desktop.org/blog/2020/08/10/principles-overview/).
But have you actually tried it?

Durden (what seems to me like the "mainstream" arcan desktop) has existed 
for almost as long as sway, yet it barely got [packaged](https://repology.org/project/durden/versions) , tbh the whole thing feels overly theoretical and focused on insignificant issues.

take this for example:

> Persistent state stores should not be some assumed default, but rather require interactive/user-initiated automation. Lifespace should be coupled to namespace.

How many programs can you think of don't use persistent state? (don't have "settings") or that will be willing to install and use but are unwilling to let them store state?
What about the added complexity? another permission you have to review before installing? Another cognitive load or something you can get desensitized to (so you end up not reviewing anything, like windows vista).
> But have you actually tried it?

It's on my bucket list. I follow its development, but I'm currently busy enough with my own stuff to play around with something that is still a moving target.

> Durden (what seems to me like the "mainstream" arcan desktop) has existed for almost as long as sway, yet it barely got packaged.

A piece of software being or not being packaged tells more about the maintainers of a distribution than about the piece of software itself. I use (and write) plenty of software that isn't packaged.

> How many programs can you think of don't use persistent state? (don't have "settings") or that will be willing to install and use but are unwilling to let them store state?

I think you completely misread the meaning of the quote about state stores.
unrelated, but how did you make the chat background transparent?

or is it just a wallpaper
I read this and the github page and I still don't know what this is
# stikman

mirror : [https://github.com/sinanmohd/stikman](https://github.com/sinanmohd/stikman)

git repo : [https://gitlab.com/sinanmohd/stikman/](https://gitlab.com/sinanmohd/stikman/)

i rekon there's a lack documentation, basically what u/featherfurl said [here](https://www.reddit.com/r/linux/comments/vzprqd/comment/igcc5ft/?utm_source=share&utm_medium=web2x&context=3)

>In a lot of chat applications stickers are like emoji but bigger and more custom. Generally you can only use a given set of stickers within the app they were made for. This is an attempt to make it easy to use the same sticker set across multiple chat apps.

one of the reason i created stikman was if you use more than one chat platforms you cant use your stickers from a with b and vise versa, and lack of proper support for stickers in platforms like element/matrix, its a lot of work and you need a self hosted server for it to work, stikman makes it easy. now that matrix basically have bridges for every other chat platform  when i asked whats holding people back from switching to it in couple of rooms all of them basically said this,  stikman comes with all the features you would expect from it sticker sharing, creation etc... .now its one less thing to worry about when choosing your preferred chat program , thanks for all the feedback and support, i'll try adding more things to the documentation and support for animated stickers.
LOL love it! I will try
What windows manager is this? It is giving me Enlightenment vibes.
What's the difference between a sticker and an image? Is it just that u can search for it faster?
Nice;)
its not transparent i blurred the static background :P
> make the chat background transparent

You can do this with a compositor like `picom`. See: https://wiki.archlinux.org/title/Picom#Opacity
In a lot of chat applications stickers are like emoji but bigger and more custom. Generally you can only use a given set of stickers within the app they were made for. This is an attempt to make it easy to use the same sticker set across multiple chat apps.
i rekon there's a lack documentation, basically what u/featherfurl said [here](https://www.reddit.com/r/linux/comments/vzprqd/comment/igcc5ft/?utm_source=share&utm_medium=web2x&context=3),

> In a lot of chat applications stickers are like emoji but bigger and more custom. Generally you can only use a given set of stickers within the app they were made for. This is an attempt to make it easy to use the same sticker set across multiple chat apps.


one of the reason i created stikman was if you use more than one chat platforms you cant use your stickers from a with b and vise versa, and lack of proper support for stickers in platforms like element/matrix, its a lot of work and you need a self hosted server for it to work, stikman makes it easy. now that matrix basically have bridges for every other chat platform  when i asked whats holding people back from switching to it in couple of rooms all of them basically said this,  stikman comes with all the features you would expect from it sticker sharing, creation etc... .now its one less thing to worry about when choosing your preferred chat program , thanks for all the feedback and support, i'll try adding more things to the documentation and support for animated stickers.
thanks
its my build of dwm, repo: https://gitlab.com/sinanmohd/dwm
Stickers generally have their own "sticker button", just like emojis.  
You also can't send arbitrary stickers, as opposed to images. Only from available sticker sets. Stickers often also represent an emoji, so its typically faster to search for them / send them, yeah.
thanks
no, the compositor controls the entire window, it can't just control a part of it, the compositor has no clue what the window is
How does sending a sticker differ from sending an image?
THanks for the explanation!
Probably not much apart from consistent formatting and maybe some custom behaviours depending on which app implements them.
On Telegram you can also send vector-based animated stickers, you can't do that with images.

So it's more like sending [Lottie animations](https://lottiefiles.com/), but I think OP's application has no support for that.
thanks, i'll look into it
I'm the primary person behind the project.  Feel free to ask me questions about it.
I can only imagine hopping on Discord or IRC with a question and joining a Debian channel, explaining you have Arch kernel and getting told you are in the wrong place. Go to Arch and they tell you it is a Debian issue. No accountability because you put a Ferrari motor on your helicopter.
I've used it. Didn't do anything crazy. Mostly used it as a way to get newer software packages on a system which originally had a stable/older base. Like putting a game from Arch on a system which was originally Debian.
I use ubiantoorch btw
I know there's a saying about looking a gift horse in the mouth, but there's also a saying about things that seem too good to be true.

What's uh... What's the skeleton in the closet here?
It's a cool project for sure but for those interested, Distrobox can achieve a similar end product if you don't mind using Podman or Docker containers to run some stuff.

As the name implies you can have easily entered containers of any distro you choose, with some tight integration including for GUI applications.
Debian: Don't make a FrankenDebian

Bedrock: Haha, hold my 49 other distros
I used bedrock at my old job where it was mandatory to use ubuntu, I was able to keep the ubuntu stratum for compatibility and at the same time have my gentoo stuff, it was great.

Thanks for all the answers the dev has given and I look forward seeing what 0.8 brings to the table
Security through Oh My God What The Fuck Is This
I use bedrock on my daily driver desktop. It helps me out for work, where I need to use CentOS or Ubuntu packages for backend/cloud/devops stuff (we do most of our deployment onto AWS using either Amazon Linux or Ubuntu). But my kernel, init system, graphics drivers and pretty much everything else non-work related runs on my Arch install (which I had going for years before bedrocking it).

I'm super satisfied with the outcome. It was painless to install. I prepared myself to be troubleshooting, but I simply ran the installer and within the hour I had it working with my original Arch and a new Ubuntu stratum. It's a fantastic project and an invaluable component of my system that keeps me productive.
I haven't used it extensively, but I have tried it. It's a great idea and the people behind it deserve huge credit for making it work and keeping it going.
I was looking for something like this! Gonna try it
"what distro do you use?"

"It's complicated..."
Frankenstein's distro
It sounds like a good idea, but I have a feeling its a nightmare to troubleshoot.
God f*cking damn it. Why do I continue to find awesome projects when Ive sold all of my gear??

Well, I'll be sure to recommend to my friends
I'm absolutely going to try Bedrock in a VM. It sounds super interesting.
Ubuntu + Arch, this is awesome
I suppose I can't mentally see anything wrong with running some form of Debian distro and using pacman to install the latest linux kernel available on the Arch repo, as long as your setup is ok with that package writing to /boot and your bootloader knows to use it.

The millisecond you start mixing and matching libraries from different distributions and enough packages for the streams to finally cross you're toast though. But small things like the kernel assuming there's no severe compiling differences it sounds fine enough.

And even then, things like fonts are just going to unpack to some directory for software to use as well. noarch stuff like that will be fine but again I cannot imagine the stability anyone would run into the moment they start mixing sources without one of the many sandboxing solutions out there at the moment to prevent software stepping on each others toes (libraries and other shared resources).

Typically one picks a distro for a more ideal out of box experience and anything outside that is either in the repos they maintain with a package manager they work with, or you're compiling from source (AUR is a good middle ground). Picking this on purpose would feel like somebody misunderstanding the reason to pick some distros over others. I mean, Arch will let you install dnf and dpkg... if you really want to use those packaging resources and be totally on your own when something goes wrong (Or at least to help with, repackaging and managing them despite not being the install destination of choice).

I don't know. To claim all these dot points with a smile makes me assume they're doing some form of compatibility segregation on these multiple sources. They would have to be to avoid major problems in a large enough install, no?


>How does Bedrock Linux Work?

>Bedrock has different strategies for different subsystems. Its most widely used strategy is to:

>    Organize the system's files and processes into units called strata. Think of these as chroots with selective holes punched in them.

Yeah that entire section makes sense for how they're preventing meltdowns. Could be interesting to play around with. I also find it interesting how it hijacks an existing install rather than providing its own booted installer. Interesting is certainly a word for this project...
I remember when Bedrock Linux first came out, how it was commonly known by some as Parasite Linux or Zombie Linux. üòÇ  On the account that it is a distro that hijacks other distros. Personally, I admire the concept as it was an interesting way to streamline Linux in general.

The idea of using openSUSE Linux and possibly using some Debian/Ubuntu developments or perhaps Arch, seems both amusing and a little crazy to me.  I have been tempted to give it a try.
It‚Äôs pronounced Frankensteen. :-D
I used to use it for having some AUR packages on fedora, although at one point I did add every distro it official supported just to freak out neofetch xD

I did stop using it after I got too bored in class and started putting the things that I used from the AUR in rpm packages, but it's still pretty cool
Used it just so I could have my at the time void setup, runnit and some pkgs from xbps I believe, but void reps aren‚Äôt as cutting edge as arch and arch‚Äôs reps are way bigger indeed, then added arch for its reps with bedrock and I found it very straight forward, wouldn‚Äôt call it perfect but it just worked as their website said, I was aware that wasn‚Äôt the only way to get the pkgs I wanted on void because you can compile them I think but it just seemed interesting. It worked great from what I remember, of course for what I was intending there was also artix (runnit and arch reps) which you could argue that was better, less complex and no performance overhead, but there was something about bedrock that I found charming. Well that was a story
Yes, Gentoo/Fedora/Ubuntu/Arch/OpenSuse. 

I was very tired of not having software in the repos. Those distros cover 99% of anything I could ever want, especially the AUR and OBS. 

I don‚Äôt recommend it though. Most distros do not like to run on a custom kernel, and Ubuntu REALLY does not like selinux.
Its like someone read about [Franendebian](https://wiki.debian.org/DontBreakDebian) and was like... I think I will.
I've done several installs and it is *great.*  My only regret is I don't have a use for it because it is a technology I wanted for a long time.  And, as seen, /u/ParadigmComplex is very engaged with the community.
Yes! I created Ubuntu with GNOME 42 using Arch's kernel for WiFi support. Worked fine
Was using it for a short month until an update to the Bedrock base broke the boot beyond repair. Cool project but I failed to find a compelling use of it to be honest.
I installed it in a VM and played with it a bit.

Absolutely destroyed the system within an hour.
Ive tried it and it seems to be slow unlike all other distros ive tried, it has a little overhead but i dont want that :/
Is this necessarily a bad thing? I think it‚Äôs a good thing, seems pretty fool. I didn‚Äôt know about this, I‚Äôll look into it now
I would like to try it 

Can anyone point any resources
I'd use it if it had Fedora's systemd init system
I thought I had stumbled into /r/Minecraft
Sounds like NixOS but more ad-hoc by pulling from other distros rather than being deeply configurable and supporting a variety of systems foundationally. With NixOS for example I could use multiple different package channels for my system packages to get whatever kernel version plus various locked program versions, and then there are many tools to convert from deb, rpm, or aur packages into nix so that they natively integrate.
Someone tagged me in this post, but no... this isn't what I'm doing. Using alternate desktop environments, that are mostly found in the distro's repository, isn't the same as what is described here.
Is that a solution for running ROS on a new kernel?
Sounds dirty.
Time to do LFS just to hijack it for bedrock /hj
I tried it a few months ago, and when I rebooted, I got kicked into a grub shell. I couldn't boot. I tried repairing grub, but couldn't. I had to reinstall. I didn't feel inclined to try again.
I've done this with that bedrock to a tiny degree. My Manjaro install supports deb & rpm. It also can run apt but I don't want to touch it because it's apparently unstable as heck and I don't want to destroy my system. Though this isn't what you're talking about this is the closest thing I've done.
I bet upgrades will be a pain.
ƒ∂
Bedrock, alma o Rocky Linux ? which one is better or that replaces centos.
Docker is a thing
Gentoo by itself and installed has been enough for me.
r/bedrockmoment
I feel like it'd make dependency hell.
Sounds like a stripped down basic version of backbox linux.
Why is bedrock needed in 2022? We have docker that allows us to run just about any Linux distro in a container? Something doesn't play nice with docker use virtualbox, kvm etc to run it virtually. Need an older version of an app run it in docker with the custom lib and package versions as needed. 

What if a distro needs python 2.7 for package manament but other distros use python3.x both expect python to be called "python" how do you cope? What about different glibc versions between distros? 

Most packages that can be built with source need 3 commands to build and install? Why another distro?

Mixing these all together seems like a recipe for settle bugs.
That's awesome, its a super clean experience, the tutorial is helpful and easy to follow. I like that the package manager manager can be configured to work like whatever PM I want.
Looks like a very interesting project but I think most people here right now are still trying to wrap their heads around the concept (like me) haha
Honestly bedrock is a really cool project and idea, just I really struggle to find a use case for me to justify using it myself. Like best I can think of would be access to aur packages from my fedora base. But I can usually find everything I need in the repos or flathub. So I'm genuinely not sure what I'd use it for except for tinkering but that'd probably be better to do in a VM rather than my actual install. Maybe at some point I could find a use case for bedrock for myself
Did you ever think about doing a PhD in computer science (if you don't already have one - which wouldn't surprise me if you did have one) using bedrock in your thesis? What you have done (by basically yourself if I read your website correctly) is a treasure of ingeniosity ~~or sheer fucking crazyness~~
I have but only one, and it‚Äôs very simple:

# how
I have thought about doing similar things with nixos. 
How does your approach compare to nixos's way of using different dependency trees for packages from different distros?
How do you handle post-install hooks from other distros that modify i.e. /etc and may thus break existing packages/services?
Thanks for maintaining this project :)
How many people are involve in a project on a daily basis. Do you have stable group / team of devs, is just you as a one man army.

If it is not a weekend project of one man then how to support you, any channels of support out there?  


Im asking because I've never heard of a project
This is going to seem random..
Would this work on a big endian PowerPC?
When will Java Linux come out?
Bedrock is a really cool and interesting project. Congrats on pushing it to where it's at, it's a huge feat!

Can you explain how an init from a specific stratum (hopefully I'm using it right) can start services from another stratum?  
Are there shims that link inits and you're actually running multiple inits?
Can i install rpm,dnf,pacman,and apt together?
Also, can i be a newsletter for every bedrock linux update?
How did you develop what must be the patience of Buddha?
How does it feel being a prophet?
Can I play Minecraft bedrock on it?
Do you feel like a mad scientist?
Did you look at NixOS by any chance?
I just want to say that your project reminds me of when I was a child, when I would stumble upon things that I really wanted, but then discovered a flaw/absence of a trait found in another thing (e.g. armour in a game that's well rounded, but looks ugly, or a beyblade being a cool attack type, but ultimately useless against anything good). Your project basically says "Why *not* have the best of both worlds?" Hell it's given me some inspiration to write my own package manager someday if I ever get the time (mix of Gentoo's portage and Nix/Guix).
Hi! Thank you for bedrock linux, I've been keeping an eye for it for a while now and am willing to try it asap! 

I'd like to ask your favorites mixes of components and if there are some that are particularly recurrent/appreciated in the community
Very epic project. Kudos!
What's the difference between bedrock and something like distrobox?
Thank you and big respect!
1. Were you so preoccupied with whether you could, you didn't stop to think if you should?

2. Has science moved too far?
You sir are a savage, and I love you for it.
I want to put this on my Legion laptop, I have Windows as a base install and have been working with Ubuntu WSL on the windows platform to test out other Linux options.

I don't want to blow up the original windows install on my Legion laptop just yet, just until I can be sure to match all the gaming capabilities and ease of use.  So that is why I am using WLS.  

But I have a second drive that I can make native for another Linux install, like Bedrock, to test it on bare metal.   WLS has some issues that are related to init() that are not the same as when I install on bare metal, I don't understand it completely but I think it has something to do with windows being the native OS.

Any help or leads on URLs would be appreciated.  I am very excited to try out Bedrock
Two questions:

&#x200B;

o Is it possible to duplicate a Bedrock deployed configuration programmatically? (think Ansible or other scripting or "golden image") - pretty sure you could redeploy the root filesystem with **fsarchiver**, but it's a matter of being able to recreate from the ground up

&#x200B;

o Is it possible to pretty-print a summary of existing strata and the parts that are "contained" by it? Think of a nice chart, graph, tree, something that could be used as a presentation to breakdown what belongs to what - this would be very useful going forward to keep track of things

&#x200B;

Example:

&#x200B;

Base "hijacked OS": Debian Stable

\-- Strata: Ubuntu - Installed software: \*.deb, kernel

\-- Strata: SuSE - Installed software: \*.rpm, kernel, etc

&#x200B;

\--If nothing like this currently exists, I would be willing to volunteer to help write at least a **bash** script to discover and print. Future goal would be to print with nice formatting and maybe some colors

&#x200B;

Note - I haven't tested Bedrock yet, but it's on the TODO list ;-)
This sounds amazing. Honestly I'd love the stability of like Ubuntu but I have a 2nd partition for Manjaro just for access to the AUR. Does bedrock have the ability to do this? Awesome concept you are working on by the way!!
Does it work with stuff like package managers etc?

I'm kinda confused, what exactly is interchangeable and what isn't; and how hard is it to run?
Can you ELI5 how this is even possible? lol This is really cool, but I don't understand how these things can be made so seamlessly compatible.
Do you have an ETA on 0.8?
Your imagination aligns with reality here.  Support _is_ a concern, as touched on in the project's FAQ: https://bedrocklinux.org/faq.html#why-not-use-bedrock

Other distro communities have no obligation to assist with issues that arise with components they provide when utilized on Bedrock Linux systems, and Bedrock's community and support infrastructure is frankly too small to provide the kind of and quantity of help some users need.

If a user is certain the issue isn't with Bedrock but rather a component from some other distro, the best bet is to reproduce it in a VM/container/etc which contains the distro in question alone - no Bedrock - and go to the corresponding distro's community with that.  If it's plausible the issue the issue is with Bedrock, the only route is to go to Bedrock's community and deal with the fact it may not have the resources to help.  In practice, this may mean Bedrock is unsuitable for less experienced Bedrock users who require a lot of such assistance.

Your vehicle part swap analogy is exactly how I've historically explain Bedrock to non-Linux-savvy family and friends.  Engine from one car, transmission from another, suspension from a third, etc.
Funny analogy but yeah this would be a pretty big issue if you had any
That is funny
this is exactly how it feels to be a trans woman receiving medical care. constantly need to explain that I have an arch kernel but I'm not running arch
That's still pretty freaking cool.
I spent about a minute trying to master saying that name.
See the FAQ: https://bedrocklinux.org/faq.html#why-not-use-bedrock

In my experience providing support for it, the biggest issue people run into (aside from just not reading the documentation) is the resulting complexity.  Some inexperienced users can get a bit overwhelmed with traditional distros; giving them the ability to get stuff from multiple ends up being yet more rope to hang themselves, so to speak.  That having been said, Bedrock does try to give users accustomed to Linux systems both a framework for how to think about a system composed of multiple components from multiple distros as well as utilities to manage it to lessen (but not resolve) the impact of this concern.
sounds like a house of cards.
Not all things work, and doesn't work with some distros.
glibc and libstdc++ ABI compat nightmares, running software against versions of system libs that software wasn't compiled against is a bad plan. See [the std::string update](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html), which still keeps some sysadmins up at night
[deleted]
Have you considered the scenario where you look in the horses mouth... and the teeth are sharp?
Every combination is literally just sudo install something with every force flag imaginable.
From what I've seen, distrobox can be suitable as an alternative to some of Bedrock's more trivial use cases, but it doesn't seem to be intended to for most of the use cases for which I actually see people using Bedrock:

- From my experience providing support for Bedrock, a lot of users inquire about Bedrock's ability to do things distrobox does not, as far as I can tell, address, e.g. get things like the installer, kernel, or init from different distros.  I also field a lot of inquiries about things Bedrock can't make just-work _yet_ but have improvements on the roadmap, like Desktop Environments and dkms.  (No one asks about Bedrock's ability to make `info` pages to just-work across distro boundaries >.>).
- From my experience providing support for Bedrock, a lot of users inquire about the edges cases for where Bedrock can and cannot automatically integrate things across distro boundaries.  In contrast, distrobox doesn't seem to automatically integrate _anything_; the expectation is users manually do what distrobox refers to as "export."
- Distrobox uses a host/container model, where containers run on top of the host.  You're stuck with the host's essential bits for the duration of a given install.  Bedrock's model is strictly more powerful: you _can_ pick a distro to get all the essential bits from and stick with it and think of it as your "host," but you can also swap any or every bit of the "host" out later if you change your mind.  Admittedly fully leveraging this doesn't appear to be extremely common in the Bedrock community, but I do see enough of it to be of note.
- Bedrock also offers qualify of life utilities to help manage a system composed of parts from multiple distros, like `brl which` and `pmm` that I see people exploring quite a bit.

That having been said, I know of three advantages for distrobox:

- You can easily uninstall distrobox's tooling, but not Bedrock's. This is because Bedrock lets you get essential parts of the system from different distros in which case removing the glue would break the system, where distrobox's comparatively limited functionality doesn't as readily let you integrate stuff deeply enough for the system to become dependent on distrobox. The main value here is ease of trialing the item in question: with distrobox you can install it on your production box, try it, then toss it if you don't like it. With Bedrock you should probably use a test environment like a VM, which is comparatively a minor hassle.
- Provided systemd host and containers, distrobox can export contained services to the host. Bedrock's planned equivalent is on the roadmap for 0.8.x and isn't ready yet. Bedrock's planned equivalent is over-all more ambitious, as it includes both cross-init-system functionality (e.g. running runit services on systemd and vice-versa) and (potentially optionally) automating the distrobox equivalent of the exporting step.
- Bedrock's setup does confuse a handful of tools such as timeshift.  Distrobox's comparatively limited scope means it doesn't try to do things that confuse timeshift.

If distrobox is better suited for someone's use case, more power to them.  I honestly, whole heartedly prefer a user go with what's best suited for them for things like this.  However, I don't know that raising it as something which can provide a similar end product is something which should be stated without quite a bit more qualification.
That sounds p interesting, thanks for the word!
This speaks to me.
Keep us posted. If nothing else, you probably learn something!
Dude find you an old think pad, or something and have some fun.
If it helps:

- Bedrock's system requirements aren't particularly high; it's mostly limited by the system requirements of the parts of the distros you're getting things from.  In practice the main limitation is disk space, as a quirk of how it works requires some redundant copies of files.  I run it without issue on a Raspberry Pi 3b+.  You don't need to spend a bundle to play with it.
- There's no real pressure to try it out this very moment.  The longer you wait to get to it, the better it'll be.  The upcoming 0.8.x series looks like it'll be a major improvement over the current 0.7.x one.
The more I understand about bedrock the more I realize the homie who invented this is a freaking genius
> I remember when Bedrock Linux first came out, how it was commonly known by some as Parasite Linux or Zombie Linux. üòÇ On the account that it is a distro that hijacks other distros.

You may be misremembering.  When Bedrock first came out, and for years following, it didn't offer a hijack install.  That concept wasn't even on the roadmap; I didn't have any idea how to, even in theory, get Bedrock to use another distro's installer.

[Maybe you're thinking of something like Frankenstein Linux?  Variations on that theme are the closest thing I can think of which Bedrock was called when it first came out](https://lwn.net/Articles/515958/).  The ability to mix-and-match features of other distros is its defining trait; the hijack thing is an implementation detail of how it does this for just one of its features.

> Personally, I admire the concept as it was an interesting way to streamline Linux in general.

I appreciate the admiration, and I'd love to take the complement, but I'm not sure streamline is the right word for Bedrock.  Bedrock's biggest issue is that it's more complex than traditional distros.  This is fundamental to its nature; combining features from distros _X_ and _Y_ is going to be more complex than either _X_ or _Y_ would by themselves.

> The idea of using openSUSE Linux and possibly using some Debian/Ubuntu developments or perhaps Arch, seems both amusing and a little crazy to me. I have been tempted to give it a try.

[OpenSUSE's default setup of GRUB+BTRFS trips up when the current Bedrock 0.7.x series is involved](https://bedrocklinux.org/0.7/feature-compatibility.html#grub-btrfs-zfs).  I have plans to work around this with Bedrock 0.8.0; consider holding off until then.
> Was using it for a short month until an update to the Bedrock base broke the boot beyond repair.

Did you report it?  I can't recall Bedrock ever having such an issue.  I take this kind of stability concern _very_ seriously.  If you can provide information adequate to reproduce it, I'll make it a priority to look into.

> Cool project but I failed to find a compelling use of it to be honest.

I think most users won't find one.  Most distros are as they are for good reason: that's what's best for most users.  Bedrock does have an audience that does meaningfully benefit from it, but it's relatively niche.
Same. I can't really think of a use case that isn't already satisfied by at least 3 or 4 existing distros. 

But I'm all for variety and wish the project well.
Did you make an effort to understand Bedrock first, e.g. work your way through the documentation or tutorial, or did you just bang your head against it while wielding root?  The latter could break most distros and arguably isn't useful news to anyone here.

Properly utilized, Bedrock is in some ways actually more robust than traditional distros:

- You can have multiple instances of essential parts of the system (e.g. kernels, inits, etc) such that a bad update to one of them won't make the system unbootable.
- You don't have to do a release upgrade in-place.  You can setup a new subsystem corresponding to the new release and test each component to confirm they work while not only keeping the other ones around, but actively in-use.  Only remove the old one once you've confirmed the new stuff is good.
As noted in the FAQ (https://bedrocklinux.org/faq.html#why-not-use-bedrock) there _is_ some performance overhead, but in practice its negligible for most workflows.  Do you know what you were doing that made the overhead noticeable?  The majority of the time people raise this it's either buggy software from some other distro (which in the past has lead to productive bug reports and resulting fixes in the software at fault) or a workflow that's trivially adjusted once identified (don't run a production database out of `/etc`).

I have ideas for the next major release to make identifying the culprit these rare instances much easier.  I don't know that we can ever fully remove the overhead, but we should be able to minimize how often it becomes a problem and improve handling of it.
The official website: https://bedrocklinux.org/

I've gotten good feedback regarding it's interactive tutorial as an introductory resource.  Consider [installing it in a test environment like a VM or spare machine](https://bedrocklinux.org/0.7/installation-instructions.html) and running `brl tutorial basics`.

I think it good idea to skim if not thoroughly read much of the documentation as well.  At a minimum, things like [the introduction](https://bedrocklinux.org/introduction.html) and [FAQ](https://bedrocklinux.org/faq.html).
Obviously the official website but YouTube has a good video of someone installing it too
Bedrock Linux does support Fedora's systemd init system.  I now await your Bedrock Linux neofetch screenshot with `rpm` in the package list.
You're right that Bedrock does not offer anything comparable to NixOS's very, very cool configuration system.

I'm not completely sure I know what you mean by "supporting a variety of systems foundationally" but naively that sounds like something Bedrock actually does do.  Originally this was more of an instrumental goal needed to avoid conflicts between parts of other distros, but over time this became a valued part of Bedrock in its own right.

Such conversion tools may certainly be adequate for users with limited use cases, but in my experience they have limitations that can become annoying as things scale should a user need quite a lot of things from different distros.  Bedrock doesn't have to convert in such a fashion; for the most part it uses things as-is.

I have other things I need to prioritize in the immediate future, but over the long term I'd eventually like to get Bedrock and NixOS to integrate nicely.  Ideally the contrast shouldn't be about one versus the other so much as NixOS alone vs Bedrock+NixOS.  I'm not sure how much that will appeal to Nix-first people, as the parts from other distros will likely not be covered by Nix's incredibly cool configuration system, which they may find displeasing.  However, I think it'd be neat for other-distro-first people who could utilize Nix's configuration system for parts of the system that don't share whatever requirement is keeping them off NixOS today.
Could you elaborate on what you are doing?  It sounds interesting.
Like in a good way?
No one is that based.  But if you do please tell the group
It's PMM is a packet manager manager that acts like apt or pacman or rpm you caN choose
Did you read the post? Bedrock is definitely not that. Alma and Rocky both act as drop-in replacements for Centos 8. It doesn't matter which one you pick, but if it's for a server you expect to still be running in 5 years time, I'd say the smart money is on Alma being the one that survives as a project.
Docker, podman, et al are great to get access to userlands with their own, separate environment.  As demonstrated by projects like subuser and distrobox, you can poke holes in the containers to get _some_ degree of integration across them / with a host system.  If that's adequate for your needs, more power to you.  However, it isn't a suitable alternative to any but the most trivial Bedrock use cases.  Consider, for example, init systems: with traditional distros + containers, you are expected to boot with host system's init.  Absent a reinstall, you aren't really expected to change the init.  With Bedrock, you can reboot to swap inits at will.
A naive approach certainly would, but this concern has been taken into consideration and resolved in its design.
> Why is bedrock needed in 2022? We have docker that allows us to run just about any Linux distro in a container? Something doesn't play nice with docker use virtualbox, kvm etc to run it virtually. Need an older version of an app run it in docker with the custom lib and package versions as needed.

Containers are great for stand-alone, self-contained things, but they aren't good solutions for scenarios where you want things to interact.  Having a font in one container and a GUI application which could display the font in another isn't super useful, nor is installing a kernel in a container.  If you want to install your OS with a given distro's installer, running that distro in a container isn't going to help much.

Bedrock's value is in making the pieces integrate together so that it feels like one cohesive system.  Not everyone has a use for things from different distros to interact like this.  Bedrock targets a niche audience.

> What if a distro needs python 2.7 for package manament but other distros use python3.x both expect python to be called "python" how do you cope? What about different glibc versions between distros?

Both these situations work fine.  The project's whole point is to overcome these kinds of problems.  This is like asking how the Wright brothers cope with gravity.

> Most packages that can be built with source need 3 commands to build and install? Why another distro?

This isn't rhetorical or sarcastic; please actually think about this: if people can build everything themselves from source so easily, why do distros exist at all?

Now ask yourself if your answer to the previous question continues to apply when scaled down to a smaller set of items being considered for distribution rather than everything.  Does it still apply for half of a distro's packages?  A quarter?  1%?  Just one package?
Happy to hear it :)
Bedrock is a very different way of thinking about Linux distros, and difficulty wrapping one's head about it is common.  It's definitely not just you.

I prioritize efforts to clear express Bedrock's concepts just as highly as I do its technologies, and frankly it's a comparably difficult problem.  The best feedback I've gotten yet on this avenue is from the interactive tutorial.  Consider trying Bedrock out in a VM or spare machine and running `brl tutorial basics`.  Hands-on experience seems to help.

I have ideas to reframe things that I'm planning on documenting with the future 0.8 release.  I'm hoping they further help.  Sadly I can't go into them now for fear of confusing users with things that don't quite align with the existing 0.7 framework.
If you do find a use case, I'll do my best to make Bedrock a good experience to fulfill it.  However, I think most users won't find a real use case.  Traditional distros are as they are for good reason: that's what's best for most users.  Bedrock does have an audience that does meaningfully benefit from it, but it's relatively niche.
> Did you ever think about doing a PhD in computer science (if you don't already have one - which wouldn't surprise me) using bedrock in your thesis?

I've thought about it, yes.  My follow through hasn't been great, though; it always feels like something else is a higher priority in the moment such that I never get around to it.  I should probably revisit my priorities or time management.

> What you have done (by basically yourself if I read your website correctly) is a treasure of ingeniosity ~~or sheer fucking crazyness~~

;)
Bedrock uses different strategies for different problems, but the most common one is to:

- Organize all of the systems files and processes into units called "strata."  This is useful both to help users and Bedrock track where everything is.  These are commonly but not necessarily one-to-one with distro installs; you could have an Arch stratum, a Gentoo stratum, etc.
- Make resources (executables, fonts, man pages, etc) available across stratum boundaries via a special directory that generates virtual files on-the-fly (think `/proc`) to make resources accessible across stratum boundaries.
- Resource producers (e.g. package managers) are not told about this cross-stratum directory.  Thus, they usually do not conflict with each other.  Think the way things act in chroots/containers, but _just_ select programs like package managers and not the whole stratum.
- Resource consumers are configured to look up resources in this cross-stratum directory.  For example, cross-stratum binary locations are added to the `$PATH` so things like `bash` can find them.

There's a _lot_ more to it than this, but that's the most commonly utilized solution.

It's common for me come up with something in R&D to unblock access to some feature from some distro previously inaccessible, but which requires a complete change in Bedrock's architecture.  Not only does the new feature require a new strategy, but it might break previous ones and require different strategies for things that previously worked.  Given this, any in-depth documentation on how Bedrock works will quickly become out of date.  Once we approach 1.0 stable and this kind of architectural churn slows down, I plan to release a white paper explaining how Bedrock works in depth.
> How does your approach compare to nixos's way of using different dependency trees for packages from different distros?

I'm not as familiar with Nix today as I should be, and so I may not fully understand your question here.  I'm hoping to learn Nix better eventually, if for no other reason than to add NixOS support to Bedrock.

Bedrock has subsystems ("strata") that are usually but not necessarily one-to-one with distro installs.  Each manages its own dependency tree(s), just like it would on the native distro.

Bedrock offers a package manager manager to manage multi/cross package manager workflows, but that largely just forwards the dependency responsibilities to the underlying package managers.

> How do you handle post-install hooks from other distros that modify i.e. /etc and may thus break existing packages/services?

Can you give a concrete example of such a concern?
You are welcome :)
> How many people are involve in a project on a daily basis. Do you have stable group / team of devs, is just you as a one man army.

Early on other people - mostly those I knew in person - contributed meaningfully such that, while I did _most_ of it, I wasn't the only one.  However, it's been years since someone with adequate background made a meaningful contribution, and it's functionally turned into a one man army.  LUGs just aren't what they used to be.

> If it is not a weekend project of one man then how to support you, any channels of support out there?

I think a one man nights and weekends project is an apt description these days.  If you mean financial support, look on the project's home page for "tipping" in the navigation bar.  Note the project isn't limited  by finances; I'm gainfully employed and can cover the associated costs out of my own pocket without much pain.  A contribution won't necessarily help the project itself.  There's really no pressure to give here, particularly given how the world's economic state seems to be going right now.  However, if one insists, consider it more like buying me a beer.  Or, per my tastes, a fancy import tea.

> Im asking because I've never heard of a project

That's intentional.  I've been purposefully balancing marketing growth with considerations like user base support time costs; of late I've been withholding marketing efforts.  Most people who _have_ heard of it today did so via word-of-mouth that I can't control.  While Bedrock works well enough to be a daily driver for many, there's still a lot left to do, and I'd rather do that than answer questions or provide support work.  That having been said, if there _are_ questions or support needs, I'd much rather answer them than leave them lingering, even if that takes a bite out of development time.  When Bedrock is far enough along I plan to push for it to better known.
Depends on how many skill points you put into Computer Use.
Probably!  Bedrock is all about choice, and such choice considerations do extend in principle to instruction set architectures.  [I build binaries for quite a number of ISAs, including both 32-bit and 64-bit big-endian PowerPC](https://raw.githubusercontent.com/bedrocklinux/bedrocklinux-userland/0.7/releases).  However, I've only ever tested the PowerPC builds in qemu, and I don't test them very often or very thoroughly.  IIRC I've had exactly one person report back about how it worked on actual ppc64 boxen, and they reported no issues, but that's not a great sample size.

Bedrock has a `brl fetch` feature you'll run into if you give it a try.  Even if all of the code works, `brl fetch`'s feature set here is going to be limited compared with the more popular ISAs.  Make sure you're familiar with `brl import` as an alternative option.

I'm considering reworking Bedrock's ISA support for the next major release, possibly doing things like adding riscv64gc and dropping i486.  Retaining this level of PowerPC64 support - put out builds, limited/no actual testing - remains the plan as far out as I can see.  It's actually one of the harder ones to support due to a quirk in how musl-libc interacts with PowerPC64's representation of long double floats, but once I have it building the marginal cost to continue to build it is trivial.
[Did you know that there was a Go! programming language before Google named its Go programming language?](https://en.wikipedia.org/wiki/Go_\(programming_language\)#Naming_dispute)  I felt sorry for Go!'s developer and did a lot of research to find an open name in the Linux context so I wouldn't clobber someone else's.  At first there [was nothing wrong with Bedrock's name, until Bedrock was some number of years old...](https://www.youtube.com/watch?v=ADgS_vMGgzY)
I don't know the state of it now, but there was a time when Solaris had a ton of its components written in Java. I'm not sure what made the cut when OpenSolaris transitioned to a Linux base. Still not really "Java Linux" but that's probably the closest thing.
> Bedrock is a really cool and interesting project. Congrats on pushing it to where it's at, it's a huge feat!

Thank you :)

> Can you explain how an init from a specific stratum (hopefully I'm using it right) can start services from another stratum?
> Are there shims that link inits and you're actually running multiple inits?

Your use of the terminology looks spot on to me.

Like a traditional distro, only one init runs at a time.  What Bedrock does differently here is let you do is select which you're using for a given session when you boot, similar to how bootloaders let you pick your kernel for the session.  You can install new inits and reboot into them at will, all while keeping the non-init stuff the same.  Same users, same `$HOME`, same applications, same fonts, etc.

As of the current release, each init just sees its own service configuration.  Arch's systemd sees Arch's unit files, Debian's systemd sees Debian's unit files, Void's runit sees Void's run directories, etc.  No cross-stratum services out of the box.  You can switch your init, but you'll also switch the set of services that are configured/enabled at the same time.

[It is usually possible to manually create cross-stratum service configuration, in which case they _do_ work, but there is limited documentation here and this can be tricky for less experienced users.](https://bedrocklinux.org/0.7/feature-compatibility.html#init-configuration)

Getting cross-stratum services to just-work is on the roadmap.  My current Bedrock task is completing prerequisites for this.  Bedrock already has subsystems which:

- Generate files on-the-fly which make resources (e.g. executables, man pages, fonts, etc) accessible in a cross-stratum portable manner.
- Selectively configure software to see these cross-stratum resources

The eventual plan for cross-stratum services is a continuation of this design.  We'll need to add code which reads service configuration (e.g. systemd unit files, runit run files, etc) and generate portable versions of them.  Then, separately, configure init systems / service managers to recognize them, e.g. symlink the generated cross-stratum version in place.  There's some non-immediately-obvious concerns like avoiding enabling redundant services (e.g. if one distro provides getty and another provides agetty) which need to be taken into consideration as well.

The end goal should feel like there's one set of services managed by one init/service-manager across the whole system.  If you reboot into another distro's init, the same services should launch, just managed by a different service manager.

It's not obvious to me that we can make every case work.  For example, I think some software has a hard dependency on specifically systemd which nothing is providing if you're using another init.  I'm content to make as much work as possible, and to document the known limitations.  Over time, push the boundary as far as we can.
> Can i install rpm,dnf,pacman,and apt together?

Yes.  In fact, I have all of those on the very machine I'm using to type this message to you.

> Also, can i be a newsletter for every bedrock linux update?

Assuming you a word there and you don't literally want to be a newsletter, there's an atom feed with news updates that might be what you're looking for: https://bedrocklinux.org/atom.xml

Sadly the current release series has a lot of noisy updates for churn on a subsystem that isn't very important.  I'm planning on sub-versioning different components for the next major release so that I can only publish update news about important updates.
The same way the Buddha did, actually: lots of thinking about philosophy.  I have it way easier than the Buddha did, what with access to thousands of years of and an entire globe's worth of preexisting philosophical thought.  I'm particularly partial to Stoicism where the phrase "patience is a virtue" is of heavy consequence.

Pertinent to Bedrock is the [philosophy of identity](https://en.wikipedia.org/wiki/Identity_\(philosophy\)).  If you install one distro, and swap out every individual file, is it still the same distro?  I don't want to go through the hassle of changing it now, but in retrospect I think something like [Theseus OS](https://en.wikipedia.org/wiki/Ship_of_Theseus) would be a better name for the project.
As I understand it, most of the biblical prophets (with the notable exception of Isaiah) didn't _want_ to be prophets; they tried (and failed) to dodge the responsibility.  My theory is they were running from the user support work.  The documentation _clearly and repeatedly_ says don't make an idol, why do I see a golden calf in the system logs? :(

Non-biblical prophets like Major Barnes from Crysis didn't have a good time, either.

That having been said, if I had to do things over again, I wouldn't change anything.
Yo dawg I heard you like Bedrock, so I put Bedrock in your Bedrock so you can Bedrock while you Bedrock.

tl;dr: yes.
[I can certainly relate to the excited rush of seeing a crazy patchwork contraption I've created actually come to life.](https://www.youtube.com/watch?v=1qNeGSJaQ9Q&t=155s)
Yup!  Various disorganized thoughts:

- Its reproducible-from-expressing-configuration thing is extremely cool.  My use of Bedrock instead of NixOS is a conscious trade-off there where I know I'm giving something up.
- Last time I looked at it - admittedly, it's been a bit - its repos lacked a lot of things that I, personally, want.  A trivial example is init system considerations.  If I have to choose between the two, I do prefer Bedrock.  However, I don't fault anyone that prefers NixOS.
- Ideally the choice shouldn't be NixOS xor Bedrock, but rather NixOS alone vs Bedrock+NixOS.  A quick and dirty attempt to Bedrock to play nicely with NixOS didn't work out.  I hope to revisit it with more time and in more depth down the road.  Some of the efforts for the future 0.8.x series include resolving known blockers for NixOS integration, although I didn't dig deeply enough to likely get all of them.
Looking forward to the day you do find the time, and - should I have the time as well - maybe integrating your package manager into Bedrock.
> Hi! Thank you for bedrock linux

You are welcome!

> I've been keeping an eye for it for a while now and am willing to try it asap! 

No rush on trying it; feel free to do so at your own convenience.  If you try the current 0.7.x series, be sure to run the interactive tutorial via `brl tutorial basics`.  If you can't get to it for a while, the planned 0.8.x series I'm working on should be even better once you find the time to get to it.

> I'd like to ask your favorites mixes of components

My setup:

- Debian (stable) by default
	- The old/stable nature means its low maintenance, which I value highly.
	- I previously tried using CentOS for this, but the repo size was so small that I kept falling back to Debian anyways.
	- Debian's obvious downsides of its packages being old can be easily resolved on Bedrock by selectively getting bits from other distros.
	- Debian's downside of needing dist-upgrades can be resolved by not dist-upgrading the production stratum. Instead, I can either dist-upgrade a copy or brl fetch the new release and `pmm` `world` the functionality over. I keep the original stratum around and doing production stuff until I've confirmed the new one is good, in which case I just remove the original and move responsibility over (with `brl rename` and/or `brl alias`).
- Arch's main repos when I want something new.
	- In my experience, the AUR is not as well maintained as main-repo packages in most major distros.  Provided some other distro I'd be using anyways provides what I'm looking for, I usually prefer that to the AUR.  Still, access to the AUR can be occasionally useful.
- Ubuntu and Debian Testing as an occasional goldilocks middle grounds between Debian (stable) and Arch.
	- Before the lib32 shenanigans, I also used Ubuntu for games, as I could be confident games were tested against it.
- Void often has packages that (outside of the AUR) I can't find from other distros listed above.
	- Its init system is the most obvious example, but also occasional random bits like `scron` and `powerpc64-linux-musl-gcc`
- Gentoo for things where I'm picky about compile-time choices.
	- In addition to USE flags and savedconfigs, I have small patches for things like `mupdf` that Gentoo has been automatically transparently applying to package updates for me for years.
- CentOS/Rocky/Alma for business/professional software that primarily targets RHEL.
- Alpine for quick throw-away strata.

> if there are some that are particularly recurrent/appreciated in the community

A few things I've seen repeat:

- Gentoo users who appreciate the ability to pick and choose _some_ packages from a binary distro so they don't have to build it themselves.
- Users who don't want to go through the rigmarole of installing distros like Arch or Gentoo, preferring something like Pop!_OS's installer, while still desiring access to things like the AUR or portage.
- Users who require something for work but prefer some other distro (they're even in this thread!)
- Users who are picky about their init.
- Users who need an old kernel due to quirky hardware but don't want to be limited to an old userland.

I'm sure there's more I'm blanking on; I've not been good about keeping track.  It all kind of blurs together after a while.
Thank you :)
As far as I am aware, Bedrock's main advantages are:

- Bedrock tries to push the ability to get cross-distro features much farther than distrobox does.  Bedrock lets different distros provide things like the install process, kernel, and init.  The roadmap includes work for things like desktop environments and dkms.  In contrast, distrobox doesn't try to do these things, and AFAIK - I could be missing something - there's no intent to go down that road going forward.
- Bedrock tries to make cross-distro features just-work out of the box, where distrobox segregates everything by default and you have opt-in ("export") every case you want to integrate things across container boundaries.  With Bedrock, things like `apk add jq && xbps-install -y jo && jo "distro=bedrock" | jq ".distro"` just-work.
- Distrobox uses a host/container model, where containers run on top of the host. You're stuck with the host's essential bits for the duration of a given install. Bedrock's model is strictly more powerful: you can pick a distro to get all the essential bits from and stick with it and think of it as your "host," but you can also swap any or every bit of the of the system from any (supported) distro baring only Bedrock's glue that holds the system together.
- Bedrock has some robustness advantages in that you can have redundant copies of essential parts of the system.  If you have the foresight to set things up with this in mind, a bad kernel or init update won't keep you from booting.  A broken package manager doesn't obligate a reinstall, or even digging into and fixing it; you can just install a fresh one and its associated packages, then toss the broken one.
- Bedrock doesn't depend on tooling like docker or podman.
- Bedrock offers qualify of life utilities to help manage a system composed of parts from multiple distros, like `brl which` and `pmm`.

Distrobox's main advantages are:

- You can easily uninstall distrobox's tooling, but not Bedrock's. This is because Bedrock lets you get essential parts of the system from different distros in which case removing the glue would break the system, where distrobox's comparatively limited functionality doesn't as readily let you integrate stuff deeply enough for the system to become dependent on distrobox. The main value here is ease of trialing the item in question: with distrobox you can install it on your production box, try it, then toss it if you don't like it. With Bedrock you should probably use a test environment like a VM, which is comparatively a minor hassle.
- Provided systemd host and containers, distrobox can export contained services to the host. Bedrock's planned equivalent is on the roadmap for 0.8.x and isn't ready yet. Bedrock's planned equivalent is over-all more ambitious, as it includes both cross-init-system functionality (e.g. running runit services on systemd and vice-versa) and (potentially optionally) automating the distrobox equivalent of the exporting step.
- Bedrock's setup does confuse a handful of tools such as timeshift. Distrobox's comparatively limited scope means it doesn't try to do things that confuse timeshift.

There is some overlap in the use case, but depending on there's also overlap with things like flatpak or NixOS; none of these things are really full replacements for any other.  If you're in the middle of the Venn diagram you have lots of choices, but if you're off in the petals they're not actually in competition with each other.  If distrobox is better suited for someone's use case, more power to them. I honestly, whole heartedly prefer a user go with what's best suited for them for things like this. However, from what I've seen interacting with the Bedrock community, plenty of users do benefit from functionality that Bedrock offers which distrobox doesn't.
You are welcome, and thank you :)
If Captain Robert Walton returns from the Artic with word of my regret, he's lying.  I regret nothing.
<3
Sadly Bedrock does not work on WSL.  AFAIK it should actually detect it's running in WSL at install time and print a message about this; I'm surprised you even got to the `init()` thing.  A WSL update might have broken that sanity check; I should investigate and fix that. I know a number of people have attempted to get Bedrock to work with WSL 1 and 2, but none have reported success. I have some ideas to make the future Bedrock 0.8 more flexible which _might_ help with WSL, but I can't make any promises.  In principle I'd like to support WSL - Bedrock's all about choice - but frankly it's not a high priority for me in light of the large number of other things I'd like to resolve first.

> Any help or leads on URLs would be appreciated. I am very excited to try out Bedrock

If you mean in reference to WSL, I'm sadly ill equipped to help further.  It's just not something I've taken the time to understand myself.

If you mean on bare metal or a VM, the official website is the best resource: https://bedrocklinux.org/ I think it good idea to skim if not thoroughly read much of the documentation as well.  At a minimum, things like [the introduction](https://bedrocklinux.org/introduction.html) and [FAQ](https://bedrocklinux.org/faq.html).  I've gotten good feedback regarding it's interactive tutorial as an introductory resource.  Once you've installed it, consider running `brl tutorial basics`.
> o Is it possible to duplicate a Bedrock deployed configuration programmatically? (think Ansible or other scripting or "golden image") - pretty sure you could redeploy the root filesystem with fsarchiver, but it's a matter of being able to recreate from the ground up

There's no official solution for that today, and it's harder to do than on traditional distros, but I am putting some work in that direction.

Bedrock has a `pmm` utility ("package manager manager") which does cross- and multi-package-manager operations.  One of the things it does is let you synchronize the set of installed packages across all the package managers against a ("world") file describing the desired state.  You can programmatically have sync the actual installed state to the config file, send the config file to another Bedrock system, and then apply it to the installed state.  However, right now this alone has a lot of limitations.  For example, it may know to install a package from Debian, but it doesn't know that it needs to enable the Debian backports repo first.  At some point in the future I'd like to either see it improved in its own right or used as part of a larger effort.

As part of how it avoids conflicts, Bedrock can have multiple instances of certain file paths.  Consider a Bedrock system which utilizes software from both Debian and Ubuntu. This will have multiple `apt` executables which expect different contents at file paths like `/etc/apt/sources.list`.  Bedrock resolves this by having multiple instances of those folders so that each `apt` sees its own rather than conflicts.  If you want to specify which instance of `sources.list` you need, you can do so by accessing it via `/bedrock/strata/<stratum>/etc/apt/sources.list`.  Thus, an automated solution should probably do something to synchronize `/etc` config files via `/bedrock/strata/...`.  I know some users have a git repo at `/bedrock/strata/.git` to track/synchronize such files in a manner similar to `/etc/.git` on more traditional distros.  Annoyingly, the current Bedrock 0.7 puts its own `/etc` files at `/bedrock/etc` rather than the more proper `/bedrock/strata/bedrock/etc`; I plan to fix this in 0.8.

There'd also need to be some effort to automate installing the desired set of distros in the first place.  I don't think this would necessarily be hard, but I haven't put any effort into actually doing it yet.


> o Is it possible to pretty-print a summary of existing strata and the parts that are "contained" by it? Think of a nice chart, graph, tree, something that could be used as a presentation to breakdown what belongs to what - this would be very useful going forward to keep track of things

FWIW I'd go with the word "provides" rather than "contained."

I think this is tricky to do in general, as sometimes it's often context sensitive, and a lot of what Bedrock does is make things accessible such that users can define context.  For example, in the past I've found some games work better against some distro library sets, and so I had scripts to launch steam against different sets of libraries depending on which game I'm interested in.  I don't know how to, in a general fashion, detect something like this.

For functionality that is inherently tied to packages, the aforementioned `pmm` utility may be again notable here.  It can list which strata provide which package managers which have which packages installed.  However, `pmm` doesn't "understand" the idea that a given package provides the kernel.

I'm actually interested in having Bedrock automatically figure out which distro is providing the currently running kernel for another purpose: it'd probably be a good idea to have Bedrock block users from uninstalling the stratum currently providing the kernel as a defensive measure.  If you want to remove the kernel-providing stratum, reboot into another kernel first.  The problem is I don't actually know how to detect this automatically in a generalized fashion.  I have ideas but they all work under in very limited circumstances Bedrock shouldn't require.

> --If nothing like this currently exists, I would be willing to volunteer to help write at least a bash script to discover and print. Future goal would be to print with nice formatting and maybe some colors

That would be awesome!  Bedrock 0.7 is currently one monolithic thing, but the future 0.8 release will probably support the idea of optional "contrib" or "community" features.  I'd be delighted to have such a script in there.
Make that 3 questions: Are there any plans to support OpenSuSE? My whole use case for testing Bedrock was to see if Debian base combined with YAST for admin convenience ;-)
Bedrock can indeed let you get features where you prioritize stability from Ubuntu while still letting you get AUR stuff from Manjaro [0].  While it's probably obvious, I should note that anything you get from the AUR is just as likely to break as it is on its native distro(s), but absent something _really_ wrong with them they're unlikely to take the Ubuntu parts of the system down with them.

If you _just_ want the AUR, I'd probably go with Ubuntu + Arch; Manjaro's other differences from Arch wouldn't come up with just the AUR stuff.

[0] [Note that pamac and octopi, both commonly used on Manjaro, apparently have some reported compatibility issues with Bedrock that have yet to be investigated](https://bedrocklinux.org/0.7/feature-compatibility.html#pamac).  Other package management related things like, `pacman`, `makepkg`, and AUR helpers like `yay` are all fine.  I don't know what sets `pamac` and `octopi` apart.
I'm not entirely sure I follow what you're looking for here.  Try installing it in a VM and walking through the interactive tutorial via `brl tutorial basics`.  I've been told the interactive nature helps where dry documentation doesn't.  See if that helps clarify things for you.
Programs have certain expectations about what they see.  If these expectations are not met, programs get very angry.

Traditional distros ensure that, provided you get everything from the distro, its programs' expectations are met.  However, they do so in mutually exclusive ways; one distro's setup might make another distro's program angry.  This is why you can't just take stuff from one distro and dump it on another and have it work.

Containers - things like docker and podman - limit what programs can see.  This way you can run two programs from different distros, where each program is limited to only seeing stuff from its own distro.  The big downside here is the programs are very limited in how they can interact; for the most part they're each in their own limited world.

Some projects like subuser, distrobox, snap, and flatpack carefully poke holes in containers.  Just enough to let programs from different distros interact in some ways that are known to be okay, but without letting those programs see something that would make them angry.  There's a lot of interactions these can't do, but they do enough to be useful.

The main Bedrock developer (hello that's me!) put a lot of time and effort into systematically understanding exactly what programs get angry in what situations for what reasons.  Instead of just blocking everything outside of a program's normal distro setup all the time (like containers), or blocking most things with a limited number of holes (like subuser/distrobox/snap/flatpack), he made a very fine-grain system to control - sometimes limiting, sometimes adding, sometimes mutating - exactly what programs see when they look around.  Just enough to ensure the programs don't see anything that makes them angry, but without blocking anything more than necessary to keep programs from getting angry at seeing stuff from other distros.  This way they can interact much more than any of the other things I've listed without getting angry.
~~Before Titanfall 3~~ My hope is to get a very incomplete 0.8.0alpha1 this year then a full 0.8.0 release next year.  However, I don't have much confidence in that; it's going to be hugely variable depending on my time availability.  For example, if there's a large impromptu reddit AMA, that may eat into a lot of the time I had planned to work on 0.8 and push things back.
I'm torn between how impressed I am with that, and between how effective it's probably going to be.

But either way, it's responsibly and very well written.  Thank you!
>Engine from one car, transmission from another, suspension from a third, etc. 

üéµüéµ"I got it one piece at a time, and it didn't cost me a dime!"üéµüé∂
I did a Void base system with the Arch strata so I could install AUR packages. It worked perfectly and funny enough was the only way I could get a certain camera viewing software working with Wine.
The "oo" part is the hard one. do you go with the gentoo sound or the torch sound?
> Don't run a performance sensitive database out of `/etc`.

Surprisingly funny FAQ.
A naive approach to pursuing what Bedrock does may run into what you're highlighting here, but this concern has been taken into consideration and resolved in Bedrock Linux's design.  Bedrock systems can not only simultaneously utilize software which was compiled against different versions of glibc, but also different libcs entirely, e.g. musl.
> the catch is that you install bedrock by hijacking an already installed distro

This is an intended selling point; raising it as a concern seems confused.

The idea behind Bedrock is to let users utilize features from other distros.  Not necessarily _just_ init systems, man pages, etc, but _everything_ we can figure out how to get.  This includes the install process.  Consider: some distros (e.g. Ubuntu, Pop!_OS) provide user-friendly installers that provide a GUI environment in which users fill out fields and click "next" a few times.  Others (e.g. Arch, Gentoo) provide hands-on, low-level control of the install process.  Bedrock lets users pick which to use via the hijack process you've raised: use the install process you like best, then have Bedrock hijack the resulting install.

> and it is effectively irreversible without wiping your drive.

This is generally true of most distros and seems an unfair thing to knock against specifically Bedrock.  The same limitation applies to Arch, Debian, Gentoo, Ubuntu, etc.

> they have a list of what works and what doesn't on their website:

> https://bedrocklinux.org/0.7/distro-compatibility.html

From context it seems like you may be sharing this link to list distros that Bedrock cannot hijack.  If so, it's worth clarifying that this link covers distro incompatibilities _in general_.  These could be issues with hijacking, or they could be other issues.

If that's not the case and you're just generally sharing Bedrock's limitations, this link is worth pairing with the previous one: https://bedrocklinux.org/0.7/feature-compatibility.html

Bedrock cannot make literally every feature of every distro just-work.  We're working on pushing the boundaries are far as we can, and we're open about limitations.  If it's useful to some people, that's wonderful, and if it's not useful to others, I certainly hope they find whatever solutions are best fit for them.
Can you elaborate?  I can't find a charitable interpretation of what you're attempting to express here.  Bedrock does not require force flags.
> Bedrock's setup does confuse a handful of tools such as timeshift. Distrobox's comparatively limited scope means it doesn't try to do things that confuse timeshift.

This is the single biggest thing standing between me and trying out Bedrock, as I run Manjaro on BTRFS specifically because of its outstanding snapshots system. It has saved my ass many, many times when I make a mistake or get a little full of myself.

For users like me, who are generally knowledgeable, a bit adventurous, but ultimately hamstrung by a need for a system that can be depended on when worst comes to worst, are there plans in Bedrock's development roadmap to address this issue? If so, when should I start paying closer attention to major releases? 0.8? 0.9? 1.0?

Thanks for this impromptu AMA. Super cool stuff!

**Bonus Question:**

What configuration do you personally use? The one listed in the first paragraph of the homepage?
How does docker in general work In Bedrock?
4 sure!
You might want to take a look at my account to find out, whilst I could, I'd rather travel and be with people
Perhaps I do have the timeline wrong. I only knew of it when you could install it within another OS.  So from my perspective, that is when I presume it came out. lol

I don't use BTRFS. At the moment, my whole system is using the XFS filing system. It has become my new standard, replacing ext4. Not only is the file system faster, but less wasteful.
> Did you report it?

Unfortunately I didn't. At the time I only have a single machine so I just reinstalled my main stratum distro base to get my machine to work ASAP.
I did follow the documentation (which is done very well), but at the time I was more interested in figuring out how everything worked than making a stable system. 

It really is a fascinating system, and I'm sure if I did things properly I'd have an easier time
Yeah that makes some sense. I guess what I meant was that NixOS supports deep built-in config of, say, several different desktop environments, different kernel versions, whole configuration layers that switch you from one network stack to another by switching a flip. That sort of power can essentially let you configure yourself a system that looks like one existing distro or another. Granted I haven't tried Bedrock yet, so I can't really compare. I suspect integrating it with Nix would look very much like nix-darwin does today (which lets MacOS manage many settings unless you tell nix to take over). But I do see more value in supporting the useful pieces of this frankenstein approach directly in NixOS, for similar reasons as nix packaging is more appealing than snap, flatpak, and friends. The programmatic configuration will always be a hurdle and something like bedrock will probably be more appealing for the masses even if it seems less technically ideal to me.
There is a webpage that explains it better than I do.

https://www.howtogeek.com/193129/how-to-install-and-use-another-desktop-environment-on-linux/
You and your compiler can figure it out amongst yourselves.
They're few and far between, but such people do exist.  Once in a blue moon I'll get a question about how well such a thing would work, only to be followed short by confirmation that it does indeed work fine.
If I do it, I will certainly post it. It will be a while though, I'll need a computer upgrade first so the compile times are bearable.
I certainly appreciate the condor of this disclaimer. https://bedrocklinux.org/faq.html#why-not-use-bedrock
Imagine if users could run `brl tutorial basics` directly on the Bedrock web page, like the Redis interactive tutorial. I think it could be an eye-opener for many.
if the distro being run was a tabletop, bedrock is a mosaic tabletop made of different types of stone tiles (components from different distros)?
I'll probably think of a use case at some point. Previously I had one but bedrock wasn't ready for it to be easily done (I think I have around a year old post on the bedrock sub asking about it). I've been able to settle on Fedora for what I was wanting out of bedrock since then though.
I doubt I'll ever have a usecase, but I'm just happy that it's even possible.
It's like assembling your own version of Linux like an assembled PC.
I wish for your continued success.
One further question.

**Why**
>I'm not as familiar with Nix today as I should be

And i should get more familiar with bedrock :D

>How do you handle post-install hooks from other distros that modify i.e. /etc and may thus break existing packages/services?

Say you install a debian *and* a archlinux package that both depend on openssh. Now you have debian-openssh in your debian stratum and arch-openssh in your arch stratum.

Both openssh versions may now attempt to place an `/etc/ssh/sshd.conf` file and start/enable `sshd.service` automatically with a post-install script.

How are such conflicts with global services resolved in bedrock?

// EDIT: As i understand [https://bedrocklinux.org/0.7/basic-usage.html](https://bedrocklinux.org/0.7/basic-usage.html), `/etc` is stratum-local (per-stratum). Looking at ubuntu's openssh-server postinst script ([https://pastebin.com/yKNm84Fm](https://pastebin.com/yKNm84Fm)), it seems it runs some `/bin/systemd` commands and then symlinks systemd files in `/etc`.
Thank you for the replay. :)  


> if one insists, consider it more like buying me a beer. Or, per my tastes, a fancy import tea.

&#x200B;

Im a tea and addition to that yerba mate guy my self.  
When i donate something i probably send you a message with few tea ideas / recomendations.  


Take care man and have a great day.
The videogame Minecraft has two editions: the original edition written in Java, and a rewrite in C++.  Much to my frustration and SEO hit, this latter edition is called "Bedrock."  When people who grew up with Minecraft see the word "Bedrock," Minecraft to mind before the geology concept.  OP was submitting word play around this theme.  [Previous generations tend to think of something else](https://www.yourememberthat.com/files/22bb28ba960e42e5.jpg).  I am inundated with wordplay around both of these to the point where I've shared [this link](https://www.youtube.com/watch?v=ADgS_vMGgzY) in discussions around Bedrock second only, I think, to the FAQ.
Epic
Well, you could add nix and the nix-daemon as a package to Bedrock. This doesn't take much space and provides a ton of up-to-date software. You can even configure it so that when you type a command and you don't have it installed, it installs it ephemerally and runs it.

NixOS is nixpkgs + a module system + a build system with tests + a ton of modules. One of the most-depended-on modules is indeed systemd, and several attempts have been made to abstract it more, but systemd is very handy indeed and abstracting it means rewriting a lot of functionality.

You'd be able to implement Bedrock, but you'd have to start from the module library and work your way up. In fact, I would really like that very much, because NixOS needs some more opinionated options for desktop use :-). You could also just have e.g. the arch kernel as a package.

And nowadays there's also https://github.com/nix-community/home-manager, which can be installed standalone and can be seen as somewhat of a userspace NixOS.
Thank you very much!
Thanks for replying! Awesome project btw
Thanks this is a great help to me, look forward to following more on Bedrock and WSL.
Depends what you mean by support.

If you mean [add myself to the maintainer column here and proactively fix issues](https://bedrocklinux.org/0.7/distro-compatibility.html), in principle I'd love to, but as a mortal human I annoyingly need time for things like eating and sleeping in order to function, and thus I have to draw the line somewhere.  For better or worse OpenSuSE fell on the unfortunate side of that line.

However, I'm absolutely open to drive-by bug reports for OpenSuSE, fixes for OpenSuSE and - ideally - someone else with adequate background, time, and patience to stepping up to be the maintainer.

Right now the only known concern with OpenSuSE is that its default setup includes both GRUB and BTRFS, [and that there's a bug with GRUB that is more like to trigger if BTRFS (or ZFS) and Bedrock are in play](https://bedrocklinux.org/0.7/feature-compatibility.html#grub-btrfs-zfs).  I have ideas to work around this bug in Bedrock 0.8.0, but that may be a while yet.  If you're using another filesystem (ext4, xfs, etc) or another bootloader (e.g. rEFInd), it's a non-issue.  In that case, the only concern is that - due to the GRUB thing - I don't think OpenSuSE is very popular in the Bedrock community and so hypothetically there could be other we don't know about due to insufficient users to find/report issues.
Ah alright, what I mean is, say I have something that requires Ubuntu but I'm not on Ubuntu; can I install the program with `apt install program` by adding Ubuntu's apt to the distro.

I'll have a look at `brl tutorial basics`
Take your time tbh, your project is great.
You are welcome :)
Gentoo yeah
Ok, then the downside is you're shipping a ton of libcs, which has its own body of concerns

EDIT: Ah, I see what you've done. Honestly I'm amazed anything works at all with all that chroot'ing and symlinking. It's not so much a single distro as it is a bunch of independent distros cleverly being tricked into believing they have their own install. I can't imagine anything non-trivial works out of the box? GNOME had a reputation for exploding inside chroots for a long time. My head is spinning just thinking about all the mounts you need for these chroots to appear transparent and make sure the /etc's don't step on one another
>irreversible without wiping your drive

This shouldn‚Äôt be too much a concern if you keep your /home folder on its own partition, maybe even /var and some of your app-specific /etc folders.
[deleted]
It was a sarcastic joke comment.
> > Bedrock's setup does confuse a handful of tools such as timeshift. Distrobox's comparatively limited scope means it doesn't try to do things that confuse timeshift.

> This is the single biggest thing standing between me and trying out Bedrock, as I run Manjaro on BTRFS specifically because of its outstanding snapshots system. It has saved my ass many, many times when I make a mistake or get a little full of myself.

I think BTRFS's facilities here **might** actually avoid the concern with timeshift, as presumably the snapshot stuff works at the physical filesystem level rather than the virtual filesystem level where timeshift lives.  However, I haven't tried it and can't speak about it with certainty.

> For users like me, who are generally knowledgeable, a bit adventurous, but ultimately hamstrung by a need for a system that can be depended on when worst comes to worst, are there plans in Bedrock's development roadmap to address this issue? If so, when should I start paying closer attention to major releases? 0.8? 0.9? 1.0?

If you mean specifically timeshift, resolving that concern isn't on the roadmap.  It's not that I don't want it to work so much as the backlog of what I'm personally prioritizing more highly extends beyond the point where I can make projections, and I haven't heard anyone else volunteer to go at it.  If someone with adequate background wants to look into deeply understanding and doing R&D to resolve the timeshift concern once 0.8.0alpha1 is out, I'd be delighted.  (Doing so against the current 0.7.x is of limited value, as 0.8.x will be _very_ different under-the-hood)

If you mean BTRFS snapshots, it *might* just-work right now.  That having been said, I'd like to personally experiment with Bedrock-aware BTRFS stuff once the initial set of 0.8.x features are out.  Per-stratum subvolumes/snapshots would be neat.  I might find they just-work at that time, or I might find I can add them as a point-update to 0.8.x.  If I find they require a big rearchitecture to utilize, I'll try to incorporate that into the design work for 0.9.

> Thanks for this impromptu AMA. Super cool stuff!

You are certainly welcome :)

> Bonus Question:
> 
> What configuration do you personally use? The one listed in the first paragraph of the homepage?
> 

My setup:

- Debian (stable) by default
	- The old/stable nature means its low maintenance, which I value highly.
	- I previously tried using CentOS for this, but the repo size was so small that I kept falling back to Debian anyways.
	- Debian's obvious downsides of its packages being old can be easily resolved on Bedrock by selectively getting bits from other distros.
	- Debian's downside of needing dist-upgrades can be resolved by not dist-upgrading the production stratum. Instead, I can either dist-upgrade a copy or brl fetch the new release and `pmm` `world` the functionality over. I keep the original stratum around and doing production stuff until I've confirmed the new one is good, in which case I just remove the original and move responsibility over (with `brl rename` and/or `brl alias`).
- Arch's main repos when I want something new.
	- In my experience, the AUR is not as well maintained as main-repo packages in most major distros.  Provided some other distro I'd be using anyways provides what I'm looking for, I usually prefer that to the AUR.  Still, access to the AUR can be occasionally useful.
- Ubuntu and Debian Testing as an occasional goldilocks middle grounds between Debian (stable) and Arch.
	- Before the lib32 shenanigans, I also used Ubuntu for games, as I could be confident games were tested against it.
- Void often has packages that (outside of the AUR) I can't find from other distros listed above.
	- Its init system is the most obvious example, but also occasional random bits like `scron` and `powerpc64-linux-musl-gcc`
- Gentoo for things where I'm picky about compile-time choices.
	- In addition to USE flags and savedconfigs, I have small patches for things like `mupdf` that Gentoo has been automatically transparently applying to package updates for me for years.
- CentOS/Rocky/Alma for business/professional software that primarily targets RHEL.
- Alpine for quick throw-away strata.
It works fine.  You can do docker/podman stuff just like you can do any other distro.  The obvious difference is, if for whatever reason you're picky about this, you can install docker or podman from any distro.  Maybe Arch has a new version of it you want, or Arch's broke in an update and you need an older version.

In fact, an under-documented Bedrock feature is that it can "uplift" [0] a docker/podman image out of the container and make it part of what on other distros would be the "host."  You can do things like boot its init or use one of its shells as your login shell.  Or, of course, you can just keep it as a docker/podman container and just do container things with it.

[0] `brl import "$(/bedrock/strata/$(brl which docker)$(docker inspect <image-name> | jq -r '.[0].GraphDriver.Data.UpperDir'))"`
That makes sense. I hope you get to see as much of this crazy rock we call Earth as you want. Can I ask ya where are you looking forward to visiting the most?
Oh damn, yeah traveling is good. That‚Äôs terrible
In that case there's no known issue, but due to the concerns with OpenSUSE's default setup I don't think it's very commonly used in the Bedrock community and so if there _are_ issues I may not know about them.  Consider testing it out a non-production environment (e.g. a VM or spare machine) first just to be extra cautious.  If that works out, enjoy :)
Gotcha.  Very cool!
> appreciate the condor

[The condor of the disclaimer](https://i.blogs.es/6f2831/condor-andino/1366_521.jpeg) appreciates you too.
In the abstract I understand the psychological temptation to build one's effort up and hide it's weaknesses, but I'm self-aware enough to realize that I don't actually benefit from such a practice.  Not only is there an obvious ethical concern, and not only will people figure it out it eventually - the hard way - and be justifiably upset with me, but they'll increase the user support load and consume time I'd rather be spending on something else.  Being up-front is better in the long run.  The hard part is painting a complete image of the trade-offs tersely.
That would be cool indeed!  However, despite my technical knowledge when it comes to Linux systems, my web dev knowledge is embarrassingly weak.  Even if someone submitted something which made it work, I'd be concerned about both my ability to maintain it should the person lose interest, and security implications of what is effectively someone running remote code on Bedrock's web server.  Also, hosting costs.  I'm unlikely to be amenable to it in practice.
Sounds more like a table made up of other tables. No two legs are the same, the table surface is a patchwork of wood cutoffs, etc.

Does it work as a table? Yeah. Is it a good-looking table? No. Is it useful? Depends on the skill of the architect.

Meaning it could either be a smooth experience, or it could be a worse experience than Gentoo or Pop.
That's a beautiful image and is fitting of the abstraction Bedrock is aiming toward.  In practice, though, Bedrock doesn't quite reach that level of abstract beauty; distro components also drag along some of their dependencies, and end up functionally redundant and overlapping a bit; as someone else here highlighted, Bedrock typically has multiple libcs.  Even if the surface is smooth, under it the patchwork of tiles in your analogy would be partially overlapping.

The geology-themed analogy I've been running with has a Bedrock Linux system being a chunk of rock composed of different [strata](https://ak.picdn.net/offset/photos/5f2c717aa75ca0db3709ca56/medium/offset_982812.jpg?DFghwDcb?DFghwDcb).  Bedrock's code itself is just another stratum - arguably a base one, as its name suggests.
Thank you :)
Well, no one else makes a distro that's ideal for my use case.  Sometimes you just gotta do it yourself.  Consider a brief overview of my setup:

- Debian (stable) by default
	- The old/stable nature means its low maintenance, which I value highly.
	- I previously tried using CentOS for this, but the repo size was so small that I kept falling back to Debian anyways.
	- Debian's obvious downsides of its packages being old can be easily resolved on Bedrock by selectively getting bits from other distros.
	- Debian's downside of needing dist-upgrades can be resolved by not dist-upgrading the production stratum. Instead, I can either dist-upgrade a copy or brl fetch the new release and `pmm` `world` the functionality over. I keep the original stratum around and doing production stuff until I've confirmed the new one is good, in which case I just remove the original and move responsibility over (with `brl rename` and/or `brl alias`).
- Arch's main repos when I want something new.
	- In my experience, the AUR is not as well maintained as main-repo packages in most major distros.  Provided some other distro I'd be using anyways provides what I'm looking for, I usually prefer that to the AUR.  Still, access to the AUR can be occasionally useful.
- Ubuntu and Debian Testing as an occasional goldilocks middle grounds between Debian (stable) and Arch.
	- Before the lib32 shenanigans, I also used Ubuntu for games, as I could be confident games were tested against it.
- Void often has packages that (outside of the AUR) I can't find from other distros listed above.
	- Its init system is the most obvious example, but also occasional random bits like `scron` and `powerpc64-linux-musl-gcc`
- Gentoo for things where I'm picky about compile-time choices.
	- In addition to USE flags and savedconfigs, I have small patches for things like `mupdf` that Gentoo has been automatically transparently applying to package updates for me for years.
- CentOS/Rocky/Alma for business/professional software that primarily targets RHEL.
- Alpine for quick throw-away strata.

Yes, I _could_ self-compile (and self-maintain!) some bits, run others in containers, use `alien` to convert, etc.  However, that doesn't scale up smoothly and becomes a problem given how often I desire different things from different distros.  At some point doing that ends up functionally making my own distro _anyways_.  In practice this was enough of a pain that just developing the Bedrock solution was less of a headache.

That having been said, I do greatly enjoy the challenge of it; figuring out how to make things from different distros interact can be an interesting and enjoyable puzzle at times.  And, /u/IDesignM correctly highlighted, it sure doesn't look bad on a resume.
Probably boredom or to have something special on their resume for future job applications?
AUR, there can be no other reason. üî®
> // EDIT: As i understand https://bedrocklinux.org/0.7/basic-usage.html, /etc is stratum-local (per-stratum).

Yep, you got it.  Where there's a concern about different processes from different distros requiring different contents at a given file path, Bedrock has multiple instances of a given file path.  Processes will see the instance they correspond to - avoiding conflicts - but still have access to other instances via another path should there be value in having them interact.

Most of the time you don't have to think about this and things just-work, but it does come up occasionally such as when adjusting one of these /etc configs and needing to specify which _stratum_'s instance to adjust.  This is an area where Bedrock's abstraction to make everything feel like it's from one "normal" distro eventually breaks down and users do need to think in Bedrock-specific terms.

> Looking at ubuntu's openssh-server postinst script (https://pastebin.com/yKNm84Fm), it seems it runs some /bin/systemd commands and then symlinks systemd files in /etc.

In practice the systemd commands only run if the postinst script is from the same environment (in Bedrock parlance _stratum_) as a running instance of systemd.  If another _stratum_ is providing the init, the script guesses it's in a chroot and just skips what otherwise would be problematic steps.  The symlinked files are _local_ as well.
You are certainly welcome, greatly looking forward to trying new teas, and you have a wonderful day as well :)
Well whoosh to me I suppose.

I also missed out on Pokemon.
From what I understand, stand-alone Nix can be installed on most traditional distros with a portable script [0] or sometimes even from the distro repo [1].  I have been told that this does work on Bedrock, and my guess is this is probably suitable for some Bedrock Linux / Nix users.  My guess is home-manager will as well.  However, these options makes it something of a second class citizen in the Bedrock world, as they lack a few common expectations:

- Bedrock organizes files and processes into units called "strata."  These are commonly but not necessarily one-to-one with distro release, e.g. you could have an Arch stratum, a Debian stratum, a Gentoo stratum, etc.  People have reported Nix working when sharing a stratum with some traditional distro.  However, since AFAIK Nix should be able to function self-sufficiently (e.g. that's how NixOS works), the normal expectation for Bedrock users would be to have a stand-alone Nix stratum.

- Bedrock has a utility called `brl fetch` which creates strata corresponding to other distros by bootstrapping them from a minimal set of functionality [2].  Ideally we should have a `brl fetch nixos` which sets up a minimal Nix stratum with just the Nix and whatever its dependencies are; just enough for a Nix-savvy user to easily bootstrap that into a full blown NixOS install.

- Bedrock isn't just about letting users get files from other distros, but anything they offer, including abstract concepts like their install process.  Some users like GUI installers with user-friendly drop-down boxes, some like hands-on running commands from a terminal; Bedrock wants to offer all such choices.  Bedrock does this by letting users install any (supported) distro they like, then running a Bedrock provided script which "hijacks" the install and converts it into a Bedrock Linux one.  Ideally we should be able to hijack a NixOS install.

AFAIK these haven't been implemented yet not _necessarily_ because they are difficult, but because no one with both adequate Nix and Bedrock experience has given them a serious try.  Learning Nix/NixOS well enough to do so is on my to-do list, but given all the other Bedrock responsibilities I have it will be a very long time before I get to it; my hope is either a Nix person gets enough interest in Bedrock to work on this, or a Bedrock person gets enough interest in Nix to do so.  However, if no one else does, I will get there eventually.

[0] https://releases.nixos.org/nix/nix-2.10.3/install

[1] https://archlinux.org/packages/community/x86_64/nix/

[2] For example, `brl fetch debian` parses a Debian mirror's package database, calculates `debootstrap`'s dependencies, downloads `debootstrap` dependency packages, extracts those into a temporary directory, then runs the resulting `debootstrap` to fetch a proper minimal Debian install.
You are welcome :)
You are welcome, and thank you.
You're welcome :)
The other thing I'm planning to test is to start with a SUSE base and hijack that, then install a Debian strata... Will see how that goes. Maybe we just write up an install guide and say "When installing SuSE use ext4/XFS for root filesystem and do blah"
> Ah alright, what I mean is, say I have something that requires Ubuntu but I'm not on Ubuntu; can I install the program with `apt install program` by adding Ubuntu's apt to the distro.

Technically it depends on what that program is, but most likely yes, you probably can.

> I'll have a look at `brl tutorial basics`

Enjoy :)
<3
> EDIT: Ah, I see what you've done. Honestly I'm amazed anything works at all with all that chroot'ing and symlinking.  It's not so much a single distro as it is a bunch of independent distros cleverly being tricked into believing they have their own install.

That's one of the better original descriptions I've seen for what Bedrock does.  The only thing I'd change is to emphasize that _parts_ of them are tricked into believing they have their own install; other parts are tricked into seeing the whole system so they interact.  Things integrate without conflicting by managing which part sees/thinks what when.

I generally describe Bedrock as a _meta_ distro.  It certainly isn't a distro in the traditional sense, but describing it as _not_ a distro ends up falling apart when viewed from the wrong angle as well.  For day-to-day usage it feels like a normal distro, just one with a gigantic set of repositories.

> I can't imagine anything non-trivial works out of the box? GNOME had a reputation for exploding inside chroots for a long time.

I model Bedrock's progress in terms of how finely I can cut things to make them work across distro boundaries.  A good example of something I can't cut is libc linkage, for reasons you've highlighted.  Something I can is `man` and `man`'s pages.

Currently, desktop environments like GNOME need to be paired with their init to just-work.  You can trivially swap which distro provides your init, and which distro provides your DE, but they both have to be the same distro release for the DE to just-work.  Provided you maintain that pairing, GNOME works fine.

The ability to get DEs and inits from different distros is on the roadmap, but still a ways out yet.  I'm currently working on prerequisites for it.

Instead of repeatedly, incorrectly speculating about problems it might have, consider actually trying it out and seeing for yourself.  Install it in a VM, walk through the interactive tutorial via `brl tutorial basics`, skim the website for known working/non-working features, then maybe explore a bit.
Nit-picking the definition of "shipping" and "ton" aside, you're not wrong that a representative Bedrock system will usually end up with multiple libcs.

The user picks the distro(s) they trust to maintain the corresponding software, including libc.  If the user trusts Alpine's and Fedora's security teams to keep musl and glibc up-to-date, they're welcome to get software from those two distros.  If they don't trust a distro to maintain a libc, they can just not get that distro's libc.  Pedantically, yes, this increases the attack surface more than just _one_ trusted distro's libc, but in the same sense that a computer with a zeroed disk and no power is more secure than one running with a single libc.

Increased disk usage is detectable, and technically there is increased RAM usage.  Disk and RAM is cheap and plentiful enough these days for Bedrock's target audience, but yes, it's a problem if you get far enough down the embedded rabbit hole.
> This shouldn‚Äôt be too much a concern if you keep your /home folder on its own partition

Yup! Just like most distros.

> maybe even /var and some of your app-specific /etc folders.

Ah, here's where Bedrock is a bit different: Bedrock systems actually have multiple `/var` folders.  `/etc` is split; parts of it are shared like `/home`, parts have multiple instances like `/var`.  The parts of `/etc` that have multiple instances are (intentionally) the app-specific folders you have in mind.

Consider a Bedrock system which utilizes software from both Debian and Ubuntu.  This will have multiple `apt` executables which expect different contents at file paths like `/etc/apt/sources.list` and `/var/cache/apt/pkgcache.bin`.  Bedrock resolves this by having multiple instances of those folders so that each `apt` sees its own rather than conflicts.

Bedrock provides a framework to think about a system with multiple instances of these things, and tooling to help people manage it, but there's an impedance mismatch when trying to do things like map concepts like a singular `/var` directory directly from traditional distros.
Ah gotcha.  Agreed: the hijack thing is certainly unusual and worth mentioning.  If the fact that Bedrock shares a single limitation with traditional distros is the only catch you could think of, I'll take that as a big complement :)
Amazing answer. I'll keep my ear to the ground! Thanks again!
Ive been able to arrange a few trips underground in the uk
Lmao I'm not even fixing it!
 That's hilarious.
Run a browser based VM and a compact image.
https://bellard.org/jslinux/ worth looking into
It can be run directly in the browser:

https://copy.sh/v86/
Couldn't that run entitrely in JS? But don't ask me how, JS is *the* webtech i'm weak in.
>Bedrock's code itself is just another stratum - arguably a base one, as its name suggests.

As a geo, this caused an involuntary twitch.
Honestly my ideal distro is Mint with access to AUR and using Pacman for package management. Definitely going to be testing this out!

EDIT: Forgot to mention, having a non-SystemD system init as well.
> Well whoosh to me I suppose.

That probably would have wooshed me as well if I wasn't exposed to quite so many people who feel compelled to express such wordplay to me.

> I also missed out on Pokemon.

If it makes you feel any better, I just learned in this thread that I made an arguably worse faux pas.  I've been explaining Bedrock for years with a geology analogy.  Apparently there wasn't enough of an overlap in the Linux and geology world for anyone to notice I've been doing it wrong until a geodude in this thread brought it to my attention.
Hmm - a "Nixpkgs" strata would indeed seem to be just the result of the install script.

However, a "NixOS" strata would be very hard, because it requires systemd in system mode (meaning it has to be PID 1, so it can't run in a container (systemd limitation)), and it manages the entire systemd configuration, so all the other strata need to be registered as systemd units.
love the informative thoughtful answers with a touch of well-earned sass
Hm, this kind of sounds like "containers, but with chroot, and some of them interact directly".
You're welcome :)
Blimey, Britain's bloody brilliant!
I admittedly didn't do my homework here.  I started with the name Bedrock before the analogy took hold, then needed more Bedrock-specific terms, and put the parts together with a bit of naivete.  If you don't mind taking the time to elaborate, I'd certainly appreciate it.  I'll be happy to update my phrasing, or even drop the analogy as a whole, if need be.
[The current Bedrock release has a known limitation where you need to get the init system and desktop environment from the same distro for it to just-work.  You may be able to get Mint's DE working with some non-Mint init via Bedrock, but you'll have to manually make some init and display manager configs, which may be challenging for some users.](https://bedrocklinux.org/0.7/feature-compatibility.html#desktop-environments)

Other stuff - like access to pacman and the AUR - should be fine.

Resolving this requirement is on the roadmap, but is a ways off yet.  My current Bedrock priority is to work on a prerequisite for it.
In that case we might start with a "Nixpkgs" stratum as a short term solution, then work toward figuring out how to resolve the "NixOS" difficulties in the long term.
A couple big differences between containers and Bedrock's concept of "strata":

- Containers run on hosts, and while you can freely add/remove containers, you're stuck with the host.  Bedrock's abstraction is more of a flat structure with no privileged "host" (or, arguably, where Bedrock itself is the host); it's just a collection strata where any feature - including host-y things installation, bootloader, kernel, init, login shell, desktop environment, etc - can come from any stratum.  You can swap out anything but the Bedrock glue that holds the system together.
- Containers contain things.  If you poke some holes in a container, it's arguably still a container.  If you remove enough material from it such that that it's doing more non-containing than containing, the word "container" starts being actively misleading.  Strata are more like a plate than a box; there are defined boundaries there, but they're not intended with the primary aim of limiting movement.
Ive always wanted to go properly
So the fundamental thing you learn early in geology is that all deposition is laid down flat. We try to [back calculate](http://www.sepmstrata.org/CMS_Images/ChronoHeader.jpg) what these layers look like.

The bedrock is what those layers were dropped on. Bedrock is thought as the primordial granite lump that is floating on the mantle. 


Just FYI stuff, Stratum is a word we almost never use. If we are looking that close we usually are being more specific. "Do you see this bed/bedding/flow?"  Strata is not used a ton, generally we use the term stratigraphic unit, sometimes shortened to unit. The big exception to this is outcrops. "Do you see the folds in this strata?"
Thanks for the response! Having AUR and Pacman is a bigger priority than the sys init program, so I can tolerate SystemD if I can't get something like OpenRC or Runit working.
The key is to keep your pinky raised at tea time.
I'm a bit burned out from answering quite so many questions here and might be having unusual difficulty processing this.  Am I correct in understanding that the stratum analogy, and description of Bedrock as a stratum, isn't strictly technically incorrect so much as it sounds really weird as no one in the field phrases things with the given terms in the given context?

Do you know of a good introductory resource covering geology for a layman?  Not enough to work in the field, but enough to where if I were at a dinner party with geologists I be able to avoid putting my foot in my mouth by calling a stratigraphic unit a stratum.
You are welcome :)
So bedrock is not a stratum, it is what is underneath it.

The technically definition for stratum would be a depositional surface. That has to be deposited on something. Think a layer of paint on the wall, there maybe dozens of layers of paint, but underneath there is drywall. The drywall is like bedrock, it's not a paint layer.   

I don't really have a good intro book for this, I do geophysics so I don't know it that well, but my dad was an old geologist so it's been drilled into my head since childhood. 

A rocks for jocks book would probably be a waste.  I can explain 90% you need to know over no more than 3 beers. but writing it down is a pain, it requires handwaving.
Gotcha.

The way Bedrock Linux was originally envisioned, it was stuff from other distros on top of Bedrock.  Over time, I found that privileging Bedrock's own code like that was problematically limiting, and so I've been moving toward a model where Bedrock's code is just another set of stuff like the stuff from other distros.

It sounds like the Bedrock and stratum (well, stratigraphic unit) name choices and associated analogy was actually somewhat correct when I chose the names years ago.  However, when I started reworking the abstract model to remove the differentiation between Bedrock and the strata, I broke the analogy, as by definition a layer of bedrock is categorically different from strata.

I'll consider either withholding the analogy or qualifying how it breaks down accordingly going forward.

Hypothetically if you ever attend a Linux convention where the Bedrock guy is presenting something, remind me about this and I'll take you up on those beers.
>Hypothetically if you ever attend a Linux convention where the Bedrock guy is presenting something, remind me about this and I'll take you up on those beers.

deal.
I like mercurial better than git and was really pushing for it.

Git is good enough, and I can't really get others to change.
Mercurial is a distributed source control system. It is used, for example, by Mozilla for the development of Firefox or Thunderbird instead of git.
Wait Mercurial is still alive? (And I mean this in a good way)
Hey, so, question.

Any reason to use Mercurial, or any version control system besides Git? Git is basically industry standard at this point, and that has to be for a reason. But if these other VC systems still exist, there has to be a reason for that too.
Mercurial just lost against git and the few advantages in ux aren't worth it. Same for darcs, bzr, ... Use it for your private projects but don't annoy package maintainers or your fellow colleagues with it.
What are the advantages of mercurial? I've only used git and cloned a couple svn repos before. I tried pulling firefox from mercurial but it was way too big and never finished downloading
Do you also point users to [hg-git](https://foss.heptapod.net/mercurial/hg-git)? With this, git repositories can also be used, within certain limits.
There's also [Pijul](https://pijul.org/), maybe the spiritual successor of [Darcs/Camp](https://youtu.be/iOGmwA5yBn0), but written of course in Rust
It is! But it seems to just be one of those things that won‚Äôt beat the overall momentum of Git at this point. 

Definitely has it‚Äôs uses and hope it sticks around for time to come.
This year, as of today, 9 new versions have already been released. So yes, Mercurial is definitely still alive.
Windows is also the standard on the desktop, so to speak. Nevertheless, we use, partly exclusively, Linux.

For similar reasons, I use Mercurial whenever possible ( in combination with hg-git if necessary). After all, Linux is all about freedom and therefore also about the diversity of programs.

But I'm not trying to convince everyone not to use git. There are enough reasons why git is the best solution in many cases. For example because the company you work for only uses git. 

But in my opinion one should reflect if git is always the right solution. I often use a version control system completely on my own. So why should I use git? Or I know a group of developers who make their code freely available, but mostly reject changes from third parties. This group has decided for example to use Fossil, which is developed and used by the developers of SQLite. One advantage of Fossil is that it is not only a VCS, but Fossil also consists of a bug tracker, wiki, forum, notes, chat etc.

And all in all, projects like Firefox or Thunderbird show quite well that you don't necessarily have to use git.
Git and mercurial are both distributed version control systems and have largely the same functionality. The most noticable difference is in what mercurial refers to as branches and what git refers to as branches. In mercurial, branches are recorded as part of commit metadata, causing it to be permanently associated with the commit(s) instead of git's ephemeral branches. Mercurial's bookmarks are equivalent to git's branches (a bookmark can be moved to any arbitrary commit at any time). Mercurial includes modified files as part of commits by default (instead of git's staged commits), but there are extensions to change the behavior including being able to select chunks to include in a commit.

Personally, I found Mercurial's CLI UI easier to grok when learning version control concepts. Having distinct commands for reset, rebase, merge, fetch, and pull help immensely with conceptualizing what's being done.

Facebook found mercurial easier to extend and modify than git, leading to the creation of [watchman](https://engineering.fb.com/2014/01/07/core-data/scaling-mercurial-at-facebook/) for keeping track of file changes in large codebases before committing. I'm not sure if this is still the case, but I've found Mercurial encourages plugins to experiment with functionality before merging them into the core.

Finally, I love the GUI tool TortoiseHG for its comphrensive ability to work with mercurial repositories. I haven't found a git GUI for Linux with a comparable interface. It's probably the number one reason I continue to use hg-git to interact with git repositories.
Perspective from someone who used hg for 10 years now using git. 

Git fees like a race car, you can blow shit up, but holy cow it‚Äôs fun to drive. Hg feels like a safe and reliable sedan. 

What others said about branches is actually my pet peeve about rigidity of hg, and my favorite nimbleness of git. 

Subrepoing in mercurial felt always a chore, and easy in git. 

But git, the race car, always feels like it has a million knobs and buttons and gauges to keep track of, and mercurial ‚Äújust works‚Äù

Anyway, my 2c
I haven't used mercurial in ages, but I remember it being more intuitive and less complicated back in the day. One of the reasons I chose hg over git back in the day was deleting remote branches. Mercurial had a straightforward way to do it. In git you had to say something cryptic, maybe `git push :remote-branch`. Git's improved since then (we now have `git push --delete`, for example), so I'm not sure how true this still is. But I've never had to worry about mercurial's equivalent of the reflog, if such a thing exists, so I'm assuming it's at least partly true now.

Mercurial doesn't have a cache/stage/index. There's no confusion over `git diff` vs. `git diff --cached`, because the distinction doesn't exist. The cache is something I've never found a use for, so I appreciate that.

I remember it being much easier to figure out which branch a commit belongs to in mercurial. Possibly even after the branch was deleted.
For me, Mercurial has the following advantages.

- Mercurial has only a certain range of functions by default. Everything that goes beyond that, you either have to activate consciously via the configuration file or you have to install plugins to expand the range of functions. So there is less risk to shoot yourself in the foot than with git which basically offers all functions out of the box.
- The error messages as well as the documentation are in my opinion better and easier to understand.
- I consider the commands that go beyond the usual hg pull / push to be simpler. 

>I tried pulling firefox from mercurial but it was way too big and never finished downloading

I just tried it with the command `hg clone https://hg.mozilla.org/mozilla-central/ firefox-source` and it worked without any problems. I therefore suspect that at the time of your attempt there were technical problems either with you or with Mozilla. However, this can also happen with any other VCS.
I liked the different types of branches, having them explicit marked as drafts not ready to merge or use was nice. If I'm remembering it correctly, used a little bit some many years ago.
Thanks maybe that could be something for me to use.
There are also other DVCS. But what does this have to do with the release of Mercurial 6.2?
Are there any compatibility shims that allow Pijul to communicate with other VCS systems? I found a [discussion](https://discourse.pijul.org/t/adventures-in-converting-git-pijul/518) on converting repositories to pijul, but nothing on ongoing two-way interaction.
> I haven't found a git GUI for Linux with a comparable interface.

I have totally given up on trying to use a Git GUI of any kind.  It is always a trainwreck.
I agree the rigidity of branches causes issues that aren't experienced in git. If I understand correctly, the evolve extension is hoping to solve some of the friction through making the process of marking changes as obsolete more inline with git's ability to push rewritten history.
There's other dumb stuff too like WHY do you need to enable an extension to shelve? Why is dropping commits requiring an extension?

These are basic things that got has had for the last decade. I expect anything else to have the same

The hook documentation is lackluster, I'm not sure what the git state is on that. A decade ago hooks were bad in git but I know a lot of the adopters have been improving it
> Subrepoing in mercurial felt always a chore, and easy in git.

Did you accidently write that the wrong way around? Because I'd say the exact opposite, mercurial sub-repos feel like a first class citizen compared to git where the sub-repos feel like a few paper-thin shell scripts over top of having two git repos in the same path.
It also supported Windows from the start. git on Windows used to be a PITA.
I use hg-git myself to push commits to Github and Codeberg (based on Gitea) and to pull changes from there. This works without problems in my case.
I see hg branches as a feature and you don't have to use it. As you said, if you want the git workflow, just use hg bookmarks instead.

With git branches you cannot know which branch a commit used to be on, because the labels always just point to the head of a branch. Sometimes that information is helpful.
I agree with you, but I still liked submodules better in git. Unless something changed, hg require you to make a massive mother repo in order to sub something. That was always a pain.
Yeah, it was slow too. Mind you, a lot of that is because Windows itself is slow

But between patches on git and patches in Windows, it has gotten better. Microsoft themselves worked on that
I use both EXWM (basically, Emacs as a tiling windows manager) and Gnome. Normally, I'll use Gnome when I'm away from home using the laptop without an external screen or keyboard. Gnome's recently added support por touchpad gestures is a bliss on the laptop. On the other hand, I tend to use EXWM more and generally feel more productive when at home and hooked to these peripherals.

I feel like as much as I really love the experience of using EXWM, I sometimes want to go to Gnome just for the eye candy or the feel of integration of a true DM. And I keep thinking of going back to KDE Plasma, which I used for years and was an amazing experience.

So in the end, I'd say each piece of software offers different ways of interacting with the machine which may not only be measured in terms of efficiency, but also comfort, familiarity or fun. All of these things seem to be important to me.
Not KDE but I switched from sway to gnome a few months back. I haven‚Äôt really had a want to switch back. gTile satisfies most of my wants in regards to tiling, though there are some things it can‚Äôt do that sway/i3 can. I customized my gnome keybinds to cover my most common use cases. 

It‚Äôs certainly not 100% there but it covers 95% of how I was using sway previously. The reason I made the jump to gnome was because I was a bit tired of fiddling with my customization to handle things that were just there out of the box with gnome. I was also finding myself installing a lot of the gnome tools for key ring and Bluetooth config and what not so I just decided to do it. 

So yeah, really haven‚Äôt felt the want to go back. I still love i3/sway. But gnome + gTile and some keybind changes just does pretty much everything I need with far less burden of maintenance.
Can you not use Plasma with any wm? Sounds easier than changing workflow imo.
I also tend to move back and forth, between i3 and Gnome in my case. I love the customizability and frugality of i3, but that also means that it requires a lot of set-up and tweaking (and a learning curve, if you're not used to a keyboard-centered workflow).

The thing is, I need quite a few Gnome GUI applications. They aren't really made for keaboard-only operation, so I still have to use a mouse pretty often. I also quite often need to use a file-manager for these apps. While the terminal file managers are great, they aren't really made for drag&drop in a GUI environment. There may be solutions to my issues, but it has been way less of a headache to use Gnome and customize a few things to my liking (keybinds, mostly).

Plus, I really like the look&feel of Gnome 4X.
I think another thing is that it's not just about raw time; mental overhead is a consideration, too. If I'm already juggling like 5 things in my head trying to make the system do something, having to think about where my mouse is and where to click could push something out of my short term memory that is actually useful. If it feels more natural to use terminals instead of a mouse, it takes less mental overhead to accomplish the task that way, and can help you keep focus. Yeah maybe it's only a half second faster or whatever, but it leaves room in your head for something else you're keeping track of, and that can sometimes matter a lot. Maybe not often, but you'll be happy to have one less thing to think about if you're operating the computer in a high pressure situation.

Of course, for most people, just clicking around in a GUI file manager takes less thinking than a terminal, so what's better really just depends on which workflow works for an individual.

Also you might look into using tmux. All the goodness of tiling WMs for terminals, but everything else operates in whatever GUI environment you want.
I recently switched to GNOME. When configured right, it kind of feels like a tiling WM, just without the tiling. But to be honest, I don't think tiling windows is as useful as people claim it is. What tiling window managers do so well is handling workspaces and a allowing for a keyboard-driven workflow. This can just as well be achieved with vanilla GNOME or KDE.

The whole "tiling and terminal is faster than floating and GUI" idea is overrated and not really true to begin with IMO.
For me Plasma and KDE apps are the best for productivity once you understand how to use them. For example how convenient Service Menu in Dolphin are? Klipper custom actions on clipboard entries based on regex?

Also there is a good support for keyboard in general and they keep improving in every version.
I use Sway and KDE.

I use Sway for programming, because I don't need to use the mouse that often.

I use KDE for normal usage, like gaming, browsing, etc. because I don't need to use the keyboard that often. I also use bismuth, because I hate floating windows.
Hardcore xmonad user from 2008 until 2017. Since then using gnome.

In my case was more "not using".  I used to enjoy my time tinkering with the system in general, neovim/emacs, xmonad, the usual suspects (dotfiles). Now years later I'm older, I have less free time and I preffer things that simply works, so I stopped maintaining my neovim config, dropped xmonad and moved to Gnome (and I'm one of those weirdos that like the minimalism of it and the lack of customization options).

I still have a very strong keyboard workflow because muscle memory being a b\*\*\*\*, just use stock gnome with tilix as terminal and I'm good to go.
A keyboard centric workflow works best for my brain. My work laptop is windows, but I spend most time in an ssh shell connected to a Linux desktop, on which I run tmux.

One thing I noticed in your comment: you describe how simple things are, like ",m" or "Alt+A", but then describe the GUI as clunky for having "shortcuts that are key combos."
After 2 years using i3, i moved to KDE. I figured out that i don't need so much tiling features. And KDE has a simple way to handle all the things that i need from a tiling wm. It's enough customizable so usually i don't need to move my hands from keyboard. I love KDE look & feel, and how it works. So i'll keep here for a long time.
Another GNOME tiling option is Pop!\_OS (or just the pop-shell extension if on another distro).
I used to be a big i3 guy and went to Plasma. I get the itch to try Sway, as I love Wayland, but also dont want to tinker anymore.  I still live on the command line, as using Discover can cause weirdness on system upgrades, especially on Arch, but Plasma provides a decent enough experience that I don't need to think about things.
I'm currently using Qtile, but I've thought about a potential future where I may want to avoid heavy configuration and simply use a DE mostly out of the box. After a few years of customising my PC, I've already started undoing a lot of steps to keep my package count minimal, and keeping to mainly popular and well-maintained packages from the official repos, and keeping my configs closer to defaults.  
There's cases to be made for both the benefit of personal touches, and also the convenience of pre-configuration, but I'd like to think I'm close to finding the perfect balance.  
I am already a fan of Xfce and Budgie though, and I'm even interested in vanilla Gnome, so maybe I'll someday reach a point where I just can't bother to spend time on maintaining my configs anymore and go with one of them. For now I still have fun doing editing config files and planning out my installs.
https://regolith-linux.org/ best implementation of i3 on Ubuntu out-the-box (and other distros) imo. Check it out.
Been using i3wm for 10 years now. I can't even look at a point-and-click desktop anymore, I'll vomit.

Oh wait, you were asking for the other way around...
Didn't switch to GNOME/KDE but MacOS (M1 MBP, not by choice) and _wooooow_ I just feel so slow and sloppy. After ~10 years of i3 I'm used to windows just appearing where and how I want without thinking about it. Now even to swap windows between sides of the display takes like 10 seconds and isn't even borderless half the time.

...and don't get me started on how annoying it is to run multiple processes of a single application. or fullscreen apps. or how minimizing works. or how multiple desktops work. Seriously, these MacOS UX designers make Windows 7 seem modern (at least it has native window snapping!).

Asahi Linux development cannot move fast enough...
Kde is bloated and heavy. I will stay in bspwm and xmonad.
I love KDE and still have it installed but only use it occasionally at this point.  I've gone through the great internal battle of DE vs WM - pros and cons to both.  Right now I'm very comfortable in my Qtile setup.  I will say though, an awesome compromise, or perhaps best of both worlds is the kwin scripts for tilling (here:  git clone [https://github.com/faho/kwin-tiling.git](https://github.com/faho/kwin-tiling.git)) 

Great dynamic tiling capability (I set it up just like my qtile bindings / behavior) and you can take any desktop and disable tiling with a key combo so you get KDE normal float.
I use KDE with a tiling window manager script called [Bismuth](https://github.com/Bismuth-Forge/bismuth/). Best of both worlds, IMO.
I use KDE Plasma with BSPWM as my window manager, which gives me the best of both worlds. To me the issue is not about which one is better, but about minimizing switching between keyboard and mouse. I can do pretty much everything using Plasma's GUIs, or through BSPWM and the terminal. Either way works fine.

If I was on Plasma + Kwin or on a custom BSPWM-based rice I would be locked into one workflow for the most time and would have to do a lot of switching between keyboard and mouse. On my office machine I cannot use BSPWM (I blame the Nvidia drivers), so I use Krohnkite to tile the Kwin windows; it works OK, but it's a hack, a proper tiling window manager is so much better.
I switched from sway to Gnome recently (previously i3 & awesome). The reason being how much configuration is needed to get additional features in a wm, compared to what regular Gnome updates provide.

Besides a clipboard manager & co, I don't use many plugins. Instead of a tiling plugin I changed tiling left and right to Alt+A/Alt+D for easy access.

This works because I make use of workspaces and usually only have one to two visible windows on a workspace. For my terminal needs I use one with tiling support.

KDE was my first DE a couple years ago and to find a useable setup I had to configure quite a few things. Because KDE has so many built-in features, I didn't need plugins. What I did with Gnome should also be possible with KDE. So I recommend just trying it out and giving yourself time to get accustomed to KDE.
I used Xfce and now bspwm. I don't want go back to a floating window manager. I'm not against using the mouse (I have a trackpoint on my keyboard) but the tiling window system is just more comfortable to use. No dragging any more, no questions about the windows hidden behind another. It can even produce floating windows if the user wants.
There is many configuration to do at the begining. I use Tint2 instead of Polybar so the work is reduced and my configuration is untouched since one year except some color changes, this is quick to do.
My ".config" directory is saved on another device. When I moved from Arch to Nixos the change was relatively smooth, most of the config work was reproduced out of the box.
I didn‚Äôt not really switch but I use gnome from time to time and windows at work.
I use xmonad mostly.

I can‚Äôt say I don‚Äôt like gnome. It‚Äôs nice and all, and I would stay there and ditch xmonad if it hat the some multi monitor behavior. 
This is the one thing tiling wms do right, having workspaces in the monitors that are completely independent from each other on each monitor.
As someone who swears by tiled workspaces, the problem, generally, with tiling WMs/DEs is that they just don't offer the flexibility of other window managers imo.

My biggest issue is that, at the end of the day, it is better for a window to default to floating, than to default to some specific segment of my screenspace. I'm just not opening/closing windows often enough to warrant such swift movement and configuration of windows. I open an application once, slap it where it ought to be, and leave it there for all of eternity or until where it ought to be changes.
I bought a laptop with GNOME pre-installed. I gave it 6 months to try and get used to it. Gave it the best chance I ever have. It was still janky, and their focus on Wayland meant OBS couldn't see all windows. Further, Debian doesn't have a build of OBS that can see both X and Wayland windows.

I felt much of my experience with GNOME was fighting its view of how computing should work. Even when I adapted, I felt like there was still a disconnect between what I wanted to do, and what GNOME *let me* do.

I ended up going to XFCE until I have the time to put together a proper i3 session again.

Wayland and PipeWire and friends aren't ready yet. The former's been "almost ready" for 10 years. The latter is a continuance of PA in the sense that it's a corporate project. Any use cases that aren't relevant to Red Hat or FD.o will not be included or addressed.

I've not used KDE since right around when 4 came out. It strikes me as a more bespoke DE, just with a better UI paradigm.

Truthfully, none of the big DEs match what I want 100%. They come with extra cruft that isn't as good as the dedicated stuff. Rhythmbox isn't nearly as good as MPD, SMPlayer, or VLC for example.

I really don't gain anything by using FD.o technology. Other software can do the same jobs with less complexity and bloat. I can adapt my system to what I want, instead of being forced to follow the XDG standards, which don't have practical community input.

I'd rather use tools that let me mold them to what I want, and are headed by small enough projects that you can bring something up and it will actually get addressed instead of handwaved as "not what you really want" or some other crap. I might even be able to submit a patch to one of the small ones! Not so much with the big guys.
amassed a bunch of home-grown scripts to perform some common actions, like mounting CD or USB.

In 2011, I had a newer computer and decided to try KDE (4.6 or 4.7 at the time). It turned out I have enough resources to spare running full DE, most of my workflow can be easily ported (like all the shortcuts for switching virtual desktops) and I don't have to waste time solving certain problems, like mentioned mounting of USB drive. I have not looked back.

As a general sentiment:

I understand the feeling of making your computer do exactly what you want it to do. But at this point of my life, I need to question what type of work are you doing where switching windows is the slowest part and you feel the need to optimize it.
I switched between several tiling WMs (bspwm, awesome, xmonad, dwm) for a year, and realized that I always use a window in a maximized state, as my laptop screen is small, so tiling WMs have pretty limited use for me. 

I am using KDE rn, and feel much better with it. Being able to tile windows quickly is still useful (for 10% of the times), now I mostly make xdotool scripts to do the tiling.

I still use terminal apps tho, mainly through tmux.
I used to use i3 and Qtile but have since moved back to a GNOME environment. Things like tiling and keyboard only driven workflows work amazing with text based workflow (ex. terminals and TUIs) but can feel quite awkward with applications that are inherently graphical (ex. photo editor, games, web browsers). By using a single window/tab of a terminal emulator in a full fledged DE with something like tmux you essential get all the benefits of a TWM for the text based workflow its great for and all the modernity, ease of use, and cohesion of a full DE outside the terminal.

  
This is pretty much how I live life within a DE with tmux for my terminal flow. I still do most things on the terminal for soft dev but when I'm done work clicking just works so much better for media consumption. Plus odd hidden benefit but the lack of config options on DEs keeps me from wasting all day configuring shit. DEs look really great out of the box and are consistent in look and feel. With options like light/dark themes and now accent colors I feel this is just enough to keep me happy and I'll leave the CSS to people who have all day to do so.
I actually did the opposite, as a long time KDE user I switched to Sway/i3 completely around 2 years ago.

I definitely agree that moving you hand/arm to reach the mouse is very annoying. I still need to do that for my web-browser (I haven't got any time to configure Firefox with something like Trydactyl).

Of course I wouldn't have switched to these kind of systems if I didn't learn touch typing and Vim/Neovim first.
People have told me that EXWM has problems with freezing due to it being single threaded. Is this true in your experience?

I have been going down the lisp rabbit hole recently and that has stopped me from moving over to EXWM.
Thanks! Yeah, i still love customizing things in Sway/i3, but i still need to install some tools from other DEs so I don't need to fiddle with terminal commands for some mundane things like connecting to other wifi networks or changing audio volume for specific apps.

And when you need to install udiskie to handle external drives, it is already half there pulling things from DEs so why not just use a full DE anyway.
Not on Wayland.
I agree the keyboard shortcuts and the way that tiling window managers force users to set up a ton out the gate is what makes them productive. The actual tiling can make windows awkward sizes and clutter up a smaller display resolution very quickly but has the benefit of keeping everything visible in a pre-set layout. Setting up a strong keyboard shortcut environment in a DE should have similar results. Realistically keyboard shortcuts are the only thing reducing the time for a context switch, which at the end of the day is all that matters.
Nice to know about these custom actions in Klipper.

I'm going to switch and immerse myself in Plasma for some months to learn these things.
Did you switch from neovim or just go with defaults?
this is me basically. I had a xmonad setup that was specific to me (and the friend of mine who made it for me lol). I had keybindings for everything and I was trying to do almost everything in the terminal. 

Now I just don't have time to maintain anything like that. And DE's like xfce and wm's live cwm (and others) fill the gap. At this point I prefer a de because it's more convenient.
When i say key combos i'm mostly talking about pressing two buttons at the same time with one hand.

,m is a key combo, but being one after another makes it easy. Like modal text editors.

But yeah, it's just a matter of getting used to new shortcuts. In the end it's all the same.
How is tiling with Pop\_os or via the pop-shell ext?
I'm almost 40yo now so I'm past that phase.

I've used both (Plasma and bspwm/sway/dwm) and the difference in performance on my laptop is not noticeable.
If my computer is not a potato, what practical impact does that bloat have?
Maybe it was in the past, but now it's as light as Xfce with ~500MB on boot
It's definitely true, unfortunately. I often work with org files containing code snippets that I export into PDF via LaTeX. The conversion process (running all the code, compiling the tex output) locks Emacs up and consequently the whole interface. It doesn't take more than a few seconds, but I can see how this could be annoying.
The reason I switched from sway/i3 was because I was spending too much time in my config. I feel like gnome keeps that urge in check. And though my Emacs memory is not that sleek, it makes up for the lost keyboard driven feel. I do use custom key binding for gnome with the windows keys.
For Wi-Fi, I use a rofi menuhttps://github.com/zbaylin/rofi-wifi-menu that displays a menu when I click the Wi-Fi module of waybar
Oh, rip :/
I have used it for 10 years now and I still learn new tricks. Also be sure not to miss Activities. You can even set commands to automatically run when start/stop/switch Activities.
I still use it for file editing now, slowly felt to the dark side (vscode) for work.
I started using pop-shell recently coming from bspwm and it is really quite good.
It supports colored borders, stacked layouts, keyboard-driven workflows but also works well with the mouse.

Only downside as mentioned is not being able to save and restore layouts.
I don't use it because I don't believe you can save/restore window arrangements, but here's some quick google search results for you:

* [https://youtu.be/-fltwBKsMY0](https://youtu.be/-fltwBKsMY0)
* [https://www.youtube.com/watch?v=8sHhWpDPZ4Q](https://www.youtube.com/watch?v=8sHhWpDPZ4Q)
* [https://www.youtube.com/watch?v=fSXUoVqIjy0](https://www.youtube.com/watch?v=fSXUoVqIjy0)
I'm using iwgtk right now. It works and it's lightweight.
The same for me. Although I don't use it reluctantly like you seem to. Maybe coc.vim would have been enough to keep you on neovim?
Multi-core processors are more common now than in 2008 though so I don‚Äôt know if this is a fair comparison.
Where are these 10 billion cores? I didn't know it was as established as this, I haven't heard if any consumer RISC-V cores outside if hobbiest boards
"5 years faster than ARM" doesn't make any sense.

12 years for RISC-V is clearly counting from 2010 to 2022. Three people at Berkeley first got the idea they should design their own ISA in 2010, but the design wasn't finished, the RISC-V Foundation set up, and SiFive founded until 2015. The first chip/board you could buy was the HiFive1 Arduino-compatible board in December 2016. That's only 5 1/2 years ago, or about 7 years ago for SiFive's founding (they're not the only RISC-V vendor, but others started even later).

ARM hit 10 million cumulative cores in 2008 \[1\], so 17 years earlier is 1991, when Advanced RISC Machines Ltd was formed with Acorn, Apple, and VLSI as shareholders. But that's not really the equivalent starting point to 2010 for RISC-V. By early 1991 there had already been 100,000 Archimedes computers shipped with ARM CPUs \[2\] and they were already on to their 3rd generation chip. 

The equivalent for ARM would be 1983, when Steve Furber and Sophie Wilson first put pen to paper on the design of the ISA.

So that's 25 years for 1983-2008.

\[1\] [https://www.androidauthority.com/arms-rise-small-acorn-world-leader-376606/](https://www.androidauthority.com/arms-rise-small-acorn-world-leader-376606/)

\[2\] https://en.wikipedia.org/wiki/Acorn\_Archimedes#Impact
Crash Override and Acid Burn were right! RISC is going to change everything!
I like the open source architecture.

But when it comes to the real world, as the silicon manufactors can easily do evil and the chips they produce can get compromised without detection, I dont know if this can really help build something truly trustworthy?
This isn't exactly surprising. Low-power low-compute cases like embedded hardware will likely all switch over to RISC-V, since that's the case for which it works best.

I'm not expecting it to beat performance for high-compute of the other open ISA, the OpenPower family, any time soon.
So... for someone who has no particular dog in this fight, and only writes (and mainly uses) software written in high-level compiled languages... why do I care?

Is RISC-V objectively better in any way that a compiler-up (i.e. no assembler / machine code) programmer is going to care about, vs. say ARMv8?  Is the "ARM tax" significant enough that it would result in more power per dollar / watt?
Will Intel develop any backdoors in Risc-V?

I'm just waiting to get Risc-v retrofit motherboard to Thinkpad X230 or a completely new laptop with 7-row.
this is so awesome
Thanks for sharing! :)
Let's not pat ourselves on the back until the production is well established...
Not to mention lithography processes have improved massively, which makes this kind of comparison even stranger.
On the other hand there are now entrenched vendors that need to be "defeated".

Not saying this is "proof" RISC-V is better , but another data point some people might want to think about.
There are also 1 GHz RISC-V processors now out, with one core. Took ARM way longer to reach it.

However, for me this isn't really special, or something that RISC-V did better.

* for many years, ARM only aimed at industrial and embedded computing. E.g. devices without CPU fans, dedicated roles, low power consumption. No one in ARM land tried 1 GHz, this only changed then ARM CPUs started to be used for Smartphones and similar applications that used a full OS (e.g. Linux kernel instead of FreeRTOS).
* today we have much smaller chip process (often measured in nanometers). So getting to 1 GHz is now "easier" than it used to be years ago.
Besides, while it may have success at the low end, when it comes down to performance:

[Technical Lead for SoC Architecture at Nokia, answers the question "Is RISC-V the future?"](https://np.reddit.com/r/hardware/comments/r1v5kv/technical_lead_for_soc_architecture_at_nokia/).  
See also the top comment on that thread.
Storage. Vendors are psyched for a royalty-free controller ISA.

https://www.tomshardware.com/news/seagate-develops-risc-v-cores

https://www.anandtech.com/show/12133/western-digital-to-develop-and-use-risc-v-for-controllers
They're becoming quite big in the embedded world. The [ESP32-C3](https://www.espressif.com/en/products/socs/esp32-c3) chip is RISC-V based, cheap at $1.10 and very capable with wifi and bluetooth support.
According to RISC-V International (the non-profit that manages the ISA), the largest user of RISC-V cores at the moment is automotive.

There are also tons of places inside chips that are built to do .. whatever ... that are mostly just combinatorial logic but need some little programmable thing somewhere. Or maybe they have a big ARM processor, but need something to control a SERDES or whatever. In the past engineers would say to management "we need a small core in the corner of the chip to do X" and management would say "it's too much hassle/time/money to license a Cortex M0, just bodge up some custom ISA". And they'd do that. And It'll probably suck. And they'll be stuck programming it in assembly language (or binary) forever because no one is going to get the time to port gcc/llvm. But now you can just grab a small well-tested RISC-V core off github and use that. And use a standard toolchain. So pretty much everyone is doing that and killing off all the custom little in-house cores with weird ISAs.
Nvidia uses RISC-V for co-processors.
You were right until It doesn't makes any sense. Because it doesn't but not because for RISC-V was even less time , but because it has nothing to do with the architecture of RISC-V vs ARM but with the advancements on technologies to develop multicores of any architecture.

It is like comparing how many time took BMW to develop an electric car with how many time took Tesla
Ughu.. it changes since 1980s... almost 40-45 years already RISC has been with us üôÑü§ê
Trust isn't really the reason for riscv being open source. It's a permissive license anyway, so vendors are allowed to introduce all the backdoors they like in it. The reason for being open source is to make it an open standard that anyone can implement and improve on.
It's all about the threat model, if you have pissed the CIA off you probably will have a hard time with supply chain attacks, although you can also be invisible and buy off the shelf hardware that has very low chances of being targeted.

This is not the only step, but nobody will attack dozens of millions of machines with supply chain attacks, they will get caught and will burn their tactics for no relevant win.
If you want trustworthy then build your own chip or use FPGA.
The one and only big advantage of RISC-V is that you don‚Äôt have to pay any fees. You can just download the standard and implement and use it however you like.

I don‚Äôt understand why people think it means open source chip designs. It‚Äôs not like the instruction set is under the GPL.
Not only this arm is from the 90s back in those days there were no additional cores, just the one. Multi core came later. And numbers of cores increased dramatically.
To me at least the fact that the ISA is royalty free means that open source cores can be developed (Some did that target high performance [smartphone](https://github.com/OpenXiangShan/XiangShan) and [server](https://github.com/MoonbaseOtago/vroom) use cases).

Any investment a programmer would make in the RISC-V software (or maybe even hardware) ecosystem could end up helping open source CPU's.
I am just a user. But I've  already met across with ARM in form of  tv-boxes. 

And what I see I can't call it funny! Dozens of linux distribution compiled by many Joes from nowhere. Lots of incompatibilities. Aged and unsupported SOCs, SOC wars... 

Probably it's enough not to poke a finger in this pond and stay with x86\_64 as long as it is alive.
Yes.

But more seriously, the attacks don't come through the CPU.  They come through the BIOS.  You need speculative execution attacks to do stuff like sniffing data from other cloud tenants.  Your laptop is much easier to attack with something like Symbiote.
Possibly but some other manufacturer can produce chips without it. With x86 you need a license from intel.
Serious question... I like to reduce my personal data leakage as much as the next guy but I am not going as far as using a way outdated, slow ass Thinkpad X230 as my PC.

What really changes or what REAL ACTUAL risks would you be exposed to if you switched to an regular off the shelf modern PC (using a Linux distro)? Are you actually being targeted by someone? Just curious...
How do you think they reached 10 billion cores without ... production?
And RISC-V is the 5th version. The previous versions didn't go far.
What are they supposed to compare themselves to then?
There seem to be more qualified people who [disagree](https://www.reddit.com/r/hardware/comments/r1v5kv/technical_lead_for_soc_architecture_at_nokia/hm8c0ga/).
> ESP32-C3
>  chip is RISC-V based, cheap at $1.10

I could not find a price at link you posted.
Ehh.....

RISC took a while to become commonplace, precisely because it took a while to develop "killer apps" (Android on ARM) to compete with x86 (whose killer platform, MS-DOS and later Windows, was the first truly standard PC design to gain traction).

Now that ARM has the scale to compete, it's pushing hard against x86, particularly in portable applications where battery life is critical and x86 just isn't cutting it.

x86 has already lost Apple, Amazon offers ARM servers at less cost than x86, and Windows on ARM is gradually becoming more common.  Apple in particular has never been too committed to its choice of instruction set (having used a total of 4 throughout the Mac's history, 2 of which are RISC).  POWER never went away either, still in use on IBM servers.
Yep, and if you have a few million bucks lying around, nobody can stop you from hiring a fab to print off your own RISC-V chips that can run regular software. Although depending on how many third parties you rely on for things like verification, that doesn't totally preclude backdooring...

(Side note, how technically "open" is x86 these days now that a lot of the patents have expired?)
>nobody will attack dozens of millions of machines with supply chain attacks, they will get caught and will burn their tactics for no relevant win.

They did it, got caught, and are almost certainly continuing to do it. Their response has been to try to assassinate a prominent whistleblower and now imprison him. There are rumours of other assassinations using vehicle electronics hacks.

Nothing technological will stop them doing this. It's an issue of political will.


https://news.softpedia.com/news/vault-7-cia-can-hack-your-smart-tvs-iphones-androids-turn-them-into-spygear-513654.shtml
The more permissive (and much less costly) licensing of RISC-V is why I presume that it'll mostly displace conventional ARM in low-power uses. 

It removes yet more of the costs involved in getting your product to market.
Of course. But if there isn't Intel backdoors on purpose in the bios, there won't be backdoors on purpose in the bios.
Wtf?

So many wrongs in just one short message.

Changing the whole motherboard into X230 makes just as outdated as the chipset+cpu combo suggests in the retrofit motherboard.

I'm not being targeted by anyone.

X230 isn't outdated by any means. It would totally good as new, if it has more than 2c/4t. 4c/4t would be amazing. It's just a question of reballing station, but I haven't been able to find one just yet.
Lemme ask you this, where have you seen it being used in public? Not in experimental or developmental settings but actual production setting?
I'm stuck with an unsupported RISC-III system which is now an expensive door holder.
The question is "what metrics are comparable", and I'd wager adoption rate might be a worthwhile comparison (so not cores built, but chips shipped?). Or maybe market share over time, since that accounts for the total number of devices.

If you're not comparing against a different time though, then you can compare other metrics like performance and transistor count.
I hope you mean the people I was quoting!!
I got the price from [digikey](https://www.digikey.com/en/products/detail/espressif-systems/ESP32-C3/14115579#).
You're missing MIPs. The architecture was an early RISC implementation and was and still is a widely used chip. They finally stopped updating that architecture in 2021 because RISC-V instead.

To this day there are routers and wireless aps running linux on MIPs.
x86 is pretty well open, but AMD64 isn't.

Edit: Obviously, patents on modern extensions of x86 haven't expired yet, but everything up to SSE has, I think. You could probably build a chip and run any x86 software from before 2000.
x86 patents largely seem to be a game of cat and mouse due to things like new extensions getting rolled out and introduced more new IP. Non-AMD or Intel SoCs and products of that type do seem to still be out there though, which is neat
That's simply not true.  Hacks exist as a thing.
What? LOL I didn't say anything "wrong" I was asking a question. Sure maybe the demands of your workflow are low end but I said it wouldn't work for ME. I was assuming the main reason you were using that model because was running a different BIOS/firmware on it. You already refereed to backdoors by Intel. My new laptop is going to have a 12th gen Intel chip in it.

I am asking what are you really gaining with a RISC-V based laptop when alternatives would be so much faster. I would love to have those tingly feelings of using a totally open source CPU/firmware but I am not downgrading. I just don't see RISC-V being being viable for a laptop beyond a very niche crowd.
For the most part you'll never know.  Up to now (and this may or may not change in future) the vast vast majority of RISC-V cores have gone into products where you don't see the CPU, don't know what code is running on it, and maybe not even thing about there is a CPU in there. E.g. Western Digital disk drives and Sandisk memory cards. E.g. any of the vast quantity of microwave ovens (over 50% world market share) and air conditioning units (25% world market share?) and other things made by Galanz and OEMd to many name-brand companies. Galanz said in 2019 they are switching all their products to RISC-V controllers, starting with SiFive cores, but they have since designed their own in their LeapFive subsidiary. Same with Huawei. Huawei IoT product have been RISC-V for some time. Alibaba are getting into RISC-V in a big way too. Recent Nvidia video cards have all had RISC-V controllers in them.

But most companies never announce it. SiFive alone says they have over 300 "design wins" for their cores. There have been official announcements of ... a dozen? ... of those. Most companies prefer not to say.  Similar for Andes, who probably have a lot more RISC-V design wins than SiFive, as they had a large existing client base for their own NDS32 ISA, who are generally switching to RISC-V for new designs. Andes said several years ago their RISC-V revenues exceed their NDS32 revenues.
I mean, considering I haven't had to swap out or modify any of the control modules in my newer vehicles just yet, the answer is "technically none" ... but that's far from the same as claiming they aren't even being produced in volume. 

You didn't say "sold", you said produced, but since you insist on doubling down on your absurdly bad take, I'll do you one better. Show me any company or collection thereof that would produce 10 BILLION of any one item that they aren't selling in quantity. I'll wait.

https://www.reddit.com/r/linux/comments/vz3k0h/comment/ig71rg9/
> Or maybe market share over time, since that accounts for the total number of devices.

But that wouldn't be a 1:1 comparison would it? I mean many machines have multiple CPU's but counting that way causes each one to only count for one. 

I think the comparison in the OP is probably fair as far as it goes. There should just also probably be an idea that you need several metrics and an accompanying understanding that even though you're comparing roughly analogous things there's still a bit of "apples and oranges" going on.

So not strange or irrelevant, just an incomplete pictures.
Yeah, but your no chump!
didn't know this existed. thanks!
Kinda.

I'm talking more about the non-embedded space.

SGI was the last company to use MIPS on desktop IIRC, and they switched to Itanium because the high-end workstation market alone wouldn't cover costs to fabricate a minimum volume run of chips, and (since SGI owned MIPS) it wasn't in a great place for use by other manufacturers.  Itanium, of course, was owned by Intel, a company that didn't have much of a finished product line of its own, and thus could sell the same exact chip to go into anything from laptops to servers.

In the end, Itanium flopped, but the same trend exists with x86 - AMD and Intel both make way fewer unique types of chip than they actually sell, AMD opting to assemble a CPU out of multiple common elements, and Intel opting to make complete, top-of-the-line chips, kill the broken bits, and sell the rest.

Apple, meanwhile, covers about a third of the smartphone market and about a tenth of the laptop/desktop market.  And even they struggled to justify custom silicon.

In the embedded market, of course, "cutting-edge" doesn't matter, so older, lower-volume processes (and the chips to use them) still have a fair chance - especially since a new production run of an existing design is cheaper than entirely new masks, and even those are cheaper than an entire new, complicated, state-of-the-art factory.  Hence, exotic, untested, limited, or simply old designs can survive.

I omitted MIPS (and the embedded market in general) because it's playing an entirely different "game" with "rules" almost unrecognizable from the dedicated computer market.
Isn't that what China is doing? 

Quick search found https://en.wikipedia.org/wiki/Zhaoxin

Kinda crazy they are using TSMC though.
Haha the image in mt head, where Intel funds Risc-V project with billions and demands one bios backdoor to every board. Then libreboot devs hacks that board right after the release, spending weeks not sleeping enough to achieve the goal of liberating the intel-backdoored risc-v.
I don't need anything else from my laptop than 7-row keyboard, maximum battery life and able to run linux. Okay, slightly lightly better performance i n Youtube than the current Risc-v SBC's offer.

For me it seems like you don't know about Intel investing in the Risc-V:

https://riscv.org/blog/2022/02/intel-corporation-makes-deep-investment-in-risc-v-community-to-accelerate-innovation-in-open-computing/


So that's why Intel backdoor derived from ME isn't far fetched to ask.
You do understand what 'being used in production' means, right ..?
That would be a 1:1 like you say, but that's the intent.

Would you consider adoption rate to be higher just because one dude has a quad CPU workstation versus a regular 1 CPU workstation? Of course not. Same goes for servers, one server with 4 CPUs would be counted as one adoption, equal to one server with 1 CPU.
Every Intel CPU from PPro (1995) has an internal RISC core. Last microcoded CISC x86 CPU was Cyrix MII (1998). Interestingly enough pure Intel CISC cores based on P5 briefly returned about 10 years ago in the form of Intel Xeon Phi accelerator.
Indeed.  And that's completely ignoring that RISC-V isn't open-source x86, it's open-source arm.
I am aware and that is obviously a concern of yours which is why asked if you think there would be any actual real world implications in your life if went with a newer X86 laptop.

Again, I would love to be able to only use OSS and an open source OS with no binary blogs on open hardware with open source BIOS/firmware but that is a fantasy. Intel ME is low on my list of concerns so not going to use a ten year old laptop as that is to much of trade-off.
Right, I'm sure all those auto manufacturers are *just sitting* on the cores they've purchased.
Okay. Then you do not use a 10 year old laptop in any case.

Intel ME is on my list that much that I have libreboot/coreboot when available and otherwise Amd. Amd has PSP but let it be. They promised already years ago Coreboot support for Ryzen but we all know why it hasn't been released.
This seems like an easy solution. Just migrate to AlmaLinux (or Rocky, it doesn't matter, they are both CentOS replacements/RHEL clones). If one happens to go under, which is unlikely considering their funding, you can switch to the other one with minimal effort.
I think you're indeed overthinking things.

Alma (or Rocky, but I'm of course biased to Alma) is your answer.  Distro age can't really be looked at because there was no need for them until CentOS changed.  Both have healthy corporate backing and aren't going anywhere.

PS - CentOS Stream is a net positive (though I can understand your frustration).  Stream provides a direct way for Alma/Rocky and other clones to contribute back to upstream which was never available in the olden days of traditional CentOS.  It also means RHEL is developed in public now instead of in private which is a win for the whole ecosystem.
Doesn't Alma Linux have a script or something that can migrate you from CentOS easily? I think Alma is your best bet.
Based on your "just NO" comment you may not want to hear this, but CentOS Stream meets all your criteria.  It also has some unique advantages.

> 1. Still keeping inline with cost saving, it can‚Äôt be commercial licensed.

CentOS Stream is not commercially license.  It's completely free to use in any way you want.

> 2. Unfrequent major releases, those of say Fedora and OpenSUSE which releases a new version every 6 - 12 months

CentOS Stream major versions happen every three years, same as RHEL and RHEL clones.

> 3. With the above, long term support on releases for at least 5 years.

CentOS Stream major versions are maintained for ~5.5 years.

> 4. Learning curve for lesser technical support staff.

There is no learning curve if you're already using CentOS, other than the routine learning that happens with new major versions of the operating system, just like with RHEL and RHEL clones.

> 5. Some sort of deployment service to ensure standardisation (this is a nice to have)

Any software like this that works with RHEL and RHEL clones should also work with CentOS Stream.  If it doesn't, then it will break when the next minor version of RHEL is released.

> My concern is, longevity especially as they are newer distros.

I know the context of this statement was Alma and Rocky, but I'll also point out that CentOS Stream isn't really a new distro.  CentOS changed.  If these changes had been done cleanly at a new major version (like it should have been), the news would have been "CentOS 9 is here early and it's different now".  Instead, the project chose to have two editions of 8 initially, one following the old model and one starting some aspects of the new model.  The only reason it was necessary to come up with the Stream name was to distinguish between these two editions.  These choices made the transition far more painful than it should have been, both for users and for the distro maintainers.

As far as longevity going forward, CentOS is now a critical part of creating RHEL.  Sustainability is a challenge for any RHEL clone, but CentOS no longer has to worry about that.  Historically CentOS never had more than a handful of people working on it, but now every RHEL maintainer is now also a CentOS maintainer, which is an exponential increase.  In addition, CentOS Stream is the only distro in the Enterprise Linux family that can accept outside contributions to improve the operating system, so there are even more people working on it that just RHEL/CentOS maintainers.

The biggest advantage in my opinion is who is responding to your bugs.  When you file a bug, what do you want to happen?  In CentOS Stream, your bug report goes to the actual maintainer of the respective package, who also maintains the same package in RHEL, and often in Fedora too.  It is also common for them to be directly involved in the respective upstream software project.  They are knowledgeable about the software and are empowered to fix the bug, or to review and merge contributions to fix the bug.  With a RHEL clone, all the distro maintainers have to do is verify the bug is reproducible on RHEL, then close it as "works as expected".  Some will go the extra mile and redirect you to CentOS Stream to file a bug that can actually get fixed, which will make it into RHEL 3-9 months later, and then their rebuild sometime after that.
Sounds like Debian would be an excellent fit. It may not be *the* solution, but I'm rather surprised that you don't even mention it as a *possibility*. It seems to fit every item on your checklist perfectly (with the possible exception of the learning curve thing--but you apparently looked at Ubuntu, which is just Debian without the stability and with extra corporate ties).
I find it intriguing that you would consider Fedora but not CentOS Stream.
I hate to say it, but you're looking for Ubuntu. 

That being said, if you can't get past the whole "let's make Linux easy for everyone" thing, Alma. Better team than Rocky. And better infrastructure.  Even if they're competitive right now, those could both end up mattering eventually.

I sure wouldn't trust any platform that young though. And I've been really happy with our Ubuntu conversion for the >5 cent servers I've had find a solution for. But I've always been a Debian guy, so that's probably a bit of a biased take
Why don't just switch to OracleLinux8 ???
Thanks for the reply. Initially I also thought yeah easy answer, but the more I thought about it, the more questions came up. Fuck it, maybe I am over-thinking and over-complicating things but then again, I don‚Äôt want to sit with this again in 3 or 5 years‚Äô time
Biased or not, thank you. My first test was Almalinux and apart for the little 32-bit lib hiccup, can‚Äôt for the life of me remember which one it was, I really liked Almalinux and kudos to the team!
As far as I know and what I briefly read, I think is it is only from CentOS8.4 but will have to confirm
Thank you for the feedback, some food for thought here
The thing is, Debian has always been in the back of my mind but it depends who you ask about it, a lot of love for it but also a lot of hate for it, which appears to be more than love.

I know that it is each to their own and that Debian has been around for a ling time and running crucial servers, I personally never worked on it so don‚Äôt have an opinion of it at the moment. But it is also a distro that I am currently downloading to test.
Hi, I only mentioned Fedora as it pops up as an alternative. I personally would not use Fedora in a ‚Äúproduction‚Äù environment. I used Fedora years back but the release I used at that was was incredibly unstable which put me off it.
Again bias or not, your input is still valued as it is exactly what I wanted, opinions of people working in the industry  with working knowledge of the distro they recommend in prod.

If I make the wrong choice, I can always blame the communityüòú
Damn mobile, I can‚Äôt quote. Ok so your ‚Äúlet‚Äôs make Linux‚Ä¶‚Ä¶.‚Äù statement stuck with me and kept on nagging away.

I am all for making Linux easy and accessible for everyone and if I am not mistaken that was the original aim of Shuttleworth to provide an O/S in order for the poorer communities in South Africa to be able to learn and up skill in IT, hence the name Ubuntu.

Each product, referring to Windows and Ubuntu, is brilliant in its own right and for me, Linux can and has been made easy in recent years without having to mimic Windows - bear in mind this is only my view point which can also be completely wrong and if so I will admit it realign and move on taking the lesson learnt.
Never worked on it so I don‚Äôt have working knowledge of the distro but in saying that, I am in the process of downloading it to evaluate it.
https://wiki.almalinux.org/elevate/

CentOS Linux 7 -> [insert_distro_here] 8 is a supported path.
Don't like ubuntu, try linux mint, either the standard version based on ubuntu or the LMDE version based on debian.
Well there you go, I stand corrected. Thank you.
linux mint official announcement https://www.linuxmint.com/rel\_vanessa\_cinnamon\_whatsnew.php
[deleted]
Beasty?! \*facepalm\*
For old.reddit.com users, here's a working link: https://www.linuxmint.com/rel_vanessa_cinnamon_whatsnew.php
There is a proposal for Ubuntu 22.10 to include webp support [by default](https://launchpad.net/bugs/1979121) in their default edition.
Forget thumbnails, last time I checked Ubuntu's built-in inage viewer couldn't even open a .webp file!
What shitload of terminal commands?Just install webp-pixbuf-loader. 

Both for thumbnails & support by the viewer. I do agree that it should be there by default but it's just one command. The real problem is knowing what you need to install. Or that there's even anything /to/ install.
webp-pixbuf-loader isn't available currently in Ubuntu stable releases since it only landed in [Debian Unstable](https://tracker.debian.org/pkg/webp-pixbuf-loader) and Ubuntu 22.10 (still in development) a month ago.
All this should be by default, good thing that many distributions are already doing it by default.
Sticking with libre office thanks. Its actually not bad compatibility wise as long as you remember to install the Microsoft fonts.
Stopped using them on Windows and Linux when they riddled their software with ads & started pushing their cloud services at me every chance they got. I thought it wasn‚Äôt worth it, this just proved it. If you need a good office suite on Linux, Windows, and/or Mac OS, get LibreOffice or Softmaker Office/FreeOffice. They‚Äôre both cross platform and have good support for MS formats. Softmaker even has odt support.
[Auto-translated version of the link](https://finance-sina-cn.translate.goog/tech/2022-07-13/detail-imizmscv1255241.d.html?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp)
I've never heard of WPS -- how common is it?
‚ÄúFree as in freedom‚Äù don‚Äôt forget this people. It‚Äôs not just the price‚Ä¶
I don't want 3rd party cloud documents or cloud text messages or cloud email. 

Many of these programs send your data to some shady server and for no good reason. This is reason #326 as to why its bad.
Noting that the Chinese author was not using a locally installed copy of WPS, but the cloud version of WPS (like Google docs) (according to the Hacker News article from.MIT Technology Review https://www.technologyreview.com/2022/07/15/1056042/chinese-novel-censored-before-shared/). Google also intervenes in some user content, though not for censorship and I think the worst Google does is remove sharing access.
Yeah I'm just sticking to OnlyOffice/LibreOffice for now
Can someone who has WPS installed test what happens if you put this in your document?

>Âä®ÊÄÅÁΩëËá™Áî±Èó® Â§©ÂÆâÈñÄ Â§©ÂÆâÈó® Ê≥ïËº™Âäü ÊùéÊ¥™Âøó Free Tibet ÂÖ≠ÂõõÂ§©ÂÆâÈñÄ‰∫ã‰ª∂ The Tiananmen Square protests of 1989 Â§©ÂÆâÈñÄÂ§ßÂ±†ÊÆ∫ The Tiananmen Square Massacre ÂèçÂè≥Ê¥æÈ¨•Áà≠ The Anti-Rightist Struggle Â§ßË∫çÈÄ≤ÊîøÁ≠ñ The Great Leap Forward ÊñáÂåñÂ§ßÈù©ÂëΩ The Great Proletarian Cultural Revolution ‰∫∫Ê¨ä Human Rights Ê∞ëÈÅã Democratization Ëá™Áî± Freedom Áç®Á´ã Independence Â§öÈª®Âà∂ Multi-party system Âè∞ÁÅ£ Ëá∫ÁÅ£ Taiwan Formosa ‰∏≠ËèØÊ∞ëÂúã Republic of China Ë•øËóè Âúü‰ºØÁâπ ÂîêÂè§Áâπ Tibet ÈÅîË≥¥ÂñáÂòõ Dalai Lama Ê≥ïËº™Âäü Falun Dafa Êñ∞ÁñÜÁ∂≠ÂêæÁàæËá™Ê≤ªÂçÄ The Xinjiang Uyghur Autonomous Region Ë´æË≤ùÁàæÂíåÂπ≥Áçé Nobel Peace Prize ÂäâÊöÅÊ≥¢ Liu Xiaobo Ê∞ë‰∏ª Ë®ÄË´ñ ÊÄùÊÉ≥ ÂèçÂÖ± ÂèçÈù©ÂëΩ ÊäóË≠∞ ÈÅãÂãï È®∑‰∫Ç Êö¥‰∫Ç È®∑Êìæ Êìæ‰∫Ç ÊäóÊö¥ Âπ≥Âèç Á∂≠Ê¨ä Á§∫Â®ÅÊ∏∏Ë°å ÊùéÊ¥™Âøó Ê≥ïËº™Â§ßÊ≥ï Â§ßÊ≥ïÂºüÂ≠ê Âº∑Âà∂Êñ∑Á®Æ Âº∑Âà∂Â†ïËÉé Ê∞ëÊóèÊ∑®Âåñ ‰∫∫È´îÂØ¶È©ó ËÇÖÊ∏Ö ËÉ°ËÄÄÈÇ¶ Ë∂ôÁ¥´ÈôΩ È≠è‰∫¨Áîü Áéã‰∏π ÈÇÑÊîøÊñºÊ∞ë ÂíåÂπ≥ÊºîËÆä ÊøÄÊµÅ‰∏≠Âúã Âåó‰∫¨‰πãÊò• Â§ßÁ¥ÄÂÖÉÊôÇÂ†± ‰πùË©ïË´ñÂÖ±Áî£Èª® Áç®Ë£Å Â∞àÂà∂ Â£ìÂà∂ Áµ±‰∏Ä Áõ£Ë¶ñ ÈéÆÂ£ì Ëø´ÂÆ≥ ‰æµÁï• Êé†Â•™ Á†¥Â£û Êã∑Âïè Â±†ÊÆ∫ Ê¥ªÊëòÂô®ÂÆò Ë™òÊãê Ë≤∑Ë≥£‰∫∫Âè£ ÈÅäÈÄ≤ Ëµ∞ÁßÅ ÊØíÂìÅ Ë≥£Ê∑´ Êò•Áï´ Ë≥≠Âçö ÂÖ≠ÂêàÂΩ© Â§©ÂÆâÈñÄ Â§©ÂÆâÈó® Ê≥ïËº™Âäü ÊùéÊ¥™Âøó Winnie the Pooh ÂäâÊõâÊ≥¢Âä®ÊÄÅÁΩëËá™Áî±Èó®
People shouldn't **only** put their sensitive data on the cloud. They should also back it up on external drive just in case the cloud provider does something fishy with their important work.
fuck the ccp.  I fucking hate china's government.
Why anyone would trust software directly controlled by the Chinese government is beyond me.
It's a bit late now, but hopefully someone will introduce them to version control in the future.
Is this product like OneDrive or Dropbox? I understand how they could block a sharing link which is going to be uploaded to the cloud but it is unclear how they have access to the local file.
What about the version without the online components?
This shit is why George RR Martin uses a DOS Word Processor.
That was just his excuse to his publisher as to why he hasn't completed his novel.
be aware the "international version" of WPS on Linux lacks all the online features the windows versions has. it also has no ads. its like they didnt bother to implement the framework for making ads and DRM work on linux.

even the "chinese linux version" is hilariously lacking on much of the cloud features.
Disappointed.  It was one of my top favorite Linux apps .
I really like WPS. It's probably the best Office replacement IMO, but I can't use it because of who owns it.

Sad.
Not sure where they thought they were living.
Why anyone would trust software directly controlled by the Chinese government is beyond me.
Virgin ChinaOffice vs Chad LibreOffice
I've told people this for years. Do not buy any software that originates from China, and uses data storage in China, or you risk this result. Also, your email will be relentlessly spammed and spoofed (happened to me more than once) to the point that you'll get blacklisted and not even be able to use your own email. Just don't do it. Chinese software and games primarily farm user data from the West. I refuse to use any phones originating with Chinese companies, and try to avoid any computers manufactured there. Sorry, the level of trust is very low now.
Just use TeX or any of its derivatives
Don't join networks that you don't trust.
Some misinformation in post. Here‚Äôs what the article states:

‚ÄúHowever, WPS said in a statement the day before, "A user's shared online document link is suspected of violating the law. We have prohibited others from accessing the link according to law. This matter was falsely rumored to be WPS deleting the user's local file." As of the press release of the Beijing Business Daily reporter, WPS related people have not responded to the latest statement of the parties.‚Äù

I use WPS - its a great alternative to office. This is disturbing, but frankly this type of thing is common in China and applies to all of the big western tech companies too. 

A good alternative is onlyoffice for those looking to switch.
Well surprise SURPRISE! I'm [shocked! shocked!](https://www.youtube.com/watch?v=SjbPi00k_ME) to learn that a hostile intelligence service would use its own product for hostile purposes! Who would have guessed?!? *Say it isn't so!*  LOL
[insert tibet uyghur xinjiang copypasta here]
So how do you know that the user is not breaking the law? Do you think dropbox never takes down files?
What is WPS?
Never heard of it thankfully.
thought police is gonna get you
WPS is the stuff that allows you to connect your printer to your router without a password. What does this have to do with user files? Clickbait.
Software sucks nowadays. I'm sticking to pen and paper for my office and paperwork needs
Libre Office + self hosted sftp server
Im using OnlyOffice. I like LibreOffice and used it for years, but i had too many issues with formatting and display issues when working with Word docs for work. Forms with lots of lines that needed printed or docs with embedded tables were especially troublesome. OnlyOffice has been much more compatible for me.
There are no ads on WPS Office on Linux.
Google-less translation by deepL:

Source: Beijing Business News

July 12, "WPS file was locked" the party revealed to the media: in May their more than 1 million words of fiction manuscript was indeed blocked by WPS, can not open to use, but never said the local file was deleted, and in the process of communication with WPS, WPS party had admitted that the audit system misjudged the sensitive words, and promised to strengthen the The system detects vulnerabilities.

But the day before, WPS said in a statement that "a user shared a link to an online document suspected of violating the law, we legally banned others from accessing the link, and the matter was blackmailed as WPS deleted the user's local files". As of press time, no one from WPS has responded to the party's latest outburst.
WPS is a domestic office software suite that can rival Office, and in the first quarter of 2022, WPS mobile monthly activity 336 million, the parent company

In the first quarter of 2022, WPS had 336 million monthly mobile users, parent company revenue of 868 million yuan and net profit of 201 million yuan. However, in addition to the "local files locked" incident, there are also complaints on third-party complaint platforms about WPS "forcing the purchase of membership" and "automatic deduction of fees when the free trial period is not completed". There are many complaints on third-party complaint platforms about WPS "forcing membership" and "automatic deduction of fees when free trial period is not completed".

After WPS issued a statement on the "WPS document locked" incident, the person concerned posted a response, "I didn't want to mention it, but the marketing number carrying my matter, on the hot one, netizens questioned my file has problems. I would like to stress again that there is no problem with the file, it is a problem with their home detection function, detecting the wrong one, and I am lying and mistaken, locking my file".

According to the person concerned, "WPS said the day after my file was locked that I had shared the file which triggered the block, and then the evidence proved that I had not shared the file externally, and WPS customer service was clear about that. So on the third day WPS unlocked my file, determined that it was not due to sharing, not due to a file violation, and apologised to me. I had asked for an apology on the first day and to fix the vulnerability, and after WPS falsely accused me of sharing, I asked for a refund of my membership fee, but WPS apologised and did not do it".

On 12 July, the person concerned also stressed to the media that there was no sharing and no violation, "In the process of communication with WPS, the WPS party had admitted that the audit system had misjudged sensitive words and promised to strengthen the system to detect vulnerabilities and improve standards, rather than the official statement that it was due to the document itself containing prohibited content and sharing it with the public ".

The person also stressed that "their own documents are fully legal and compliant, with no prohibited content, and said that the blocked documents had not been shared with anyone else except for WPS staff in conjunction with the WPS investigation. I usually have the habit of backing up my work on multiple platforms, and the same documents are archived in multiple other cloud document platforms, so there is no indication that the content is illegal. I never said that the local files were deleted, but they were blocked and could not be opened", and provided a screenshot of the communication with WPS customer service.

In addition to those involved in the incident, a number of users also said that they had experienced being locked out of WPS. A user named "momobuai520" posted: "WPS authors should be careful, my document has been locked for a week and still not unblocked, and the appeal has not been approved."

This is not the first time the person concerned has posted about this matter, before WPS issued a statement on July 11, the person concerned had posted that "WPS snooped and blocked the manuscript on its own, access is strictly prohibited. It says the file contains prohibited content and access is forbidden, who gives it the right to snoop on users' private files and deal with them on its own, not to mention the entire content of Clearwater, which the website can send out".

So does WPS have the right to view the content of user files and block them? Yao Kefeng, director of Beijing International Law Firm, explained to Beijing Business News, "If WPS is used as a localized office tool, it is not allowed to review user files. If the user is using the cloud document function or wants to share documents, WPS becomes a dissemination tool. In this case, WPS has the right to censor or block user files based on technical means such as sensitive word monitoring. This is a requirement placed on the platform by regulators in the Personal Information Protection Act, the Security Act and other legal regulations".

In response to the incident, WPS said in a statement released on 11 July: "A recent link to an online document shared by a user was suspected of violating the law and we have legally banned others from accessing the link. The incident was blackmailed as WPS deleting the user's local files. We will reserve the right to defend our legal rights through legal means." In response to the parties' response following the release of the above statement, WPS sources have remained silent.

WPS was founded in 1988 and is the core product of Kingsoft Office. According to the financial results for the first quarter of 2022, Kingsoft Office reported revenue of RMB868 million, up 12.26% year-on-year, and net profit of RMB201 million, down 21.78% year-on-year. As of March 31, 2022, Kingsoft Office's main products, WPS Office and Kingsoft Wordmaster, had 572 million monthly active devices, an increase of 14.86% year-on-year. Among them, WPS PC version had 232 million monthly active devices, an increase of 17.17% over the same period last year, while the mobile version had 336 million monthly active devices, an increase of 14.29% over the same period last year. Although, WPS's main competitors did not disclose their share of the domestic office software market, WPS is recognized as the head of the domestic office software products.
Beijing Business News reporter Wei Wei
[This office software you've never heard of is one of the most popular business apps around: WPS Office has more monthly users than Microsoft Office on mobile platforms](https://www.techradar.com/news/this-office-software-youve-never-heard-of-is-one-of-the-most-popular-business-apps)

WPS Office, along with OnlyOffice, is (for better or worse) often recommended on this subreddit to people who complain about aspects of LibreOffice.
It comes pre-installed on a LOT of Chinese made shit.
No idea but it appears to be Chinese and closed source => I'm not touching that with a 10ft pole.
Don't know how common it is, but for MS Office compatibility, the only thing better is running MS Office in Cross Over, in my experience.  For linux, Kingsoft ships .deb and .rpm for $0. It is not open source. Apart from extremely high fidelity, it has pretty good performance with large, complex spreadsheets, although LibreOffice has dramatically improved in the past 12 months. LibreOffice keeps getting closer and closer to MS Office fidelity, but WPS Office was conceived from the start to be a clone of MS Office, which is different to LibreOffice. 

It also shares Excel's weaknesses with CSV files. For those, LibreOffice is miles ahead.

I guess those suspicious can turn on traffic sniffing.
However this censorship happened in the cloud version of WPS, not on a version installed on the user's computer. Under Chinese law cloud providers have to take responsibility when a user publishes content by sharing it. It sucks to live under such a government.
That‚Äôs entirely correct. And if you store or upload pornography on Google Drive or OneDrive they suspend your account. Does not matter of you have a paid subscription. In short, they have their laws and we have ours.  We should stop demonizing countries that do things differently - human rights abuse aside.
imma try this. installing wps on my arch vm
tried this, saved it on desktop in a VM, created an account in WPS Cloud (with fake IDs ofc), nothing happened. File is still unlocked. Im going to let the VM run with internet connected for a while and see what happens.  


(VM is windows 7 btw)
My fear is that external drives will be more hard to come by and will be more expensive if every normie accepts the Cloud blindly.
People don't know. It's our job telling them
He says on [reddit.](https://techcrunch.com/2019/02/11/reddit-300-million/?guccounter=1&guce_referrer=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnLw&guce_referrer_sig=AQAAACd-6Smyn86ayJTVoapX4Jz-uElK9_Kg7gfJfLqLlu7Jd9DOU6B52YXLI5ePjnh_h9pB9WY-YZQcdSuqoEQfdI2uO5wBsy8eyF1tT9XuGRxGm8BJvPeyLC6wtv9EVZpJ-irXJenyl9bwXGpLl20WDPztEPqWNM6x9qW3NGF03XwX)
What version control do you recommend for office/writing purposes? Git? Or is there something specifically tailored for office use?
Offline app with cloud sync feature
Spooky, ain't it?
Fortunately LibreOffice is pretty good and it's getting better every single release
TikTok User: Whatever, look at this dog butt wiggle.
Yep, that's what WPS has claimed. But the user claimed that WPS locked the local file as well, which is basically equivalent to what ransomwares do. WPS's statement so far only said that they didn't delete the user's file.

Here's another article on this, which contains the screenshot of the user's original post.

https://wallstreetcn.com/articles/3664613
I guess those are not important issues for you? Just because it's repeated doesn't make it any less concerning
The idea that somebody can break the law simply by having a file can only happen in a dystopian authoritarian regime.
[WPS office](https://www.wps.com/)
I could be wrong but I think it's actually a really really old wooden ship.
Sir, the scribe quarters are over there.
a nice self hosted nextcloud
I agree. Only Office is a better alternative
It aint free
You are right, they do not have ads in Linux. And I see that I wrote that in a way implying that it did. Thank you for pointing out my mistake. I meant that I switched from them on Windows and Linux when they riddled the Windows version with ads.
they're owned by Kingsoft, which also owns (or used to own) Cheetah Mobile, a company that would develop adware for Android that would pretend to boost and optimize the device, before they got from banned from Play Store for ad fraud.
That clause at the end is doing a lot of heavy lifting.
It comes pre-installed on pretty much every Chinese phones (too many to my liking) and has a very bad behaviour of asking to open everything with it (and sometimes not even bothering to ask)
I've been using it for years now. Far better interoperability than libreoffice.

Think it's time to switch to onlyoffice.
We should stop people suggesting that thing and stick to good old LibreOffice.
>"on mobile"

lol
Just to be clear OnlyOffice is not related to WPS, it is an FOSS project and HQ is in Riga, Latvia. I often prefer it over Libre tbh.
Any updates?
Shouldn't that be the opposite? External Hard Drive's supply stay while the demand is lower, resulting in lower price.
Lol sure, r/iam14andthisisdeep

Tencent has a ~10% ownership stake in reddit, a pseudo anonymous forum where I waste some time when I'm bored, which when considered within the context of this post, is *totally* the same thing as using software owned and operated by a Chinese company to write a huge novel that criticizes the Chinese government.
There are multiple options depending on how you're composing your documents.

I use and recommend LaTeX for most things, and its source files are good candidates for versioning with Git. Some proprietary document formats aren't easily diffed and are less than ideal to keep in Git. I can't speak to WPS in particular.

If you don't mind using proprietary software, Helix Core (Perforce) does a good job of versioning arbitrary large binary files - that's the primary reason it's so widely used in the game development industry. It's free to self-host for up to 5 users.

Volume snapshots can also work but likely require more foresight and technical planning than I think the average PC user can be expected to muster, and can also require significant storage space. Btrfs deserves a special mention for its ability to clone / snapshot individual files - or, with a bit more work, everything within a particular directory (or set of them); this saves on the space requirements. You (and others in /r/linux) probably wouldn't have any trouble getting this working, of course, and it's more "set and forget" than any other option. Just don't forget to also have an external backup in case of disk failure.
Regular snapshats of the filesystem are usually good too.
Their claim would be technically correct too. After all the file is still their. Their behavior is more insidious than just deleting the file. I'm glad I'm able to just use LibreOffice. It's interoperability is good enough for me.
Hmmm‚Ä¶ ok‚Ä¶ that‚Äôs compelling evidence.
Thank you.
>The idea that somebody can break the law simply by having a file can only happen in a dystopian authoritarian regime.

Have you read the stories of people who's account got closed by a Microsoft or Google? No appeal, no evidence, not even a legal contact. Multinationals are dystopian but with better marketing.
Do you know that having copyrighted material is illegal in many Western countries? Not to mention pedopornography. If you ask me, yes we are in a dystopian authoritarian regime.
We all know a self made web server hosting the files, running on a completely self compiled and patched BSD is the best
This! Add it to docker as a container!!! Living my best life
It is free and open source
I do not see how Google can ban anyone for ad fraud, one of their main products "Youtube" just promotes scam ads all the time. Google/Youtube should be financially liable for all those scams and dangerous medical ads.
Only office is really poor and lacks basic features. I can't select a portion of a document to see how many words it is. It's infuriating.

Among the non free stuff, Softmaker Free Office is fine. Otherwise it's Libre Office or the highway.
Nah, onlyoffice is a Russian operation. So trust for them as well.

I guess we're all stuck with Libre for now.
You want to suggest libre software on r/linux? You're going to get downvoted and then banned!
nothing happened. I'll see again tonight.
> pseudo anonymous forum

It's pseudo to the point where it's not even worth mentioning the word "anonymous." They may have not de-anonymized particular users but one should just assume they know who you are or can at least provide de-anonymizing data to third parties.

> is totally the same thing as using software owned and operated by a Chinese company to write a huge novel that criticizes the Chinese government.

I wasn't replying to the OP though. I was replying to your comment which implied to me that you didn't know reddit had a Chinese ownership as well.
Yes there are horror stories that aren't given enough visibility.

Richard Stallman is right about using the cloud. You're renting resources on somebody else's computer. Even people who rent housing have more privacy and legal protections.
Yeah, but you have to run it on a custom architecture on an FPGA. Or you're gonna tell me you trust Intel, AMD, whatever not to have backdoors in hardware?
But you have to have the know-how for that one, fancy pants.
>>completely self compiled and patched BSD

> Arch flair

I am confusion.
I was on their website and there was nothing free there about the 30 day trial.

I am not going to scour their website for footnotes on how to get to their free product while bombarded with enterprise options.

Libre has a nice big download button - that is free
is that something you need to do often?
Only Office is GPL, WPS is proprietary.
OnlyOffive is a Latvian software not Russian
Line office is pretty good feature wise. I don't understand what's better in WPS or OnlyOffice
Ahh ffs...
Russian propaganda?
The horror
What even is this AMD and Intel? I only use a custom, self made RISC-V CPU
Who says I'm using hardware encryption?
LFS was no problem for me, even on a 15 year old laptop. BSD, without a wiki, will be harder ofc
https://flathub.org/apps/details/org.onlyoffice.desktopeditors
Often enough for it to be irritating unfortunately.
You're wrong. It's only registered in Latvia to circumvent sanctions. Otherwise, fully russian.

More over, this company has a OnlyOffice variant made for the russian market. It's called "–†-7 –û—Ñ–∏—Å"(R-7 Office). The interesting thing is, R-7 is the name of the first russian intercontinental missile. That rocket was even on their site at some point. Here is a screenshot - [https://imgur.com/a/bR4lDpJ](https://imgur.com/a/bR4lDpJ)

So here's that. Hope people read about it and that cheap ploy of registering in Latvia doesn't work out.
No, it's Russian with headquarters in Latvia.
The difference is pretty big. For instance, if you are editing an MS Word document that needs to be precisely placed on the page, such as template for a printed form or for a label, WPS will almost certainly get it right, and LO office almost certainly not.

Or go to [office.com](https://office.com) and download some brochure templates for Word, and compare how well they open in WPS vs LibreOffice. In LO, background graphics get moved around, sometimes onto a different page.

There is also a performance difference. Open a spreadsheet with 500000 rows and 1000 columns and try deleting say 50000 rows based on a filter, or insert a column. LibreOffice used to take minutes to do this, even after the much longer delay in actually opening the file. LibreOffice is much better now, but WPS Office is still faster. I consider LO good enough now.

Funnily enough, nothing beats Excel running in Crossover, even though it is only 32 bit and running via Wine.

OnlyOffice is hopeless at these tests. It usually just crashes.
Idk about WPS, but OnlyOffice's internal format is Microsoft Office's. No import and export filters to go from DOCX to ODF and back. This improves compatibility and rendering consistency with Microsoft Office documents a bit.
What do you mean?
I made all the lithography work with my very own hands. You have to step it up.
Thanks, now on mobile I also see the ‚Äòget free‚Äô on their webpage also.
Isn't it a pretty common move to base in Latvia specifically to avoid the problems with state control that we're worried about? Or are you saying that's a ploy?
May i ask you for a source for your says?
The compatibility problem is that LO supports Microsoft's OOXML Strict standard. MS Office does not by default. By default it saves in OOXML Transition, which means MS Office's proprietary blobs are still littered inside. You can actually force it to save in Strict in the Options but you'll lose access to some features. I'm not sure what they are though.

Microsoft created an interoperable standard so the EU would stop fining them 15 years ago. They've been 'in transition' to their own standard since then.

OO has a goal to be as compatible as possible with MS Office. LO's goal when it comes to that, is simply to support the standard.
>onlyoffice is a Russian operation

I think that's Russian propaganda. OnlyOffice has got an HQ office in Latvia, part of the EU and offices in Dallas TX, The UK, Armenia, Singapore.
It's a ploy used to circumvent sanctions put on russia.   


The owner is Lev Bannov from Novgorod, russia.
No. It's not russian propaganda. Quite the contrary, the creator of OnlyOffice is a part of it. You should know that almost any IT person of weight in russia becomes part of FSB. For example, Kaspersky is an ex-KGB officer; Durov, the creator of VK(russian facebook) had to flee russia because FSB demanded he cooperates with them and now they basically own VK. The same goes with Bannov, who is just a imperialistic and militaristic russian swine who is happy to work with the russian government.

Here I'll copy the text I already posted here a couple of times so that you can see.

"More over, this company has a OnlyOffice variant made for the russian market. It's called "–†-7 –û—Ñ–∏—Å"(R-7 Office). The interesting thing is, R-7 is the name of the first russian intercontinental missile. That rocket was even on their site at some point. Here is a screenshot - [https://imgur.com/a/bR4lDpJ](https://imgur.com/a/bR4lDpJ)

So here's that.Hope people read about it and that cheap ploy of registering in Latvia doesn't work out."

You can see the creator of Onlyoffice gladly put a launching nuclear missile on their site which tells me he wouldn't mind cooperating with FSB. And all this opening offices around the world is only to circumvent the sanctions put on russia.
Some Russian opposition media is in Latvia specifically because they're doing *good* things that Putin's regime would shut down. Is Bannov some kind of oligarch crony?

Like, I don't think we need to ban everything touched by a Russian (or Chinese) person without further info. Maybe that further info exists and he's just the worst, I have no idea -- that's why I ask.
Bannov is a regular imperialistic-militaristic russian businessman that, most probably, has ties to FSB(as any IT person of any weight in russia; Kaspersy is an ex KGB officer for example).

I'll copy the text from another message in this thread.

"More over, this company has a OnlyOffice variant made for the russian market. It's called "–†-7 –û—Ñ–∏—Å"(R-7 Office). The interesting thing is, R-7 is the name of the first russian intercontinental missile. That rocket was even on their site at some point. Here is a screenshot - [https://imgur.com/a/bR4lDpJ](https://imgur.com/a/bR4lDpJ) 
So here's that.

Hope people read about it and that cheap ploy of registering in Latvia doesn't work out."
Okay, yeah, I'm onboard then -- that's the kind of thing that really is a big red flag to me.
I like how this looks. It's very clean. Thanks for sharing!
https://github.com/sannfdev/grabby
Rice? What do you mean?
Cute fish? Wasn‚Äôt that abandoned?
I like your hostname!

I preffer printf for bash tho, nevertheless good job!
Love the simplicity!
Thanks and yw
Customising your desktop configuration, sometimes with an over emphasis on eyecandy rather than functionality is referred to as ricing. And what you end up would be a rice. Well while you're at it, it's actually fun but time consuming.
Thanks, noted, and thanks!
Thanks!
Weird. I always thought this was called theming. Any idea why it's called rice instead?
The reason why it is called rice is historical, and perhaps surprisingly, explicitly *racial*. 

"Rice" comes from the world of car modifications. It was originally in full ["rice burner"](https://en.wikipedia.org/wiki/Rice_burner), and was a derogatory term referring to Asian-made cars (which is why "rice") with "useless" or "cosmetic" modifications.

The term's original racial connotations seem to be not quite as well known nowadays, especially in the computer customization communities.
What the fuck?  Why are people using a racist term to describe Linux themes?!
Well, as I said I don't think a lot of people nowadays know the origins of the term. And of those who do, a lot of them consider the term as having been "reclaimed", i.e. it has lost its original racial connotation, and is no longer used in a racial manner.
It‚Äôs definitely a term I don‚Äôt hear much of anymore. I was born in the late 80s but as a kid it always was used in terms of cars. 

Nothing derogatory was really meant by it as far as I know (maybe I was too young or innocent). It‚Äôs similar to other sayings from the past that we no longer use often.

But yeah, if you riced out your car or in this case your Linux theme (lol) it means you gave it a lot of eye candy that by itself doesn‚Äôt serve much functional person. 

The Fast and the Furious.
Cool, do you have plans to upstream this?
While I very much like the simplicity of pledge, portraying Linux as requiring you to write raw BPF code is a little bit unfair. Nobody, not a single reasonable soul, writes the BPF code by hand, but compiles C with LLVM.

Besides, that mechanism allows your program to run unmodified, as the filter can be applied externally.
I wish I knew more about security and OpenBSD and such, but this seems incredibly cool. It seems like a very lightweight sandbox I could actually see myself using.

I've been reading your blog since finding out about Cosmopolitan and APE and I really appreciate your work! I hope APEs become more mainstream, perhaps at some point we can package applications in a way that's portable to any operating system.
If your Justine, I love you for doing this so f\*\*king much. Especially making a command line version that can be used on any executable!
This is great. Looks sort of similar to Firejail from the user's point of view, but maybe a bit more low-level, a bit more fine-grained in some aspects?
All very interesting research work. But I‚Äôm not sure why this is superior to the _many_ pre-existing wrappers for setting up eBPF-based protections? Also, you say it‚Äôs not clear how to implement unveil, but can‚Äôt that be done by simply filtering openat and friends? Not to mention namespaces, which could even act as a second, redundant file-masking layer.
You mean donate()

/s
Some day, proper application isolation on Linux will exist, similar to Android, but working for the user and not against.
TIL that OpenBSD has had a steady user base of 7000 users between November 21st 2002 and July 13th, 2022.
It's beautiful in theory, I love UX, but damn. BPF, are you serious? The one that found vulnerabilities and has privileges above root. I don't enable it at all and don't want to.
This should be called Amber Heart patch. Since she pledged all her money.
It's a userspace thing, this implementation doesn't need kernel support
I hate to judge large groups of people (though I guess this group isn't really large) but it always seems like BSD desktop users such a high level of ironic shark its borderline insane.

Talks smack about Linux and how it can't do things with unrealistic examples.

 Its like that joker saying Linux isn't good for gaming because releasing your game on every distro's repo would be hard.


7000 hipsters with next to no wifi drivers and game support aren't part of an elite group, they're the juggalos of PC platforms.
It already exists in the form of properly sandboxed Flatpaks. We‚Äôve just gotta work on getting more of our apps to fit inside.
If you actually follow the link you'll find it's a snarky article about the bsd's dying. So I wouldn't take it seriously. 

R/openbsd has around 15,000  subscribers. Which while might want to think it's a large part of the internet it's a small part.

While the openbsd community isn't the biggest It's sub-projects are used everywhere, OpenSSH for example.
I see, thanks.
I use OpenBSD and I do Minecraft mod development and play Minecraft on it basically daily. WiFi supports works well. It covers my use case pretty well (LibreOffice, Firefox, IntelliJ, Minecraft, coding with JVM / Golang / Haskell / Idris / Agda, Discord and MS Teams)

Granted there aren't many games in OpenBSD and Linux has a lot more software available, but describing BSDs desktops as this barren wasteland with 0 supports for anything isn't accurate.
I think you have a very different core ideology about what computers are useful for, compared to most BSD users.

I use my computer to do the work I need to perform for my job (mostly text editing, occasional graphical work). In my free time, I use it to listen to music, watch videos, and as a thin shell to the operating system that is the modern internet.

Very rarely do I run into situations on my computer where I go, "Darn I really wish I could do this, but I can't run it", and, generally, when I do, it's old proprietary creative software which doesn't run on Linux, either.

To describe me as a "hipster with no wifi drivers or game support" would be like the carpenter in the Zhuangzi's parable of the useless oak, who sees a grand tree‚Äîundoubtedly "useful" to itself, and to its ecosystem, being described as capable of shading thousands of oxen‚Äîbut nevertheless declares it useless, for its timber would be of no use in crafting boats or doors or fences or furniture.

I don't know if my operating system is useful for playing video games, but I've never been inconvenienced by it in any of the tasks I actually depend on it for.
"I hate to judge large groups of people"  
*proceeds to judge a large group of people*  

> 7000 hipsters with next to no wifi drivers and game support aren't part of an elite group, they're the juggalos of PC platforms.
  
C'mon man, literally the exact same thing could be said about Linux only a couple of years ago
Wait, the OpenBSD group is a gang? Do they beef with Bloods or Crips?

https://www.theguardian.com/music/2011/nov/01/juggalos-classified-as-gang-fbi
> It already exists in the form of properly sandboxed Flatpaks. We‚Äôve just gotta work on getting more of our apps to fit inside.


Sometimes I want a program to be able to run with some privileges - othertimes without them.   

I.e. I don't want Zoom to always be able to watch my entire screen; only when I intend to do a video call where I'll screen share.

Does Flatpak support this?
I dont know if Flatpaks are really sandboxed against programs that actually want to break out. I read different opinions about that but from what I understood, when using X11, there is no real sandboxing, with Wayland, a big maybe.
I'm really interested in trying out either freebsd or openbsd but internet isn't supported on my laptop on freebsd and I'm guessing openbsd as well and I use my desktop for gaming a lot so linux is much better for that use case. I might look into a small USB WiFi card for my laptop as a solution though, I'll have to see.
> Granted there aren't many games in OpenBSD and Linux has a lot more software available, but describing BSDs desktops as this barren wasteland with 0 supports for anything isn't accurate.

I didn't say there was 0 support but anyone promoting desktop Unix while attempting to take shots at Linux is a prime example of trying to punch up.

Like, for real. Hardware support is still a concern with desktop Unix, mean  while I'm throwing Linux inside everything willy nilly like Ghangis Khan threw his seed with no real issues.

And for software its not just games. Linux has almost every thing and what it doesn't have it still has via WINE/Lutris/Proton/others. Desktop Unix isn't even in the running. Hell now you can just install Windows programs using snaps/flatpaks/appimages.

If OpenBSD works for you then use it, same with anybody else but if people like OP want to jab at more useful platforms they should expect to be called out.
Does the Linux binaries work well on bad?, I'd like to carry over, but I need Discord and steam (gotta grind that Factorio playthrough).
> I think you have a very different core ideology about what computers are useful for, compared to most BSD users.
> 
> I use my computer to do the work I need to perform for my job (mostly text editing, occasional graphical work). In my free time, I use it to listen to music, watch videos, and as a thin shell to the operating system that is the modern internet.
> 
> Very rarely do I run into situations on my computer where I go, "Darn I really wish I could do this, but I can't run it", and, generally, when I do, it's old proprietary creative software which doesn't run on Linux, either.
> 
> To describe me as a "hipster with no wifi drivers or game support" would be like the carpenter in the Zhuangzi's parable of the useless oak, who sees a grand tree‚Äîundoubtedly "useful" to itself, and to its ecosystem, being described as capable of shading thousands of oxen‚Äîbut nevertheless declares it useless, for its timber would be of no use in crafting boats or doors or fences or furniture.
> 
> I don't know if my operating system is useful for playing video games, but I've never been inconvenienced by it in any of the tasks I actually depend on it for.

This reply contains much of the very snark I was referring to and may even qualify as an r/iamverysmart post.

I always go with the "right tool for the job" mentality for everything from knives, boots, and jackets to PC parts, laptops, ans operating systems.

So answer this, what does desktop Unix actually offer the user that isn't already covered by Linux? And I mean in the literal sense, not spewing vague nonsense like OP's post.

People make the claim doas is somehow so much better than sudo so Unix is better despite the fact doas is available on Linux too, not that it really gives a benefit to desktop users in any meaningful way using one over the other.

People claim bin/sbin being "cleaner" in Unix makes it better despite that meaning next to nothing to desktop users.

People also throw around "Unix is built with security in mind!" like some how Linux isn't.

The benefits of using Linux is obvious but I've yet to see a single good reason to use Unix over Linux for the desktop.
Even a decade ago, Linux was absolutely dominating on anything except for desktop
> "I hate to judge large groups of people"
> proceeds to judge a large group of people
> 
>     7000 hipsters with next to no wifi drivers and game support aren't part of an elite group, they're the juggalos of PC platforms.
> 
> C'mon man, literally the exact same thing could be said about Linux only a couple of years ago

Seems like that joke flew right over your head. 

Should I have that dude from family guy explain the joke to you?

That said unlike desktop Unix, Linux's growth was held back by myths and a straight up lack of people knowing it existed (and gaming but that only effects people who play games).

I have been using Linux since Intrepid Ibex and onward. Even back that hardware wasn't really a concern when I was installing Linux. Desktop Unix still has hardware support issues.

You suggest that just a couple of years ago Linux had only 7000 users and wifi trouble which is INSANELY inaccurate. 

Not only does Linux have the best driver support out of any OS but entire countries have used Linux as their official OS (such as north Korea).

While there isn't a real way to measure Linux's desktop population accurately the 1~2%  number has always been in question. How could there be so many more people using Linux now and so many more companies issuing Linux laptops and it only goes up less than a percent?

And finally the real question, why Unix over Linux?

You end up with a system that is functionally what you would get with a Linux Install but with less usability and support less hardware.
> Wait, the OpenBSD group is a gang? Do they beef with Bloods or Crips?
> 
> https://www.theguardian.com/music/2011/nov/01/juggalos-classified-as-gang-fbi

Just in case this isn't a joke I should make it clear Juggalos are not in fact a gang but a bunch of misfits that were rejected my normal misfits and also listen to ICP.

Calling Jaggalos a gang is akin to the cops seizing airsoft guns claiming they can be made to shoot bullets.

Its classic old man syndrome.
FlatSeal can be used to change the permissions for a given Flatpak. The changes are persistent, but there‚Äôs no reason you can‚Äôt just keep changing them back and forth.

However, if you want to confine an arbitrary program that isn‚Äôt packaged as a Flatpak, look into bubblewrap (the underlying tech used by Flatpak) or Firejail.

Edit: I didn‚Äôt really your message fully. With Flatpak things like camera access are handled dynamically. When it tries to access that kind of thing, you‚Äôll get a permission prompt to accept or decline. You can choose to have your decision be remembered or to ask you every time. It‚Äôs like on Android / iOS.
I don't know about Flatpak, but Firejail is good for situations like this.
With X11 it isn't possible, period.

Well, except maybe if you want to run a full Xorg instance for EVERY single program.
> I'm really interested in trying out either freebsd or openbsd but internet isn't supported on my laptop on freebsd and I'm guessing openbsd as well and I use my desktop for gaming a lot so linux is much better for that use case. I might look into a small USB WiFi card for my laptop as a solution though, I'll have to see.

Honestly used Unix a bit for a limited time for work years ago and it was fine.

Server side of things if both support your needs you can  use either  on a functional level

That said, software support, hardware support (not as big an issue on server side), flexibility, and software options all point to Linux more often than not.

As far as the desktop goes, theres not much of a reason running Unix other than learning/tinkering. There isn't a single task/job/thing a desktop user could do better using Unix instead of Linux.
Try OpenBSD anyway. The BSDs don't all use the same drivers, and porting them (if that happens) takes time.
I refuse to use an operating system where I have to buy an external Wi-Fi dongle in order to have internet work. That's a silly expectation.
No. Linux binaries do not work at all on BSDs natively. They are completely different platforms like MacOS or Windows. Afaik FreeBSD and NetBSD both have some sort of campatibility layers that can run linux binaries with added overheads, though I am not sure how well they work, while OpenBSD offers none. 

Discord desktop app isn't ported to OpenBSD because due to electron. That said, the webapp with Firefox / Chromium works well enough in my experience. 

Steam and Factorio aren't ported to OpenBSD as well. Not many games can be run on OpenBSD because wine cannot be ported for some reason (something to do with 32bit support I think). Only games I know of that has been ported to OpenBSD are Minecraft (easily ported because it uses Java) and Stardew Valley (idk much about this game).

There is [/r/openbsd_gaming](https://www.reddit.com/r/openbsd_gaming) if you are, for some reason, interested in gaming on OpenBSD. It's a very niche subcommunity of an already niche community so good luck.
I primarily use Linux on the desktop, but I will happily admit it has a number of problems which some of the BSDs (GhostBSD/FreeBSD, OpenBSD) do well.

You've already listed some of them - cleaner design, security focused by default. Linux has security tools, but they usually aren't enabled or configured properly out of the box to be useful.

The BSDs are much easier/smoother in their upgrade process. Usually lighter on resources, less coupling between the base system and third-party tools, etc.

Linux is, for a lot of people, more practical. I completely agree with that. However, just because it has some benefits doesn't mean we should ignore the positive aspects of the BSDs that could be adopted.
Thanks for the detailed answer.  Exactly what I was looking for.
Its not an expectation, plenty of wifi cards work with BSDs, just not mine. Its the same situation with Linux. The sad reality is that if you want the best hardware compatibility then windows is the best because the vast majority of consumer desktop hardware is going to be made with windows users in mind.
That‚Äôs not true, FreeBSD has a Linux compatibility layer that implements the Linux system call interface and can run unmodified Linux binaries.
> You've already listed some of them - cleaner design, security focused by default.

Again if the only example of "cleaner design" you have is bin vs sbin then Unix doesn't really have a cleaner design in any meaningful way.

And again Linux is also security oriented at its core as well. Simply saying "security focused" with no real examples doesn't mean anything.

>Linux has security tools, but they usually aren't enabled or configured properly out of the box to be useful.

Any examples?

>The BSDs are much easier/smoother in their upgrade process.

This again is a vague statement and can either be true or not true based based entirely on what Linux distro you are talking about. I wouldn't exactly categorize this as a Linux vs Unix thing as someone could simply just choose a stable distro and now suddenly this statement is meaningless.

>Usually lighter on resources,

Again distro dependent but also depends on how you use your PC. From what I can tell most BSD users are using KDE or Gnome. If I use MATE on ARCH does that make Linux lighter than BSD?

>less coupling between the base system and third-party tools, etc.

This could have been written a little different. 

Are you saying less integrated or more integrated with the system?
I mean they're programs, you install them and if they work the way you want them to then thats it.

Any real world example of pros and cons to this concept with BSD vs Linux?

>Linux is, for a lot of people, more practical. I completely agree with that. However, just because it has some benefits doesn't mean we should ignore the positive aspects of the BSDs that could be adopted.

I'm not talking about abandoning BSD, my whole point was originally about OP essentially being the nerd at the table throwing shade at normies.

If there are real benefits of using desktop Unix over Linux I'd love to see them. Infact I've combed the internet for just that just incase I was missing something like when I first started using Linux.

But in all honesty alot of the people using BSD on forums, youtube, and else where that make content haven't actually pointed anything meaningful out.

Listening to them is like a verbal copy pasta.
Ah, I should clarify it doesn't work natively. Thanks for pointing that out.
Cute name
Cool project! Thanks for sharing this. :)
zbar with `zbarcam` `zbarcam-gtk` `zbarcam-qt` and `zbarimg` handle this pretty well too. I like how yours also appears to regenerate the QR code on the terminal after scanning it in that's really nice.

A good addition to the CLI would be a way to just drag a box over my portion of the screen with a qr code and have it process the resulting screenshot data instead of having to resort to the above utilities.
I use something similar for wifi  when travelling.

- Connect with your phone first (scan qr or type code) 

- Use Android to share connection via qr

- Terminal command to scan phone QR 

Wifi on laptop easy as.
I need exactly this, but for datamatrix codes instead of qr for work :O
How can this be used by online apps installed in servers that run on linux?
I got a similar feedback before. However, I strongly believe that it's beyond the scope of this terminal tool. A screen capture utility would be a gui tool which better stay decoupled.

However, it's possible to improve the api to allow piping images as binary data from another tool like a screen capture utility. I'll try implementing it and documenting how to integrate.

EDIT: Here's the PR: https://github.com/sayanarijit/qrscan/pull/5
[ShareX](https://github.com/ShareX/ShareX) can scan qrcode from screen capture and also generate qrcode from text. It has also a cli [https://getsharex.com/docs/command-line-arguments](https://getsharex.com/docs/command-line-arguments).

&#x200B;

Edit: sorry I got confused. it is only for windows.
> drag a box over my portion of the screen with a qr code

heres a cool snippet:

`scrot -opfs - | zbarimg -q --raw -`

Uses scrot to screenshot an selected area and zbarimg to read it. Outputs the result as text.

More usefull in a desktop environment would be adding xclip to it, so it puts the result to clipboard:

`scrot -opfs - | zbarimg -q --raw - | xclip -selection clipboard`

Best to use by putting it behind a hotkey.

(Ofc this can be used in other ways too, I have actually made an [small script handling screenshots](https://codeberg.org/Okxa/simple-screenshot-handler-script/), which might be of interest, even as a example if not just as is.)
What's the use case?
Can you make it so it scans off from the clipboard?

The workflow would then be:

1. Scan area with your favorite screenshot util
2. Save to clipboard
3. Generare QR with your tool

It'd be handy, just an idea - cool tool
Nice. `qrscan -` also works in a similar way.
Nice suggestion for a usage example.

No need of clipboard with this https://github.com/sayanarijit/qrscan/pull/5
Yeah I was thinking piping from stdin probably works too, awesome - thanks!
Rocky is too slow on this.

Alma released the build on May 26 which is almost 2 months earlier and was pretty close to RHEL 9 (\~10 days).

When it comes to the community rebuild of RHEL, speed is basically the only key weighting factor. And Alma has been winning almost all the time.
Don't see what the benefit of Rocky is, Alma seems to be capable of delivering updates (both big and small) quicker.
I still am confused as to what the relationship between Rocky Linux and Ctrl IQ is? I find it quite alarming that a large number of Ctrl IQ employees actively work on Rocky Linux.

Isn't corporate meddling what got us here in the first place?
Nice, seems like only yesterday RedHat destroyed CentOS.
Are security updates slow, or just major updates? Because I'd wager that speed of security updates is what really matters, and speed of major updates is just nice-to-have.

I agree that Alma is looking better overall.
Hi there, I'm from Release Engineering at Rocky. Yes, you're right, it did take us a while to get our release out. We definitely wanted it out a bit earlier, but things don't always pan out that way. 

There were a few reasons for this though: we have our new build system that we were rolling out, trying to rebuild everything within the new build system (after an initial bootstrap), and then the introduction of two more architectures (ppc64le and s390x) to try to be in parity with RHEL. Since this is a .0, these typically are a big deal and a lot of effort to wrangle, especially with just a beta set of packages and some stream bits where necessary. (imagine the difficulties that centos had way back in the 4/5/6 days...). When you take those things and pile on the new build system, it brings in a lot of unknown variables (like bugs in the build system and package building bugs). I would say those things is what took up most of our time. (As an aside, SIG/AltArch will be starting up armhfp and risc-v builds sometime in the future. That's likely going to take a longer time than what we did for 9.0!)

We're generally pretty good at getting minor releases (eg for 8) out within a 5 to 7 days. Our regular updates are pushed to our tier 0 mirror within 24 to 48 hours of upstream's release. The same will be true with all other releases going forward.

With our new build system, we're hoping to be more efficient on not just minor releases, but major releases too. Ideally, we want to be able to put out betas too, because we feel that is important and we weren't able to achieve that with our old build system. Perhaps I'm idealistic, but I do enjoy putting in the work for this project, and I look forward to seeing what can be accomplished.
Speed of new releases is the only key weighting factor? That makes no sense at all. You're ignoring CPU architecture support, documentation, support options, types of editions, security updates & disclosures.... and in favour of whether a new version comes out within one month or three months? 

People who run these sorts of systems professionally spend months testing and then run them for 5-10 years and you think a month or two on the release cycle is going to matter to anyone, let alone be the most important factor?
Alma uses subkeys to sign packages, unlike RHEL or rocky, so I literally cannot use it, it‚Äôs not compatible enough with upstream.
CentOS Stream delivers them more quickly still. :-)
Hi there, I'm from the Release Engineering team. There are a few of us on my team in particular that are from CIQ (originally starting out as volunteers), with myself and a few other members plus majority of the other teams all being volunteers. It's definitely not a majority overall. Since I'm one of the volunteers co-leading RelEng, that's all it really is for me. I spend a lot of my free time working on Rocky and I quite enjoy the effort I put in. I definitely understand your concern on meddling.

I can't speak to the relationship between us and CIQ other than that CIQ is one of our principal sponsors, since really that's all it has appeared to be over Rocky's life. I am one within that is willing to ask the questions or raise concerns about particular issues that do not sit well with me if something does come up. It doesn't too often, though. As for our board (in another reply, you mentioned a board), we are getting ready to solidify the structure and such. We've just not been able to get everyone together consistently due to `$life` taking priority but we know we need to get it taken care of very soon. Unfortunately my brain is a bit frazzled from the lack of sleep, so I'm forgetting a lot of details.

What I will say is we do welcome anyone to bring up these types of questions to us at our mattermost, forums, and so on if you ever want to ask more questions!
It doesn't matter, it's a multistakeholder game now. Alma and Rocky have a ton of different corporate sponsors, but none of them have IBM's level of control.
So what happened? And does it affect Fedora? I'm a casual Fedora 36 user.
I believe that the security updates are done somewhat timely. But the major/minor version upgrades are all significantly slower on Rocky.
Thanks for your explanation. I highly respect the Rocky team for being active and open, and  also my respect towards the Rocky community who are willing to offer knowledge and help.

The world need RHEL rebuilds, and the road to improve is not ending here. I do hope that Rocky could get better in the future.
Yes. Alma supports the same 4 arch like rocky. The documentations make no difference. The support type are all community-based. So it is a pure win on the alma side.

Alma and rocky are no more or less than 100% rebuild of RHEL so everything comes from CentOS repo directly and every thing goes back to RHEL. The package tests and QAs are mostly done by RH. They do not and cannot revise the code content besides rebranding because it would defer the purpose being a 100% binary compatible clone. Thus what alma and rocky tests are also only about their rebranding and compiling toolchain to see if everything works properly.

This is not a brand-new release model. This is a RHEL clone. So, speed of new releases is, yes, basically the only key weighting factor left here.
Can you elaborate further on this? I do not see how subkeys have impact on compatibility rather than integrity.
One of the last major CVE's that affected RHEL was patched within 2 hours of the vulnerability being announced. I'm not sure when it hit Centos Stream or even Fedora for that matter. Took Alma about 8 hours to release the patch and Rocky nearly two days. The only reason I know that is because I was evaluating a CentOS successor for work at the time and was running RHEL on my personal workstation, Alma and Rocky on VMs on a hypervisor host so I was able to compare the delays in getting stuff released.
Stream makes you seem like a time traveler!
The entire point of Rocky and Alma is, of course, that Centos Stream is Not What Is Wanted.
But CentOS Stream is not a RHEL clone and therefore cannot guarantee that "what works on the current RHEL release will 100% work on it".

It delivers everything much more quickly, but as long as the binary compatibility is broken, it is something completely different than alma or rocky.
Nobody wants CentOS Stream, they should just rename that project and stop confusing people. CentOS is no more, dead, muerto, sleeping with the fishes. If there is no need for the RHEL binary compatibility then just use Fedora.
They are all literally just repackaging RHEL, so IBM "controls" them too.
Both Alma and Rocky have large central corporate backings. I‚Äôm not concerned with who‚Äôs donating a few hundred bucks to put a logo on a website. 

More importantly, these projects are driven by teams lead by those that are affiliated with the backing company. I‚Äôm not sure how the board structure works for Rocky, but in Alma‚Äôs case it had employees of CloudLinux and friends of business partners (they‚Äôre tightly integrated with cPanel stuff).
Ignore the vitriol you see about this.  Instead of CentOS being based on RHEL, now RHEL is based on CentOS.  Fedora is not affected.  I explained it in more detail (with diagrams) on Twitter.

https://twitter.com/carlwgeorge/status/1439724277746573314
They pulled a 90's / 00's era Microsoft with CentOS:

Embrace, extend, extinguish.

RedHat took over control of the CentOS project, then effectively "killed" it by fundamentally altering its core mission.  It was meant to be a bug-for-bug compatible, unbranded RedHat. Now it's pre-RHEL testing platform for new updates.  There was a period in between where they did both.

Hence we now have two major CentOS successors: The spiritual successor from the original CentOS days, Rocky, and the corporate successor Alma.

Alma has a big money corporation backing it which is probably what leads to it being more readily on top of things but it's a corporate backing so... What happens if the corporation changes vision?

Rocky isn't as resource rich but has one of the original CentOS founders leading the team (and it's even named in memory another founder).  They are more slowly finding their footing but their mission is clear and baked into their core: Community driven enterprise Linux, today, tomorrow, always.

While my bias is clear I'm really not anti-Alma.  If Rocky hadn't come along I'd be championing Alma for sure.  I just trust folks more than I trust corporate board members.  Folks CAN also be corporate board members... Unless stakeholders raise a fuss about it.
We appreciate that, and I do enjoy trying to help where I can! I'm hoping we can continue to improve and get better in the future as well!
Nope! You are assuming that everyone out there is sitting around to test new versions like your thinking suggests! Maybe you should tell them about your skills and volunteer to speed up releases.  Oh  wait, you are busy here whining about delays of a few days.
> They do not and cannot revise the code content besides rebranding because it would defer the purpose being a 100% binary compatible clone. 

They claim to provide some images with optimisation for Google Cloud.
It‚Äôs tied up in building packages on that distro, I can‚Äôt get mock to properly build my rpms on alma because they use subkeys instead of directly signing packages with their signing keys, rocky works fine.
Of course. But if the speed of update delivery is a concern, then CentOS Stream could be Exactly What Is Needed.
The EPEL dnf countme statistics say otherwise.

https://twitter.com/mattdm/status/1547580016178839553
Binary compatability is not broken. The ABI is stable over the lifetime of a major Stream release, just as with RHEL itself.
CentOS Stream follows the same compatibility rules that RHEL does across a major version.

- [el8](https://access.redhat.com/articles/rhel8-abi-compatibility)
- [el9](https://access.redhat.com/articles/rhel9-abi-compatibility)

If it broke compatibility, then the next RHEL minor version (and the next minor version of RHEL rebuilds) would break compatibility as well.  It's not something completely different.  In fact I just checked and 90% of the package versions in CentOS Stream 9 match RHEL 9.  93% of the package versions in CentOS Stream 8 match RHEL 8.  It can't be any different from RHEL than RHEL is from one minor version to the next.
A lot of nonsense is written about Stream. An example is the claim that it lacks binary compatibility with RHEL.
Latest EPEL stats on Twitter seems to disprove this.

Rocky is more popular than I expected (ahead of Alma), but for systems with a greater than 2 weeks existence, centos stream outnumbered both Rocky and Alma.
Well yeah. You're gonna have to use Debian or Ubuntu if you want to remain 10ft away from that influence at all times while still having a free operating system.
Care to enlighten us how IBM can exert control on Rocky or Alma?
> RedHat took over control of the CentOS project, then effectively "killed" it by fundamentally altering its core mission.

Historically CentOS was run quite poorly, and was repeatedly in danger of collapsing.  Red Hat stepped in and offered the core team jobs to keep the project alive.  It wasn't a hostile takeover like you're implying.  And speaking from inside the project, it's far from dead, it's doing better than ever.  In the past CentOS was created by just a handful of people.  Now every RHEL maintainer is also a CentOS maintainer, which is an exponential increase in engineering resources.

> Hence we now have two major CentOS successors: The spiritual successor from the original CentOS days, Rocky, and the corporate successor Alma.

Both Rocky and Alma were started by CEOs that now sell support for those distros.  Neither one is more corporate than the other.  The difference is Alma's creator started a non-profit and turned over the Alma trademark to it.  Rocky's creator said he would create a non-profit, but instead started a public benefit corporation (that he solely owns) to owns the Rocky trademark.

> Alma has a big money corporation backing it which is probably what leads to it being more readily on top of things but it's a corporate backing so... What happens if the corporation changes vision?

Both Rocky and Alma have "big money corporation backing".  If one were able to total these up as dollar amounts, I suspect they would be comparable.  But funny enough you can't do that because Rocky doesn't disclose all their sponsors.  In politics this is referred to as "dark money".  Regardless, if one of Alma's corporate sponsors changes vision, then all that can happen is they stop sponsoring the project.  Other sponsors can continue, and the non-profit continues to control the future direction of the project.
It's ironic. They wanted to kill free RHEL clone and two more emerged... *and* they lost control.
Any source more detailed on this? I assume most optimizations for clones are done on build flags rather than code revisions. But I may be wrong.
But you can still use even unsigned binary packages on the system right? RPM -i --nosignature should do the install while yum can also do --nogpgcheck. So I would rather not call it specifically a "compatibility" issue.
Most people with those claims are ignorant parrots.

Those stats make sense if we also consider this: https://sigs.centos.org/hyperscale/

The small bunch claiming that centos is not wanted are simple a noisy minority.

Having said that we're glad at work that this happened because we're pretty Happy with Alma.
Binary compatibility IS broken, when we see bugs happen on and only on CentOS already. It does not necessarily need to be related to an ABI break.
Yes but it is still different consider vendor support. On CentOS stream it breaks immediately after the update while the break will only happen on RHEL's next point release. So users are more problem-free and vendors get more time fixing and testing their stuff and hopefully their thing is ready at the next RHEL point release. 

This is what I see the centos stream is good for: a solid testbed for the next RHEL.
*SUSE has a little cry.*
Only insomuch as whatever IBM does with RHEL, they pretty much have to use to remain 100% compatible.  That is why I put quotes around control.
This is just ridiculous.  If Red Hat wanted to kill free RHEL clones, they wouldn't publish the source RPMs to git.centos.org.  All the open source licenses require is providing sources to customers.

I was one of the CentOS team members and was in several of the conversations leading up to the project changes.  Everyone knew that new clones would emerge, and other existing clones like Oracle and Springdale would grow in usage.  The predominate attitude towards this was "good luck, have fun, we're going to do something else".  Red Hat doesn't have to be the one creating a RHEL clone.  In fact, there are clear advantages to Red Hat not being the one doing this.  Alma and Rocky both have invested far more resources into their rebuilds than Red Hat ever did into classic CentOS.  This is already bearing fruit with much shorter release delays.  Alma got 8.6 out in 2 days, and 9.0 out in 9 days.  Rocky got 8.6 out in 6 days, and 9.0 out in 58 days.  Both of these are way faster than CentOS historically.
When the whole point of the distro is perfect compatibility with RHEL, anything that works in RHEL and doesn‚Äôt on your distro is a problem. I mean, if the fedora folks running epel can‚Äôt get mock compatible with alma I‚Äôm sure as heck not going to bother.
If that is the case, then those same bugs will appear in the next minor release of RHEL, Rocky and Alma. Do you have an example of such a bug?
"broken", "breaks immediately", it sure seems like you're intent on implying that CentOS Stream is broken.  Just because you aren't interested in a distro doesn't mean you need to speak negatively about it.  It's much more than just a testbed for RHEL.  It's a solid operating system with a ~5.5 year lifecycle and the ability to accept contributions (unlike RHEL rebuilds that by design must match RHEL, and thus can't change anything).
Is the free version of SLES, openSUSE, actually that interesting on servers? It seems it's rarely ever heard of for that use case. It seems they do have ISOs specifically meant for servers, so maybe it is a thing.
Assuming your username matches across sites, kudos to you for having [filed a bug about this](https://github.com/rpm-software-management/mock/issues/877).  It's often difficult to get people to do that.  But you left out an important detail with your "can‚Äôt get mock compatible with alma" claim.  You were trying to use the Alma 8 mock chroot on an EL7 host.  EL7's yum and rpm don't support subkeys.  This isn't a mock bug or an Alma bug.  You're trying to use a feature on a platform that doesn't have it.  The answer is to upgrade to a newer host that has that feature.

To be clear, mock **is** compatible with Alma, both as a host and as a chroot target.  The issue is EL7 is damn old and is showing it's age.  More details [here](https://bugzilla.redhat.com/show_bug.cgi?id=2017069).
I believe what they create is just "binary compatible" with RHEL and gpg signatures and verfication implementations should not be something related to this (they cannot use RH's signature anyway). But I do see your point here. Maybe submit a bug report for them and ask if they can provide with another signing method?
I do remember seeing something particular on CentOS, but need to search for the source and cannot get it to you right now.

And it doesn't necessarily need to appear in the next minor release of RHEL, because it can be fixed at anytime prior to the release. On the other hand, the CentOS Stream users got to live with it for some time.

We can do a thought experiment here: say the upgraded minor version package in upstream introduces a bug or vulnerability X, which got past RH's QA and landed into CentOS Stream. Then this bug can affect definitely the CentOS Stream, but once got fixed later and patch applied before the next minor release, it may not exist anywhere in RHEL.
I do respect the CentOS maintainers and CentOS stream itself being whatever purpose it wants to. However, it is by definition more prone to break (notice that I am not talking about it being "less stable" on ABI, but more prone to bugs/vulnerabilities and other potential issues) compared with RHEL. Fedora tends to break more than CentOS Stream, and stream more than RHEL. That is how upstream is gatekeeping, which is not anything about speaking positive or negative, but just about facts.
I'm familiar with people having used it as such, but haven't had any direct experience of it myself.

My understanding is that openSUSE Leap 15 is fully binary compatible with SLES/D 15, so fundamentally there's no reason why not. In theory the relationship between them isn't any different to RHEL's with (old) CentOS.
> I do remember seeing something particular on CentOS, but need to search for the source and cannot get it to you right now.

OK.

> And it doesn't necessarily need to appear in the next minor release of RHEL, because it can be fixed at anytime prior to the release. On the other hand, the CentOS Stream users got to live with it for some time.

Unless it's a security issue, then the bug won't get fixed prior to the next minor release of RHEL.

> We can do a thought experiment here: say the upgraded minor version package in upstream introduces a bug or vulnerability X, which got past RH's QA and landed into CentOS Stream. Then this bug can affect definitely the CentOS Stream, but once got fixed later and patch applied before the next minor release, it may not exist anywhere in RHEL.

Yes, that scenario is possible.
All distros have bugs, including RHEL and RHEL rebuilds.  Yes, it's possible for a bug to happen in CentOS Stream and it gets fixed before it gets into RHEL.  What you're missing is the fact that CentOS Stream can resolve bugs faster than RHEL.  When (not if) bugs happen in RHEL, they often aren't fixed until six months later when the next minor release comes out.  If that bug is noticed in CentOS Stream, it can be fixed in a matter of days or weeks.  This same dynamic is true for Fedora to CentOS Stream.  There is so much more nuance here than your oversimplistic "Fedora tends to break more than CentOS Stream, and stream more than RHEL" statement.
Huh, that's cool. I always kinda assumed that SLES diverges from openSUSE way more than Red Hat diverges from CentOS
A quick search gave me this: [https://bugzilla.redhat.com/show\_bug.cgi?id=1911827](https://bugzilla.redhat.com/show_bug.cgi?id=1911827)

A brief read got me the idea that they enabled wayland on stream updates, but decided to revert it back in RHEL 8.4 release because of issues. Things like this can hurt the stream users quite much and it is thus a reason to push users to downstream like alma/rocky or RHEL itself rather than the upstream CentOS stream.
>Update: It looks like this change slipped under the radar of DRM subsystem maintainer David Airlie. He is now seeking Intel to revert/fix the firmware handling still for Linux 5.19 release candidates so as to not break the firmware handling / backwards compatibility. Stay tuned...
Update: there's clarification on the rules of firmware compatibility:

[https://www.phoronix.com/scan.php?page=news\_item&px=Linux-No-Firmware-Breakage](https://www.phoronix.com/scan.php?page=news_item&px=Linux-No-Firmware-Breakage)

&#x200B;

I don't think this needs to be spelled out but maybe it does.. One of the reasons for compatibility is that there are real situations where people need to switch between kernel and firmware versions, often to track down issue with something else or maybe just using a fallback or something else. I cannot believe someone would ignore something so fundamental willingly these days.
TIL business cards are still a thing.  The last time I got a box of these, the company was sold the next month.  If my next card isn't running linux, I'm gonna be pissed!
Now let's see Paul Allen's card
[I'm coming for you](https://youtu.be/6SFs655p3Eo)
Oh shit, that's (you, OP?) the dude who ran Linux on 8-bit AVR!
If it doesn't run Gentoo then you aren't working for me ;)
Holy cow, very impressive project.
But, more importantly, can it run Doom?
Nothing new: 
https://www.thirtythreeforty.net/posts/2019/12/my-business-card-runs-linux/
Damn it's like that nsf/air tag diy shit. Why don't make it wireless
Would be cool to make a resume version of this. Instant job to whoever you hand it to, lol
Even tho i don't understand much about them this are my fav type of projects. thanks for sharing!
My old company gave me a box never used one. My newest company gave me an email with options which all costed ME so screw that. Then covid so yeah no business card exchanging.
Just need egg shell white PCBs.
yup, this project is a follow-up :D
I'm not /u/dmitrygr, but he also did the Linux on 8-bit AVR (also mentioned in the article)
It's really really cool!!! You're amazing
God how the hell does one even understand this much low level computing? Kudos to you. Embedded electronics devs scare me lol.

What's interesting and funny is that you literally just randomly replace TLB entries in the pursuit of needing as little compute power as possible. I'm no embedded engineer but shit like this impresses me. The various ingenious methods people come up with to not want to use unnecessary computing power.
Will it run on a Dreamcast? SuperH <3
[deleted]
It would be pretty epic if Rene could integrate the original SLAX module-builder from Tomas and allow a fully customized "build" of linux systems, for both novice and experienced folks. This could be used via a web interface too; perhaps via donations that could work.

t2 is a great idea and it is awesome to see Rene still on it after like ... 20 years or so. But it kind of lacked traction (and SLAX is sadly no longer as flexible as it once was; I did not like the transition to debian for instance. Now I am stuck with systemd with the "new" SLAX, written by that guy who now works at Microsoft ...)
You will have a bigger issues dealing with the 2.6 kernel rather than finding a distro with support.
Never made it upstream 

http://linux-vax.sourceforge.net/index_____oldsite.html
Absolutely proud of where we are today on Linux, my main work is with Unreal Engine and to those people who say "Windows is the only way for gamedev". You are absolutely wrong, i've been using UE on Linux for about 2-3 month now and its been nothing but joy.
Great stuff! I built UE5 GA in a CentOS Stream 9 VM and was able to run it on my Fedora workstation. Is UE running in XWayland or natively on Wayland? Can't remember the SDL2 config on that, IIRC UE5 uses 2.0.20 (will double check that).

How long did it take to build for you? It was several hours for me in a not-high-core-count VM...
Somewhat unrelated question: Why does UE5 have such good looking  default scenes, and Godot doesn't? I've never used UE, and I've tried Godot a couple of times. I can drag in some cubes and setup a plane or some rectangles for walls or whatever, but the lighting is always just garbage looking. By contrast, any scene I see from UE or Unity always has nice, smooth looking lighting by default - I don't think people are tweaking it all that much.

What does UE do differently? If Godot is capable of it (I'm assuming it's not a technical limitation, and just the lack of good defaults), I'd love to make a Godot template with better lighting and materials and such.
Nice work! Thank you for sharing this u/Drostina. :)
I just wish there were pre-built packages/binaries of UE5. Having to compile everything yourself is quite cumbersome.
now make Quake source port kekw
Very useful comments
I was not able to get Wayland run with Nvidia card. What did you do to get it to work?
Nvidia doesn‚Äôt play nice on my arch install, X11 it‚Äôs fine no issues but dang that‚Äôs nice üò≠



Next laptop I get will be team red


There is no reason why Nvidia should be difficult üò≠


Radeon is next
Very cool, I couldn't even get UE5 (main branch) to compile on Windows ü§£.
Nobody cares about Nvidia + KDE + Wayland :(

Edit: Duck I got downvoted to hell and people didn't get what I wanted to say. 
Basically I'm sad there's still no good support for the above mentioned combo. All the Wayland talk is always about Gnome and never KDE which is a major reason I use Linux at all.
Hey mate, could you please post your instruction on how you got the wayland working under kde? last time i did it and it worked pretty good but the start menu and widgets didnt want to open and work
FYI you don't need to compile from source if you install using Epic Asset Manager. It's way faster than cloning and compiling from scratch.

It basically downloads a precompiled unreal engine binary which doesn't have the source files so it take up even less space.
Thank you! I can confirm it is running on XWayland when checking with xwininfo.

My specs are pretty highend fortunately due to the nature of the work so it only takes about 45min max to build for me.
Epic and Unity are companies with a lot of resources. A lot of work is done on onboarding. But Godot is in an opensource project with limited resources.

If you want to participate, you can contribute or donate to godot.

* [https://docs.godotengine.org/en/stable/community/contributing/index.html](https://docs.godotengine.org/en/stable/community/contributing/index.html)
* https://godotengine.org/donate
By default Unreal Engine have preconfigured directional light/sun, AO enabled, skylight to lit the shaded area, auto exposure, and I think several other post-processing effects because I remember it has a post-processing FX volume comes in the default scene. At least on UE4 from what I rememeber

While when a scene/3D node is created in Godot it just creates an empty node. It's possible to make Godot looking nicer with the World Environment settings that basically works similar to UE's post-processing FX volume (they have AO, SSR, auto-exposure, tone tapping, etc.), and tweak the directional light colour.
Note that this is my opinion completely, I love Godot and where it is going but this is just my experience.

I work with Unreal Engine everyday but have used Godot here and there but defo not experienced with it.

Unreal Engine in size alone is around \~50GB on windows and 120GB when built from source and Godot when I last downloaded was around 50MB if I am correct. That is a massive difference and it is for a reason. Unreal Engine has many many complex tools that let you do anything. That's why AAA companies use it in the first place.

For lighting alone you have ray tracing in UE4 and in UE5 a new tech for lighting is out called Lumen which is incredibly fast and accurate as it gets with shadows and raytracing. Not sure if Godot even supports ray tracing.

There are many other things that are different (for example):

Unreal Engine now uses nanite which is incredible at rendering super high detail meshes all whilst costing almost nothing in terms of FPS/performance. Godot does not benefit from such tech neither does do well imo with 3D games (that's just my opinion)

Unreal Engine has different tools that let you control .. cloth physics, 3d collision settings, 3d animations tools, render images and scenes, DLSS, ANSEL, etc etc.

Godot is developing really well, but it just needs the time and a couple of big names under its belt to really take off
There are a few technical limitations, but really it comes down to having sane defaults, which Godot just doesn't have imo (that will change with 4.0).

[You can learn more about that here](https://github.com/godotengine/godot-proposals/issues/348)
You can find decent setups for Godot in the official Material demo. That can be a starting point for your project.
My pleasure :)
Absolutely agree, every update I have to do this, then maybe even have to recompile my plugins on top
For me on Fedora this took no tweaking at all, it just worked out of the box after installing the nvidia drivers:

Enabled the third party repos -> sudo dnf install akmod-nvidia

I am using a 3080
Fedora 36 (Gnome) is using wayland by default with Nvidia proprietary driver. You need to have 474.xx+ driver. It works best with GTX 16xx and rtx series GPU.
Ive had the same issue with Arch, Only had this all working on Fedora. Even Unreal Engine had a ton of issues building on Arch for me and Unity was janky as well. Wayland didn't work etc.

Therefore I made the decision to stay with Fedora as in my case it just worked much better out of the box. Really gonna miss the pacman and AUR.
Thank you! I didn't do it on Windows when I was using it, just used Epic Games launcher to do it. Not fun to sit there and watch it compile every update, really hoping one day they would provide a better solution.. :(
I don't run Fedora nor KDE, just did a test on it, yet I can appreciate both of them. Even as a tiling manager user
I'm running exactly that, however, I moved over to X11.
I switched over to Wayland on my work pc 5 days ago. KDE + Nvidia T600 + Ryzen 5 iGPU, 5 monitors total.  Been good so far. Running Tumbleweed so latest everything. Have been using  it on Optimus laptop for a while, using intel for the desktop and use the offload to Nvidia for games. No issues.
I didn‚Äôt do anything out of the ordinary, just installed the nvidia drivers, then reboot and it worked out of the box, I checked widgets and start menu but they also work fine but might be just my case or not enough testing. It was super janky before the drivers though.
It pulls the docker snapshot and I‚Äôve had issues with that. I was unable to recompile plugins for the docker version, have you been able to use plugins by any chance?

Also had some issues with C++ and editors. If you have any experience with the two, it would be super useful to me!
Nice! Yeah, my build was definitely single threaded thinking back on it. If I were to have done it on my actual workstation it would have been a fraction of the time. My only gripe with using UE is just how much space is required to build the engine. The end-result engine is on-par with how big the Windows pre-compiled builds are, but I needed to allocate a 200GB disk to my VM to be able to complete the build.

There is some work being done to get UE to be Wayland-native capable, can't wait until it's fully realized:

https://nitter.net/flibitijibibo/status/1511530172553605124
I'll probably donate in the coming months, Godot seems like the de-facto standard for Libre VR stuff.
Unity doesn't have good looking default scenes either
>That is a massive difference and it is for a reason. Unreal Engine has many many complex tools that let you do anything.

That is not the reason though. The reason is that unreal bundles a shitton of assets and static libraries.

The tooling advantage is still real ofc, it's just not responsible for the size difference
Haven‚Äôt tried fedora on it yet, I might tho
Hmm idk about plugins much. Compiling destroys my laptop so that's why I don't do it.

I do however have some experience with c++ and editors. I highly recommend Rider as it works out of the box with unreal engine perfectly with autocomplete n everything that comes with an IDE.

It costs 10$per month but you can get it for free using your college or uni ID, or you can buy a permanent license for like 100$. Otherwise there's not really any other working C++ editor for linux, though Rider is the best one compared to all the other editors windows and Mac have.
It would be absolutely fantastic if they provided prebuilt binaries for Linux + support for marketplace natively. They do offer docker snapshots atm but sadly it doesn't work well with my testings, neither could I recompile my plugins for it, so had to stick to building from source. It would really be great if they provided proper support for Linux someday :)

Currently the build is taking 120GB on my disk if you are curious

I had no idea about the native Wayland client, that would be sweet!!
Do bear in mind, you cannot make profit from Rider if you do get it from using your college email.
> Compiling destroys my laptops so that's why I don't do it.

That sounds like something you might want to check out.
Isn't CLion their C/C++ IDE?
Not to mention their full suite is absolutely worth the price anyway imo, so many tools for so many languages.. all with consistency
Yeah, when I go from just learning unreal to professionally using I tend to buy a permanent license, at least unlike adobe  its affordable and well permanent.
Well hearing loud ass fans with 80C temps for 12hours straight isn't the most enjoyable thing. Though it was definitely worse when I had to do it on my old laptop with 6gb of ram and core i5 gen 6 processor lol.
Yes but it doesn't work with unreal engine, it's just C/C++.

Rider which was a c# IDE was then turned into a game dev IDE and supports C/C++. And I think they'll add support for Godot soon. Unity already has support with Rider as well.
Yeah JetBrains products are definitely worth the money. Just waiting for native Wayland support, but (at the moment) I‚Äôm not having any issues running through XWayland so I guess there‚Äôs no rush.
>Not to mention their full suite is absolutely worth the price anyway imo, so many tools for so many languages.. all with consistency

This guy codes.
If you have other systems available, distcc can be quite effective. I use it for cross-compiling on low powered systems with Gentoo.
Ah I see, other than throttling the CPU by restricting core count, there's not much you can do about that beyond using a cooling stand and *maybe* replacing the thermal paste if it's past due. But <=80C sounds like normal *high* usage temperatures these days (laptops are weird, I'm still used to desktops sitting steadily <40C; and while normal, I'm pretty sure ~80C constant isn't benign but that's a problem with the thin laptop form factor in general).
I see, thank you
To add to this (my own conclusion on why Rider and not CLion supports UE): Rider uses Resharper as its main engine to drive completion and such (the VS plugin) and CLion doesn't. So since Resharper(++) has built-in support for Unreal it probably was easier to integrate that into Rider than adopting everything into CLion (I might be wrong on this tho)
Are there plans for Wayland support in the JVM?

Wonder how their new Fleet client fits into all this, it looks and feels like VSCode, but their docs say it's still a JVM app
Yeah I sadly I do not have that. I sold my old laptop when I got a new one. But definitely good to know if I ever end up with a bunch of old computers that I don't know what to do with.
Yeah the laptop is only like 6months or so old at this point, it has a very powerful CPU and like you said 80C is the usual working at max capacity heat, I've seen this when playing games as well, only once have I seen it go above 80C but other than that it actually holds that temperature without going above pretty well.
Oh I see, that might be right, because you used to be able to mod the code in unreal engine just slightly to work with Clion, but I couldn't get it to work last time and no one has, it might have been changed.

This was because unreal engine would delete definitions n such before Clion could access it, so the fix was so literally delete or comment out that part so that the definitions are available for Clion.

And of course you had to compile the whole thing after you changed the code as well.
JetBrains are working with Oracle on the [Wakefield project](https://wiki.openjdk.org/display/wakefield/OpenJDK+Project+Wakefield+-+Wayland+desktop+support+for+JDK+on+Linux) to bring Wayland support to JDK.
Would it be viable to farm it out to aws during the render / compile ?
Have no clue lol. It would be probably be like a small spec compared to the other beasts lol
> dpkg /dev/sda1

What? Surely you mean a different command?
Most reliable for me has been Debian with Cinnamon and Mint Xfce.
at the last place I worked I was tasked to setup a lab with 30 computers for a highschool. I chose to use dumb terminals with a terminal server. I had to use Linux since Windows terminal server was so expensive to license and was outside our budget. I chose Lubuntu since it was the most performant, other heavier WMs did not play well with the remote desktop solution but Lubuntu worked great. Awesome solution, the students could use Gimp and LibreOffice with ease. Much happiness.
Lubuntu has always been good. I'm not a fan of the modern theming personally so I have stopped using it since it switched to LXQT. However it is very solid.
I tortured Debian recently in VM by installing almost all available WMs and DEs. 

It survived.. well started to work slowly after I manually deleted XFCE, LXDE, LXQT.. etc :-)

But I dunno why and probably there is some background, but my fears are that there might be problems with browsers, videos and other modern stuff if it is being run in these not mainstream environments. I am also  doubtful because of using remote desktop that it might not work well with such environments.
I too love XFCE but use GNOME because of how east it is to change it with extensions and the notifications center. It's very important to me for the nofications to save and for when I click on them to take me to the app in question and message (with email for example). Excluding those two XFCE / LXQT is all I need really.
fsck. I said I don't know nuthin!
I also stopped using Lubuntu around the time it switched to LXQT. It felt more janky, less stable, and heavier on my system.
I did the same w Ubuntu Studio on a quest to make up my mind.  KDE got sick.  A profusion of accessory apps, no clue what went w what.  Lubuntu is running much more smoothly and simply.  I don't need everything that comes w Studio by a long shot.
There is actually a GTK3 port of regular LXDE floating around. It's in Arch BTW. I hope it gets more traction.
üòÉ nobody forbids you to try everything you want. 

And then you will develop your own preferences in choosing the right DE for yourself. 

At least all that LX-\*-de(s) pretend to look simple Win98-ish. 

Mine in Debian was (actually were, because I installed LXDE&LXQT)  some sort of screwed up in terms of  desktop icons, cursors and might have been window decorations. But yet again I had installed a zoo of DEs.
I find the new gnome confusing.  Right clicking... Does nothing. I can't cope with that.
i update a machine when i log into it... i like the rush i get from watching all the letters zoom by
There are two sub-categories of updates.  Security updates fix, well, security problems, so they should be applied ASAP.  Regular updates handle non-security bugs, deal with them when convenient; you could wait for months.  Debian conveniently makes the distinction between those two sub-categories; some distros don't.

The best workflow IMHO is to have some sort of background process that checks for security updates a couple of times a day and warns you, via whatever mechanism you prefer (e-mail is typical).  As there are a gazillion ways to handle this, and as I'm not a Debian expert, I'll let someone else help you deal with details if that's the way you want to go.

(I tend to use some sort of Nagios alerting system, which is overkill for a single desktop).
Depends on how much risk you want to take vs your distro's release cycles. Unless you keep up with CVEs and tech security news, you probably wouldn't be aware with some new vulnerability that may affect you is discovered and when you'll update.

Linux desktops tend to have a much lower tempo compared to Windows desktops of these types of discoveries but it does happen from time to time. Malware is also less of a concern but it does exist.

Furthermore, in the home, most of your security comes from your home router's Many-to-One NAT (Network Address Translation), so if ports aren't forwarded to your Linux PC, then someone reaching out to it will gets its packets dropped anyway. The NAT is not a firewall and is more or less security by design convenience.

It is also typical for a Linux distro's own firewall to come with some restrictive rules on top of that.

All and all, you're pretty safe with Linux in a typical home network, so long as you aren't serving things from it on the Internet. Downloading malware is probably the biggest concern for attack but, once again, there's less effective malware around for Linux.

I would say if you are serving content or services (VNC, ssh, http, Plex, etc.) over the Internet then once a week is reasonable. If you aren't then you can likely get away with going much longer for sure. At that point, I would be more concerned about wanting bugs squashed in the software I use.

This is a great question for discussion though. I'm glad you asked it.
I've used Gentoo for more than a decade. As a new user I remember checking for updates every day, but over time the interval has stretched...and stretched...now I check for updates every couple of months.
As a debian stable user (Bullseye), once every 5 months.
I run pacman -Syu once per day. It's on my daily routine to-do list.
It depends on the distro. The rolling release ones have daily builds, so if you're not updating daily, what's the point?

Ubuntu forces security-critical updates on you whether you like it or not. As for the other updates, if you use the GUI updater once a week, say, you're as well running a proper apt update and upgrade from the CLI every couple of months just to keep things sweet.

Personally, I find weekly updates to be perfectly reasonable on most point-release distros. And with a distro as old and stable in its package base as Debian, some weeks there aren't even any updates to be had.
I would say a "reasonably relaxed" rate would be somewhere between maybe twice a week and once every week or two. But I also see no reason not to update more often if that is your vibe.

Personally, I am a compulsive updater, no real logical reason, but I like it, and will get security updates and new features/buggies marginally *sooner* and i can't blame Arch (began with compulsive *sudo apt- update && sudo apt-upgrade* in my Ubuntu/Debian days, then worsened with Arch and *sudo pacman -Syu* compulsion and has stuck with me in the Fedora/Red Hat *sudo dnf upgrade && Flatpak update*)
When I feel like it
On every Debian Stable instance I manage, I literally have unattended-upgrades set up. This ensures that systems are updated daily without me having to pay any attention to it. I also have email notifications about what the unattended-upgrades did so that I can log in manually if there is something that needs restarting (which is maybe once a month or so).

On my "main" PC with Debian Unstable I update daily. That's the point of using rolling-releases in first place as far as I can tell.
It depends on what you use the machine for.

If you are running a internet-facing service (a web server or a SSH server) then it's important to have those updated promptly. As soon as the vulnerability is announced commercial firms will begin scanning the internet for vulnerable machines, adding them to their products (either selling DDoS 'stressors', or proxy services, or access to the machine to stripmine it for credentials or cryptolock it).

If the machine isn't connected to the internet, then a more relaxed stance is fine.

In larger firms, you're basically 'connected to the internet' (ie, the hacker comes in through one machine, then scans from inside the firewall).

It's also worthwhile being aware that vendors hate causing inconvenience -- a small set of users will whinge loudly forever. So updates tread carefully. Perhaps too carefully.

If you take the view that you don't care if there's is inconvenience -- update automatically, perhaps reboot, and damn the torpedoes -- then your computer's security stance is much better.

Some distros split 'security' and other updates. That's just too naive for the modern era. Apply all updates. Even better are distributions like Fedora, which update to the latest release of the product with an issue, rather than backport a security fix. There's been a bad run where such fixes didn't fix.

edit: I'd also add that raising the bar on access to your machine is really worth it. Network-wise, if you want to run SSH then use a security key as a 2FA and consider limiting logins to only the IPv6 link-local addresses (which Avahi can make a nice user experience -- ssh kombiwombi.local).
Arch daily, Debian stable weekly
I run an `eix-sync` (Gentoo) once per day to see what updates are available.  How often I *actually* update depends on when I think that those updates are important.
Usually around 20 times a day (arch)
Actually for Ubuntu it would probably be better to rely on unattended updates alongside standard Software updater in desktop variant. In case you can't manage it yourself. At least security patches would be delivered in time.
You should be safe as long as you don't connect the internet.
If you don't do it every 15 minutes, your machine will 100% brick.
I backup and update my system once a week and I run Arch. For raspbain I update it whenever I remember my rpi exists or if there is an important security patch
I've been using Arch for a few years now, and on my desktop I run `paru` (same as `pacman -Syu` but also covers the AUR packages) every night as I'm getting ready to be done for the day. It doesn't need to be that often by any means, but it's a habit by now. On my laptop, which I've been using far less often since I've been out of college, I'll turn it on every month or so and do the same. As for my NAS, which is Debian, I'll run `apt update` and `apt upgrade` along with updating my Docker containers every month, rebooting my system if any of the patches require that to take effect.

The desktop updates are a mix of wanting the newest stuff and security updates, while Debian is pretty much just for security. Whenever I see people showing off their computers that have hundreds or thousands of days of uptime, I just think "I really hope that's not connected to the internet, because applying those updates to a running kernel without rebooting is probably more of a pain than they're going through. 

&#x200B;

But yeah, I'd recommend most people to just update their daily drivers once every 1-3 weeks, and they'll probably be fine. If they use software that has a known vulnerability that they hear about, obviously update sooner, but in general that's probably fine.
I use Fedora so I do twice a week on Tuesdays and Saturdays
Every time i am bored, or curious what's new :-D
Pretty much every time I log in.
Whenever there are new snapshots released.
I'm on a rolling distro. I update it whenever it tells me there are updates ready. Why would I do anything else?
>How often do you need to update to stay safe?

Honestly, approximately never. Even if your computer is vulnerable, the chances of actually being affected is very small. This is less true for the web browser because that is constantly running untrusted code. But modern browsers show you a notification when there is an update, so I just hit the Update button when it appears. As for the rest of the OS, I update when Pop OS gives me the annoying popup saying to update. But that's it.
Well depending on your usecase, you don't *need* to update at all. Most 'Basic-Windows-User'-Friendly distros have some software, like yourbsoftware store or smth like that, that will automatically look for updates. I for myself update when I feel like it (like once a week on systems I have set up fully) and when I'm installing new software. (I use Arch and I can just say *# pacman -Syu software_title* and it installs the software and any system updates it finds) Hope this helps
rolling releases
mmm, I'd say a few times a month on my active machines. though, my servers tend to take the back seat on updates, just whenever I feel like doing it. I'd try to at least keep them on a build that gets active updates.
It really depends. I try to update packages about every month or three. I also keep an eye out for critical patches (for example the heartbleed issue? i patched the moment they put out a release for that). 

I also sit on the LTR releases, I do NOT update on each distro (for example, I only go with 20.04, 22.04 etc on ubuntu, I do not upgrade on their insane every six months bullshit).

Mind you, I maintain about 200 or so servers plus a bunch of VMs so I am \*not\* about to try to update daily or weekly, even with ansible.
RHEL 9 servers, daily.

1432 Ubuntu 22.04 VDI's (Desktop vm's trough VMware Horizon) unatended upgrades.

All managed by salt, now and then we fire off something specific trough the whole fleet.
I run updates most of the time when I'm powering off a computer for the day. That goes for my personal Void laptop and my Arch desktop. 

The Ubuntu laptop sometimes gets updated several times a week, and sometimes not for several weeks. I disabled the autoupdate systemd timers so updating needs to be done manualy. It all depends on when I need to use the laptop, which is rarely during the summer months.
I use Arco (Arch).  Although the project lead has great videos I have come to realize that he is an enthusiast and thus fiddles and experiment a lot, reinstalling all the time.  He is currently off on another big tangent.  

I use mine for real work so I fiddle the minimum, thus I update once a month after their next ISO release.  Partly due to the fact that I only want to merge my config files once a month.
Configure daily unattended upgrades and forget about it
I update my workstations completely every time a new version of Fedora comes out (every 6 months). I have some HTPCs, compute servers, and a few raspberry pi's that get updated every few years, if that.

Or did you mean just downloading updates? I do that at least every week on my workstations. It only takes a second and doesn't interfere with my work at all.
Whenever a security patch (read: upgraded or rebuilt package) is available. I'm subscribed to the "slackware-security" mailing list, so I usually find out by email.
As soon as they pop up, which is once a week usually
I choose key packages that I consider "necessary" to stay safe, the package set depends on what use case the system has.  For my laptop, these are:

1) Browser

2) Secure shell

3) Kernel

4) Programming language and runtime (Erlang and gcc)

I dont htink that 'whole system updates' for security is something that I care about, that can happen once a month or every other month and I have no problems sleeping at night.
First thing, every Saturday morning...

    yay -Syu
I run Debian, and just run updates when I get the notification in the tray that there some available.
Every time my distro presents me the news about updates. So when I use the computer, at least once, usually. Gladly in between also, or before switching off once more. It's faster than checking emails.
For Debian systems, weekly-ish. Depends on the system. Most updates don't need a reboot and they've been super stable for years. Fedora would destroy itself whenever an update happened. Debian had never done that to me.

Ubuntu I think checks for security updates daily, that's very good to do. General updates I do weekly-ish also. 

Reboots I aim, usually, for monthly unless there's something interesting that would cause me to want to sooner. 

I do much prefer letting auto updates for Linux do their thing. On Windows, everyone knows how that goes and the horrors of it. But on Linux, almost never have automatic updates been a problem.

Most places I've worked have auto updates enabled at least a little bit. Daily or weekly usually. I would recommend letting auto updates do their job.
Weekly or mid-weekly for me. On Arch.
I usually update my Arch installations once a week. Usually on Saturday. Only if a serious security vulnerability has been discovered that affects computers that can be reached via the internet do I carry out the updates as soon as possible.
Daily. The first thing I do every day is running "topgrade" an utility to update all my package managers (dnf, flatpak, cargo, pip, npm, etc etc).
Depends on the context. Ubuntu? Weekly. Arch? Hourly
On Arch, whenever I have 150 or more packages out of date.

On Ubuntu, every half a year or so.
I'm on arch and I update every 2 weeks
TLDR; Yes  


Longer answer: When ever I feel like. Anything from once a day to once a week.
I use Fedora and the frequency of updates are so high I end up updating couple of times a day sometimes. I know I don't have to but it's exciting in a way.
I generally update my (Arch) desktop systems when I know I'm going to reboot for whatever reason, which can be anywhere from a couple times a week to once every couple months. That way kernel updates get applied immediately and there's no kernel module or (less frequently) application weirdness. Being on Arch I used to update way more frequently, but that quickly becomes inconvenient when you're actually using your computer for more than just endlessly checking for updates and installing them (there used to be days where the only thing I'd do on my computer was install updates).

I update my physical servers and server VMs and containers less frequently, generally every 1-2 months. Debian stable, which is what I run on most of them, doesn't have a lot of updates most of the time so trying to update more frequently doesn't result in any updates.

The exception to all of that is when I learn about a major security vulnerability in something I'm running, in which case I'll update everything affected immediately.
Using manjaro.

I update whenever the software center says there are things to update.
I just set everything to automatically update. I am on a conservative distro though, so I am really only getting security and bug fixes. I do have a comprehensive backup strategy as well.
Daily, usually before shutting down my PC, I use Arch btw
every time it booted up and i use arch linux btw
i update only if the update has more beneficial stuff rather than cosmetic stuff. otherwise i dont bother.
I let my package manager decide when to update automatically.
I'm doing a pacman -Syu every few days, It's not fun having 50 concurrent downloads on pacman if there is only 10 updates :P

In all seriousness I update about once every other day
Last Linux install I did I found a script over at Github ([https://github.com/VirtualZero/up](https://github.com/VirtualZero/up)) made a Cron job for it and had it run every night at midnight. 

Never had to think about updating my system again.
Sorry don't know...
Cron does that for me every time my machine is booting up
Sudo dnf update, motivation for life
weekly
Stepping out of the Arch meme for a second, I do use it on my laptop and only run updates when I feel like it.
That may mean going a week without updates, especially if I don't use it a lot in a week.
I usually run updates once every sunday, just a general habit of mine.
An anacron job that updates onboot

dnf -y update 

No more worries
daily, it's part of my morning routine
Whenever i remember. Void Linux never breaks.
I do it whenever I feel like it. Sometimes every few days, sometimes once a month. If it is work computer I never update during the week, only Fridays towards the end of the day.
It depends entirely on what needs updating. Is there a critical bug somewhere? Update immediately. Is there some nice to have feature? Update when you have time.

Thing is, it is not a fixed cycle since reasons for updating are different.

If you aim to play recent games you need to be prepared to update often as there are new API extensions and other things released almost weekly or monthly and often you need those to run something new.

So, take a look at what your use case is and judge the need for that, not by something someone else might have said at some point. It is depends on what YOUR case is, not what someone else might need.
An hour.
No strict schedule, usually daily (with no fixed time of the day) if I'm not busy and not away. Sometimes can skip a week or two, but not very often. On Arch it is.
I do it once every 2-4 weeks. No issues for 5 yrs as a home/desktop user.
Do you know about the unattended-upgrades package? By default I think it installs only security fixes but it can be configured to install all updates. I'd recommend using it so you get security fixes installed quickly, even when you're on vacation or otherwise unable to do it manually.
If you are running a Debian variant, you might consider installing the [unattended-upgrades](https://wiki.debian.org/UnattendedUpgrades) package and configure it to automatically install security patches.  Then you can do bug/feature updates as frequently or infrequently as you like.
When I have time to spare to deal with any issues that arise from the update. Sometimes a piece of software I use often will have new or changed features and I need time to check them out.
I run updates right before I go to bed in my desktop (gentoo). On my server (Ubuntu) I have auto updates set up and running once a day at 4 am.
Fedora user, I update every Sunday morning and reboot.  I also do my local and remote backups at this time.  

If I read about some absolutely critical security update then I will update sooner.
Every boot, first thing I do
Fedora Silverblue/Kinoite: every 6 months I guess (without counting the Plasma theme and extention updates).
I was going it weekly for a while. Then I got lazy. My last update involved nearly 800 packages, and I had to tell pacman to ignore three of them so that the updates could go through because there was an error importing some keys. For my distro, I really should be doing daily updates to minimize problems.
i have a cron job which runs every week
I run 'sudo dnf update' at least once per day.

Personally, I've found Fedora to be the happy medium between rolling distros like Arch and "point" releases like Ubuntu.
At the very minimum security updates should probably be automatic. Personally my security updates are on auto-install and I have automation that updates everything every 24 hours. I'm on Ubuntu LTS so there's next to no risk in updating regularly. Debian stable should be even safer to update regularly.
Every few times I use them. As for reinstalls, every four years or so.
Usually before turning my computer off at night.
I enable automatic updates and don't even think about it.
openSUSE Leap, check for updates every morning. something comes every 3-4 days...
Honestly, for a more recent Linux user, your intuition is pretty on the money in terms of what you *should* want. Fortunately, there's really no such thing as excessively updating your system; updating Linux isn't like consuming too much magnesium - you're not gonna shit up your system by doing it (unless you somehow slip into "Dependency Hell"), you'll just get really fast at typing the command. I've always been a big Debian-based distro guy as well, and the mentality you have is also correct: Update as often as possible, upgrade as soon as possible. You're already behind the more bleeding edge shit, but a good trade-off is that you're pretty stable as a result.

Best practice is to always update as much as possible to improve security updates and whatnot, but if we're gonna be honest here, you could probably go months or years on a minimal build without suffering too greatly. Probably even longer if you use OpenBSD.

If you feel like you're getting too OCD about it though, you can always just set up a cron job to update and upgrade for you at regular intervals every couple of days, or once a week and then log the output to a file so you can comb over everything.
Assuming you're talking about a desktop system, you shouldn't have to manually check for updates in the first place. Your DE, software store or whatever should be giving you a notification when updates are available
I just let my OS handle updates. Flatpak app update on boot by default and Ubuntu pops-up a message if there are outstanding updates  ¬Ø\\\_(„ÉÑ)\_/¬Ø
For my personal Linux systems, I update whenever the little pop-up tells me there is an update.
arch, i do it like roughly every couple weeks, sometimes more, sometimes less
Me personal computer a few times durinf the week.
The work computer usually at the end of the week or at the start and if need something that needa repo syncing during the week I check the impact and decide if I do it or not.

Obviously I'm using arch linux.
Slackware user here. Last time I updated my system was when I installed it.
Daily. I enabled auto-updates on Fedora Silverblue, so it'll download the update and apply it at shutdown (takes a minute or so, laptop lid closed).

So the system is usually on one of the latest patches.

Because of how Silverblue works, if I shut down the system while updating nothing breaks. And if something goes wrong I should be able to roll back (similar to what scheduled btrfs snapshots provide).

Edit: I like to apply updates daily, because if not I'd have to look out for severe security issues and then apply updates. If updates are painless and don't break, I see no reason not to update daily.
tbh if you like the idea of the beading edge id recommend a noob friendly arch based distro like manjaro, but for Debian based distros I dont think you need to update often at all unless you have an app specific situation like wine, but even then its not like they update wine every day so neither should you, arch is complacently different in that sense, but im sure you've heard something along the lines of arch is "unstable", but in reality you'll find it very hard to brake, and if you use a btrfs backup solution like time shift, it can be suuuuper covenant for experimenting without the headache of loosing something on the disk, but if your content id just update every week or month
depends

on my OpenSUSE tablet, I update it at least once a week. on my Pop!_OS machine, I do it whenever I get a notification from the Pop Shop
Once a week at work except for VTK which goes on vendor schedule and high priority alerts which are done ASAP.  At home - when I think about it.    At the end of the day, balance the time it costs you with the risk that you are willing to accept.
On Manjaro whenever there's a large update I usually wait a few days and I check reddit to see how many technical issues people are having.  If it looks like easily solvable problems then I'll update, if it looks like problems I wouldn't want to deal with personally, I wait a few extra days.

This isn't a Manjaro specific thing at all, this goes for any distro.  You can choose where on the edge you want to be.  You can be as soon as the update is pushed, or a month later.  Whatever your comfort level is.
all the time.

I haven't been able to update POP OS for 6 months because I guess it's out of space in the storage partition where the OS is?
I update when two conditions are met. 

1.	Updates are available 
2.	I have time to deal with the fact that they‚Äôre probably going to fail

My system has not been very good about applying updates through the shop. It marks them as updated even though the update failed though. So I usually have to fiddle around in terminal to get it to actually update. Today it did update flawlessly after I completely reinstalled the shop itself. I think I may have fixed it! So I turned on auto update and I‚Äôll see how it goes.
I update whenever I turn on the computer. For some computers that's daily, for some weekly, some go months between updates
I just update on login
Every couple of weeks.

I use Arch by the way. ;)
Nice try, NSA.
All packages up to date :(
I do this even in Manjaro and openSUSE Tumbleweed. I‚Äôve never had an issue.

During my Arch days I subscribed to the users mailing list and a devs mailing list and regularly checked the news section in the official page just to see if any updates required manual intervention. That limited how often I would update which I did every Monday and Friday and really reached a point where I felt I was working for the machine and no the other way around. Which caused yet another distro hop round.
sudo pacman -Syu, every boot every time, Im actually thinking of putting it into a script like i did in kubuntu years ago
how about security updates ?

if your answer is NO

 shouldn't move to more prebuild distro like arch or manjaro
Is that updating your distro or package manager?
yay
**topgrade** does it all
&#x200B;

You mean cronjob.  ;)

My cron  runs a script to `sudo apt-get update`  and then `sudo apt-get -s upgrade`. So I get an email every day showing what packages can be upgraded. I then can choose to upgrade or not manually. In general, every few days I will upgrade packages.
Pacman -Syyu bruh
Why not run a 3am cron job?
Weekly updates is my general practice, too. I use Arcolinux and Arch on my desktop and laptop(in that order), and Raspbian on my Pi4, and it doesn't cause me problems.
> ...Ubuntu forces security-critical updates on you whether you like it or not...

By default that's good for normal users. For advanced users it's easy to disable/change that behaviour.

    perl -pi -e 's/"1"/"0"/g' /etc/apt/apt.conf.d/20auto-upgrades
> The rolling release ones have daily builds, so if you're not updating daily, what's the point?

To have up-to-date software when I need it. If I am working on something I usually don't need to install anything (everything is already there) so I keep the machine running for two or three weeks. Only then do I do a full update (or when I indeed need a new package in between).
you can choose to skip auto updates on ubuntu when installing it. at least for the server, maybe the desktop is different. even so you can still disable autoupdate.
Ubuntu doesn't force security updates, in the background. You can have it show up in the GUI app instead.
I'm on Arch (btw) and I usually update every day. Sometimes twice a day if I get bored lol
Tumbleweed on my work PC I update first thing Monday mornings so if there is an issue I have time to fix it (hasn‚Äôt happened yet; touch wood) and if I need to reboot for a kernel update etc then it doesn‚Äôt interfere with my workflow. Tumbleweed on personal laptop I update every couple of days.
Confirm weekly updates or even less.  
Already did not to update for months, if not some critical patches have been released.  
No, I hate those who use "Never change a running system", because most of the  
time those persons produced stuff that is old whacky garbage which can implode  
any moment.  
But if not needed, it is not needed.
I came to say this. Debian derivatives get auto security updates, and I may manually update if I'm fiddling with something, but not much otherwise.
What's the difference between stable and unstable? What one is bullseye?
Ubuntu server here, but ditto. Unattended upgrades runs daily early in the AM, as does watchtower for docker stuff.
That's a long time between updates for an Arch guy. I don't use Arch BTW
You could also just run `yay`. This is an alias for `yay -Syu`, so to speak (https://github.com/Jguer/yay#examples-of-custom-operations).
I liked Fedora because it felt like a nice medium between Debian and Arch, except DNF is unbearable to do anything. I could read a book in the time it takes to update. Also, I'm a KDE maximalist and I don't like that the spins aren't hosted on the official website. I also started on an Ubuntu based distro and every one since has almost always been Debian/Ubuntu based, so I'll type a sudo apt command in fedora and get confused why it doesn't work
I haven't had that happen on any of my distros. It'll say "0 updates" and I'll run sudo apt update, and as soon as it's done it'll say "100 updates" always have to run the update in terminal for it to realize there's things that actually need updating
btw i wrote this, just didn't realize my google account wasn't connected to my actual reddit account
Darn. Fooled again. As a word to the wise, we have this thing called "NSAkey". It's a key that keeps us pesky government agents out of your system. It's already built into almost all versions of Windows. You may want to consider getting it. It's Snowden approved I'm sure
vibe killed
The number of events which require manual intervention in arch is very rare: https://archlinux.org/news/

Zero events in 2021, and three in 2022(but only if you use specific packages - keycloak, wxwidgets,  pipewire).

I personally have never had issues with updates breaking things in over a decade since the rc.conf to systemd transition.
> reached a point where I felt I was working for the machine and no the other way around

This. That's why I don't use Arch anymore. There is a line between Linux as a hobby and Linux as a normal daily use computer system which should just work to get stuff done.

Funny thing is that Arch gave me a lot of knowledge and now... I use Ubuntu. But yeah, without snapd, with custom kernel and Nix package manager.

In the end of the day I can tinker shit out of "normal" Ubuntu if I want to but the point is that when I don't - I just don't have to even think about it and just go on with my life.

And for strange experiments I have virtual machines too.
Or they could do what works for them ü§∑‚Äç‚ôÇÔ∏è.

Maybe their desktop is simply not critical security-wise. Maybe they have security on their network that they trust to do what they need. You can't really tell someone they should switch from Gentoo simply because they update every couple months
*Yes*
paru
I didn't know about this; it even works on Windows and macOS. It even handles `tlmgr`, `conda`, and some old emacs installs I forgot about.  
The only notable omission is running `pacman -Syu` in MSYS2 and maybe also Cygwin.
Oooh, I'd never heard of this tool, thanks for the recommend.
I've just been using `alias update="paru; flatpak update -y"` in my .zshrc but maybe I should give this a go!
Don't (!!!) use `-Syy` without good reason. Use `-Sy`. The second `y` forces pacman to ignore all caches and redownload the full metadata. That is unnecessary bandwidth for you and especially for the mirrors you pull from. The second `y` is only an emergency measure for when something got corrupted.
Running a cron job for a system upgrade isn't a good idea. You might end up breaking some part of your environment if packages conflict or need to be replaced.
Even better. For OP's case, if it's Ubuntu they're using, this should give them peace of mind regarding what's "safe" as an update frequency.
>perl oneliner text edits

It's been a long while since I saw people offering ancient wisdom on this subreddit.
`sed -i 's/"1"/"0"/g' /etc/apt/apt.conf.d/20auto-upgrades` should work too.
It‚Äôs an option on desktop too. Even has a graphical pull down to select it.
Bullseye is the current stable. Unstable always has the name "sid".

Unstable Debian is *basically* a rolling release version of Debian with some caveats, probably best described [on Debian wiki itself](https://wiki.debian.org/DebianUnstable).
Is your dnf speed problem on install or actually fetching?
It happens on Fedora, I can confirm.
Make a cron job
15 packages can be upgraded. Run 'apt list --upgradable' to see them. :)
Well I was there for the change from SysV Init to systemd, and also some gcc libs required something, don‚Äôt really remember. Some pacman updates here and there needed my assistance and what not.

I completely agree that the interventions were rare, far in between and in the off chance that something really needed a fix the worst was to chroot and do some magic.

It was a learning experience and I had to fight the urge of reinstalling but it was worth it. But eventually checking the news and mailing lists got old.

I really appreciate blind, zero intervention updates, not gonna lie. I also enjoy the elegance of unbloat and total control Arch provides. It‚Äôs just that nowadays my priorities don‚Äôt completely align with Arch for now.
how about performance wise ?

or how about bugs
Tbf, I switched to paru too. I just aliased it to yay.

Here's a freebie that I've caught sometime ago, for interactive installs:
`yayi='paru -Slq | fzf -m --preview '\''cat <(paru -Si {1}) <(paru -Fl {1} | awk "{print $2}")'\'' | xargs -ro paru -S'`
Yup. Every time I see anyone mention "pacman -Syyu" without the full context for when it's appropriate, I always think "Tell me you read an Arch install guide but didn't understand it without telling me you read an Arch install guide but didn't understand it."
at least if you do, you could probably also schedule a backup before the upgrade
This, plus it takes literally two seconds to type the command, and a couple minutes to let it run in the background. It's not really taking any significant time out of my workflow.

And it gives me one extra thing to check off on my daily to-do list, which is weirdly fun, so there's that.
Fair point.
By the way, as a fellow NixOS user (judging by your flair) what's your update strategy for desktop use? Assuming you use NixOS on your desktop of course.
yeah but pe(a)rl is a gem
sed is missing [a lot of RE features](https://stackoverflow.com/a/67943782/1133275) so it's not my tool of choice.
Interesting. Thanks
Fetching. Install could be a bit lower, but fetching is unbearably slow for anything
You made my day!
(drake yes meme)
So tell us why? (yay vs paru)
That's a hell of a one-liner, thanks for sharing!
That's really dependent on your internet down, but maybe try the fastest mirrors dnf plugin. depends on your mirrors
E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution). :(((
I don't have a strong reason. IMO, both work just fine. I switched at some point (IIRC when the main maintainer of yay, said he would be stop working on it in lieu of paru) to try it out and I never changed back, because it worked fine - and showed the diff of the PKGBUILDs by default, which was a nice surprise.

Shout out to `aur-utils`, which is the real MVP. You create a pacman repo with your AUR packages and when you do `pacman -Syu` it installs them from there. Nowadays, I only have a laptop with Arch, so it felt too much overhead, so I killed the remote server that had it, but it was great as the package building occurred on it and I didn't have to build AUR packages on my local setup. I've been meaning to try it out with aurto, which also comes with timers, which seems great, but I kinda don't want to have packages randomly building, when I might need the CPU/RAM.
Rust?
Glad to help!
Have never come across this and I don't want to ü•∂
Simply change apt to aptitude, it does some magic stuff and usually resolves the issues
Seems positive that he started as a developer at RH
Though I don't use the distribution in my personal life, Red Hat is a wonderful company that does great things for not only its customers, but the FOSS community.
It is unfortunate that the Red Hat administration is not as much known to the community as Canonical and Shuttleworth are. For some reason, people only hype the shits like Shuttleworth and Poettering instead of the other devs working close to the community.
i really want this guys be half of tired about Android phones like me and try something for create a Linux phone
Red Hat is fantastic for business use though. Motherfuckers will write a bespoke kernel module for you if that's what it takes to fix your issue. Best money my company spends every year is for that support contract.
Red Hat and it's devs create a lot of the creature comforts I use for my machines. Podman, Cockpit, systemd, Pipewire, Gnome, NetworkManager (lifesaver) .etc
RH executives aren't as involved in the day-to-day operations because they basically can't be with a company that large. You have to get down to people like Poettering before you really get to the point of "prominent person with enough technical involvement to execute some sort of long term plan." which is what I think you're referring to.

I don't know if either is really super hyped up but if RH executives are just these faceless beings to the broader FOSS community, that's why.
Poettering and Shuttleworth are decidedly not hyped within the community. Maybe outside, but I've almost never heard any hype for them anywhere.

And Shuttleworth is just wish.com Steve Jobs.
> Poettering

Now a Microsoft employee.
Red Hat developers tend to be pretty adversarial towards anyone who doesn't pay for RHEL.  And they've done the community dirty in the past too.

They're an IBM company so they should get absolutely zero respect now.
Not Red Hats market at all.   They have almost no interest at all in the consumer market.
Absolutely!  That was part of my point.  Customers aren't paying for the OS, but rather the astonishingly awesome support.
Even if you are looking to get the certification but can't affrod the subscription (only the exam), they'll give you tons of pointers, including lab.redhat.com

Sure, they want me to be a customer anyway. But there working to get people in and working to get people to pay first.
Even though I don't use any of those applications / systems, I appreciate the contributions.
There's no need to insult Shuttleworth
I used to hype Mr. Space Shuttle, but I didn't know about Mr. Poetter until just a week ago.
If only as many companies involved in OSS did as dirty as frequently.
They've contributed heavily and with very good results to the current Linux stack. We have a lot to be thankful for for RH developers
Google/Apple was not in the phone market and now is the king there
what type of stupid comment is that?
Yep, don't know how that rogue "though" got in there, completely changed the tone of my comment.
Simple. Cause apple want to sell and google want to create an ecosystem they can manipulate. Red Hat? I dont think they give a flying fuck on what you need, thats why custom rom projects or even companies like purism or pine64 exists.
Going from one customer facing product or service to another is easier than a server OS company pivoting to make phones for some reason. If someone made a Red Hat Phone there would be next to no brand awareness amongst the general public like there was with Apple and Google.
maybe this new president in RedHat wish a ecosystem or maybe not
my comment was about that
This is a very interesting read. I like hearing about how Google internally handles tens of thousands of computers. Google had a similar presentation about bespoke Mac tooling back in 2013 that I loved. https://www.usenix.org/conference/lisa13/managing-macs-google-scale
> The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback.

What does this mean for stable/LTS Debian-based distros? Is their way of freezing the versions of tens of thousands of packages for every release still the right approach? Would this also apply to RHEL and its derivates with their much smaller frozen-in-time repos?
Reminds me a lot of /r/openSUSE Tumbleweed with OpenQA just Debian based and probably with a lot more manpower behind it. Interesting that they build everything from source. The fact that incremental updates reduce stress for their server admins makes sense yet was unexpected from me. It is a bit sad that this is closed source.
Very intriguing.  I used to do desktop engineering for Windows based clients before moving into Network Infrastructure.  I've always been fascinated with Linux and especially Debian.  Their journey is an absolutely incredible undertaking.  It makes sense that it took many years.
>In the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.

Interesting. I remember seeing a talk about switching from Goobuntu to Rodete by an Argentinian Debian developer years ago (I think her name was Margarita Manterola), and it sure looked like a Herculean task. It seems that Google has developed a very interesting tooling for their rolling cadence, maybe in the future Debian could use some of that to deliver a new branch with a rolling cadence similar to Testing/Sid as Debian CUT tried to do. However, it seems like it would take a huge amount of work to pull it off.
Finished reading. 

But Google itself is a software workshop. 

I dunno how their workflow would fit other industries and small enterprises.
I'm glad google uses testing and works upstream where possible. Debian Testing is downstream from so many desktop distros.
>Goobuntu
More proof that Google likes to spend money on projects to give the impression that its success is just down to the innovation and cleverness of its people, when in reality Google's success is really due to them being in an unassailable monopoly.  Ultimately it is bad for innovation and bad for progress and technological progression.
That's a very interesting read an all, but for me it seems that some Googlers has too much money and free time on their hands...
*"One issue that we've run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it's possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community. "*

I dont quite get this, could someone please explain what is meant there?
gogol can go blow a horse and choke on it
LOL, at the same time they are still providing backward compatibility in their Android ecosystem. üòÇ Sort of.
I particularly like this one about how they "live-converted" Red Hat Linux to Debian https://www.usenix.org/conference/lisa13/technical-sessions/presentation/merlin
The most important point is: You are not Google! ;-)

Google had several pain points with going Ubuntu LTS and a 2 year release cycle, and upgrading was as massive PITA.

As always, there are different solutions with different tradeoffs, and I am sure the smart folks at Google made a good decision for their pain points.

I am not Google and fixed releases (+ flatpaks and backports) are massive quality of life for me. 

My browser on the desktop should be the latest version, but everything else should just work(TM) and not change at all, unless I want it to change. 

For me, the biggest PITA are unplanned updates or breakages at the wrong time. I'll happily take 1-2 days every 2 years to adopt my Ansible scripts for a new release of Debian. (... and occasionally I need 3 minutes to bump versions of software I need for development and rerun Ansible scripts. 

As always, understanding why Google does something and clearly understanding your own PITAs and problems are much more important than contemplating abstract questions like the end of LTS releases or following the fads/fashions of the week. 

Rolling might be the perfect tradeoff for you, an LTS or everything in between, as always: 'It depends'.

My 2 cents. :-P
If you think about it, both are experimenting with the rolling model, just look at CentOS Stream. And Debian Unstable has always been something of a rolling release as well. Anyway, yeah, smaller incremental changes are not only easier to handle, but they also solve the problem of migrating to new major releases (or not migrating because it's too much of a pain, and just leaving that Cent5 instance as it is until it blows up).
RedHat definitely goes in some interesting directions. CentOS Stream which uses [AppStream](https://www.redhat.com/en/blog/introduction-appstreams-and-modules-red-hat-enterprise-linux) or immutable Distros like [Atomic/CoreOS](https://getfedora.org/coreos?stream=stable) or [Fedora Silverblue](https://silverblue.fedoraproject.org/) baseod on [OSTree](https://en.wikipedia.org/wiki/OSTree) which have a minimal base system and with Flatpak as the main way to install apps on top.

Furthermore Ansible is also driven by RedHat.
Stable/LTS distros are still essential for servers and specific use-case workstations.

For general use-case desktops everyone and their grandmother are moving towards a rolling release model.

I understand that a rolling release model is easier to test and maintain but personally I'm hoping the 6 month release model doesn't go away, it gives a lot of peace of mind to set-up a system and to know that for the next 6 months nothing major will change and you don't have to touch any settings and can focus on what you really want/have to do.
>of freezing the versions of tens of thousands of packages for every release still the right approach?

That was the  design/engineering mistake made in 1990s when the whole distribution fit 2-5 CDs of 650MB. From the killer feature it turned into the dependency and DLL hell as well.
LTS Distros are still best for  the majority of server environments. Servers are much easier to upgrade on mass between releases.
>Is their way of freezing the versions of tens of thousands of packages for every release still the right approach?

Yes, because breakage still happens.

However, it's also up to the admins in question, and Google's people in charge of workstations have decided this is the way to go.  I wouldn't blame an org for going with RHEL for a decade, either.

You can even run on a full rolling distro like Arch if you wanted, it works for the Arch project, but again it's up to what sort of work the admin wants to do and can do, instead of one way being right and the other wrong.
There's two pools: packages built in unstable and packages built in testing. When a new package is built it is built with its dependencies and its build dependencies, and that lives in unstable. That finished package can then be promoted to the testing pool. Since Google's process is to rebuild a package from scratch for their distro they could be in a situation where the runtime dependencies might all be in testing after this new package was promoted, but one or more packages required to *build* the newly promoted package from scratch are not because it got stuck in the pipeline.
Packages that have critical release bugs can't migrate from Debian Unstable to Testing. Also, there is a minimum of 2-10 days for packages in Unstable to migrate to Testing. Additionally, if a critical release bug is found on a package in Testing, it will be removed from Testing until the bug is fixed. Furthermore, before migrating to Testing, a package needs to be built for all the supported architectures by Debian and should not make the operating system uninstallable. All of this are possible scenarios that would impede building packages from Testing as dependencies might be stuck on Unstable, removed from Testing, might make the distribution uninstallable and/or may fail to build on a given architecture.
Way back in the early 2000s, they were really open about orchestrating their servers and using bespoke hardware. They really discuss a lot of this stuff openly.
Nahh. The problem that small business who are not in Software industry just can't afford employing so many admins!!! Naahhh, even admins have disappeared, there is a new word - DevOps. So if you are a small factory with a dozen of CNC, what the hell is rolling release for them??? 

How many DevOPs do they need and how many mechanical engineers positions would have the factory  sacrifice?
> CentOS Stream. And Debian Unstable

Thank you. I'll make a note of it. I was disappointed once: I had installed Mint LMDE (with Debian Stable) on my laptop, thinking it followed this rolling model. Alas! only the Cinnamon desktop (which I don't care very much about) did.
As you seem to know about this question, which Linux rolling releases would you recommend for a PC? Because this is truly my main problem with Linux, which I really like.

You install a brand new version you're all proud of, and after a short time you realize that it's already outdated, then that you can't install the latest version of a software anymore. It seems to me that a long time ago, I could practically, by cheating with sources and repositories, switch from one version to another, but that was DIY and I was younger.
Steam isn't a rolling distro though. It's just a release stage before RHEL.
But aren‚Äôt they handling the updates to the stable branches in small little updates to the dev branches that eventually gets merged into the stable branch every two years?
Because not many people would dig through 60.000 of Debian packages!!!!  A needle in a hay heap dilemma.
I am fed up with 6mo release cycle in Windows... Probably MS has heard a lot from consumers thus reverting it to annual.
Thanks a lot for your explanation! 

So did I get this right, it's all about that a package is built in unstable and then after testing moved to the testing pool. Then when Google trys to build the package from source it could be possible that it depends on some other packages which are still in unstable, right?

If I got this right, one advantage of building everything from source is, that they only get packages which dependencies are in testing too?
Thanks for your explanation! I think I got it now (see my answer on the comment above).

It would be interesting how often it happens that a package is removed from testing and how many critical security flaws are generally in testing. Do you happen to have any numbers on this topic?
The yelling examples are assembly line in China, where equpment is still ruled with WindowsXP or Windows7, but they are assembling phones with SOCs released a few months ago... ü§£
I haven't tried CentOS Stream yet but have tried a number of times using Debian Unstable as my personal desktop.

It can work well for a while and then _something_ you want to do later just becomes impossible due to the current weather forecast of Unstable recently. I had one install working for several months and then I wanted to install Wine (so that I could run the Rollercoaster Tycoon 2 installer to get the data files out of it for OpenRCT2 -- note that OpenRCT2 now can use the installer _itself_ and extract the assets, but anyway..). The Wine available on Unstable only supported 64-bit Windows exe files and gave errors trying to run 32-bit exe's (of which a vast majority by a wide margin, especially if we're talking about legacy games, are 32-bits).

The condition of Unstable at the time had dependency hell and 32-bit Wine support _was not installable_, full stop. The rest of my Debian desktop worked well enough, but I ended up just going back to Fedora. The latest release of that has all the bleeding edge software (very like Debian Unstable) but brings less surprises since the latest Fedora release is the officially managed one (like Debian Stable) where the occasional breaks are fixed quickly. But unfortunately, it has that 6 month release cycle and heavy upgrades. Fedora Rawhide is an analog to Debian Unstable, I'd tried Rawhide a couple times before and it brings the same kind of system crushing (or hopes & dreams crushing) instability at times.

It's always something little like that (and sometimes it's something big - like when glibc itself was having a major upgrade, and Debian unstable (and testing too, IIRC) was _very_ tempestuous.
[deleted]
OpenSuse Tumbleweed is a lesser-hyped but great option in addition to the usual Arch-based suspects.

But any stable distribution + Flatpak versions of Firefox/LibreOffice/other software is another option to get up-to-date applications.
After moving around, I settled on Debian testing for my home desktop (and stable for online servers). Debian testing seems to have the best balance between freshness of packages (aside from the period just before a new stable release when it goes into freeze as nextstable is polished), and stability (aside from the brief period after the freeze ends when a lot of packages are upgraded, so avoiding updates around this time is key).
Most modern stable distros do let you upgrade between releases
Just go with Arch, it's rolling release and packages come as vanilla as possible. Whatever you want to do with it, is up to you, regular binaries are rolling release too, no need for flatpaks. Plus, you get the AUR that includes practically every program you'd ever need.
Garuda is a good one for beginners who don't know what they need to install, otherwise EndeavourOS starts with a relatively clean setup (not "bare" like Arch) and lets you build from there. Both are Arch based and use the Arch repos so they get bleeding edge rolling updates.
No I know.  They tried to sell it as rolling initially but it didn't turn out that way. They might have gotten push back on the concept
CentOS Stream is RH equivalent of running Debian testing during the freeze period before a release.
> So did I get this right, it's all about that a package is built in unstable and then after testing moved to the testing pool. Then when Google trys to build the package from source it could be possible that it depends on some other packages which are still in unstable, right?

Yes.

> If I got this right, one advantage of building everything from source is, that they only get packages which dependencies are in testing too?

This is an indirect outcome. The other goals that they talk about in the description specifically about knowing what's in the binary. Google is a big enough and worthwhile enough target where someone trying to slip in a malicious binary package upstream is an actual risk vector. Additionally Google invested in build tools to scan the source and build-time checks/linters to prevent vulnerabilities from entering the stream and its much easier to do this at build time.

"Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code."
It varies, as no one knows exactly when or if a critical bug will appear, or how long will it take to find a fix for it.
Yea, I remember the glibc update sat pending for months. They [Debian] also often have libvirt and virt-plugin decouplements happen all the time, and Debian's decision to manage the Node.js package the way they have is just not to my liking.

However, I've been running a rolling Debian testing for 4 years - have gone through 2 drives already (your typical 2.5" SSD 500GB, then my first M.2 500GB, now currently on a 1TB WD Black SN750), but have otherwise had no problems other than dealing with the pinning + dependency fail I might have to do from unstable when freezes are happening - though I just held specific packages back and waited when needed.

Now, with Snap being so popular and so capable - I've found more recently that so long as I am careful to leverage my knowledge of Debian and its ecosystem and to look out for Gnome updates that might bunk my next restart; I'm never actually at a stand-still. 

> I go with a snap-based approach during freezes, etc., at this point.

Snap has its advantages - such as essentially sandboxing every package it offers. They come with the dependencies required so there's no need to worry about Debian's temporary state (when it happens); I check for when a freeze is over or apt dependency issues resolve and switch back.

I couldn't imagine a better daily driver than Debian today.
I thought it was a rolling release by default. Before wiping the system off and installing the Ubuntu-based Mint Cinnamon distro, I thought for a while of changing the repos, but I renounced, as I had also an issue with poor wifi, and I wasn't sure at all an upgrade could fix it.
Well. I've often been reduced to doing this, or to using Appimages as long as they don't require shared resources I don't have, but Flatpak is not the lightest way to install a software...
Every time I tried in, say, the last ten years, the upgrade broke before the end. Probably this is partly due to the fact my systems are too much customized. I suppose one must be very cautious, disciplined and religiously respectful of all prescriptions for this sort of upgrade to be successful. Maybe I should uninstall all the apps I built or wrote by myself, all those from optional repositories, etc., before upgrading, but this would be yet more tedious than formatting and reinstalling from scratch, and the upgrade would risk to be less stable.
Can‚Äôt recommend (or disrecommend) Garuda due to lack of experience with it, but I know people who have had good experiences with endeavor. Alternatively, just install arch.
Yea I do think there is room for a true stream. A centos stream that is downstream from rawhide would be quite interesting.
Thanks!

Of course it's mainly a security benefit, the side effect is pretty cool too,
Makes sense. But are there any statistics of the past? I can't really find anything. I'd like to compare the numbers from debian testing to a real rolling release like arch...
It's only a "rolling" OS if you use the **channel** (*i.e. `stable`, `testing`*) names as opposed to the *code* (*i.e. Bullseye, Bookworm*) names.

**Sid** is the only exception. The code name for experimental will never change so that channel will always be rolling.

You can hit EOL sticking to a testing codename, following it down to stable and its ultimate death.
Flatpak is lighter the more you use it. I've had entire GUI applications be distilled into a <20 mb size because I had other flatpaks that use the same dependencies so the flatpak just used those runtimes that I already had on my system. That's kinda it's main feature
> all optional repositories
That's probably why, you should use as-little custom repos as possible and if you need the newest version of an app use Flatpak
> I can't really find anything. I'd like to compare the numbers from debian testing to a real rolling release like arch...

Hmm, I don't know of any statistics, but you could take a look at the list of packages in the Testing branch (https://www.debian.org/distrib/packages#view). It would be better to compare it with Debian Stable or Ubuntu as Arch and Debian packages sometimes have different names or are splitted differently (Debian is the binary-based distribution that splits packages the most among Linux distributions). You can also take a look at https://tracker.debian.org/ to see the state of a (removed) package in all of Debian's branches. Overall, I would say Arch is more comparable to Debian Unstable, as Testing sometimes has periods where it may become unusable or have packages removed for long periods or time. For example, [Blender has been removed from Testing](https://tracker.debian.org/pkg/blender) since 2022-03-27.
For me, if you can't have fun anymore and you have to be even more disciplined than with Windows, it's not worth it.
> It would be better to compare it with Debian Stable or Ubuntu as Arch and Debian packages sometimes have different names or are splitted differently

Makes sense, I didn't know that there are differences between the distro, nice to know, thanks!

&#x200B;

>Overall, I would say Arch is more comparable to Debian Unstable, as Testing sometimes has periods where it may become unusable or have packages removed for long periods or time.

Wait, I am confused a bit now. Isn't testing more stable and usable than unstable? 

Sorry if I'm a bit slow in this matter, but I have never looked deeper in details like this and just used Linux without that deep knowledge. Sadly you don't get to be teached much about in school about (some sort of technical college would be equal to my school) it.
Yes but even on rolling distro you need to be careful with advanced customizations because it can indeed break.

So your point is kinda of mute, rolling, 6 month or 3 year release distros always require some degree of attention when upgrading.
> Wait, I am confused a bit now. Isn't testing more stable and usable than unstable?

Not necessarily. Bugs do appear more often in Unstable than in Testing, but they are also fixed more swiftly in the former. You can take a look at this for a more in depth explanation: https://www.debian.org/doc/manuals/debian-faq/choosing.en.html#s3.1.7

>Sorry if I'm a bit slow in this matter, but I have never looked deeper in details like this and just used Linux without that deep knowledge.

Sure, no problem.
Difference is that a "stable" distro plus extra repos to compensate for the decrepit stuff is probably going to be more painful when upgrading compared to a rolling distro with no extra repos, plus with a rolling distro the maintenance is going to be more incremental compared to big upgrades that touch every single aspect of the system in one go.

I guess I'm probably quite biased though, I happen to like having new stuff and I'm another one of those people who have been screwed over by version upgrades, while my first Arch install has outlasted every single "stable" distro release I've tried without my ever considering reinstalling it.
Thanks! I always understood these two repos were like stages a packet goes through. 

I really like how well everything is documented in Linux, you just have to find the right information\^\^

Have a nice day!
Any reason you did not set the cover in TeX?
You should take a look at optimizing the PDF ‚Äì 4mb is quite large for something that's 90% black text on a white doc. Even just using Adobe's default compression (which crunched the images a bit) got it down to **401kb**.

Understand this isn't the main part of the project, though! Nice looking layout.
You can download this at no charge [here](https://git.kjodle.net/kjodle/the-codex).

Also, a lot of people gave me some very useful suggestions, so check the source code for credits. (Thank you for those. There were pretty useful!)

There is also a forum link, because some people asked me about contributing or collaborating.
Can we post a pdf of this somewhere so mobile users can view it?
I like the end result when using latex and use it often, but i never heard anyone call latex "fun" lol
Hey congrats on your second book. 

I've never heard of Latex before so I looked it up and it seems interesting, especially for complex math and technical journals/articles. Or, if you were just a command line kind of person and didn't want to bother with complex word processors. I'll give your books a read!
Latex really does output impressive results. Congratulations! I love LaTeX too
Should be called "The CoDeX"
The codex Astartes supports this action.
Hell yeah, I've been waiting for a new issue since I saw the first one, excited it's here!
I love LaTeX, but if you really wanted to be in the Linux/UNIX spirit of things then you could have used the original UNIX typesetting system: troff. It comes installed on every Linux system, as groff (the GNU implementation). Troff is much more UNIX-y than LaTeX, which is not surprising since troff was created by the guys who created UNIX. Traditionally you'd use pipes to chain together the various preprocessors for, say, pictures, tables, and math:

    pic some_troff_file | tbl | eqn | troff

Groff has command-line parameters that accomplish the same thing. It seems that most Linux users nowadays are Windows refugees who know troff only&mdash;if at all&mdash;from its use in producing man pages. But it can be used for general typesetting. The classic K&R C book was typeset in troff, and new books (primarily in computer science) are still being typeset in troff. I actually prefer the look of troff's output to LaTeX's. I'm tempted to re-typeset your zine in troff, including the cover. :)
As you mention using LaTeX without a GUI in this issue, I would highly recommend using `latexmk` for all of your future compilation needs (should be available in the standard repos), be they of the LaTeX, XeLaTeX, or LuaLaTeX variety, as it really makes the whole process _much_ simpler (for a quickstart, you just run `latexmk -pvc [MAIN_FILE]`, and it will automatically monitor and recompile your project the correct number of times when it detects changes to any of the source files).
My biggest problem with LaTeX is that the error messages are cryptic. Most of the time the problem boils down to a package missing, but you could hardly guess that from the error messages.
That sketch kinda looks like the guy that invented Git!
My issue with things typeset in LaTeX is that they look specifically like things typeset in LaTeX, and I can't shake off the feeling I'm reading uni material.
As a rich old white man, I am sorry for what I did to people.
Is this like a Linux magazine?
Your ideas are intriguing to me and I wish to subscribe to your newsletter!
Because to build a LateX paper from scratch, we must first invent the universe
As I recall, it was something to do with wanting a different font. Fonts in LaTeX are pretty limited, and I don't know how to create a new font package, yet.
Thanks!

I don't have access to Adobe. Any suggestions for FOSS alternatives? This is something that bugs me, as well.
my first computer was a commodore 64 too, and in page 39 of the pdf the diagram has no transparency, the circles are just stacked one on another, that is, latex is in the blue circle.

(firefox's embedded pdf viewer)

&#x200B;

edit:

if i open it as raw it works correctly
Purchased volume 1!
Link?
[issue 001](https://git.kjodle.net/kjodle/the-codex/raw/branch/main/001/build/codex-001.pdf)

[issue 002](https://git.kjodle.net/kjodle/the-codex/raw/branch/main/002/build/codex-002.pdf)
Please
Trying to get a float in place is the fucking  furthest from fun imaginable.
It is the standard for the vast majority of academic publishing~~, especially~~ in math and mathematical sciences. It's not uncommon for math students to use it for personal notes, homework, powerpoint-style presentations (with Beamer), etc.


edit: Thanks /u/dualfoothands for correcting me: this is *only* true in math and mathematical sciences. In all other fields, most authors use Word. It makes sense, LaTeX takes time to learn and it's only really *necessary* if you're doing a lot of math. But there's a reason a lot of us use LaTeX for everything once we learn it!
It shines in situations where you need to build things from scratch that just aren't possible/too difficult with other software.
Thank you!
Ah, yes! Someone did recommend pronouncing it as "the codek" last time.
Thanks for waiting! It took longer than I thought it would.
I'm working on re-typesetting it just because it's a decent chunk of material to use to make sure I can still work with LaTeX and the -roffs.

Side note: you mean to say `roff`, commonly cited as being short for "runoff", as the slang to mean "print something" ("Hey John, I'm going to go **run off** 5 copies of this book..."), Which was a basic command for basic formatting of text to be output. This split into `nroff` (new `roff`) for terminals and line printers, and `troff` (typesetting `roff`) for typesetting systems, such as the GSI C/A/T.

`groff` is GNU `troff`, and has some additional niceties, such as being able to run the preprocessing steps using CLI flags, instead of a chain of piped commands, as you mentioned.

Random history lesson.
This is also on my to-learn list.
Agreed. But most of them are out there somewhere on the internet, so with a bit of searching, it's often of only medium difficulty to figure out (instead of requiring the kind of mind that can bend spoons!).
I think I know what you mean. It has to do with Computer Modern, the default font in LaTeX. No one outside of the LaTeX world uses that font, and I can understand why. I've never liked the look of it&mdash;it's too thin and spindly for my taste, which is why I always use a different font in LaTeX.
Username checks out.
You might want to have a look at lualatex and the fontspec package. That allows you to use basically any font without creating a package.

Btw, I hope this didn't sound too negative, great work from you! It's just my limited viewpoint of using LaTeX for too many things for too many years...
I think it's likely the size is in the images - could you possibly call something like [jpegoptim](https://github.com/tjko/jpegoptim) on the images as part of the build phase?
Thank you!
Thank you
I think this is the biggest pain point with latex. Math equations are easy and beautiful. Referencing is a breeze. If you use luatex or Rnoweb, inline scripting allows for easy importing of data and table generation. But fucking hell, getting an image right fucking *here* sometimes feels impossible.
Eh, I don't know about vast majority. I've got lots of friends with PhDs in English and Anthropology and they just use word (I know because I've asked). I'm an economics PhD, I use latex because I find the math bits much easier, but a substantial number of journals only want word documents so they can copy and paste into their own typesetting system (which isn't latex). I've recently had to transfer a whole book chapter I wrote into word because the publisher wouldn't accept latex. Also, a little less than half the colleagues I regularly coauthor with use latex.
I do have a couple pure math colleagues and they use latex. 

I don't have much data on this but I'd be pretty surprised if even half of academic output was produced or even drafted with latex
> where you need to build things from scratch

really? I tried using it to take notes on my classes and it was kinda cumbersome. Guessing what package I need to do X, compilers not working, leaving garbage files, idk. Basically I lost the incentive to learn it, it seems really cool but, it's just inconvenient for my usecase compared to markdown/libreoffice
Worth the wait
I'm about a quarter of the way through with a troff version, with the hardest part done: wrapping text around the external images in section 1.1. Troff doesn't seem to have a convenient way to do that, but I came up with a workaround that's "good enough". Got the custom paper size working, too. The rest of the document looks much more straightforward, shouldn't take me too long to finish.
I finished the troff version. I can put it up in its own repository, or would you be open to including it in yours?
That's what initially attracted me to LaTeX in the first place, many years ago.

Ironically, it's the lack of font choices that meant the cover had to be made in LibreOffice. Such is life.
Thank you for that suggestion! I will look into it.
Definitely the images. For example, the c128.jpg file alone is 2.4MB. Its resolution is 3948x1932 and he's pulling it in as is and then rescaling it in LaTeX, rather than rescaling it first and then pulling it into LaTeX.
\usepackage{placeins}

\FloatBarrier
Yeah, I think these are the easiest footnotes I've ever managed. Floats, though‚Äîit's a lot of trial and error (and mostly error).
I guess I was too biased by my own field. Based on [the only data I could find](https://d197for5662m48.cloudfront.net/documents/publicationstatus/27103/preprint_pdf/4afbc42d46d7317f5aa67eaa2cb617b1.pdf), it seems like LaTeX adoption is nearly 100% among math journals and fairly high among other "hard sciences," but almost 0 in other scientific fields (and presumably low in non-scientific fields). 

I doubt journals in these fields are publishing documents made in Word, but it's fair enough to say that the researchers themselves aren't using LaTeX. However, I'd still recommend LaTeX for most computer-savvy academics, even if you're not sending .tex files to publishers. If your papers contain a lot of figures and tables, but not much math, then it might be too tedious.
LaTeX is cool, and while I have heard of people using it to take notes, I don't think it's a good idea. Apart from the fact that it takes a while to compile, the code isn't really readable on its own.

This is where markdown (or plain text for that matter) shines: It's fast to write, hard to mess up and it can be easily converted to a variety of formats, including LaTeX.

Depending on what you're planning to do with your class notes, you can write the initial draft (aka the notes you rapidly take during class) in markdown, then when you're done you can convert it to LaTeX and add all the fancy stuff you need or want for printing or publication.

Or you can convert the whole thing to HTML, slap a cool CSS on top and publish it online, which these days is probably easier to use and distribute than a PDF.
Be careful and aware when disputing with TeX sectarians! üòÑ

(\*)Tex is a programming language for your "documents/reports/articles/thesis", not a wysiwyg system for reflecting surrounding world into the text as quick as possible. So you don't blame yourself for failing to use it as "class notes".
Now I'm curious. Mind showing me the workaround?
Your own git repo is fine. I'm watching it on github now.

And thank you!
Yea, this tracks with my experience. Math, Statistics, and Physics are the only ones with majority latex documents. Collectively they're a minority of academic output. I'm surprised it isn't higher in general engineering, but I'm no engineer, so what do I know.

I love using latex and try to encourage others to use it, but without a ton of success. I hope one day to be able to use git to version control my data, code, and document, and use pull requests to collaborate on the text, but that feels a ways off.
That data you linked to is from 2009 and 2014. For mathematics it was about 97% in 2009, then 92% in 2014. That downward trend might not be a statistical blip. It was Office 2010 that brought in the big improvements in Word's math typesetting capabilities. Since then more and more scientific journals&mdash;including math journals&mdash;started accepting submissions in Word. These are journals that used to be 100% LaTeX for submissions. The decline in overall LaTeX use is not surprising, but its declining use in math is.
>LaTeX is cool, and while I have heard of people using it to take notes, I don't think it's a good idea.

I did absolutely everything (notes, homework, etc.) in LaTeX for my undergrad and masters in math. I wouldn't recommend using LaTeX for live notetaking to a beginner either but I would recommend learning LaTeX anyway! In particular, I recommend it for anyone who either produces a lot of structured documents (i.e. stuff intended to be read by others, not just scratch work) *or* works with a lot of mathematical expressions. There is a steep-ish initial learning curve, but after the initial hump it's really quick and easy to work with. It can be much faster than WYSIWYG editors once you have experience.

There are plenty of reasons to write your structured documents in LaTeX. LaTeX documents  look beautiful and professional (almost) by default, and you also have a massive amount of control over the appearance of the document. LaTeX/BibTex handles references and bibliographies nicely. The support for mathematical expressions is absurdly good.

>Apart from the fact that it takes a while to compile, the code isn't really readable on its own.

Both of these things are not true. A simple document containing a few pages of text will not take more than a second or two to compile on any modern processor, and there's online services like Overleaf which will automatically compile (at least) that quickly on any browser. The language is a markup language; for simple documents it might contain very little except for the plain text of your document (and some preamble). It only gets hard to read when you start doing (very) unusually complicated stuff.

>This is where markdown (or plain text for that matter) shines: It's fast to write, hard to mess up and it can be easily converted to a variety of formats, including LaTeX.

A markdown-only document converted into LaTeX would look almost identical. Making text bold like \textbf{this} instead of like \*\*this\*\* is not that big of a headache.

>Or you can convert the whole thing to HTML, slap a cool CSS on top and publish it online, which these days is probably easier to use and distribute than a PDF.

You can do this with LaTeX too via, for example, LaTeXML. [arXiv](https://ar5iv.labs.arxiv.org/) is looking to adopt this approach over hard-baked .pdf's at some point, and it already works great: here's a [demo](https://ar5iv.labs.arxiv.org/html/1910.06709).
> convert it to LaTeX

Yesss! `pandoc` is magic! As much as I love LaTex, I love not having to write LaTeX directly even more!
I'm trying to be as careful as possible lol

Yeah, I read some people used it for note taking, and I wanted to try it out. But I guess they were really good at typing or something?
I put it up [here](https://github.com/monsieurmoneybags/the-codex). You'll see the image trick in the codex-002.roff file.
I finished the troff version last night. So the secret will be revealed soon. :) Just have to figure out the best way to make it available. By the way, the PDF is only 289kB, compared to 3.6MB for the LaTeX version.
git is a great complement to LaTeX, but I also don't know any other math people who know git. I would encourage them to use it, but I don't get the feeling that most of them like using a computer too much. I bet there's a solid chunk of comp sci people who know both LaTeX and git, plus some other tool I've never heard of (which they'd recommend I learn of course).

I'm happy to recommend LaTeX in general on r/linux because I figure there's a lot of people here who won't be intimidated by a markup language (and that really is the biggest obstacle). Normally I only bring it up to people who are doing some advanced math in college.
Agreed. LaTeX is text-based, so it makes collaborating via a version-control tool like git a lot easier than say, Google Docs. 

It's a bit of a learning curve, but well worth it in the end.
LaTeX is not going anywhere in math. Not until something better comes along, and Word is *not*, even with a LaTeX compatible equation editor.
Just to add, my colleagues who use word also have extensions to do the equations in latex syntax, which takes away a lot of the pain. A counter point however is the rise of overleaf, which may be helping latex hang on
I have a feeling that's because OP for some reason isn't using standard pdf compression (compressed object streams). Another commenter got it to some ~400 kB I believe just with standard compression
The problem for LaTeX is that Word doesn't have to be *better*, just "good enough". And for a growing number of people in math it is. I love LaTeX, but I'm not blind to the decline that's occurring. The mere fact that in 2014 it was *only* 92% LaTeX in math journals is shocking to me&mdash;for years it was 100%. I would not be surprised if it's below 90% today.
Including a 2.3MB image file didn't help either.
It's not shocking at all. What was once impossible is now simply inconvenient. Of course some people are going to use Word, it has a lower barrier to entry. It also produces worse results and is less convenient to actually work with. LaTeX is part of the math culture, it's not being replaced by what is at best an accessibility option.
What do hardware video encoders actually *do*?

Do they just return the motion vectors between two frames?    Because with motion vectors, the rest of video encoding is pretty cheap and easy to do in software?
I don't know what is problem you are trying to solve here. You say it's "desirable", "of interest", "potential" without stating the problem you are actually trying to solve. It looks like intellectual masturbation to me.

Environments variables are just a list of key values strings that are passed the same way command line arguments are when a process is \`exec\`ed. On top of that the C library is the one propagating them through processes and exposing a simple API to query and modify these values. That's basically all there is to it.

If on top of it, some applications have bare-bone parsing and validation, but it's usually pretty primitive. The most sophisticated use you could find are CLI frameworks that will try to find parameters in CLI arguments, then environments, then a configuration file.
If you need typed variables you might as well use a proper language (at least with type hints and checking during development).

This seems like a "solution looking for a problem". Just... Don't.

Also if you are setting a variable based on some network code you check exit code of whatever you executed (wget/curl/whatever) to make sure it succeeded. You don't need to do crazy mumbo jumbo with typed variables and magical error detection.
Environment variables in shell scripts, like all shell scripting variables, do have a type. That type is string, until it isn't, at which point it befells on none other than you to make it so that it is. And when you dive into this unholy mess where dragons eat your brain for breakfast you'll surely find out why people prefer to use actual progamming languages instead.
If you like environment variables, you'd probably like Docker and Kubernetes.  They use it all over the place to pass data around.
Environment variables aren't specific to any kind of shell/programming environment. They're a small section of string-type data that the operating system "handles" (I'm being vague on purpose, the details don't really matter for the point I'm trying to make). It's inherently serialized bytes, which is pretty suitable and intuitive to just be treated like strings in most shells/scripts/programs. 

Having said that, there are some examples of shells that do take variables to the next level while still allowing that ease-of-use of shell programming. PowerShell allows variables to be objects, not just strings (although, environment variables specifically can't be objects). I think something like that might be up your alley, you get a little bit more of the normal features of an object in a programming language, but it's still a normal shell. I think fish shell also has some higher-level collection types (sets, lists, etc) that are more feature rich that Bash's arrays, but it's been a while since I've used that.
"Environment variables" are sets of text of the form "*A*=*B*" made available when a program is loaded to run. The program can access the environment variables via the third pointer passed to the entry point. The Unix entry point looks like this: **int main(int argc, char \*argv\[\], char \*envp\[\], char\*auxv\[\])**. For boring portability reasons access to the **envp** array of environment strings is in practice done via the **getenv()** library function.

This is slightly different to "variables in the command shell", which are usually sets of text of the form "A=B". For example **A="abc"** then **echo "$A"** (which prints "abc"). The **export** command traditionally marks a shell variable as being copied into the environment variables of any programs started by the shell. For example, **export A**.

When started, the shell copies the environment variables into its shell variables. So if a shell calls another shell the result is very satisfying: shell variables marked with **export** end up in the shell variables in the child shell.

Shell variables do not make for a good programming environment, as the shell variables are effectively implemented using expansion -- **$A** inserts the text of the shell variable **A**. In our example **echo "$A"** is expanded to **echo "123"** before that shell line is interpreted. Quoting -- using **""** and **''** and **{}**  \-- are used to control the moment at which expansion occurs. That's both complex -- multiple layers of quoting like **"\\"$A\\""** rapidly appear -- and eventually ineffective.

If you need to write a shell program of any complexity, consider Python or Perl instead.
This is absolutely intellectual masturbation. Or rather, it's just 'hacking away for the sake of hacking' to see what's possible. It's usually by playing that new ideas arise. Now, before I started to play a bit, or dream up some silly ideas in my sandbox, I wanted to see if anyone had built LEGOs, and why they might have done so.

I did have some specific ideas in mind due to past experience with genetic programming and metaprogramming. And, of course, there's always the entire field of either obfuscation or 'getting things to work covertly, or do things they are not supposed to do'.
Yeah, maybe this is a dumb idea. It just seems really cool to have variables dancing between programs, between programming languages, and possibly evoking/evaluating some type of dangerous yet possibly very clever metaprogramming techniques.
' **help declare** '

You can force a variable to be integer or upper/lowercase, for example
Maybe a reference to the dragon book. If not, well, there still be dragons here. Thanks for the input.
Thanks! I was wondering what a higher-level message passing system might be like via this medium. I will keep that in mind.
Well, good luck with that then. I'm skeptical you'd find something inherent to environment variables with what you've described.

I guess that if you really want to go for a DSL/language, you could treat values as code and use the way these are propagated between child processes to do some clever polymorphism and composition. But then, you don't really need to create your own language to try that and there has been shown better ways to build on that.
Sounds like global variables, but across programs. If so, then please no
It's not new, you could always do that. It's called shared memory.
Thanks. I agree. I also mentioned DSLs at first. I was wondering if some project had done anything like this in the past. Seems not, and that's good to know.
Indeed, it sounds terrible at first. And maybe it is. 

I was also wondering about potential upsides for widescale hardware integration.

In any case, it's nice to know that the current state of the art is: "WTF dude -- this is ridiculous. No, of course, no one's doing anything like this because it's stupid. `goto` the F out of here."
It would be nice if there were types that weren't strings though
Amazing extension!
I thought this was an Apex Legends related post
Thx <3
You do realise games adopt words and phrases that predate them right?
that's so not true, Tim Apex Legends has invented the words like Flatline, Havoc, Rampage, Wingman and Devotion. he also created the name Marvin which has never been used by anyone before, ever.
So Mozambique didn't get its name from Apex Legends?
> that's so not true, Tim Apex Legends has invented the words like Flatline, Havoc, Rampage, Wingman and Devotion. he also created the name Marvin which has never been used by anyone before, ever.

Almost all of what you said was true, except he didn't create Marvin..... he bought the name as an NFT.
The magic of FOSS.
And here I was thinking that the most famous GNOME extensions would be [Just Perfection](https://extensions.gnome.org/extension/3843/just-perfection/) and [Burn My Windows](https://extensions.gnome.org/extension/4679/burn-my-windows/).
I thought it's vice versa zorin forked it from these extensions
I doubt there wouldnt be a Taskbar extension if not for zorin, since the same people that developed zorin would probably develop these extensions instead
Honestly, Zorin OS is pretty cool. The only reason I'm really not interested in using it is because I haven't heard anything on their plan for GNOME 4x which I assume would come for their rebasing to Ubuntu 22.04.

That, and because the macOS-like layout is locked behind the paywall (though I did find a torrent for it... somewhere) and their tweaks tool don't play well with normal gnome-tweaks.

I think there's value in Zorin but unfortunately it doesn't fit with how I use the GNOME DE or how I use my Linux desktop in general.

Though, I do wonder why they haven't just released a KDE version. By all accounts and my own experience, maintaining GNOME modifications in-between versions is hell, enough that Pop is making their own DE and Garuda throw in the towel for that. And it's not like KDE can't do their vision for Zorin's user experience, given what Feren OS has accomplished with it.
[https://github.com/ZorinOS/zorin-taskbar](https://github.com/ZorinOS/zorin-taskbar)

>Zorin Taskbar
>
>The official taskbar for Zorin OS.
>
>Based on the Dash to Panel Gnome Shell extension and the Dash to Dock extension by micheleg.


[https://github.com/home-sweet-gnome/dash-to-panel](https://github.com/home-sweet-gnome/dash-to-panel)

>Significant portions of code in this extension were derived from Dash-to-Dock.
>
>Additional credits: This extension leverages the work for ZorinOS Taskbar (used in ZorinOS) to show window previews and allow the dash from Dash-to-Dock to be embedded in the Gnome main panel. Code to set anchor position taken from Thoma5/gnome-shell-extension-bottompanel. Pattern for moving panel contents based on Frippery Move Clock by R M Yorston. Ideas for recursing child actors and assigning inline styles are based on code from the extension StatusAreaHorizontalSpacing.
Zorin rocks ngl
The most famous extensions I have never used?

But on a more serious note I can spot the user who never used gnome 1 or 2. The first gnome releases had the [gnome foot start menu](http://nielssp.dk/guis/gnome-1.2/screenshots). 

The extensions, of which there are multiple seperate ones serving the same or similar purpose, are recreating what used to exist by default. 

If you want real fun try some old releases with gnome, enlightenment and compiz. Or get the extensions to emulate some of it: [Desktop Cube](https://extensions.gnome.org/extension/4648/desktop-cube/) and [Burn my Windows](https://extensions.gnome.org/extension/4679/burn-my-windows/) emulate a few of the effects that that combo came with.

I can still recreate it with more effort, but I miss borderless, title bar less windows with 100% transparency. I used to keep terminals open like that so that the text looked like it was part of the background, it was a nice effect.
Is gnome pronounced like gnome, or genome? ü§î
Gnome doesn't get enough funding for the amount of benefit they're bringing to the community
I am slowly starting to have a headache!!

ZorinOS, BolgenOS... where do you guys catch all those big fishes? 

RH, Canonical, SUSE pumping dozens of millions of dollars into their infrastructure but can't catch up with Windows...

And then suddenly 1 man or small team projects are proclaimed to be the best solutions...:-)))
> To learn that Gnome wouldn't have had a taskbar or start menu if not for ZorinOS is kinda mind blowing.

Mind-blowingly incorrect.  Gnome 2 had this stuff decades ago.  It's not exactly new tech.
Proof?
Proves the Point that Gnome is a terrible Desktop Environment.
Hey Is there a good dash to dock extention one with intellhide. Also should work on Wayland if possible
Looks good, just tried it, "show applications" under Pop!_OS 22.04 doesn't work as expected, it fades out the desktop like the middle applications panel is going to appear but it doesn't and the active application windows just reappear.
> To learn that Gnome wouldn't have had a taskbar or start menu if not for ZorinOS is kinda mind blowing.

does gnome classic mode not count?  Although I doubt the menu is any good for serious usage.
LOL zorin os wtf
And software piracy is just a "surprised fork".
[deleted]
Fork Of Someone's Shit
Hey, they're pretty famous as well.
I tought it was Dash to Dock.
Yeah, when i think of gnome extensions, I always remember those two, especially just perfection
I went and installed Burn My Windows after reading your comment earlier this evening and I honestly think that‚Äôs the coolest, most fun effect, I have ever seen on any OS. I love it! Thanks for mentioning it.
[deleted]
After the forked extensions got more popular, ZorinOS rebased their extensions and started contributing to Arc Menu and Dash to Panel.
Yeah definitely not, but we wouldn't have the same extensions I guess. In a parallel universe, things are different ;)
Or someone would have developed it. It's a popular UI mechanism.
> That, and because the macOS-like layout is locked behind the paywall 

You can actually install it for free, since it's open source. Pro version includes it by default, that's the only difference.

Here's a script for it: https://github.com/channchetra/Zorin-extra-Layouts

> haven't heard anything on their plan for GNOME 4x which I assume would come for their rebasing to Ubuntu 22.04.

Modifying libadwaita would probably take some time, not an easy task.

> and their tweaks tool don't play well with normal gnome-tweaks.

I haven't faced any problems yet. I stick to the default theme though, personally think it's the most modern and polished gnome theme I've ever used.

> Though, I do wonder why they haven't just released a KDE version.

KDE would feel like an overkill I think. Way too many options and way too much redundancy in the UI for a distro that's supposed to be simple and easy to use.
What you're seeing is the new repo. Read my other comment: https://www.reddit.com/r/linux/comments/vx67jm/comment/ifu3ydw/
[deleted]
I, too, love being charged for free software.
There‚Äôs also Blur My Shell
for a GNOME 2-like experience, you could also try MATE on a modern distro. iirc Linux Mint MATE comes with Compiz as an option for the compositing window manager, and Fedora MATE-Compiz has Compiz as the default.
Officially it's guh-nome, but either will do.

You even have some of the "big" tech reviewers calling it "nome" knowing they are saying it wrong, but at this point very few people care.
Yes.  Also guh-nome.
[https://gitlab.gnome.org/Teams/Engagement/AnnualReport/uploads/f66cb1302c634fb4ad8fe5646adc7268/2020AnnualReport-Digital.pdf](https://gitlab.gnome.org/Teams/Engagement/AnnualReport/uploads/f66cb1302c634fb4ad8fe5646adc7268/2020AnnualReport-Digital.pdf)

&#x200B;

Income 2019 2020  
 Administrative Fees $962 $1,292  
 Advisory Board $153,500 $103,500  
 Conferences $98,818 $45,640  
 Donations $632,914 $759,233  
 Events $392 $390  
 Internships $7,700 $8,000  
 Interest $19,810 $6,194  
 Other $1,551 $939  
Total $915,646 $925,189
Go donate to the cause then!
> RH, Canonical, SUSE pumping dozens of millions of dollars into their infrastructure but can't catch up with Windows...

That's because none of these companies focus on consumer level desktops. Their focus is on enterprise alone, which is why none of them have been successful in making Linux mainstream while KDE, ZorinOS, Pop!_OS have been shipping laptops and desktops with their OS pre-installed.

Here's an example, compare https://ubuntu.com with https://zorin.com/os

The homepage will tell you everything that you need to know, Ubuntu is made for enterprises and corporations. ZorinOS is made for my family and friends.
I wasn't talking about Gnome 2 though.
https://youtu.be/y1T4iIin3D8?t=1242
Gnome is fantastic.
GNOME is the uncontested champion in regards to touch screen support.

It's just so happens that within the tiny slice of the market that uses Linux on general purpose user facing devices, only a minute sliver of said market is running it on devices where touch screens is the primary input mechanism.

So, if anything, one could criticize GNOME for being a GUI that bet the farmstead on a touch-driven future that not only hasn't come to pass for the vast majority of Linux users, there is no indication that such a future would be seen as desirable by the vast majority of either the existing Linux userbase or the wider computer userbase, seeing as touch has failed to become much more than a gimmick outside the mobile phone and tablet space.
Nah, it proves that a large portion of Linux users are just as limited and stuck in their ways as windows users
It's called KDE.
Not really surprising. Pop has extensions to hide the applications.
Zorin is better that whatever shittly half put-together arch system that needs to be babied everyday so it doesn't crash and lose your data.
[deleted]
No I'm not wrong. Just go check the commit history of both the extensions, check who made the first commit.

Both extensions are based on Zorin's extensions. They later got popular, and people started adding more features (including ZorinOS) and then ZorinOS rebased their extension on Arc Menu because it was a bigger project with more contributors by the time ZorinOS 16 came.
Hmmmmm.... Tastes like it isn't supposed to be eaten.
Fork Of Someone‚Äôs Software
I also love the clipboard indicator. Saves so much time.
Right, this is an extension for the perfectionists, who enjoys these small tweaks here and there.
Oh, I should've know that there's going to be people making it available for free. I already have a Zorin Pro ISO but that'll be good for the next version since I only really want that layout from the Pro version.

Ah, yes, libadwaita. I experienced the hardship from using Bottles (via Flatpak) on Ubuntu Budgie - had some weird ass window decoration inside window decoration issue. That said, on Pop, I managed to use WhiteSur-dark's libadwaita just fine, so I imagine it's mostly just polish and edge cases now. Good luck to them on that.

I'm mostly a WhiteSur adherent but other than that, I really like to tweak my GNOME experience to fit to my preference. Unfortunately, my preferences is very close to Unity's and that takes way too many tweaks to fit with Zorin- heck, it was too much even for Garuda (which introduced me to it, with their Unity-fied KDE experience) that they throw the towel and just ship vanilla GNOME for their GNOME edition. It's a wonder that Zorin and Manjaro managed to do so much with GNOME and maintain it.

Not necessarily on KDE. Feren OS made it pretty simple to use, even have a UX Layout changer inside. The only issue I'd imagine is KDE's menu withing menus and the rows upon rows of sections in the settings. That said, I think that's where Zorin could bring a lot of value- by properly simplifying KDE, hiding what's not needed by most users, and cleaning the presentation up. I just think it's more sustainable for Zorin's UX than dealing with GNOME, but hey, it's up to them.

Good talk regardless, though since you seem more in tune with Zorin, do you have any news on their plans for the next update? They seems to be pretty quiet to me since last year, at least compared to Mint and Pop.
[https://www.youtube.com/watch?v=y1T4iIin3D8&t=1250s&ab\_channel=LinuxForEveryone](https://www.youtube.com/watch?v=y1T4iIin3D8&t=1250s&ab_channel=LinuxForEveryone)

&#x200B;

Embarrassing
You don't have to pay.
Someone doesn't know what "free" means
Except for that
"Free as in freedom"
Except that you can still install everything. Pro just means that they preinstall it for you.
Free software =/= Freeware
I stand corrected. 1M a year is a lot more than what I was imagining
that's basically no money at all for any big project. Although clearly that's not where most of the work is coming from. Since that's my folks working for other companies
Lenovo ships laptops with Fedora (GNOME) installed. It‚Äôs a pretty standard GNOME environment too, one that is used in Red Hat‚Äôs enterprise OS. Lenovo works with Fedora upstream to make sure hardware support is available. To an extent, Dell does the same. 

I have spent the last 12 years or so supporting RHEL on the desktop in the workplace. GNOME has a lot of features that are quite useful for enterprise customers that is missing in their environments. 

For example, lockdown dconf settings. Very useful for kiosk like systems, or in an environment where we need to simplify settings.
Oh well I have to accept your opinion. Partially. 

I only worrying about security errata pouring like a waterfall thus there are always huge doubts about small teams being able to sort out patches really quick.
You said:
> To learn that Gnome wouldn't have had a taskbar or start menu if not for ZorinOS is kinda mind blowing.

The fact that Gnome already had this stuff indicates that this claim is nonsensical.
https://www.reddit.com/r/linux/comments/vx67jm/til_gnomes_most_famous_extensions_dash_to_panel/ifumyt8?utm_medium=android_app&utm_source=share&context=3

??
I'm most productive on gnome compared to any DE I've ever used (including windows & MacOS)

Why? Because I don't ever need mouse while navigating through Gnome. I press super key and start typing or use personal shortcuts.

When I first used Gnome on Ubuntu few years ago, I hated it. After I learned how to use it, it became my favorite
What part of Gnome is particularly good for touch screens? The overview could work well in theory, but the only way to open it on touch devices is with the small Activities button in the top left corner, which is not ergonomic at all. Have you ever tried using Gnome on a tablet? It's really not great.

Gnome works best with a keyboard-centric workflow imo - I really don't get where this weird misconception came from that it was designed for touch screens.
Gnome 3+'s giant iphone inspired menu aside, I think people overlook how mouse and keyboard centric and usable gnome is and has been since the controversial 3 update. It  encourages easly switching windows with hot corners(something myself and a lot of other people used to set up in compiz and was very useful pre touchpad gestures and with a regular mouse), you can launch anything by just typing super and then starting the type the name of the thing(something during the gnome 2 days people would like install synapse or gnomedo to use to launch), and for heavier window organization you can either just hit super or hot corner(or in more modern gnome three finger swipe on touchpad/screen) and drag that hot potato into new work spaces that you can dynamically create and delete on the fly.  


Its different and mousecentric and not to everyone's tastes but they havent abandoned desktop space in favor of mobile dreams. Even if they keep removing customizability and simplifying things more and more.
yes, but for mobile devices you have android (I can feel the angry downvotes already)
So it's a good Thing you need millions of Extensions that are incompatible with each other Just to get a usable desktop?


That you need a third Party Tool to get a stupid Dark theme?
Like kde extension?
why are you assuming i use arch??

plus i advise you to study a bit if that happens to you. probably not normal.
found the debian user
[deleted]
r/angryupvote
One man's shit is another man's software
The fact that you need an Extension for that makes me Love KDE even more.
> hiding what's not needed by most users, and cleaning the presentation up. I just think it's more sustainable for Zorin's UX than dealing with GNOME, but hey, it's up to them.

Yes I believe so to, if they can manage to pull it off but it won't be an easy task for a 2 person team.

> do you have any news on their plans for the next update?

17 will release next year around August. 16.2 is coming in September.
For Zorin Pro? The website says you do.
Compare Zorin Lite, Core, and Pro. Software that is $0 on Ubuntu is locked behind the Pro paywall with Zorin.
Considering the programs they're pay walling is freeware, it kind of is...
Now imagine what the entire FOSS community could do if it had the budget of Microsoft or Google...
that's basically pennies for a project the size of gnome.
A mid sized team, maybe 2, in a private IT company would already cost something like that. And they would develop a much smaller product.
Still, they are operating at a loss in the ~$100K territory
> Lenovo ships laptops with Fedora (GNOME) installed. It‚Äôs a pretty standard GNOME environment too, one that is used in Red Hat‚Äôs enterprise OS. Lenovo works with Fedora upstream to make sure hardware support is available. To an extent, Dell does the same.

Oh thanks, I didn't know. Never seen them in the wild or the news so wasn't aware.
I think we also have to factor in the threat model when talking about security. I think even without the latest patches, consumer desktops are relatively safe due to the nature of their workings. For corporations and data centers though, it can cause huge losses.
> had

is the keyword.

Gnome also 'had' typeahead search but it doesn't now and I don't think any kind of extension can fix it.
[https://www.reddit.com/r/linux/comments/vx67jm/comment/ifu3ydw/](https://www.reddit.com/r/linux/comments/vx67jm/comment/ifu3ydw/)
To be fair that's exactly the same on KDE. Hit alt+space to get access to krunner where you can access pretty much anything from the keyboard including individual files etc
You should try a tiling WM.
Iirc you can open overview with a three-finger swipe on the screen on Wayland, like the trackpad gesture.

Where i had difficulties with touch support was the gnome file manager ( cant recall the name ). It was unintuitive as hell.
>The overview could work well in theory, but the only way to open it on touch devices is with the small Activities button in the top left corner, which is not ergonomic at all.

The hated "hot corner" gesture was enabled by default before Gnome 40, which is nice on touchscreen but horrible when using a mouse. On Gnome 40 and up, the three-finger swipe works well.

>Have you ever tried using Gnome on a tablet? It's really not great.

Yes I have, and it's the best UI available for Linux right now for tablets IMO (even if it's certainly not perfect). Phosh also works decently, but I prefer Gnome Shell.
> What part of Gnome is particularly good for touch screens?

The widgets, which have more empty space than content.
or all the Linux on mobile interfaces
Dont forget all those 2 in 1 tablets out there
Gnom is well-usable by default.
What do you mean?

[https://i0.wp.com/9to5linux.com/wp-content/uploads/2022/03/g42-1.webp](https://i0.wp.com/9to5linux.com/wp-content/uploads/2022/03/g42-1.webp)
GMOME 42 has a dark theme support via settings.
GNOME is usable by default. If you're incapable of adapting to change, I'm very sorry to hear that. Not everything has to be the same set-up.

For the record, GNOME no longer requires GNOME Tweaks (which isn't a third-party tool either, btw) to enable a dark theme.
Install the KDE Desktop and never Go Back to Gnome again.
nah i was joking.  i've used arch and it doesn't do that.  but zorin is the most easy to use, no fuss distro i've used.
Artyom talks about this in Linux for Everyone's podcast: https://www.youtube.com/watch?v=y1T4iIin3D8&t=1242s

I've posted on ZorinOS forum for further clarification: https://forum.zorin.com/t/artyom-said-that-dash-to-panel-is-a-fork-of-zorin-taskbar-and-arc-menu-is-a-fork-of-zorin-menu-is-that-true/19445

Hopefully Artyom can tell us the full story :)
Explains all the infighting within the FOSS community lmao
It's not very soft when I'm constipated..
Why do You capitalise some Words?
They'll only be rebasing to 22.04 on August next year? That's... quite long. It'd be less than a year away from 24.04. Do you have any links for planned changes for 17 (and maybe 16.2 as well) because it gets me curious as to what are the changes that Zorin has planned.

Also, I'm kinda surprised it is just two people. With how much monetization was pushed, I would have expected them to have more people. Mind, monetization and team sizes aren't the be all and end all, considering how much Feren, Garuda, and, indeed, Zorin managed to accomplish. I just expected a bigger team...
Well yah, that's the Pro version. Get the free version.
"free" doesn't mean that you get everything for free, it means freedom. The Pro version is FOSS. 

The Pro version doesn't offer you that much more than the Core version, only some themes and bundled FOSS software. You could easily replicate all of these on your own.

If you want these features so bad, get a job and pay for it. It's not expensive and you'll be supporting the developers (I know, heresy on Linux seeing as everyone thinks everything should be without cost).
Ubuntu is still the worst distro to Date.
> locked behind the Pro paywall with Zorin.

what software can be obtained on Ubuntu that isn't a sudo apt away from Zorin Core?
They could get greedy and useless like Google and Microsoft?
I mean, in most of the world you can have a decent small software dev company of around 20 people with 1M a year.    
     
No silicon valley salaries but most of the world doesn't pay that. And not to undermine Gnome's work but 20 fulltime devs sounds like it could be enough
ü§¶‚Äç‚ôÇÔ∏è
Nah,

> wouldn't have had

is the keyword, because you are making claims that are larger than "it doesn't now".
Nah I would rather use my ubuntu with a smooth gnome than use a half baked tiling wm
Ah, that makes sense, last time it tried it on a touch device I was still using X11. That does make it significantly better.
> The hated "hot corner" 

Hated by who? This is one of the first thing I re-enable when using Ubuntu.
If Linux phones existed, GNOME would be the most popular one. KDE is also doing a lot for Plasma Mobile, but I haven‚Äôt heard much.
>If you're incapable of adapting to change

or you dislike the workflow Gnome was developed for
It's usable by default right until you need an app that has a status indicator, then you need an extension.

Yes maybe apps having indicators possibly  does not follow GNOME's design language, yes maybe the way they work is a horrible hack that needs to be modernized, but either way there are apps using then and them just being removed with no alternative for years other than a third party extension is ridiculous.
I'll keep my Win XP layout KDE Plasma, thanks.
It's usable, but inefficient.
Well I used kde for few months, I liked it, but I want to try something new üòÖ
to me its a pile of shit with monetized locked features to make things even worse.
Because my phone tries to autocorrect English into German and I'm too lazy to correct every single word.
> They'll only be rebasing to 22.04 on August next year? That's... quite long

Yeah but I don't consider it a big deal since their interface is vastly different from vanilla gnome and since the introduction of flatpaks, there are no issues like outdated packages.
Also you can think of it this way, they release a new version a year before Ubuntu.

> Do you have any links for planned changes for 17

https://help.zorin.com/docs/system-software/when-will-zorin-os-17-be-released/

Also from Artyom:

"We can confirm that we're actively developing and updating Zorin OS and have no plans to stop.

Our current focus is on completing development of a direct upgrade method which will allow you to upgrade between major versions (eg. Zorin OS 15 to 16) and editions (Core to Pro) without needing to re-install the system. We're also aiming to release Zorin OS 16.2 this September.

The release of Zorin OS 17 ‚Äì our next major version ‚Äì is tentatively planned for mid-2023."

> I just expected a bigger team...

Same. Since this is their only job, they have every incentive to drive up sales and adoption and I hope they can hire more people in the future. Imagine ZorinOS having a team as large as Pop!_OS', they could change the game for Linux. It's one of the most beautiful distros that's super focused on making Linux usable for Windows and MacOS users.
I‚Äôd rather stick with Ubuntu. Quality software without the price gouging.
You could also avoid all of this and install Ubuntu. All the pros, none of the cons.
In what way(s)?
*laughs in linuxfx*

edit: Also, on a more serious note it's not like they pulled the plug on any release CentOS 8 style, I can't blame anyone for losing all trust in RH-maintained distros because of that, for example.
What software is available through the GUI installer that isn't in Zorin Core‚Äôs GUI installer?

‚ÄúBut you can use apt in the terminal.‚Äù

Okay, but we're talking about new users. If you're pushing someone towards Zorin and they've never used Linux, why are you expecting them to use the terminal on day 1?

Hands down, Ubuntu offers a better new user experience than Zorin.
With the way Gnome developers have acted in the past and made changes that never satisfy the community, I don't doubt it.
20 full time devs is not enough, but even so, your calculations are waaay off.  Regular salaries in the rest of the USA plus western europe are still about $60K and that doesn't include taxes, insurance, or what it costs to run HR for it. It's likely the costs are at least $100K per employee. 

Paying anything less than $60K would be an insult, even to those living in say eastern europe or india.

I'd suggest trying to run your own consultancy like Igalia or Collabora and see how far you get with $1M and 20 devs, even if you avoid  the USA and western europe.
Nothing to facepalm about, it's true.

Even with all the patches, most consumers are never going to get hit with vulnerabilities that require physical access to the computer or port forwarding from user's side. It's very rare that thousands of people get hacked at once, especially on Linux when most security patches are updated regularly.
Tiling WMs Like sway or BSPWM are Not half baked if you don't half bake the Config File.
Then don't use GNOME in that case. There's a difference between finding something unusable without modifying it (which is what the person I responded to was stating) and not liking the intended workflow(s).  


I like both KDE and GNOME, both have their own sets of pros and cons, in addition to their own flaws - but I'm not going to blindly champion or hate software. Choose what you need for your workflow.
That's a very small thing over-all in the grand scheme of things if I'm to be honest, but I don't like that they don't include support for them by default either. Whenever I have an install running GNOME, that is one of the three extensions I do install. However, in terms of actual usability, you don't actually NEED them though - even if they are great to have.
For you. It's entirely subjective.
It depends upon your workflow really, but yeah it can definitely be inefficient in some cases. Personally, I don't agree with some of the decisions that GNOME goes with either.
Locked? or simply just not pre-installed? ZorinOS' codebase is open source, you can go on their github and download the extra layouts available in Pro version.

This is no different than downloading a KDE Theme or Latte Dock. The payment is only for supporting the project and getting installation support.
the "the monetized locked features" include, free and opensource apps bundled, and more desktop styles.  that's it.  in the free version, it's got windows 10, windows classic, mac os, and good old gnome.  that's good enough for me.
> to me its a pile of shit with monetized locked features

to be fair they don't lock out not already going EOL (less than) 1.5 years after release *stares at what Fedora was like until VERY recently*
Ah, I see. I know that German capitalises nouns, if I'm not mistaken.
Jumping in on this thread... Daily Zorin user here. I respect how Zorin team works but I would really like that devs were more transparent about their plans. Now that I learned it is a 2 man team üòØ, I kinds get why they may not "waste" time with that. BTW, where did you see those news from Artyom about 16.2? 

I really hope they figure out a way to theme libadwaita properly, I'm hoping a theming API comes out with Gnome 43 to make that easierü§ûüèª. I love their themes but they kinda break a few small things in apps like LibreOffice, Inkscape, etc. Hope stuff like that happens less frequently moving onwards with.

> KDE would feel like an overkill I think. Way too many options and way too much redundancy in the UI for a distro that's supposed to be simple and easy to use.

I thought the same for the longest time, until I saw [cutefishOS](https://cutefish-ubuntu.github.io/). So now I have mixed thoughts about it. Would be kinda hard anyway, with just two people releasing gnome and xfce themes already.
[deleted]
Well, a lot of people use Zorin because of its look. It does have a very aesthetically appealing look that is unique to what you usually see on Linux
There are no cons.
> You could also avoid all of this and install Ubuntu. All the pros, none of the cons.

Cons? No nvidia drivers pre-installed. No Windows apps support by default. No installation support. No proprietary WiFi drivers. No flatpak integration in the store. Snaps installed by default. Doesn't look better than Zorin (Subjective).

Zorin wins when it comes to the OOTB experience, it's tailored towards less tech savvy users.

Ubuntu on the other hand is made for enterprises in focus, not users.
Snap, terrible Gnome Implementation, Spying on Users, snap, snap, snap
> ‚ÄúBut you can use apt in the terminal.‚Äù

I don't think you *really* expected me to mean that literally, but I used sudo apt as shorthand for just obtaining them through the software store.

Fine, I'll rephrase. Which software does Zorin OS' build of GNOME's software store block?

> but we're talking about new users. If you're pushing someone towards Zorin and they've never used Linux, why are you expecting them to use the terminal on day 1?

Are you expecting them to use knock-off Unity with the Start button moved to the bottom on day 1?
That's what. I simply want a desktop that works out the box. No extra configuration. Ubuntu's GNOME implementation makes me feel at home.
Hard to do when Gnomes gtk changes start affecting other desktops that use gtk.
Well, if the intended workflow is too different to your own, it is practically unusable for these people.

Here an extreme example: My father has a co-worker who prints Emails out. He receives hundreds on a daily basis and puts them all on ONE daily pile. When my father went to him to talk about en Email he sent, the co-worker just asked from when the Email is, and then pulled the Email out of the middle of the pile.

The intended workflow is quite clear (I think), but would you find it usable?
In some cases you do actually need them, for example try quitting Discord without using the indicator. Closing the window or trying to quit it from the dock simply hides it, still running but with no trace of it in the desktop. Some other apps that close to the tray do the same.
Who is this workflow good for then? I just fail to see what user GNOME developers have in mind.
> This is no different than downloading a KDE Theme or Latte Dock.

which i also wouldnt do :)
> I thought the same for the longest time, until I saw cutefishOS. So now I have mixed thoughts about it.

CutefishOS isn't KDE. It uses its own apps and modified Window Manager. It has a few KDE apps but it's really not KDE.
I‚Äôm not forced to pay for software given freely. Where‚Äôs the profiteering?
https://github.com/ZorinOS/zorin-desktop-themes

Install any of these on an Ubuntu install.
There are no pros to Zorin either.

Aside from the theme packs (which can be installed on any distro running Gnome), it's Ubuntu but you pay for it. Then you have to re-buy it at every major release.
Nvidia drivers do come installed, assuming you weren't an idiot and checked the box labeled ‚ÄúInstall non-free drivers‚Äù. Same with WiFi.
Snaps are worse than appimages but better than flatpacks.

The default UI is Gnome with 2 extensions installed. If you think their implementation of Gnome is bad, then every implementation of Gnome is bad.

Ubuntu doesn't spy on users.
If you only use your Computer twice a year ubuntu's Gnome is enough, yeah, that's true.
I mean, that's not really GNOME's problem if I'm to be completely honest there. They can't be expected to not do what they want with their own software just because other people who are mostly unaffiliated with them use it too. If those other desktops have an issue with it, they are free to fork from whatever version of GTK they prefer and move in their own direction or use a different tool-kit (like Budgie is doing).
Just because an intended workflow is extremely different from your own doesn't mean that it's inherently unusable. Workflows are adaptable in most cases, but there may be some instances where it's not possible - those are usually edge-cases.

This is, for example,  seen in what we currently offer for accessibility tools, unfortunately. At the moment, we don't really offer many decent tools for those with impairments such as blindness or hearing. This is why I would recommend Windows for people with such disabilities until our current offerings improve. Their workflows suffer on GNU/Linux as a whole.
Personally I wouldn't really bring Discord up as an example due to the fact that they don't really like to support GNU/Linux in general, but you should still be able to change that behavior in your settings if memory serves. Most programs that offer that functionality offer the setting to disable minimize to tray or don't even have it enabled by default.  


If you're using software that doesn't offer the ability to disable that functionality, I recommend finding an alternative if possible. If you're stuck with it, that definitely sucks - but you can't really fault GNOME for something that isn't within their control. You should be able to find the process in System Monitor and end it there or just kill the process via Terminal (if you prefer that method, anyway).
> try quitting Discord without using the indicator

ctrl q
People who like using their keyboard... Such as me
Works perfectly fine for me as a dev. I'm trying out KDE every now and then but it just doesn't do it for me (although I wish the KDE team all the best).
As someone who dailies between win/linux for work/home... Gnome works just fine as a general desktop PC. Open apps, move files around, print shit, browse the web, play some music, etc. 

Would I want to be a multiple-server admin and need 37 tiling ssh sessions managed? Probably not, no. 

Regular daily use workflow though, zero issues. Plenty of people give a gnome based OS to their elderly parents and they have no issues with it either, it's very much mac-like intuitive for "regular" usage.
Then that means it's not for you. Don't know why you'd judge other people for having their own choices.
why are you such an asshole?
Oh, I guess I assumed that it is based on KDE bc it used QT, silly of me. Thanks for the FYI.
You are the payment
Well, I've tried installing Zorin themes on another distro, but they look different to Zorin and break in some areas.
So you want to install the theme on Ubuntu but you can't use ZorinOS' free version to install the Pro features in the same way? Hmm...
> There are no pros to Zorin either.

The pros are supporting the developers and getting installation support. That's more than enough of a reason alone.

Aside from it you get proprietary drivers pre-installed, 0 fuss when installing. Ubuntu at launch wouldn't even boot on RTX mobile chips due to the nouveau driver lacking support. Zorin comes with many proprietary drivers by default that you wouldn't get on Ubuntu. Zorin was the only Ubuntu based distro that had the support for my WiFi chipset, Ubuntu wouldn't even budge.

If people like spending their time on things that they shouldn't, Ubuntu can be a great distro.

If you like not wasting your time and something that just works, Zorin is a no-brainer. And yes, that includes the free version.

So stop trying to judge people who want to support the developers, not your money, not your expense.
> Nvidia drivers do come installed

https://help.ubuntu.com/community/BinaryDriverHowto/Nvidia

"By default Ubuntu will use the open source video driver Nouveau for your NVIDIA graphics card. This driver lacks support for 3D acceleration and may not work with the very latest video cards or technologies from NVIDIA."

Did you not read what I wrote? Ubuntu wouldn't even boot because it doesn't come with Nvidia by default.

> Same with WiFi.

Zorin adds extra drivers for WiFi chipsets. Totally ignored my point.

Let's leave it at that. You obviously have a preference for the corpo loving OS more than the user centric one.
Ubuntu does spy on Users
LOL are you nuts? I use it because it is comfortable, I am used to it. And Ubuntu's GNOME is so smooth. You are a really biased person.
I would argue given the scope of what utilizes GTK the upstream devs have a responsibility to not push changes down that force other DEs to do things the way they do things.  If that can't be done maybe Gnome should fork off a Gnome specific version for themselves.
>Workflows are adaptable in most cases

I cannot fully agree with that statement.

Sure, you can adapt your workflow to a certain extent.

But if the workflow is completely different from the way you think, it is unusable for you.
Okay then let's say BitWarden or Element or some other application that closes to the tray, Discord is just the first example that came to mind.

I can turn the feature off, but why should I have to? Do we expect a regular user to know to do that when on every other desktop environment I know of as well as Windows and macOS this just behaves as expected?

A lot of changes are more debatable and I would agree that sometimes people are just stuck in their ways and don't want to accept change, but in this case it is GNOME breaking functionality of third party applications for (from a user perspective) no real obvious reason, with no viable alternative at least that I know of.
I mean yeah I know I can do that, or kill it through the terminal or the system monitor or whatever, but needing the user to use workarounds to get around applications working unexpectedly in a certain desktop isn't very good UX in my opinion.
KDE has Keyboard shortcuts that make more Sense than gnomes. And If you are such a Keyboard centric User, why use a DE in the First place?
why are you getting vulgar over an OS? i cant have preferences? you cant handle it?
In what way?
Yes, because installing a distro that paywalls popular free (as in beer) software that the distro doesn't own shouldn't be supported.
I love installing Ubuntu and checking the convenient box labeled ‚ÄúInstall non-free drivers‚Äù and not have to worry about nvidia drivers. They're automatically installed for me.
Yeah, the ‚Äúcorpo loving‚Äù OS that gives away their distro for free vs the ‚Äúuser loving‚Äù distro that charges for full access to 3rd party apps and charges a new license every upgrade.
Please share your sources.
You're arguing that GNOME has a responsibility to basically be incredibly limited in what they do with their own tool-kit because others who are mostly unaffiliated with them chose to, of their own volition, utilize their tool-kit - and if they can't or won't do that, should fork their own software - which they are one of the core maintainers of? You realize this would essentially mean upstream gets abandoned and the fork would then become the new upstream, effectively being a complete waste of time and effort, right?
I said 'most cases'. I do acknowledge where there are cases that workflows may not necessarily be able to be adapted to the intended workflow. That being said, it doesn't indicate that the desktop as a whole is unusable at any rate. I still do recommend choosing what you need for your workflow, as I do for mine (which, I'm fairly flexible depending upon what is being done).
Well, if you want the program closed, you'd obviously have to disable that functionality. I'm aware that is a 'gotcha' question. That functionality isn't necessary either, notifications exist for a reason - all you have to do is just view them. So, let me ask you then, why must GNOME keep functionality it no longer wishes to maintain strictly because of third-party programs? Those appindicators don't provide any essential usability to the DE, nor the OS, itself.

The original argument being made is that the DE is unusable when it comes to appindicators, but no real necessity for them has been shown. Am I aware there is more functionality that can be added to appindicators such as prompt the program to check for updates without opening the program window itself or such as in Steam's case, open a recently played game without opening the Steam window first? Certainly, but those are just quick actions which are simply just a quality of life feature. They aren't essential, and every function I mentioned can still be done in the actual program itself.
that's a discord problem, not a gnome problem though.
I use kde as well with bismuth, love it. I can see myself switching to it full time, in the future. 
Thing is, gnome forces you to change your work flow and it kinda leads you to use your keyboard; you can transpose this to kde as well. But, by default, kde kinda makes you want to use your mouse. That's how it feels to me at least
no, you can have preferences, but you just act like an asshole.
User data
> Yes, because installing a distro that paywalls popular free (as in beer) software that the distro doesn't own shouldn't be supported.

So, Ubuntu Pro?
Ubuntu is still a Shit distro.
I can't even argue at this point, you obviously do not want to understand or accept that the free version is as good as paid and that paid version is to support the devs and get some few extra features in return.

But accepting that wouldn't be very convenient for you at this point, you're better off judging others spending their money where they want to and act like a messiah of FOSS when you don't even want to understand what FOSS truly is (Hint, it's not freeware).
Ubuntu is the Corporate bullshit distro in your comparison.
Just Google canonical snap telemetry. You'll find loads of sources.


Also flatpak is better than snap in any way.
Technicly it isnt "their" kit.  It was originally developed for GIMP and other programs.  Yes. I dont think GNOME should force design changes via GTK given the size and impact it has outside of GNOME itself.  It pisses me off thay they make changes for their DE that negatively impacts other DEs.

And i recogize at least in this sub this is unpopular given everyones use of the downvote button as a disagree button but this is a hill i'll die on.

Its why I switched to KDE and why I avoid GTK programs these days.  

I'm tired of fighting the DE to use my computer the way im used to using it and im tired of people who have no relation to the DE I may be using breaking things because its not theirs and they dont care.
Well I guess this whole argument depends on whether or not you think desktop Linux (with GNOME being the default in most popular distros) should appeal to the average user or not.

If the target is technically knowledgeable users as is the vast majority of the current user base, then a change like this that may need apps to adapt or the user of said apps to change settings to fit into the paradigm of the interface they're using can be seen as justifiable, these users have the knowledge that they can simply choose something else if they don't like it, or go install that extension to get the feature in GNOME. From this perspective this is largely a non-issue.

If we want desktop Linux to be something that can be used by the average user who nag not know what a desktop environment even is, or how to open the settings to change this behavior we need to be somewhat more pragmatic. We can't simply break by-default enabled functionality in apps, as that leads to surprising and unexpected behavior that the user will not know how to work around - and GNOME is not even close to big enough in market share that developers will accommodate its differences from other platforms. So you have app developers not adapting their apps, and users being hit with surprising unexpected behavior, all you can really do to solve this problem is to support the feature.

I'm arguing from the perspective of someone who has used Arch as primary OS since around 2011, with just about every desktop you can think of that does not involve a tiling window manager and ended up buying a Mac because I don't want to have to tinker with my computer or change settings, or worse configuration files anymore for my system to work as expected. Desktop Linux may just not be for someone like me who wants everything to work out of the box and get out of my way.
GNOME removing a feature that app developers can consistently rely on existing in all other desktop environments and OSes that they run on with no proper alternative is not a problem with the app, no.

Discord was maybe not the best example to bring up given that we're on /r/Linux, so feel free to replace it with any FOSS app that also has a tray icon and my point still stands.
ok so you are just incapable of talking like a decent person, best of luck . i gave you a chance.
What user data?
Not at all. What's installed with Ubuntu Pro is the same as what's installed with Ubuntu Desktop. They don't limit what's available in Desktop to encourage Pro sales.
In what way(s)?
Not only have they taken Ubuntu and reskinned it, they don't even do anything to KDE Connect besides rebrand it.

Zorin literally is Ubuntu with a new skin.

You're paying for other people‚Äôs work.

This isn't a libre vs gratis debate, this is one of ethics.
So, you don't understand what telemetry is, thanks for admitting that.

They do send basic install and usage count stats back to canonical but do not monitor everything you do in the app.

As for Flatpacks: https://flatkill.org/2020/
>Shits on Ubuntu and Canonical for having very little telemetry that is even present in KDE

>Uses Google
> Also flatpak is better than snap in any way.

I can't imagine how Snap is worse at supporting LXD than Flatpak (or Waydroid, though I don't see Waydroid supporting Snap anyway considering the low appetite everyone seems to show for that. A shame tbh).
I'm very well aware of GTK's origins, but it's clearly GNOME's tool-kit now. As I mentioned before, if those DEs have an issue, they can fork GTK at whatever version/commit they prefer and move in their own direction, or use a different tool-kit - again, Budgie decided to move away from GTK (to EFL) for this very reason. GNOME has no responsibility to limit themselves for everyone else just because they chose to use their tool-kit.

For the record, I've not down-voted you at all. That being said, I'm glad you've found a solution that works for you. If that's the hill you're willing to die on, I wish you the best of luck with that in the future.
Isn't this the point of libadwaita? So gtk won't be effected anymore by gnome specific things. If cinnamon changes to gtk4 for example then they can just use gtk4 and no libadwaita which allows theming to work still.
I mean, desktop GNU/Linux doesn't have to appeal to everyone - that's a pretty unhealthy mind-set to have as it causes unhealthy amounts of feature creep, which is its own set of issues. I still don't understand the obsession most people in the GNU/Linux community have with that, honestly (not saying you are obsessed with that, btw - just a general comment).

That being said, most people always recommend a distro with Cinnamon or KDE pre-installed for newer users, so I still don't see how this is an issue for desktop GNU/Linux adoption being that we consistently see adoption by newer users who will typically go on Linux Mint (which I still dislike that people recommend that distro with all of the bad security practices they have, but anyway, uses Cinnamon of course), Ubuntu (which is not stock GNOME), Pop!\_OS (not stock GNOME again, and I don't even remember if they even announced their new DE written in Rust yet or not - all I know is that they have an extension suite called COSMIC still), or the newer SteamOS 3.x (which uses KDE).

I'm sorry if your experience didn't pan out well in the past, but assuming that you haven't touched GNU/Linux at all recently, you really should give it another go. Unless you're doing something that isn't average end-user-like, you shouldn't really need to edit any config files. I could just do a fresh install of Fedora WS right now and be up and running in a few minutes. Of course, I default to Wayland these days, so maybe you are still referring to X11 experiences? Or something else? Possibly audio configurations from the past? Lots of issues have been resolved thanks to PipeWire, Wayland, and a few others (such as Valve, WINE, and GloriousEggRoll on the gaming front - but gaming on GNU/Linux still has a decent ways to go). For the record, the average end-user is only going to be doing things like viewing videos and pictures, listening to music, checking E-Mails, browsing the web, and possibly some light gaming. Anything beyond that is no longer average end-user because that is a deviation from what most people do.
Dude, he is right. Deal with it.
canonical is like microsoft.  They want to collect as much user data as possible.
I would rather not ask u/theRealNilz02.

Because it's always the Snap, "forced firefox and snaps", bloat (this is false) and all other bullshit.

I love how some elitists think Valve has done more for Linux than canonical.
King of misinformationüëë
I use the Word Google as a synonym for web search Just Like everybody else does. Also what does my choice of search engine have to do with the fact that canonical is Just as Bad as Microsoft?
Thanks.  Kde deff has its own warts, but for now it stays out of my way more than Gnome does.

I disagree but understand your position and i'm not nessisarily arguing gnome is "wrong" to do that with "their" toolkit.  

But I think when they own something that a large portion of the ecosystem relies on they should have a responsibility to minimize breaking changes that affect projects not their own.  I don't feel they do and as a result I don't view them to be good stewards of the GTK toolkit.

But thats just like my opinion man :-P

Hope you have a good day!!
So what you are saying is noone should work on GTK because if you do anything that doesn't align with gnome expect the api to break and become worse overtime as an actual tool kit instead of just debranded gnome
If we're conceding that desktop Linux doesn't need to appeal to everyone, then I agree with what you've been saying.  If the target is mostly technical users then needing a bit of customization to match your use case is probably totally fine.

I still do have a Linux install that I use occasionally because some things are honestly just more convenient to do on Linux than everything else. Been using Fedora Silverblue with GNOME and I have been quite impressed with it so far. GNOME 4x has solved most of the things I didn't like about it other than app indicators and changing the trackpad scroll speed being impossible without workarounds, desktop Linux is slowly but surely moving in a direction I really like and I'm quite excited to see where things go in the next years.
How? The optional user data collection that's totally 100% optional?
Valve has done a Lot more good for Linux than canonical in the Last 5 years.
If that's what you want to believe, you are entitled to your opinion.
The fact that you said Canonical is as bad as Microsoft shows that you are very misinformed and uninformed.

Besides, it's still ironic the fact that you used the word Google instead of "look it up" or "search it" when you're care so much about being private.
Hey, I definitely understand where you're coming from when you say that. It's just that it's GNOME's infrastructure that's hosting GTK along with their other resources being used to maintain it, so that's more where I'm coming from in regards to this - but I see where you're coming from generally. I also don't agree with some of the decisions that GNOME makes over-all either. I'd like to revisit Budgie as I used to enjoy that DE, but their lack of Wayland support has really killed any chance of that.

Hope you have a good day as well! :)
I never stated that. I stated that if any DE had an issue with the direction GTK was going, they could either fork from a version/commit they preferred and move on into their own direction, or move to a different tool-kit.

As a developer, if you were to depend on an extremely important piece of software as a dependency, and you have no control over the direction of it, you should hold the expectation that it may not necessarily evolve in a direction that you agree with and you may need to either maintain a copy yourself if possible, or replace that software and rewrite your software to accommodate the change.
Absolutely, if there are some people who prefer Windows or Mac OS over GNU/Linux, power to them. Not everything has to be built for everyone - hell, that's also the power of FOSS, where you can just fork something and make it your own. Of course, this causes lots of needless fracture if there is no actual meaningful difference from upstream, but the option is still there.

Glad to hear about your more recent experiences though, and yeah I usually hear fairly good things about SB, just haven't really had the time to give it a shot - I suppose I should fire a VM up whenever I next get the chance. Regarding the trackpad issue, yeah I'll admit that is probably something that should be more easily adjustable due to how finicky some trackpads can be.
It's just the principal of it that people don't like.
But as a whole Canonical has done so much for Linux.
If you want to be an elitist neckbeard, sure.

If you want Linux to be approachable for the masses, not really.
Why do you keep defending a company whose Sole Intention is to piss off Users?
Couldn't they just submit pull request for the changes or open bug reports for regressions? And use social pressure to change the minds of the gtk devs. Since it is an option beside walking away or picking up the entire project
That's understandable. But don't misrepresent what's actually going on.
20 years ago maybe. Now they keep adding Features that Piss Off their User Base.
For Linux to become approachable for the masses, canonical needs to Stop making their distro worse with every Release.
They can submit a Pull Request or Bug Report, but it doesn't necessarily mean either will be accepted. If you were to try to force a project to bend to your will by consistently using social pressure, you earn the ire of the project's maintainers - which isn't good in any case; you especially don't want to earn their ire if you plan on doing Pull Requests or Bug Reports, as those require communication and collaboration, which is something they won't be as willing to do with you. It also does cause some of these developers to get harassed, which just isn't cool.

At any rate, there's nothing wrong with adapting to a vision that isn't what you intended at first. Sometimes you end up liking the way something turned out. If you end up not liking it and you've tried to work around it without success, then you can attempt to work with upstream to see if a resolution can be found, and if not, my recommendation is to reevaluate that piece of software and see if it is time to swap it out for a different dependency. GUIs aren't necessarily a quick rewrite, so I can see how there is hesitation, which is why I recommend to reevaluate as it's a big decision to move to a different GUI tool-kit, especially if you're a multi-platform program.
Considering the average computer user, they've been making it better.

Worse for power users, maybe (but not really since you still have full access to all the tools you've always had access to), but that's such a small portion of computer users.
So you are saying that gnome should be flexible to adapting a vision that isn't theirs
Removing Programs from the repositories is making a distro better for the Average Computer User?
I was referring to other developers as I was explaining the process regarding if something you didn't like was done to their dependency, but I would also say that GNOME should have some flexibility to a degree on certain subjects. It really depends upon the subject, though. Not everything is equal in terms of what can be flexible, as some things have to be done in a certain way.
Considering the software is still available, it's a net neutral effect.
I understand.

I think the only part where we might disagree is that social pressure is force. Like all it does it make it public as to what a specific userbase wants and known who is in the way of that. Like one wouldn't say I forced by boss to get into trouble if i let it slip to the customer that he was having us send them out of spec parts cause it was the actions of my boss that got them into trouble.
It isn't. The slowness of snap applications is noticeable, even for the Average Computer User and adding PPAs is hell even for experienced Users.
Only on the first run. After that, it's just as fast as the native app was.

Plus, the security benefits of being able to update Firefox as soon as a security patch is released as opposed to waiting for the package maintainer is a HUGE benefit.
Automatic Updates are an anti User Feature IMO.
If you've ever spent time supporting anyone other than yourself, you'd realize it's a good thing.
It isn't.
You're right. People getting infected and attacked through security flaws that have been patched weeks ago is a bad thing.
Automatic Updates are the wrong way to Go about this.


They Take away the Users Control of their OS, making an automatically updating Linux distro nothing better than Windows NT.
Okay, so what's the best way to patch a security flaw that doesn't leave the user vulnerable for longer than absolutely necessary?
Having the User Update their Software themselves. 


They can Check which Programs are Updated, get notified when something went South and can choose based on other peoples' experiences with certain Software Versions If an Upgrade will function or Not.
So it fails to protect the user from external security flaws.

You've immediately lost the attention of every security expert out there, meaning Linux will never gain market share.
Every Security expert will Tell you that Automatic Updates are a Bad Thing.
If you believe that, you're lost.
Honestly I think it's a good idea for Canonical to switch the Firefox deb to snap. My high school staff still uses Ubuntu 12.04 and in some classrooms (which are gradually being replaced by smart boards with Windows 10) and they have very outdated versions of firefox. It is so outdated that it doesn't even have the photon redesign which came out in 2017. They are aware about this but don't care. If Firefox was a snap they would still be secure with a newer version of Firefox.
I see the screenshot is taken on Fedora 36.  
I'm looking forward to the Fedora Budgie spin as it seems to be the perfect combination that I'd recommend to my friends and family. Is that expected to be ready for Fedora 37?
How can I get the budgie desktop on Fedora 36?
u/JoshStrobl are you planning to use Budgie as your daily driver going forward?

I‚Äôve been looking for something new to use and kinda didn‚Äôt like a few things with the most recent Gnome in F36.
What? Fedora Budgie!! When, how? A spin with v37?
Where Budgie 11?
I'm glad this desktop is growing. We added it in OpenMandriva some time ago - but it still has a development status, but we are slowly refining it.
Assuming no surprises, yes it will be in for Fedora 37. It may even be backported to 36. Whenever I am compiling the packages locally as part of the review process, I am doing so against both f37 (rawhide) as well as f36 (my daily driver).
This is something I am packaging and will be the official maintainer of. It is not in the official repos quite yet, it has been undergoing review process and as I have been quite busy, I only recently returned to finishing those up.

You can expect some Buddies of Budgie post (blog and social) when all the components have landed in the repo. Two of them (budgie-desktop-view and budgie-screensaver) already have. Budgie Control Center is what I am currently working on addressing in terms of review, then it is the desktop itself.
Can't, there's no maintainer for it unfortunately.
What do you mean? Budgie has been my daily driver for like 7 years now. I wouldn't create an organization around it if I wasn't going to daily drive and work on it.
Sorry, read comment, wicked news, cheers Josh and congrats
I would encourage you to read the May post from the blog, you will get plenty of information about Budgie 11.
Oh I'm aware, i just saw the ss and wrongly assumed it was already ready for use for everyday users.
Apologies, obviously haha. I was typing fast without thinking. 

I meant using it on Fedora as a daily driver unless you‚Äôd been packaging it that way for yourself all these years.
How are you finding EFL so far? Is it nice to work with?
Gotcha. The screenshot is under Fedora since that is simply what I use :D
Ah. Yes I will be daily driving Fedora. All my machines (personal and work alike) run it :)

Aside from just compiling all the necessary components myself, I have been packaging Budgie Desktop for inclusion in the Fedora official repo. 3/4 of my patches have been approved, 2 of them are already in the f37 repo, the other will be once the repo is created by Fedora Engineering. The only thing left is budgie-desktop itself, after that it should be as straightforward as me asking for the f36 branches to be created, and merging my work into those to get into current Fedora.
Oh awesome! Maybe I‚Äôll be able to get it into 36 first then.

I‚Äôve only played around with Ubuntu Budgie but really is quite a smooth and luxurious DE.

Edit: Thanks for all your hard work!
If they provide ovpn files you can just import those into Network Manager.
It's possible I'm missing something. But I don't really understand the purpose of this. There are enough VPN providers that support Linux directly and some of them also offer a client with a graphical interface. Why not just use these providers?
Surprisingly useful, lots of gaming communities are switching to Radmin. Unfortunately this is a ridiculous number of steps.. Might try it when I'm bored some day. Thanks for sharing though!
Not sure how feasible it would be to use casually, but thank you very much for writing this. 
I remember I couldn't play from linux a game once as my mates were using radmin
Ex user of Radmin VPN here for Minecraft servers, switched to ZeroTier and never looked back, less hassle, cross platform, more stable, highly recommend
Hey im not experienced on that kind of stuff but couldnt you use a windows docker image container to have even less resource usage?
Windows client VPN is the worst. You have to go into 5 different screens to setup what should be done in one screen. If I wanted to copy a built-in VPN client, it would be MacOS. One of the few things they did right was their networking.
Well, if they could.
Radmin is popular for making LAN gaming work outside the LAN
I am glad, that it might help you.

It is not really not that complicated.

One day I will make video about it.

Also, I have heard from other people about ZerotTier, that is kinda like Radmin, that is cross platform.
One day I would publish video.

It is not really that complicated or too much work, I just tried to explain every step as much detailed as possible.
Got ya.   
I am going to look at it one day.

Thank you!
I am kinda not experienced with Docker.

Maybe it would work, but I have never used it.

Does it have a GUI (since bridging requires GUI)?
ZT is better but it's more complicated so I don't blame people for using Radmin
https://github.com/drduh/YubiKey-Guide

There's also this for anyone needing more YubiKey resources.
[deleted]
Me, who doesn't want to touch GPG after reading a bit into it once: Interesting *sips a bit of Spezi*
Oh nice!  Using a YubiKey with SSH is awesome!
Wow, that is so detailed. I'm impressed. I'll add a link to it.
There are some alternatives to a Yubikey that offer the source code on Github, for example. But in some cases, the software preinstalled on the respective sticks cannot be updated (Nitrokey Pro 2, for example). So even with these solutions, you have to trust the provider.

And with those that offer an update of the software on the stick, I usually miss some functions like Fido. That's why I deliberately chose Yubikeys. I would prefer an open source alternative. But only if the range of functions suits me and I can also update the software myself.

But apart from that, I don't see it as particularly security-critical for my use case if the software and hardware of such a stick is not open source. Most of the time, it serves me as a 2FA, so the stick itself is worthless.
Unfortunately the open alternatives are just not there yet‚Ä¶
You can use GPG with Trezor and Ledger devices.  Trezor is fairly open.  Not sure about Ledger.
Why does the Linux Foundation not employ its own Root Key in TPMs which will sign distributions certificates for Trusted/Measured/Secure Boot?

and distributions can register/request there?
There should be another set of signing keys that *must* be accepted and those should be in the hand of a selection of distributions/vendors like RedHat/Fedora, Debian.

They should not be in the hand of a company that was already on trial for anti-competitive practices
I feel like a broken record in pointing this out, but Microsoft has two carveouts in their [WHCP policies](https://docs.microsoft.com/en-us/windows-hardware/design/compatibility/whcp-specifications-policies) nowadays (from Win11 22H2), in Systems.pdf under System.Fundamentals.Firmware.UEFISecureBoot:

>19. For devices which are designed to always boot with a specific Secure Boot configuration, the two requirements below to support Custom Mode and the ability to disable Secure Boot are optional.

As well as:

>(Optional for systems intended to be locked down) Enable/Disable Secure Boot. A physically present user must be
allowed to disable Secure Boot via firmware setup without possession of PKpriv. [...lots more text]

Back when Windows 10 launched (Win10 1511), this carveout read as follows:

>On non-ARM systems, the platform MUST implement the ability for a physically present user to select between two Secure Boot modes in firmware setup: "Custom" and "Standard". Custom Mode allows for more flexibility as specified in the following: [...lots of stuff including the disable option]

At some point the "non-ARM systems" got changed into "systems intended to be locked down" which isn't defined in the policies anywhere, and thus, can seemingly change at a moment's notice. It looks like we're starting to see the effects of this now, *and the policies can let it get so much worse*. The option to ship a Windows-only laptop is now seemingly very real.

The by-default provisioning of the "UEFI CA" third-party key itself has also had an ambiguous, otherwise unexplained carveout for it (for a long time):

>Microsoft UEFI CA key MUST be included in SecureBoot DB unless the platform, by design, blocks all the 3rd party UEFI extensions.

We fought (realistically I think some lawyering behind the scenes happened somewhere) to even have the Custom/disable option added in the early Windows 8 days, and because the campaign worked, people have forgotten that the threat was genuine.
> Given the association with the secured-core requirements, this is presumably a security decision of some kind.

Or a marketing and product management decision that's conveniently wrapped in a plausible technical decision.

The fact that it marks an apparent reversal of course, and does a (currently weaker) version of exactly what Microsoft swore UEFI and signed bootloaders were not meant to do -- block third-party OS installation -- kindda strengthens my gut feeling that this has very little to do with security.

Sound technical solutions to real world problems tend to muddy the waters around these decisions. Marketing material may show the stuff that comes from the techies along the stuff that comes from the suits, but they don't always belong together: any sound technical solution customer problems can, in the right hands, also be used to solve company problems, even against users' interest if they are sufficiently well locked down.
Pfft. The moment it got to a point where I as a user cannot simply slam any random USB or optical disk into my computer and just press enter on a screen that asks me if I want to boot from external media, because booting from external media might be dangerous, was the moment it ceased to be *my* computer. I don't want it to be signed by anybody, especially not Microsoft. Except, perhaps, myself.

But I'm a clued in user. Just as every digital game and movie requires an online account so that the vendor can wreck my shit after taking my money, I know that gradually making it more difficult to boot whatever media I want on my personal PC, is all about eventually creating two tiers of PC, the workstation; (will cost 4x as much), and the consumer crap which will only run approved software and nothing else. When this transition is complete, if you crack the cases of both machines open, you will find that the hardware inside is exactly the same, or nearly the same. The only difference will be the malicious firmware in the CPU of the consumer model that only runs code approved by Microsoft and the MPAA.
i.e., Microsoft have returned to their old ways and are now preventing non-Windows boot loaders from working on new machines out of the box.
But I thought Microsoft hearts linux!  Have they lied to us *again*?
To be honest, I'm actually in favor of Microsoft phasing out the 3rd party cert.

It allowed booting everything. Just edit the grub.cfg and boot whatever you desire.

That completely defeats the point of secureboot, as it'd allow you to boot manipulated payloads.

I know it sucks, but it fundamentally broke the chain of trust, because grub was unable to produce such a chain at all. My systems are better off without this.
Lenovo sucks. No idea why people keep buying from them. They've been doing shady shit like this for years. Not a friend of FOSS.
why is microsoft in charge of every x86 pc? why isn't intel or eff?
Just turn it off and be done with it. As far as I can tell the main reason it exists is to inconvenience users of alternative operating systems anyway. Even if it wasn't inconvenient, the fact that it is tied to Microsoft is a very good reason to not use it.
> Why does the Linux Foundation not employ its own Root Key in TPMs which will sign distributions certificates for Trusted/Measured/Secure Boot?

Probably because while they can do it, no manufacturer will actually install or even pay attention to that root key, the only reason the two microsoft keys are prevalent is due to manufacturers wanting to stamp that "Compatible with Windows" logo on their products. 

I think there are manufacturers who actually include either Canonical or Red Hat's key but there is no widespread use of either the two.
>Root Key in TPMs

Mostly because secureboot keys are not stored in the TPM. Secureboot is unrelated to the TPM, the TPM only measures secureboot events.
I would more say that the set of singing keys should be in hand of someone completely independent with as little stake in the whole thing as possible.

So, maybe someone in the UN, like a UN UEFI bureau?
> and the consumer crap which will only run approved software and nothing else.

also known as a "smartphone".

And the workstation will be just as locked down. After all, Adobe etc still need to extract their measure of blood each month. To this day various industrial and professional software rely on hardware dongles as DRM.

People adopted the micro computer because it allowed them to run software without interference from the mainframe sysadmin. Now the micro computer is becoming ever more mainframe-like, thanks to the massive use of micro hardware in building racked computing farms.

Hell, take a look as the latest generations of games consoles. Or why RMS created GPLv3. It is sad to see him more mocked and vilified these days, when he warned of all this coming for decades.
It's horrible your pc is not yours anymore, and it's bloated with spyware, that kind of stuff makes me want to live in the woods outside of the grid
Hello EU? I would like to order one "Beat company to bankruption" trial please
They never stopped being themselves. Although they managed to convince a bunch of new kids in the last two decades by baiting them to think that MS was cool and different. That monster will never go down.
has been like that for last decade , its not new
I'm shocked, trully shocked.
As long as you use it via Azure or WSL, sure...
Your wish has been granted. Intel is in charge of every PC courtesy of the ME running Minix on its own CPU which you can't shut off and is network aware.
Intel was the one that developed the original EFI spec. The UEFI spec is owned by an industry body called the UEFI Forum:

> The Unified Extensible Firmware Interface (UEFI) Forum is an alliance between technology companies to coordinate the development of the UEFI specification. The board of directors includes representatives from twelve "Promoter" companies: AMD, American Megatrends, ARM, Apple, Dell, Hewlett Packard Enterprise, HP Inc., Insyde Software, Intel, Lenovo, Microsoft, and Phoenix Technologies.
well, i don't say there should be no windows key, but there can be TWO keys. Or even a handful, where we separate that from real vendors so they cannot do fidget around with this. Or have a few for vendors or (supra-)national organizations or some nongov-entities. These are public keys or even certificates for gods sake.

this is so annoying and aggravating. microsoft only signs a shim because they do not want to sign the public key because of GPL reasons o_O
> Probably because while they can do it, no manufacturer will actually install or even pay attention to that root key, the only reason the two microsoft keys are prevalent is due to manufacturers wanting to stamp that "Compatible with Windows" logo on their products.

Having a key from another source would side-step any claims that they might be acting in collusion should Microsoft do something that seems anti-trusty. In that scenario they can just point out the MS key being industry standard and that they actually support non-MS keys.
Someone without enough knowledge to prevent being easily manipulated?
There's already organizations that handle these sorts of things. If it was handled by the IEEE, I'd be more than pleased.
Or break up the company into competing splits?
\> this is so annoying and aggravating.   
microsoft only signs a shim because they do not want to sign the public   
key because of GPL reasons o\_O

Care to elaborate? What's this public key we're talking about? Is it publicly released? I'd understand them not wanting to make the key public, as it would kill the purpose of Secure Boot.

Or is it related to some incompatibility with the GPL?
Yeah because that works so well for our very knowledgeable politicians
Both? First take their money and then split it up and hope it vanishes
> Or is it related to some incompatibility with the GPL?

to my knowledge it is that. i currently dig into the whole thing to understand it in full.
>Care to elaborate? What's this public key we're talking about? Is it publicly released? I'd understand them not wanting to make the key public, as it would kill the purpose of Secure Boot. Or is it related to some incompatibility with the GPL?

By signing the shim bootloader, Microsoft effectively signs the certificate of that distribution, which is embedded inside the shim.

What they are not going to do is sign GPL licensed software directly (shim is BSD licensed), since they fear that by signing a GPL licensed binary, the private key could become "infected" by the GPL. Someone could argue they break the GPL by not releasing the private key and sue them over this.
[deleted]
but why do we need a shim in the first place and not just a certificate which is signed?

&#x200B;

yesterday i read several documentation and today i will read the shim sourcecode?
Oh, I see.

IMO it wouldn't make much sense to release they key just because it's a GPL licensed binary, but I guess that's always the risk with the GPL.
> Lol what? That's like saying a server on which you build GPL software must have unauthenticated telnet access.

The idea of the GPL is that everyone can get the source code and build their own version of the program. But if the binary needs a signature to run, you cannot run your custom built version of it (at least not without disabling the signature check).

If building that binary relied on a piece of tech only available in that server, then the GPL might as well require access to that server. But I dont know, I am not a lawyer, and reading the GPL is annoying.

When GPLv3 was released there was a modification to explicitly prevent this (they called it tivoization if you want to look it up). Its one of the reasons why the kernel is licensed as GPLv2 only. And probably one of the reasons why MS plays it safe and doesnt sign any GPL software.
> And probably one of the reasons why MS plays it safe and doesnt sign any GPL software.

They do sign GPLv2 software as Rufus' UEFI driver for NTFS partitions has been relicensed to GPLv2 in order to provide signed drivers when you create UEFI boot drives formatted with NTFS(instead of prompting users to disable secure boot)
[deleted]
Then the shim topic doesn't make much sense...

They could just sign the binaries and not just a grub bootloader shim...
Is that from the GPLv2 or the v3?

I believe that would make serial keys invalid.
IMO their problem is that they do not want to sign random software with their private keys.
It still makes sense since you don't need to get every kernel update/new driver signed by microsoft, instead use the shim with your certificate built in and self sign. Probably GRUB could replace the shim but since it's licensed with GPLv3 it cannot be actually signed by Microsoft.
no, there are also alternative ways for this. the certificate which is signed by microsoft is an intermediate certificate and it just signs certificates by your distribution. and THESE certificates then sign the kernel. 

&#x200B;

i do not think that is the explanation? but if they need to sign PROGRAMS, that would explain it.
As others have said, it's mostly aimed at servers. Even with EPEL and other 3rd party repos you are going to be missing a lot of software you might want for desktop usage or it will be very out of date if it is in the repos. Fedora would be what you want for desktop.
CentOS is fine for servers. But for coding and desktop, you'll probably have a better experience with Fedora.

Fedora is upstream of CentOS Stream which is upstream of RHEL.

Fedora also has desktop focused distros/spins such as [Fedora Workstation](https://getfedora.org/en/workstation/) and [Fedora KDE](https://spins.fedoraproject.org/kde/), [XFCE](https://spins.fedoraproject.org/xfce), [etc](https://spins.fedoraproject.org).

Fedora has fast release cycles and very fast updated software releases and its normally stable.
Great for servers, super computers, and systems with many users. I would choose something else for a personal workstation, desktop, or laptop though.
Also note that nowadays you don‚Äôt need to use CentOS to get free Redhat - they have a zero-cost personal license for up to 16 devices:

https://developers.redhat.com/articles/faqs-no-cost-red-hat-enterprise-linux#

I‚Äôm actually surprised I didn‚Äôt see this mentioned below.
"More or less free RedHat" was why I gave CentOS a try.  Ive been using it, and now CentOS Stream, for ~3 years full time.  I'm liking it. And i like that it is basically the same as RHEL so skills transfer to $work.  Actually we have more Centos/stream VMs than we have full fat RHEL now at $work.  (Its mostly a windows shop )
It's good. Professional grade.

Centos Stream has turned out very well too.

Tour reason however is nostalgic so even if it wasnt excellent you might still enjoy it.

EDIT

Dont let others disparaging of it push you away, it is still most excellent. It isnt a beta.

It just doesnt wait randomly for 6 months before giving you an already approved update. It also.doesnt have the multiple week support gaps that the downstream experience every time rhel releases a new point release. It really is excellent.
It sounds like the spirit of his recommendation was that RHEL is good, and you could basically get it for free via CentOS.  These days, if you just want free RHEL, there's the Red Hat Developer Subscription, which gives you 16 free instances of RHEL.  CentOS is no longer a clone of RHEL, but is still very similar and for most workload it functions exactly the same.  Even better, because CentOS can now diverge from RHEL, it can accept public contributions, which become part of the next RHEL minor version because now RHEL is based on CentOS.  Both are worth checking out.
I really like redhat as a company.  They've done a lot for Linux desktop
Wow, the community is really conflicted about CentOS and IBM. I think that regardless of what people say, good or bad, just try it for Gramps' sake. I've tried so many different Linux distributions over the years that to me they're all the same thing, just packaged differently so there's no right or wrong answer here. If it helps, after a decade of distro hopping, I landed on Fedora and absolutely love it. So if you end up really liking CentOS, I see no reason why you won't enjoy Alma, Rocky, or Fedora.
I have experience with both CentOS and Ubuntu. I personally find Ubuntu more friendly and easier to work with.
If you're a redhat person and you need a stable server OS and nobody has bought RHEL for you, then CentOS.

If you wanna play games, learn linux, use a desktop etc and you have no particular attachment to RHEL or CentOS, there are other options, either staying with the redhat space (Fedora), in the Debian space (Debian, Ubuntu, Mint, tons of others) or other stuff (Arch etc).
If it's an RPM-based personal desktop you want, you'd be best off with Fedora Workstation (or one of the Spins if you don't like GNOME).

CentOS Stream is more of a server/cluster distro - in its current form, it's more or less the in-between of Red Hat Enterprise and Fedora.
I wrote up my thoughts on CentOS Stream for my employer, last year: [I'm looking forward to the future of CentOS Stream](https://www.reddit.com/user/gordonmessmer/comments/noseph/im_looking_forward_to_the_future_of_centos_stream/)

tl;dr: CentOS Stream is a stable LTS that is an improvement over the old CentOS project for the vast majority of use cases.  

For many entry-level use cases, RHEL can be licensed free of charge.  And if you aren't looking for an LTS release, Fedora is also an excellent stable release distribution.
What a cool grandpa you had! Sorry for your loss.
Redhat is more or less the standard in corporate data centers and the cloud, though Ubuntu/Debian is also represented, and there is some SUSE presence.  


Centos used to be a red hat clone, but at this point, it's just upstream of RHEL. It's still useful for any scenario where you want a red hat OS without necessarily needing a 100% identical OS. As long as you're mentioning Centos, we should mention Oracle Linux and vzlinux, which remain RHEL clones, built from the Redhat code base.  


Pretty hard to go wrong with Redhat on the server side, but speaking as a redhat pro, I've been using Debian derivatives on the desktop for the past 14 years
This is a [good ZD Net article](https://www.zdnet.com/article/centos-linux-8-is-about-to-die-what-do-you-do-next/) that even states IBM suggested their customers use CentOS as it was basically a free branch of RHEL. CentOS Stream is different and is like a beta version of RHEL.

Most people looking to replace the deprecated CentOS Linux, have switched to [AlmaLinux](https://almalinux.org/)
Lots of yakitty yak in these comments. Just install Mint or Ubuntu and enjoy a stable OS managed intelligently and securely.
*Boom*
CentOS was a redhat clone, but is now upstream and some Kind of beta for redhat. The thing you should look for is either Rocky Linux or alma Linux, which are both exactly the same thing. A redhat clone, since CentOS isnt. 

The "desktop" redhat Linux is in fact fedora, so the updated Version of your grandpas recomendation would be fedora for a desktop or alma/rocky for server, if fedora core doesn't fit your use case.
I work with RHEL7 & 8 daily, as well as Ubuntu 18 & 20.
For stability and security, out of the box, RH is amazing.  Built like a rock, which makes it great for server builds.
For dev, tools, ai or anything else, you'll want Ubuntu or some other Deb based OS.  A lot more flexibility. There's just a mountain of stuff that works better on them.
After reading these comments it seems a lot of people misunderstand the relationship between RHEL, CentOS Stream and Fedora. I‚Äôd recommend reading this and taking a look at the graphic. https://www.redhat.com/en/resources/centos-stream-datasheet
For what purpose?  If its for your desktop, I think it'd be a pretty bad idea.  I mean, sure try it out, but imo its not built with your use case in  mind.
Since he died CentOS changed a little. If he were alive now he might like Oracle Linux, or Rocky, or Alma. (all now more or less free Redhat)
Its what we base all the serious stuff on, from microservices to devops to db clusters
I use alma Linux, which is the same thing, on my laptop. It‚Äôs basically what centos used to be. It‚Äôs rock solid and always just works, and the support length is insane so I don‚Äôt have to worry about reinstalling every 6 months or breaking with a rolling distro. It‚Äôs not a ‚Äúfun‚Äù distro, tho. Everything works, you set it up once and forget about it.
I liked it.  but Debian had a particular package i like.
I've worked at two jobs now where Red Hat/CentOS was used for servers. It's stable asked pretty much works. With Red Hat you get support and a few more bells and whistles but you could go a long time without noticing the difference. It works great for servers. Licensing costs are a thing though...

For my laptop I use Manjaro and would avoid CentOS/Red Hat but that's a personal preference. There's nothing preventing you from trying it out for a home machine.
Your feelings could change a lot how you perceive the distro.

As you possibly know, the old CentOS perfect clone of Red Hat  has been discontinued. The current CentOS stream is upstream to Red Hat, Red Hat offers a number of support less free subscriptions and the two free clones are Rocky Linux and Alma Linux.

I'm a desktop user and I recently tested Red Hat 9 with a free subscription and its clone Alma Linux 9 because I was attracted  by the 10 years support.

Despite I appreciated the solid professional structure, because I run Linux on weak / old hardware the two of them were so heavy to the limit of being unusable.

If you have decent hardware and you are not a very last package release maniac, Red Hat Linux  is a solid option.
CentOS 7 is great for building binaries that would be compatible with as much Linux systems (that are neither too old or too new) as possible. You'd probably want to upgrade the compiler to at least GCC 9 though.
He‚Äôs right. It is basically free red hat. It is a great platform for enterprise environments. It‚Äôs very stable, works really well. I don‚Äôt feel it takes as much maintenance as others. 

Downside: don‚Äôt use it on desktop or in an environment where you‚Äôre going to need bleeding edge stuff.
[deleted]
I would only use an RHEL-based distro for a home desktop if you also use such a distro at work. If you want to stay in the RHEL family but don't use it at work, then Fedora is an obvious choice.

We use Almalinux at work and I like the 10 years of free updates it gives you (Rocky Linux is an alternative, but I'd avoid Oracle Linux because, well, Oracle are evil), so you can upgrade at your own pace. I'd recommend waiting at least a year at work/home to upgrade to the latest release because it takes that long for all the repos you'll need to populate to a decent level.

One recent big deal for RHEL desktop distros is that MATE is coming soon to the official EPEL repo (rather than relying on the Stenstorp repo), though it's in a mix of epel/epel-testing at the moment and is missing some packages (themes is an obvious one). I hate Gnome/Gnome Classic with a passion, so MATE is a "must have" for me.
I think CentOS is/was a great option but, you probably might want to try Fedora Workstation instead. 

It also depends on what you might be looking for in a distribution, bleeding edge features are more of a Fedora goal as well as ease of use.

 If stability is what you need then OpenSUSE line might be a better fit. I've hoped over each of these distributions over the years and loved them all for what they all could do for me.

Though I've probably installed CentOS and OpenSUSE on servers more that I have on Desktops.
One word, fedora.
The key part to know about CentOS is that as of today it's two different things under largely the same name:

* Historically CentOS was literally a "free RedHat Enterprise Linux". Same packages, same releases, same everything except licensing and support. This was fully the case up to CentOS 7 and partially for CentOS 8. This is a good choice if you want to take a look at what enterprise distribution of Linux looks like. It was also commonly used in enterprise if you wanted a RHEL equivalent but didn't need official external support.
* Recently the project has completely shifted gears into being a rolling release distribution that is a testing ground for RHEL (called CentOS Stream). It's still *basically* RHEL, but instead of copying its releases, it is ahead and rolling. I'm not sure why anybody would want to use it unless you are a developer targeting future RHEL releases.
A good portion of the worlds web servers and backends still run CentOS. Due to impending EOL aspects, many are looking to alternatives such as Rocky Linux. Look it up, it aims to be a fork of red hat. (Fork means a copy of a code base for modification for adding to existing work.)

I‚Äôm going to take a shot in the dark given the context of your grandfather‚Äôs line of work experience that he is speaking of cli focused distributions, since that‚Äôs what the workforce uses. GUI based Linux desktops seem to be more hobby enthusiast based. Nothing wrong with that.

Another thing he may be trying to convey is getting you to learn cli linux because it‚Äôs the actuality of what‚Äôs going on under the hood. The majority of professionals utilize the cli to perform OS configuration, service management, and script writing. 

Colleges generally show you the CLI first for these reasons. You‚Äôll be working with and in it for the majority of serious tasks and be expected to use linux from a somewhat low level point. 

I would boot up a virtual machine(virtual box, VMware fusion is fine) of centos or rocky linux, get connected to the cli, and just learn to move around, create files, change permissions, delete files, learn what man pages are and so on. Then if I were mentoring you, I‚Äôd maybe make you build gentoo from scratch. Then id have you learn vim from playing a game like vim adventures. 

I suspect this was your grandfathers loaded intent, to see if you enjoy the tinkering and hacker mindset of playing with an operating system. I know ‚Äî it‚Äôs a hop skip and a jump, but I think probably in the ball park given his experience. Cheers.
What I like about CentOS, is that apart from a vast multitude of server systems using it out there, it comes pre-installed with SELinux, which sort of allows you to fine-tune how your processes interact with an external network or a database, therefore providing greater security.

Probably doesn't mean much for a single user, but at an enterprise level, this sort of thing can make a huge difference in terms of fishing out undesirable bugs preemptively.
I'm sure I'll get down voted for this, whatever, but I recently tested all of the redhat/alma/rocky/centos/fedora servers, and compared to other distro's, it was too bloated, sucks up too much ram too quickly, apps like cockpit have too many dependencies, also pushes you into networkmanager and firewalld, and just seems slower than the leaner distro's out there.  I did like the podman and nspawn implementations, those worked fine.  Reminded me of Ubuntu though, they also push people into stuff they make like snap, lxd and netplan.
Linux user for 20+ years here.  I use mint.
Well let‚Äôs see. The problem with using CentOS Streams is vendors don‚Äôt support it. So it‚Äôs not helpful when you have it in a non Production environment like a lab where you want to test out stuff like builds and automation scripts. For my current purposes, Jfrog and Mongo don‚Äôt support CentOS Streams. There may be others.

Red Hat does have several subscription models where you could install a Red Hat distro under ‚ÄòSelf Support‚Äô in addition to Layered, Standard, and Premium. A developer installation in a virtualized environment can be registered as well.

But the Red Hat subscription model can be pretty painful to deal with.
Like everyone said, its a great OS, but for ease of use you want to go with a RHEL-compatible fork like Alma or Rocky. Oracle Linux is also nearly identical to RHEL, Free, but very few people recommend it. I'm not sure why, I've had a good experience overall.
You should use Rocky Linux, it's almost the same as RHEL/CentOS, because Red Hat killed CentOS a while ago.
Redhat developer would be the way to go imo
I wish I could go back to using it, honestly. the last six months or so I've been working at places that run Ubuntu and I didn't like it as much.
Personally:

- Ubuntu for web servers
- CentOS for academic and scientific computing
- Arch Linux for desktop

RHEL has great documentation that also applies to CentOS. For example, I'm currently migrating from CentOS 7 to CentOS Stream 8 for a research-computing cluster and relying heavily on [RHEL's docs for Directory Server](https://access.redhat.com/documentation/en-us/red_hat_directory_server/11) (which is actually just 389 LDAP otherwise).
CentOS Linux was basically free Red Hat Enterprise Linux and a decent enough server OS. It was cancelled and now CentOS Stream is a testing ground for RHEL, not something you're likely to run on a server or desktop.
Meh, ... I highly prefer Debian.

But if your objective is to spend lots of money on the distro, licensing, support, and pay for lots of commercial 3rd party software to pile atop that ... then Red Hat is the way to go.

CentOS is dead.  Red Hat / IBM killed it.  But if you want, theres CentOS Stream - essentially Red Hat Beta ... of if you want Alpha, there's Fedora.

If you want "more or less free Red Hat", go with Rocky Linux.  Started by same person who originally started CentOS, and with CentOS's original intents/aims in mind ... but not taken over by IBM / Red Hat and turned into what they want - rather than what their user base wants.  Rocky Linux is essentially as close as one can get to Red Hat for free - it's highly Red Hat compatible - as CentOS was, before IBM / Red Hat mucked that up and replaced it with CentOS Stream.  CentOS is upstream of Red Hat - essentially Red Hat Beta sitting between Fedora (essentially Alpha) and Red Hat.  Rocky Linux is immediately downstream of Red Hat from Red Hat free source - as close as one can get to Red Hat without the restricted proprietary Red Hat bits - so for the most part it's quite highly compatible ... even down to binary package level.  So, ... if you want really close to Red Hat, but free - Rocky Linux is likely your best bet.  There's also one other distro that has quite similar - AlmaLinux, so one might consider that instead of Rocky Linux.
CentOS is a good distro, but you will want to use flatpak packages if you plan on using it as for a desktop.
The current "Free Enterprise RedHat" standard bearer seems to be [Rocky Linux.](https://rockylinux.org)
I use RHEL because I have to at work. I prefer Debian systems though. 

What IBM and RedHat did to CentOS is basically killed it. Cent is dead. We have some legacy systems that we are moving away from RHEL and CentOS, cause that path is dead. 

Rocky Linux is what CentOS used to be. So if you want "RHEL for free", use Rocky Linux.

That said, I wouldn't use any of them for a new user that's unfamiliar with Linux. For that I'd suggest Kubuntu. It's Ubuntu but with a better desktop.
CentOS was THE distro of choice for any use case that needed high stability and reliability, like servers and enterprise use. But that is no more, since CentOS Stream, a commercial operation by CentOS's owner IBM/RH.

If you need such functionality, go for one of the many alternatives that sprung up since this happening, as things stand atm AlmaLinux is the most developed and with the best commuinty.
rockylinux or almalinux are the true replacements for centos going forward.
RHEL or Redhat is an enterprise grade distro, if you are looking for your personal use go for some other distro like mint which you already have or ubuntu
I like loved CentOS but rarely used it as Debian  more ubiquitous in web hosting.
Tried CentOS Stream in VM, I didn't like it due to frequent updates like a rolling tumbleweed üòÇ

But actually I guess it's possible to consider one of Alma|Oracle as a home server for some other stuff like running Ubuntu & Windows VMs (if your videocard  drivers supports virgl in case of linux vm)  for a long period of time not being afraid of broken support. You can even save up some electricity by using those VMs from your living room TV as there are RDP and !NoMachine clients for Android.
That advice was spot-on, two years ago, before the traditional Centos model was changed.

Today, it's most closely translated to Rockylinux or Alma. Both are 100% free rebuilds of RHEL. We've been using Rocky 8 in production for over a year, it's great.
CentOS is not ideal for home use. But if CentOS is suitable for a use case, then Alma Linux is the way forwards as they've killed CentOS proper.
Centos is fine if you are fine with semi rolling release which is centos stream.  
Steble branch was shutdown for good. If you need a stable replacement for centos, which is dot for dot a redhat binary compatibility then you need  rocky linux.  
It is redhat with no commercial support and a different logo. It was created by One of centos core developer and the community..  
There is a huge chance no one will buy them off because bsd is their licence of choose.
Is it Ubuntu? No? Then it's all good.
I love Redhat... the distro that introduced me to Linux... but I've never needed Redhat support so I've used CentOS in a corporate environment for longer than I can remember... but now CentOS has splintered... so HELLO Rocky!
If you mean for a desktop, then I'd suggest Fedora.

Fedora, CentOS Stream, and RedHat Enterprise Linux (RHEL), and Rocky Linux are all basically the same OS, but at different stages of the development lifecycle.  Fedora is where new stuff happens and is meant primarily for desktop use.  CentOS Stream is a server OS, forked from Fedora, that undergoes a lot of updates.  RHEL is a solid server OS, forked from CentOS Stream.  CentOS (not stream) no longer exists.  Rocky Linux is a community driven replacement for CentOS, that is nearly identity to RHEL, but is updated slightly behind it.

For a personal server, I'd go with CentOS Stream.  For a professional server that must be very stable,  I'd go with RHEL or Rocky Linux.
Redhat and CentOS are very related, tho RHEL is expensive (but as far as I get it, it's worth the pricetag if you need that kind of support).

Both are gold standard, workhorses that'll get shit done. Not the freshest, but they are well tooled and solid, long term choices. Strictly speaking CentOS non rolling release is dead tho, look toward RockyLinux or AlmaLinux (I use both currently).

A catch on the fedora train is RHEL support is a bit more work against actually getting a free RHEL for development if you are going that way (considering Redhat has a link specifically for that on their website)
I have a "Rocky" relationship with it.
Horrible. RPM systems are horrible.
Rocky Linux is supposed to the "successor" to CentOS if you want to give that a test run
Red Hat can go fuck itself for taking over and wrecking CentOS
I mean, I have always thought of Red hat and its distributions that comes from it to be more business based, from the exception of fedora. I don't have much of an opinion other than that. And I have always personally had more troubles with redhat, and I do not know why.
I only have one experience on CentOS. Maybe 10 years ago.

I was building a software in freelance, it was a celery backend and a wsgi service. I am as doing Docker already for development and staging for my client.
My client sells the service for one of ots client and I have to install it on a bare metal server without docker running CentOS. I was terrified by Python version shipped with this distro. Can't remember at the time but outdated version.
Debian ships Python 2 as default python (maybe it ha changed) and old Python interpreter stays for a while but I think  CentOS is worth.

So I would say. Maybe it is a great distro, but it depends of the usage.

(I tried to search the current version of python in CentOS at the moment but Google can't find it in seconds)
l might say  the old man had a point!

He had seen a zoo diversity of OSes and UIs. And probably in the old age he might have wanted a stable and safe environment with rich web browser and e-mail client.

I really don't know if Skype and Teams work in CentOS.
If I weren't using Arch i'd definitely be on Fedora
CentOS was a great option, now I am genuinely uncertain.  

RHEL rebuilds have been a thing from day 1 of RHEL, and for a long time the best option was CentOS, hence why Red Hat bought em out.... and changed the policy.

If you need 16 or less systems, you can get a free RHEL entitlement for those prior to needing to pony up.  Self support.  I have my freebie at about 9 or 10 if the 16 consumed.

If you want RHEL, without Red Hat,  Alma is the distro of choice.  The do 1:1 rebuilds AND are corporate backed from a hosting co.  Rocky is also 1:1 rebuilds, but fixed zero of what allowed for several debacles in CentOS from reoccurring. 

There is also Springdale (Yale Uni CompSci dept rebuild, or some other ivy league, forget) which is nice, and Oracle which isn't a bad distro by any stretch, and cheaper than RHEL, and offers a few things RH hasnt done, yet.

Alot here shit on RHEL, having never used it.  RHEL8/9 are very good on the desktop -- been using both.  8 for a few years, 9 just came out.  I can do work, game, manipulate my 3d printers, ham radios.

That said, RHEL isn't that noteworthy on the desktop, until you put 30 of em in an office and need to manage em.  Then the streamlined managent tools pay off.

Servers same thing.

And all of this also applies to SuSE Enterprise & OpenSuSE Leap.  Another great linux vendor there that needed to pay attention to similar details needed for the enterprise.

Check out RHEL, Oracle, SLE, and OpenSuSE Leap if you haven't.   Excellent workstation and server products that IIRC all are free to one extent or another.

I prefer RHEL fwiw.  Ive also paid them an assload via my consulting co for clients over the years.  The made IT bearable to do for a living.  Windows and Citrix was making me look elsewhere.
If you are on desktop I would go with fedora workstation.

Heck, for running my models I use the fedora server
My opinion, based on experience:

RedHat is for "professional, business level systems" where they NEED a contract and want to PAY for support, even if they don't use it. I joke that they won't consider anything a serious product unless they pay heavily for it

Ubuntu/Debian is heavy in developer space

Arch is a hobby OS like Gentoo, LFS, and other compiled systems

Alpine is for containers

But different tools for different needs.
Whatever you do, don‚Äôt even bother with Larry Ellison OS
RHEL is the cream of the crop for servers and workstations. CentOS is dead, but it's been quickly replaced by Rocky Linux and Alma Linux. Both try to accomplish what CentOS does which is more or less free Red Hat.

RHEL and its free derivatives can be used as a Desktop OS. If you don't mind the somewhat older software stack in the repos. This could also be seen as an upside to some who value predictability and stability a lot.
This topic made me feel so curious about over-hyped RHEL tectonic movement that today  I finally installed a couple of virtual machines of Alma and CentOS. ü§£
I have to use CentOS, personally not a fan.  I'm sure it was ideal for business a few years ago.

It's just no longer viable, and it's difficult to find modern stuff for the OS.

It was the peak of security and stability, but is being phased out everywhere it was popular with Ubuntu.
CentOS has been sunset by IBM
[deleted]
Try it for the sake of grampa, then later for various reasons, try Rocky Linux, what CentOS was meant to be like.
CentOS was more or less free RedHat until RedHat purchased it few years ago and since then drifted it away from what it originally was. It seems like it is basically something similar to Fedora, except Fedora is intended for desktop and CentOS is intended for servers.

Personally I really liked openSUSE from the traditional distros, until I learned about NixOS, but with the later one, a bit of warning you likely will have to thrown out of window anything you know about Linux and learn it over again.
You misspelled ‚ÄúDead Rat‚Äù and ‚ÄúCementOS‚Äù /s

We use them everywhere. I hate them because they‚Äôre so goddamned reliable and very easy to ignore simply because of that fact, and thereby fail your security patch audits you spring on yourself üòÅ
The last few times I tried to install any under that umbrella:

1) It didn‚Äôt notice my nvme
2) It doesn‚Äôt notice two of my keyboards
3) Tried to install on a old surface and it didn‚Äôt notice that keyboard either

This has been across numerous of the distros the last three years.
I was dabbling in CentOS until they started their bullshit limit stuff a while back. Switched back to Ubuntu after that.
Cent OS should be discarded in favor of Rocky Linux.
Redhat and CentOS have always been work-related obligations for me, over the course of Linuxing since....Slackware debuted.

It seems like there is always one or two RH/CentOS boxes floating around the infrastructure, wherever I work.  They are usually unloved and we are Forbidden to update or touch them because they are a dependency for some ancient software we no longer have a support contract for.

I've never *enjoyed* working with Redhat or CentOS.  For me, it's been Debian for decades.

But, there's really no good reason for that, its largely personal preference and my own experience.  If my grandfather recommended CentOS, I'd probably check it out =)

EDIT: apparently as mentioned elsewhere in the thread CentOS is dead but there are alternatives.  So, that kind of tells you how much I love Redhat and CentOS....
A few years ago, it was a viable option for servers. We've moved away from it since the change to stream, and I wouldn't really recommend it anymore.
We used it here until they pulled that CentOS 8 crap with EOL. We have mostly switched to Ubuntu since it's the other one most people in my company know. We also wanted a support system behind it with Ubuntu.
 centos and redhat are everywhere in real enterprise. i‚Äôve made a 25 year career of it. but centos will fade with the loss of 7 coming soon
Not really. RHEL Workstation, or AlmaLinux are very well suitable and are meant for desktops.

They are just the LTS version of Fedora, basically.

So, yes, for most private use cases, Fedora would be the way to go, but a 13 month support window is just not feasible in any corporation setting or in academia. So for professional use, you'd normally see RHEL or Alma, or Ubuntu LTS on the workstations, managed by an IT department. The Ubuntu interim releases, with their 9 months support cycle would be considered long-test beta in those circles :-)

Only problem with RHEL is that it may not work on many new builds that are not certified. If you buy the newest HP workstation (other manufacturers for Linux certified HW exist) then it won't be a problem. The newest PCBuilder PC may have problems and may need the newer Fedora kernel.
EDIT: as people do not seem to understand

CentOS 8.0 = EOL Dec 31st 2021
Aka dead so don't use it and use Alma linux instead if you want to stay downstream of RHEL

CentOS stream 8 = current version that shut be used but be aware it is upstream instead of the old CentOS 8.0 which used to be downstream.

Dont just say use CentOS but also mention stream as CentOS revers to the old version which is EOL.

For the people that still don't get it and seem to be disliking this comment which are straight facts from centos them self
https://www.centos.org/cl-vs-cs/
At this point the release cycles probably affect desktop hardware support more than anything. 

Most developers I'm familiar with are already using VM's and containers for most of their local development. For example you might be writing PHP for five websites with completely different patch levels and there's no way to get your host system to match all those mutually exclusive configurations.
Personally, I use openSUSE Leap. For some reason I just feel better avoiding the Ubuntu/Debian branch. However, I recommend Mint for casual users who are interested in Linux.

Anyhow, I agree, Fedora is a better choice if you wish to go the Red Hat route.
> Fedora has fast release cycles and very fast updated software releases and its normally stable.

These two things are mutually exclusive. "Stable" in distro context explicitly means "no/few software updates", so you can't have both stable and updated software.
IBM Linux For Enterprises, IBM Linux Testing and IBM Linux Unstable. Call them what they are now.
Can I ask why you think this?
Interesting, good to know. What about clusters? I‚Äôve been playing with them recently trying to learn more about them.
[deleted]
thats the first ive heard about it, might check it out.
I love CentOS Stream haha. I always feel guilty about it around here, but as a homelaber it's a real sweet-spot of new and stable. I totally get they completely changed what CentOS was and so it's no longer fit for its old purpose, but the new purpose is pretty cool...
This is a misinformed comment. This user does not understand the relationship between RHEL and the current downstream projects.

My recommendation is AlmaLinux. Rocky Linux works too. They are the true continuation of what CentOS used to be. Hell, they're better than CentOS ever was.
IBM (who bought Red Hat) deprecated CentOS because it was competing with their paid market share. CentOS was a bug for bug compatible version of RHEL. Red Hat originally stated an End of Life of **2029** for CentOS 8, yet they've already deprecated it. 

The fact that they released a new product that is an upstream of RHEL is perfectly fine. However, they shouldn't have called it CentOS. They only called it CentOS to pretend they weren't killing off the real CentOS to regain market share.
beta isn't bad and wasn't ment to disparage, but you simply can't deny it's upstream of redhat now and most of its fame came from the status of being an exact copy.
And there are reasons for an exact copy of redhat. Thats why alma and Rocky came out fast after centos8 got canceled.

Redhat -> CentOS -> fedora.

In the end its all Linux.
CentOS stream is good for people developing for the environment, and maybe home servers, but it certainly isn‚Äôt viable as a RHEL alternative.

For the simple reason that security fixes can be delayed for weeks (I think the current record is 42 days) because Stream doesn‚Äôt get the fix until ALL applicable versions of RHEL get it. And sometimes things like EUS, AUS, and real-time can seriously delay that. 

It‚Äôs not uncommon for Alma and Rocky to have fixed packages before Stream. 

And frankly, with Alma and Rocky out there, as well as D4I, no reason I can see to run Stream on a home server.
It is a beta
Unfortunately there is nothing else, but DEbian/Ubuntu and AUR repos for everything. 

Probably Snap/Flatpak would take a few more years to grow and try to kick off those 60.000 packages repos that RHEL doesn't have.
My work finally switched to Ubuntu when CentOS stream happened. To be fair, all of us were more familiar with Ubuntu/Debian, but regardless, it has been easier to work with since the transition.
Thank you! He had a lot of cool stories. Well except the military, he‚Äôd never talk about what he did for them regardless of if it was classified or not lol
This is misinformation. Centos Stream packages have passed QA and are merely being accumulated ahead of release to Red Hat, so it isn't a beta.
Is centos Stream upstream, downstream or parallel to rhel?
Was going to write something along these lines. OP, try Fedora as it's a little more desktop-oriented and part of the RH family.
I was thinking to use it as my coding machine and focusing on learning more about security tbh
I mean he died in October

Edit: we‚Äôll in all fairness it was a few years before he died he recommended Cent
Just static link everything in this age of cheap disk space.
The change was a huge improvement.  CentOS can now accept contributions.  Bug reports actually go in front of the real maintainers and get fixed, rather than being closed as "matches RHEL, works as intended".  Users get features and fixes as they pass QA, rather than waiting for a huge batch update (minor release) months later.

The trade off for all these improvements is moving from a 10 year lifecycle to about 5.5 years, but anyone familiar with the ecosystem knows that the distro gets increasingly difficult to use in those later years due to old software versions.  And if you really need those full 10 years, you can still use RHEL (paid or free) or one of the other RHEL rebuilds.  One course of action I've seen people discussing is using a CentOS version for 5 years to enjoy the above benefits, then deciding if they want to migrate to the next CentOS version or to the same version of a rebuild for the extra lifecycle.
Most of the time vendors don't need to explicitly target it.  If a vendor is targeting a RHEL major version, then things will likely just work on CentOS Stream.  It can only be as different from RHEL as RHEL minor versions are from each other.

If a vendor explicitly targets RHEL minor versions, then you may run into issues.  Kernel modules are the first thing that comes to mind.  I suggest encouraging those vendors to additionally target CentOS so they are prepared for the next RHEL minor version.  That's what ZFS does.
Oracle Linux is 100% binary compatible, but they change the core system which makes it different from other RHEL and the clones (mostly with their no-downtime kernel patches thingy(I've been told this breaks more than it fixes in production long run, but I have no personal experience on the matter)). 

The main reason for not recommending it is because it's Oracle, and they do not have a good history or business practices at all, being toe-to-toe, and often worse, with Microsoft.
>testing ground

It's not really a testing ground.  Every change in Stream has already passed the same QA it would have gone through to get into a RHEL release, before being released into Stream.  

It *is* an actively developed distribution and you won't get patch rollups every 6 months where everything is documented with nice release notes, which makes it less suitable for some roles.  I just want to make the point that it's not as unstable as people think it is.
The actual "free enterprise Red Hat" standard bearer is the Red Hat Developer Subscription, which gives you actual RHEL, for free.
Why go that route when you can literally get RHEL for free for up-to 16 devices using their free developer subscription.
Fedora should be the natural choice here as he is clearly interested in the red hat ecosystem.
CentOS Stream averages new updates about once a week.  That's nowhere close to openSUSE Tumbleweed (if that's what you were implying) or rolling release distributions.  Also the types of updates are different than what you'd see in those other distros too.
CentOS Stream is still stable.  It follows the RHEL compatibility rules.  Calling it rolling was a blunder in the original marketing.  What actually happened was it ditched the minor versions, and only has major versions now.
Default Python versions (of supported releases):

- RHEL 7: Python 2.7 (with Python 3.6 available).
- RHEL 8: Uses a platform python, 3.6. Allows for installing 2.7, [3.6], 3.8, and 3.9. User gets to set the unversioned Python symlink.
- RHEL 9: Back to traditional setup, Python 3.9 (Python 2.x is no longer shipped). More side-by-side installable options will be added over time, but 3.9 will always be there for the full 10 years.
This is actually exactly why he told me he used it. He said a lot of companies used it also so it was good to have an understanding of it. 

Oh when he retired he pretty much never used software on his main machine unless he could review the source code him self. He‚Äôd hotswap hard drives to one of the others of his windows one for things like Skype
PUIAS Springdale is Princeton and the Institute for Advanced Studies (which is basically across the river from Princeton). 

Yale‚Ä¶ that‚Äôd make some folks really upset, if they saw it. :)
I have yet to work for a company that doesn't use a debian based distro, and I only work for large companies. But I also only work for tech companies.
Fedora is not ‚Äúintended for desktop‚Äù any more than RHEL or CentOS Stream. Fedroa is just further upstream. What you see in Fedora now ( feature-wise ) is what you'll likely see in a later major release of RHEL and CentOS. For example, RHEL 9 is loosely based on Fedora 34.
Availability of software is a huge factor.  Many RHEL customers refuse to update to the next major version until the software they need is available in EPEL.  It's significant enough that last year [Red Hat decided to fund headcount for the Community Platform Engineering (CPE) team to officially help with EPEL](
https://communityblog.fedoraproject.org/cpe-to-staff-epel-work/).  This has already paid off with [EPEL 9 being stood up months before RHEL 9](https://communityblog.fedoraproject.org/epel-9-is-now-available/), rather than months afterwards as happened historically.

You also touched on the hardware.  The way I think of it is that when you chose RHEL or a RHEL like distro, you're mostly picking Linux hardware compatibility from when that version was branched off from Fedora.  For example, RHEL 9 was branched from CentOS 9, which was branched from Fedora 34 in 2021.  So you mostly have 2021 Linux hardware compatibility through RHEL 9's end of life in 2032.  There are hardware enablement backports that happen during the first 5 years of the lifecycle, but those tend to focus on specific hardware platforms that customers push for, which tend to be server hardware, not laptops or workstations.

All that said, if your hardware is compatible and you have all the software you need available, there is absolutely nothing wrong with using RHEL as a workstation.
Depends on if you're talking home or work use.  At home Fedora works well and I use it for my desktop VMs.  

At work, Fedora wouldn't fly.  The lack of support would be a non-starter.  Even getting past that, it's a load of paperwork, security scanning, reports, remediation, and automation to add in another OS.
For this use-case, where it's not a production/dev server trying to 1:1 RHEL, what would be wrong with CentOS Stream? I've been using it on my homelab for awhile and been very pleased.
Just a clarification, CentOS Linux 8.0 stopped getting updates on 2019-11-05.  After that it became 8.1, and continued on until 8.5, which was the final minor release.  You're correct that it's no longer maintained.

Minor versions in RHEL rebuilds are just points in time, they don't continue to get updates after then next one comes out.  The only really safe way to stick to one is to pay for RHEL EUS.
You seem to be trying to offer clarification, but I think you mean "CentOS 8" when you say "CentOS 8.0".

"CentOS 8.0", the point release, didn't EOL on Dec 31 2021, but on Nov 5 2019, when Red Hat released RHEL 8.1.
As you well know it's not dead. They changed concept very slightly (more regular updates) and call it Centos Stream.
Yup, and thus CentOS Stream if you are running it as a server no longer has the same security qualifications as old, EOL CentOS which was downstream of RHEL.
EDIT: as people seem to want the truth. 

I keep seeing people lying about CentOS being killed and recommending AlmaLinux, so I figured I would spread some lies too.
Sure, but that doesn't mean you can't use containers and VMs in Fedora and the  software tools to do those things are still regularly updated.

As an added bonus, there's also Fedora toolboxes where you can run containers/virtual OS (not VMs) of different versions of Fedora (and really, any OCI from Dockerhub) with most of the sandbox constraints to the host removed. I actually have an Arch toolbox inside Fedora on my laptop.
> openSUSE Leap

Will be discontinued after its next release. Its replacement is something called openSUSE ALP but how exactly that's going to look is pretty nebulous at the moment.
> However, I recommend Mint for casual users who are interested in Linux.

Please stop this. Mint isn't just a "casual user" or "beginner" disto like many people like to parrot in this sub.
Yeah, but many don't know what "stable" means and think it means "my computer doesn't crash". Don't know how often I have this argument with people claiming Arch is "stable".
I disagree. You're talking about software repository conventions where things from maintainers move from untested to stable eventually.

They are not mutually exclusive nor is it 'explicit' in all contexts. A system can be stable with newer software releases. A software's development lifecycle is also separate from a repositories release cycle. A release schedule is also not aligned with every other distribution.

Stability of the system itself is simply how stable the system is which as you already gleaned from the context was what I was specifically talking about.

If 'in distro context' stability is 'explicitly no/few software updates' then you're saying that Fedora is wrong about the context of their own distribution.

For example, under [Fedora Myths](https://fedoraproject.org/wiki/Fedora_myths) in the Fedora Project Wiki:

>MYTH - Fedora is unstable and unreliable, just a testbed for bleeding-edge software  
>  
>Fedora has proven that it can be a stable, reliable, and secure platform, as shown by its popularity and broad usage. Additionally, our well-managed packaging and review process adds an extra layer of safety not found in some other distributions.

You could criticize the opinion of stability sure but that doesn't change the context.
Stable is an overloaded term.  For some it means compatibility.  For some it means reliability.  For some it means as few changes as possible.  Each individual Fedora release is stable, as per the [updates policy](https://docs.fedoraproject.org/en-US/fesco/Updates_Policy/#stable-releases).  But there are new releases every six months, and each release is only maintained for about 13 months.  That balance (stable but room to innovate in new major releases every six months) is working really well for the project.
If you need to conceptualize the release branches, it looks like:

 *  Development: Rawhide
 *  Stable: Fedora
 *  Stable LTS: CentOS Stream
 *  Stable LTS with parallel stable minor releases: RHEL

The "Testing" and "Unstable" branch names are probably references to Debian's model.  Trying to line up Red Hat family and Debian releases results in a lot of places without direct analogs, but where they do come very close to aligning, it looks more or less like:

    Unstable/Sid  :  Rawhide        :  Rolling development
                  :  Fedora         :  Frequent (6m) stable
    Debian        :                 :  Infrequent (2y) stable
    Debian (LTS)  :  CentOS Stream  :  LTS
                  :  RHEL           :  10y LTS with 2y LTS for (selected) minor releases
RHEL isn't aimed at the desktop user, though many thousands of Red Hatters are using it as such for their daily driver. It works fine on the desktop. Funky hardware, odd configs, bleeding edge features are all going to be absent from RHEL though, as it's not what Red Hat builds for. Fedora aims to be on that edge, or just behind, and is much better suited just by mission for desktop than RHEL would be. Honestly I'd have no issue running Fedora in most server applications, the biggest problem in that space is that Fedora will EOL after a year and while upgrades have been pretty smooth for quite a while now I completely get not wanting to do a major OS update annually.
Yeah perfect for clusters. All the clusters at my university use CentOS. From what I've seen government clusters use redhat and universities use CentOS.
I help run a few academic clusters. We used to run Centos, now we're in the process of migrating to Rocky. Unless you're running a heavily containerized workload, Stream poses issues if you need a controlled environment.
> What about clusters? 

Clusters of what? Like HPA clusters? RHEL (and it's derivatives) are popular choices for that. There are bespoke operating systems for Kubernetes clusters as well if that's why you're thinking of.
Please don't say that. If you need RHEL then you need the stability which Fedora cannot (and is not intended to) deliver.

13 months is not a viable upgrade cycle in an environment that needs RHEL.

RHEL for workstations is RHEL Workstation or Alma Linux, or CentOS Stream (upstream from RHEL, but if you keep following it, it will track just ahead of RHEL and be supported for the full cycle).
The new rebuilds have been able to put more resources into their distros than Red Hat ever did into CentOS Linux.  The direct result is they've done a good job getting releases out quickly, especially AlmaLinux.  I agree they're better than what CentOS Linux was.  But I personally think that CentOS Stream is better than any downstream rebuild.  CentOS Stream can actually fix bugs, not merely reproduce them.  When you file a bug, do you want it to be handled by someone who can actually fix it, or someone who will close it as "reproducible on RHEL, works as expected"?
> CentOS was a bug for bug compatible version of RHEL

No, it wasn't.  Ask the CentOS QA people, and they'll tell you, "We came up with the phrase "bug-for-bug" compatible during EL5 as a GOAL to aim for. CentOS was NEVER bug-for-bug compatible"

https://www.spinics.net/lists/centos-devel/msg19564.html

And neither Rocky nor Alma are, either.  It simply isn't possible to create reproducible rebuilds without build environment information that Red Hat does not and has never published.
For the umpteenth billion time, IBM had nothing to do with it. 

FFS, IBM was making money by selling CentOS support contracts.
Nice conspiracy.
I've been on the receiving end of lots of the CentOS backlash, and I assure you that most people who keep insisting that CentOS is a beta now mean it in a disparaging way.
It is upstream of red hat, but only gets updates already approved for the next point release.

Alma and rocky have their place, but do not have additional benefit for most other than emotional comfort and ability to push off updates for a short while.

They also have gaps in their support cycles when support moves from one point release to the next.
> beta isn't bad   

In the context of what CentOS' single reason to exist was it is bad.
Read the original post.

The person wants to try centos because his late father recommended it.

He osnt trying to run high frequency trading for a fortune 500 company.

For a big enough deployment you will need a well versed staff abd/or support. In those cases a rebuild may not be the best option anyway.

As for the delays in security updates, that is interesting. I would like to read more about it.
There are many reasons to use CentOS Stream on a home server.

- You want to get access to bug fixes months before RHEL and RHEL rebuilds.
- You want to get access to new features months before RHEL and RHEL rebuilds.
- You want to use an operating system that you can directly contribute to.
- You want to be able to file bug reports with the actual package maintainers who are empowered to fix it.  D4I doesn't allow you to open support cases, and all rebuilds can do is verify the bug exists in RHEL.
> For the simple reason that security fixes can be delayed for weeks

In this context, I think it needs to be mentioned that CentOS security updates were *regularly* delayed 6-8 weeks, twice a year, after every point release.

> (I think the current record is 42 days) because Stream doesn‚Äôt get the fix until ALL applicable versions of RHEL get it.

That's not now security updates work in Stream.  There is no policy that requires that all RHEL releases get a fix before Stream can.

The only updates that are *expected* to be delayed in Stream are those that are under security embargo.  And for that small relatively rare set of patches, fixes start when the embargo is lifted (when information becomes public).  Work on Stream can start as soon as the vulnerability details are made public.  They aren't required to wait until all RHEL releases are patched.
>For the simple reason that security fixes can be delayed for weeks (I think the current record is 42 days) because Stream doesn‚Äôt get the fix until ALL applicable versions of RHEL get it. And sometimes things like EUS, AUS, and real-time can seriously delay that. 

Are we talking about CentOS Stream 8 or CentOS Stream 9?  Because the release process for Stream 8 is in a bit of a transitional state and not as streamlined as 9 has been.
What I meant was the fact that your grandpa used Linux.

While we all say things like "I use Arch, btw"... you can say "well, my grandpa used Linux, B.T.W"
Is CentOS Stream upstream or downstream of the "RHEL x.y beta" snapshots that get published a few months before a "RHEL x.y" release?

(This is not trolling it's a genuine question!)
> This is misinformation. Centos Stream packages have passed QA

Hi, I work for Red Hat. This is currently true for CentOS Stream 8. ~~I don't think it's true for 9.~~ Edit: correction below

I wouldn't worry about it much, because updates are conservative.
It is a beta for RHEL. It is kinda solid and stable beta due to its QA, but that does not remove its property as RHEL beta ground.
It's more of a server OS, if you want a desktop oriented version you should use Fedora. 

If I'm not mistaken the pathway of features and bug fixes used to follow this trajectory: Fedora -> CentOS / Red Hat

It is now like this: Fedora -> CentOS Stream -> Red Hat
CentOS died in december. Sort of. I still have it on some servers after using it for 15 years. Next time I will probably go for Rocky or Alma. Writing this on Red Hat BTW
That's not an option for some projects that depends on glibc and can't be compatible with another libc like musl.
Cool so how about RH support broth RH stream and Centos linux which is downstream of RHEL.
The vendors I mentioned specifically said they don‚Äôt support CentOS because of the short time between minor versions. Since they have multiple distributions they test, CentOS‚Äôs short time between minor versions means they just don‚Äôt have the resources to check every time CentOS changes.
That is not how VENDOR SUPPORT works. Support means that you get everything guaranteed to work without trouble and once you encounter anything new you get others to back you up.

Unfortunately no vendor targets on CentOS stream. So anything may still work on it, but that is different from support.
Mainly because I need a lot more than 16.
If down voting something It will be good if you know what you are doin and talking about.

It is stable but not production ready stable. 

In terms of server application I *specifically* write "Centos is fine if you are fine with semi rolling"  

Fedora is a full flag testing for redhat which is solid stable with 7 or 10 years support. Centos is placed right in the middle of that. 

Stable?   
Yes. 

But is it enough stable for some?  

The specyfic OP question was "How do you guys feel about Redhat and CentOS?"  
So I feel exactly as I wrote and this is based in facts.   
Here is a full reference https://www.redhat.com/en/topics/linux/what-is-centos-stream  
Quote   


>When you use CentOS Stream, you benefit by gaining early access to the same source code Red Hat developers and engineers use to produce the next version of Red Hat Enterprise Linux. The platform provides a continuous stream of content with several updates per day  
The platform provides a continuous stream of content with several updates per day  
If that speaks stable for you then good for you. it is not enough stable in many use cases

&#x200B;

>Calling it rolling was a blunder in the original marketing.

I never said that 

I call it **semi rolling** or if you prefer constant development yet bit slower then in fedora.  
So the right replacement for what centos once was is rocky, alma, oracle linux not centos stream.  


If you Seek for dot for dot rethat you do not want stream if not then go for it.  
And that is based in reality
Pity we don't live 200 years long. Anyway it's nice to hear that old man tried to entertain himself by reading the code and ruling his own Linux installation. Usually people complaining  about annoying elderly relatives who panic from accidentally pressed buttons in UI  or disappeared Favorites in  their browsers. üòê

But actually majority of us needs only the browser to find casual entertainment and e-mail client to keep bills and other official notifications.

Anyway I would probably vote that  RHEL&clones are only good for a stable environment in business processes or supervised home/office usage. 

For young generation RHEL universe  would look like a very boring thing.
> Many RHEL customers refuse to update to the next major version until the software they need is available in EPEL.

That's what I used to do. At the moment our main cluster is on CentOS7.9 which is what I keep the small one I manage on (to be compatible between them and to be able to just copy the compiled software over).

On the workstations I am on Ubuntu LTS mostly (currently actually Pop!_OS, but that's close enough on the basis), since most of the specialist software is developed on Ubuntu and needs to be built from scratch on Redhat (which is usually not a big deal, but not something that I want my students to have as their first exposure to Linux).

We only have HPs at university which are all Linux certified models (Z-Stations and Z-Books). Unfortunately our IT is not really supportive of Linux, so I have to manage our research group myself (or rather support colleagues and students in installing what they need).

Edit: the commercial simulation software we use is currently only supported up to RHEL8.5, RHEL9 is on the Roadmap for Q3 2024! I don't know how they can get away with it at ¬£15k per seat per year.
> Depends on if you're talking home or work use.  At home Fedora works well and I use it for my desktop VMs.

Totally agree. For home use, definitely Fedora.

My argument is with the "RHEL is for servers only" crowd.

Myself I'm Ubuntu based (Pop!_OS), and use LTS for work and the regular ones (6-month releases) on some of my private machines.
There is nothing wrong with stream. But saying just CentOS is CentOS 8.0 not stream. There is a difference.
For work, support. Management wants everything supported down to dev.  That means RHEL for everything.  

At home, I'm using CentOS 7.  Moving to 8, I'll probably switch to Alma or Rocky.
Lol was the pedantics necessary though ? Since its dead anyway .
Yes i meant 8. Sorry for that my bad
Exactly CentOS is not the same as CentOS stream. And there is a clear difference and they shouldn't be blindly talked about as being the same. They are close but not the same.
Yeah, but it's not the same Cent OS, it's basically a RedHat testing branch now.
As this is my jam, can you be explicit what you mean by this ?
You got any sources for that?
That is the biggest BS
> I keep seeing people lying about CentOS being killed and recommending AlmaLinux, so I figured I would spread some lies too.

That's not how you fight disinformation. That does more harm than good. Idk what your original comment said, but just stop.
Is it Chinese?
Where did you get that from?
> and the software tools to do those things are still regularly updated.

I guess but the basic features of containers and VM's are pretty mature nowadays so I don't really think many are going to see the benefit of getting the latest bits unless they're a developer that specifically works with VM's and containers in some sort of non-trivial way (to where they'd need to use some obscure and new feature or option). Whenever I talk to developers the main concerns seems to be to get to where they can develop on the latest versions of PHP or Python or whatever without potentially breaking the host system.

> Fedora toolboxes 

Yeah I've actually seen that but I've never sat down to really explore it. I use Silverblue and my only exposure to them is starting a fedora toolbox to search for a package since `rpm-ostree` doesn't have a search feature but all the package names are the same.
still at least 4 years to go until there is no more openSUSE Leap. And then it will continue just with a different design to the distro (immutable core it seems).
I didn't mean to say that Mint is not a good distro for advanced users. Rather that it is well suited for beginners. There is a difference between those two statements!!
I consider myself an advanced user. i use Mint because, honestly, after a long day of work, I don't want to come home and fight dependency hell for 4 hours. I just want to game.
I just don't get the appeal of Mint. I get the appeal of Fedora, of Ubuntu, of Pop, of Arch, of OpenSuse, of Elementary and of Debian (despite its old packages, which I don't like, but I understand other people might).

But Mint uses old, LTS Ubuntu releases adding nothing except am ugly theming. Well, it uses flatpak so for the people who hate snaps there's that...
The problem here is that we have two different definitions of the term "stable".

In my context (research, where reproducibility of results is paramount), stability means that versions of libraries, compilers, software, etc. stay the same. In a developer context, that's what you tackle with environments, which ensure that everybody works with the same version of packages, etc.

In other contexts "stable" simply means that your desktop is reliable and doesn't crash.

Fedora is "half-stable" within a release, since it does not upgrade key libraries between, e.g., 36 and 37, but does upgrade software. In many applications that's OK (and in many, it's the Goldilocks ideal).

The "Fedora has proven that it can be a stable, reliable, and secure platform, as shown by its popularity and broad usage." is, with no disrespect to the Fedore maintainers, incorrect in many ways: (a) the definition of stable changes between "Myth" and counter-argument (although the "myth" seems to use the latter definition as well, but most people I know that don't use Fedora mean the former when they say it's not stable enough - referring to the 6/13-months cycle), (b) the logical fallacy of "it is used often, therefore it must be good" is wrong in almost all fields - popularity was never a measure of quality or correctness.

Fedora is great and definitely a really good option for a workstation (that is self-maintained) but "stable" in the first sense it isn't. It is, however, really stable in the second sense - and that is a huge feat by the maintainers, considering the leading edge technologies that it is a testbed for. And that is what it is, it is, aside from being a good distro in its own right, a testbed for what is going to go into RHEL or not.
> You're talking about software repository conventions where things from maintainers move from untested to stable eventually.

The repositories and packaging is 99% of what makes a distro IMO, so this and the distro itself is the same.

> A system can be stable with newer software releases.

The moment it receives a non-patch software update it becomes unstable, so my opinion is that it is. If you've got a release that during the supported window receive non-patch updates then it's not a stable distro.

> then you're saying that Fedora is wrong about the context of their own distribution.

They're muddying the waters unnecessarily, yes. They should have picked a different word that doesn't already have a different meaning in that section.
>  That balance (stable but room to innovate in new major releases every six months) is working really well for the project.

I'd say that's the unique selling point. And it's what makes Fedora interesting. And the maintainers seem to do a brilliant job of it.
Hmm, then it sounds like this part isn't correct and that the releases are in fact stable.

> and very fast updated software releases

From that text I'm reading that they don't have updated software releases at all.
They are just being obtuse because IBM owns Red Hat.
Oh perfect! Looks like I have a good project for it
The university I work at is 100% RHEL.
CentOS Stream must follow the RHEL compatibility rules, because the next RHEL minor release branches off from it.  There have been some early growing pains with the transition from downstream to upstream (especially around getting RHEL maintainers onboarded into the CentOS project), but it's a valid choice for most workloads, not just containerized ones.
[deleted]
If I need the binary compatibility with RHEL on a cluster, I'd choose Alma.

The bug-fix problem is not really a problem, since critical bugs get patched between point-releases on RHEL, so it's RH's certified bugfixes that you'd get. All bug-fixes and new features that get introduced in Stream, but not RHEL, will be non-critical and will be in the next point-release.
I don't think one is strictly better than the other. I agree with the purpose of Stream. I think having a proper target for upstream submissions and using it as a preview for upcoming changes makes a lot of sense.

But we've already been shown that getting security patches out in a timely manner isn't a priority at all. It's still just a sandbox to me until the gap between RHEL getting a security patch and Stream receiving it is minimal. It's *weird* when the clones receive patches before Stream does...
This old lie never dies.
I've genuinely never heard it called a conspiracy before. I thought it was just the generally accept series of events. The depreciation of CentOS the entire point why Rocky and Alma Linux even exist. When CentOS 8 was originally released, they put the End-of-life as **2029**, but have already deprecated it, which left many users unsupported.
Rocky and alma have the same benefit from rhel that rhel has from CentOS stream
There is a policy that CVE fixes rated Important or Critical (embargoed fixes will be in one of these categories) go to RHEL customers first before CentOS Stream.  This is similar to how Fedora has worked (without an explicit policy) for many years.  Ideally CentOS Stream gets the fixes within a day or two after RHEL.  There have been notable instances of CentOS Stream 8 having much longer delays than that due to problems with how it's built.  I'm not aware of any long delays happening in CentOS Stream 9, where RHEL maintainers are directly responsible for their CentOS builds.  It's also worth clarifying that CVE fixes outside that policy (Low and Moderate) are delivered in CentOS Stream *months* before RHEL or any RHEL rebuild.
Regarding the delay on security patches ‚Äî yes, that was an issue around point release, and it occasionally became a problem when an important or critical bug was only in the new point release, but that wasn‚Äôt particularly frequent. 

But the delay of critical and important security fixes on Stream 8 is now a common place thing. Yes, you can often get low/medium fixes early (and hey, so can RHEL customers! I‚Äôve done it many times for mine), but those aren‚Äôt the ones where C levels ask about the status. 

Oh, and the new rebuilds aren‚Äôt taking weeks on point releases. They‚Äôve been getting them done in days.
I mean, I‚Äôd say his history of coding for the military, Atari and ibm is way more interesting then him using Linux
Upstream.  Those betas are branched off from CentOS Stream, get a little bit more development work (literal release blocker bug fixes), and then become the next GA minor release.  While that's going on, CentOS Stream moves on to content for the next next minor release.

That's the minor version betas.  The major version betas kinda work like a minor release before .0 (e.g. 9 beta was like 9.-1).  You can see that in the diagram in this post.

https://blog.centos.org/2021/12/introducing-centos-stream-9/
> This is currently true for CentOS Stream 8. I don't think it's true for 9.

I also work for Red Hat.  It is true for 9 as well.  CentOS updates must pass RHEL gating checks before being included in a production compose.  You can learn more about this process in [this talk](https://youtu.be/yf1wO5Iu8uY) (gating discussion starts at 25:11).
I believe it was "sold" as including QA when stream launched. Interesting that it was dropped for 9 then? Especially given people's aversions to it... 

Anyway I agree on your last point.
It's amazing how many people in this thread don't understand the term beta, and as a result keep insisting it isn't one.
The old model was more like Fedora -> RHEL -> CentOS (and others).  Now it's Fedora -> CentOS -> RHEL -> AlmaLinux (and others).  I created some diagrams to help explain, which you can find in [this Twitter thread](https://twitter.com/carlwgeorge/status/1439724277746573314).
It sounds like they don't understand CentOS at all.  RHEL has new minor versions every six months.  CentOS has no minor versions.  They both have new major versions every three years.  Any vendor that supports RHEL would benefit from also supporting CentOS, because if your software works on CentOS you know it will work on the next minor release of RHEL.  This especially helps vendors that block you from updating to the next RHEL minor release until they've tested it.
I would love to see a vendor contract that says "everything guaranteed to work without trouble" or even something close to that.
Then you should consider getting the subscription as apparently you make money from linux somehow if you have 16+ devices running it, and you will need all the support you can get, plus as a rule of thumb, you get what you pay for, if it's free, it probably has crappy support compared to what RHEL offers.

> If down voting something It will be good if you know what you are doin and talking about.

One, I didn't downvote you, that was other people.  Two, I'm one of the CentOS maintainers, so I do in fact know what I'm talking about.

> If that speaks stable for you then good for you. it is not enough stable in many use cases

The daily thing is outdated information from early planning days.  We typically push updates once a week.  And that just means the updates are available, every user is still in control of when they apply those updates.  Also the type of updates matter here, these are updates that are appropriate for the next RHEL minor release.  It's exceedingly boring compared to actual rolling release distros.

> I call it **semi rolling** or if you prefer constant development yet bit slower then in fedora.  
So the right replacement for what centos once was is rocky, alma, oracle linux not centos stream.  

It's not rolling or semi rolling.  It gets updates in the same way other distributions without minor releases do.

> If you Seek for dot for dot rethat you do not want stream if not then go for it.

Most people that think they need that, don't actually need that.  But they have options for that if they insist, including actual free RHEL.
Yup, I use RHEL at work as a VNC desktop and it does the job I need for handling Linux and our container platform.

I don't really use Ubuntu much outside of training labs that have it - common if it's a Kubernetes lab from what I've seen.  Work is entirely RHEL so Fedora and CentOS at home work well as I'm already comfortable with the commands.
> But saying just CentOS is CentOS 7.0 not stream. There is a difference.

Oh, I did not read that it way, my bad. I just thought he meant the most recent version of CentOS (Stream 9). Makes perfect sense now.
Extremely close.  Last time I measured package versions it was about 95% the same (CentOS Stream 8 vs RHEL 8).  Remember, CentOS Stream can only be as different from RHEL as RHEL minor versions are from each other.  It's not a radically different thing.
I prefer to think of it as a RHEL preview. Every package in Stream has gone through Red Hat QA. Every few months, a snapshot of Stream becomes the next RHEL point release.
Testing isnt a fair term, there is no unstable or QE that doesn't go into stream that doesn't go into rhel.

The -only- exception to this is that embargoed security flaws get tested MORE in RHEL, because they are in a 'private' branch that isnt in centos streams git repos.  However, on public dates these hit centos streams at the same time that rhel does.

Source: I work in product security for rhel kernel.  Retbleed sucks.
So, security and compliance folk want to ensure that all of the libraries and core system components compiled into the distribution are the mainstream, stable release versions. Red Hat's team qualifies that everything in the base distribution meets those criteria, so it's a "security qualified" distribution, borrowing Red Hat's security controls. Old School CentOS was a downstream distribution of Red Hat, so it also inherited all those security controls. 

Fedora generally compiles the latest builds from every distribution library that supports it, usually the libraries with the most vulnerabilities associated with them. It's an upstream distribution. CentOS stream is now also an upstream distribution, so though it is not as unstable as Fedora, it still is ahead of GA. As we know, many of those tweaks in an OS can happen at the last minute.

The practical application of this I've seen in the wild multiple times is that upstream distributions that are not fully GA and security qualified get hacked. In every one of those circumstances where it was a library that wasn't from a stable release or branch that was the culprit, I've rebuilt systems with Red Hat, and they don't get hacked. Some companies just can't afford RHEL, so they need to know what they are getting into if production security is a concern.

Maybe somebody has solved for this in the past decade with a technology I'm not aware of, but its just been my practice to always run downstream of the LTS version of whatever version is out there. This applies as much to Suse as Ubuntu, or the other dozen distributions I've worked with, but less to Suse since it's always been more security stable.
I probably just don't know your use-case but for the most part you should be installing software in toolboxes or flatpaks on Silverblue to get the advantages of having an immutable OS.
Does this mean leap and microOS will merge? (Sorry if that's a stupid question but those immutable systems are hella confusing for me üòÖü§∑)
Exactly! This person gets it.
Mint just works, at least for me. That's the appeal. Of all the distros I've tried on my desktop setup it has always worked the best without needing to tweak much of anything. Even PopOS gave more more hassle when I tried it out vs. Mint. 

Mint now has tons of custom GUI and CLI based tools baked in which are super useful to new and seasoned users. Their built in backup/snapshot tools are regarded very highly.

Also Mint does follow the Ubuntu LTS train but that's a good thing for most. Not everyone needs or wants bleeding edge libraries or rolling release garbage that randomly breaks things.
> Fedora is "half-stable" within a release, since it does not upgrade key libraries between, e.g., 36 and 37, but does upgrade software

As a *policy* matter, Fedora is a stable release.  There are a small number of packages that have an exception, and those are listed in the policy document:

https://docs.fedoraproject.org/en-US/fesco/Updates_Policy/#stable-releases

It has happened in the past that a maintainer has pushed a major version update during a release, but that usually results in a bug assigned to the maintainer pointing out that this isn't supposed to happen, and shouldn't happen in the future.
A good chunk of software packaged in Fedora only gets updated in Rawhide, and then the new version is delivered in the next Fedora major release.  Even at that pace, it's delivered faster than most distros.  Updates that are applied to the existing stable releases of Fedora must follow that updates policy.  Some packages have standing exceptions (as documented in the policy), usually due in part to their proven track record of delivering compatible enough versions upstream.
They have some. But I'd consider Fedora to be "stable" within the release.

But for many the release cycle will be too fast. Or more importantly, the support cycle too short, to be considered "stable" enough for enterprise use - which it where CentOS/RHEL/Alma come in.

Fedora is better than the Ubuntu regular releases, though, since you can skip releases (and essentially get a 1-year upgrade cycle*), where Ubuntu only allows 9-months support for regular releases.

\*In a conservative way this could mean: upgrade to 35 when 36 is released, and to 37, when 38 is release - basically offsetting by one release to avoid teething problems.
Have fun!
Have a look at:

http://www.openhpc.community/

https://github.com/openhpc/ohpc/wiki/2.X 

I'd NOT go for CentOS, though, but for a full downstream clone of RHEL, like Alma (or the free developer version of RHEL), since CentOS is upstream it is basically testing for the next minor release of RHEL, so some of the RHEL recipes may not work as they are, since CentOS is part of that upgrade/preparation process.

Edit: I am NOT saying that CentOS Stream is not suited, but if you follow guides for RHEL 8/9, then the minor changes between point releases can trip you up. CentOS Stream is much more stable (enterprise grade) than Fedora (and most other Linux distros, incl. Ubuntu LTS).
Facebook runs centos streams, they seem to do quite ok.
Fedora is not an equivalent of RHEL, it is the upstream community distribution that will eventually become RHEL.

RHEL9 (which was released recently) was branched off Fedora 34.

Fedora is great, but with 6-months release and 13-months support cycles, it's not aimed at the same target.

Forget the RHEL is only for servers idea. RHEL or a derivative is (AFAIK) powering most of the worlds super-computers, true. But the Linux workstations that are connected to them are often also running RHEL Workstation.

I compare RHEL to Ubuntu LTS (with a support contract), Alma Linux to Ubuntu LTS (without a support contract), and then you have Fedora which is equivalent to the interim Ubuntu versions. 

Ubuntu also have a 6 months release cycle, but an even shorter 9 months support cycle - so Fedora can (theoretically) be used in a very progressive enterprise environment, since it gives the chance to skip a release and update after 13 months.
I know as a former RHEL customer there were times I was frustrated having to wait for the next minor release for fixes or features.  It wasn't about security fix severity, it was usually "not having this blocks what I'm trying to get done" and having to delay plans while waiting for it to be resolved.

Of course that's just my personal experience, but I do believe it's a valuable thing CentOS offers now, especially when combined with the contribution model.  I remember a time in RHEL 7 that I figured out a bug in the docker package (even provided a patch in the bug report) and had to wait over a year for it to be resolved in RHEL.
It's already been reduced drastically between 8 and 9.  8 workflows are a mess, it's still actually a rebuild, just rebuilt from source code from a different branch. 9 is where RHEL developers are doing their own builds directly.  I totally agree that it's weird when clones get a patch before CentOS does, but you have to realize how rarely that happens compared to all the other things that show up in CentOS first.  For example, each time a kernel CVE has made it into clones before CentOS, I've checked and seen 5-10 other CVE fixes that are in CentOS and not yet in the clones.
It's a reddit version of events. It foesnt really match reality.

As for Rocky and Alma, they are rebuilding from the Centos git, something that Red Hat could prevent if they wanted to, but dont.

The truth about Centos is it died before rhel 7. Without red hat investment they were failing at making rebuilds and a rebuild of rhel 7 (or was it 6?) was struggling to make it out. There was also almost no community - all the work in rebuilding and in releasing centos was almost exclusively being done by red hat too.

With the changes that have happened, Centos has been revitalised. It actually has a community now. SIGs actually have a purpose and contribute to centos and rhel development now.

Alma has also been a great addition to the community and offer support and features that everyone benefits from.

Rocky I am more iffy about. I probabpy havent heard anything bad in 2022, but sour tastes can remain for a long time and I dont see what their value add above Alma is.
It is a conspiracy theory, and it has been disputed by Red Hat employees repeatedly and consistently.  IBM was not involved in the decision.

Stream was announced in September 2019, and while it is unfortunate that the decision did not come sooner, discontinuing CentOS was really the right thing to do in Dec 2020.  It simply didn't make sense to spend another 3 (or 8) years building *both* CentOS and CentOS Stream, when Stream was a better option for most users.  It would have created the impression that Red Hat lacked trust in CentOS Stream.  It would have also continued to be a massive strain on resources, which probably would have caused more delays or other release problems with one or the other product, which would have created even more negative impressions.

I understand that people are/were frustrated that they'd started deploying systems on a release that was discontinued early, but I also understand why Red Hat discontinued it early, and I think that all things considered, it was the right thing to do.
Not really.  RHEL maintainers do their work in CentOS to get changes into RHEL.  Downstream rebuilds aren't doing work in RHEL.  It's a much different dynamic.
Thanks for clarification!
Honestly, this sounds rather strange on Fedora. It can be interpreted as that Fedora is not actively doing its own independent security maintenance on said Important or Critical.
Thanks :)
>Interesting that it was dropped for 9 then?

Remember CentOS Stream 9 is the opposite of CentOS Stream 8: c9s is upstream of RHEL 9 and open to community merge requests, whereas c8s is downstream of RHEL 8 and just released faster.

I don't watch them closely myself. Might be wrong about the specifics. Maybe packages really are held for QA and I just don't know about it.
It wasn't dropped in 9.  In fact it's better in 9 than in 8.
With their backwards thinking, if Centos streams were beta, every other release of rhel was beta too, it has essentially taken its place in the release mechanism.
You basically get something like this when you purchase a RHEL or SLES subscription. And read the last part of the sentence again.
I've not needed their support or subscription thus far. Rocky works just fine.
Oh ok take care man and thank you for clarifying some stuff :)
I wasn't too clear in my original reply sorry for that.

I just feel like it shut be clear that there are 2 types of CentOS and one is EOL. As many servers still run the old CentOS 8.0
You are completely correct and don't get me wrong not saying stream is bad. But CentOS 8.0 and CentOS stream are different as CentOS 8.0 is pretty much frozen and stream isnt.
Yeah, but CentOS was bianry compatible with RHEL. That's why it was so popular. You could develop on CentOS and deploy on RHEL without trouble. Not so much with CentOS Stream though
Sorry, I misused that term very liberally. I meant to say, CentOS used to be the RHEL just without the enterprise support.

Stream has changed it. Maybe it's F.U.D. but different distros stepped up to fill the void left by OG CentOS.

P.S. - you've got a really sweet job! I wanted to work with RedHat back in school, when it wasn't a part of IBM.
You can use `rpm-ostree install <packageName>` to do the package installation as a layerd package where it essentially re-plays the install of the package every time you update the OS.

It's kind of a hassle because you have to reboot to see the change or do a `rpm-ostree ex apply-live` which is only safe to do if you're installing something like `lsof` where there aren't going to be many dependencies where something might get updated out from underneath the running software. 

But yeah I do use flatpaks for desktop stuff, just not everything is desktop enough to be done in a flatpak.
sorry i really dont know about the relation between the future opensuse ALP and current microOS. it could be that both will exist in paralel. im nothing but a simple user.
While I don't agree, I appreciate your opinion
The kernel, e.g., is on the list of exceptions. Which makes it semi-stable in my book.

I have also had a few of those "should not happen" upgrades in my testing period. May have just been bad luck.
> I remember a time in RHEL 7

Hey, you make that sound like it's ancient history. All our HPC clusters are still CentOS7 :-)

Yes, CentOS Stream is a valuable model, and on a personal workstation I'd choose it over Alma*. On the other hand there are good reasons to stick with RHEL/Alma/Rocky, over Stream. Horses for courses.

\* personally I am stuck on Ubuntu/Pop!_OS since most of the software I use for my research is developed on that.
This workflow situation sorta demonstrates how they should have committed to the original timeline IMO. To this day the entire thing just seems poorly thought-out, and the break point would have been cleaner at a major EL release.

That said, we have more committed clones and a better process for submitting patches to RHEL. The positive generally outweighs the negative.

Live and learn I guess.
I'm saddened to hear that you have a sour taste from Rocky Linux. Whatever we've done to cause that, certainly wasn't intentional. There was quite a bit of mud-slinging early on by multiple projects, and that was highly unfortunate. If you have anything specific, I'd be happy to talk about it.

While I'm obviously biased, I'll try to answer your question about what we feel the value of Rocky Linux is...

Rocky Linux was built specifically to ensure that what happened with CentOS going EOL could not happen to Rocky. As I was literally part of the creation of CentOS, I had a very good vantage point of what we did right, and what we could have done better. With Rocky, we created a series of checks and balances within the project to ensure that a diverse community always retains control by interdependencies among people and teams. This means that no single person or company controls Rocky Linux as a whole.

Additionally, we have multiple members the community actually owning critical and core processes as opposed to a single company. All of the code we use to create and maintain Rocky Linux is free and open source, and the community shares in ownership of the critical build assets like all of our signing keys and our secure boot shim. This is so critical as any company controlling those assets, code, or process could hold the entire project hostage.

The Rocky team feels so strongly about this, that we refuse to release any version of Rocky unless all dependencies are also freely available at the moment of release such that anyone else can replicate and/or enhance what we've done. This is also why we've been a bit slower on major version releases as this diligence and effort takes time.

Hope that helps, Greg.
Red Hat CTO Chris Wright said, ‚ÄúCentOS Stream isn‚Äôt a replacement for CentOS Linux"

They are two entirely different products, neither of which I have a problem with. My primary complaint is that they named their upstream build after their downstream build which adds confusion to the situation. 

More importantly, Red Hat committed to CentOS 8 End of Life in **2029**. Now, how can the community trust Red Hat commitments? This isn't a "reddit version of events", that's exactly what happened. Please fill me in if Red Hat never committed to a 2029 EOL.

> Centos has been revitalised

CentOS Linux is dead. I'm excited that CentOS Stream exists, as I'm excited for any new linux project. But CentOS Stream is simply not a replacement for CentOS 7/8. They're simply different projects with different goals.
Thank you for the answer.

> It simply didn't make sense to spend another 3 (or 8) years building both CentOS and CentOS Stream, when Stream was a better option for most users.

This is the part that trips me up. CentOS and CentOS stream are simply different products for different users. People like being downstream for stability. That's also why people buy RHEL 8 and RHEL 9. I'm fine with the claim that Stream is a better choice for many users than RHEL 8 or CentOS 8. If they had announced that CentOS 8 was the last CentOS release, this entire conversation would be very different. 

However, they reneged on their guarantee of supporting a product through EOL, which now renders any other projection of EOL also fairly useless. 

> It would have created the impression that Red Hat lacked trust in CentOS Stream

Meh. I can easily see a world in which Fedora, Red Hat, CentOS, and Stream all coexisted, especially until stated end-of-life. This is especially true in light of the release of Rocky Linux (named after a CentOS founder). We will likely have an ecosystem in which these things do all coexist.
It is the same dynamic as CentOS Linux and rhel
It's complicated.  Sometimes those security fixes are created by Red Hat engineers, released in RHEL first, then Fedora and CentOS.  Sometimes the fixes don't apply to Fedora because they're already shipping a newer software version that isn't affected.  Sometimes the same person is doing the work in all three distros, and sometimes the Fedora maintainer is a different person.  The tendency in Fedora is to update to a new upstream version with the fix, rather than doing "independent security maintenance" to backport a patch.  If the fix is already public upstream, a Fedora maintainer might apply the fix before it's released in RHEL.
I know my kernel patches go through streams first  for non important CVE's, I see it there before rhel packages. 

I know you're correct about el8 , I have seen similar behavior for 9, but I imagine that someone internal will be working to standardize this soon so its less confusing.
\[Disclaimer: Red Hat Employee for the last 10+ years\]
Sure thing, happy to help.

P.S. Thanks for pointing out that section on the redhat.com page, I was able to find the right people to fix it.  They removed the "with several updates per day" part.  As one might expect, the update frequency (available, not applied) is more often than RHEL, but less often than Fedora.
No one should be using 7.0, as it hasn't had updates in over 7 years.  7.9 is the current version of CentOS 7.
CentOS is still just as compatible with RHEL as RHEL minor versions are between each other.  Even though it's no longer identical, it still follows the [RHEL compatibility rules](https://access.redhat.com/articles/rhel9-abi-compatibility).

You can still develop on CentOS and deploy on RHEL without trouble.  In fact it works better than it did before, because now you know that what you develop will work on the next RHEL minor release as well, not just the current one.
The ABI is stable throughout the life of a major RHEL release, just as RHEL itself is.
Hehe, it feels like a long time ago.  I believe that example was from 2014 or 2015.  RHEL 7 ended full support in 2019.  Since then it's been in maintenance support, meaning it basically only gets critical security updates, no new features and no new hardware enablement.  I still use it in a few places as well unfortunately.
Since you mention it. On a personal workstation, anyone should probably rather go with one the "faster" distros, think Ubuntu non-LTS or Fedora, IMO. LTSes like centos and similar can become frustrating because you end up with an old desktop environment, an old graphics driver, an old python version, etc etc. Not a problem on a server, but kind annoying (imo) for anyone with a workstation or laptop.
> This workflow situation sorta demonstrates how they should have committed to the original timeline IMO. To this day the entire thing just seems poorly thought-out, and the break point would have been cleaner at a major EL release.

I 100% agree.  8 should have been left alone under the old model and lifecycle, and the new model and lifecycle should have started with 9 going forward.  But it wasn't my call.  Having a dual upstream/downstream model in 8 has led to endless confusion and stress.

> That said, we have more committed clones and a better process for submitting patches to RHEL.

I'm not aware of a clone that's gotten patches into RHEL yet.  All of the external patch contributions I've seen so far have come from people that aren't affiliated with the clones.  There may be some that I haven't seen yet (I'm actively looking for them so if anyone knows of some please let me know).  I have seen them starting to file more bugs, which are still a valuable contribution, but not the same thing.
Damn I‚Äôm eating some popcorn reading this back and forth.
> Red Hat CTO Chris Wright said, ‚ÄúCentOS Stream isn‚Äôt a replacement for CentOS Linux"

That's selective editing.  What he wrote was "CentOS Stream isn‚Äôt a replacement for CentOS Linux; rather, it‚Äôs a natural, inevitable next step intended to fulfill the project‚Äôs goal of furthering enterprise Linux innovation"

CentOS Stream *is* what CentOS *should have been* but couldn't be as a post-release rebuild.  Stream is a *better* system.

https://www.redhat.com/en/blog/centos-stream-building-innovative-future-enterprise-linux
Hi, I work for Red Hat. Truth is:

* CentOS Linux deprecation was indeed a Red Hat decision, not pushed by IBM.
* Red Hat makes support commitments for Red Hat products, which have "Red Hat" in the name and little red fedora logos, to customers and partners. CentOS Linux was not a Red Hat product.
Yes, but that's not what you said.
Yes. I edited the reply meant to say 8.0 not 7.0
I didn't know that. Thanks for correcting me
From an enthusiasts point of view (Linux enthusiast) I agree with you there. From a sysadmin's, manager's, researcher's, <anybody using the computer for work> I don't.

Most people (not this sub's users) don't really care about having the newest version of DE, graphics driver, (Python excepted, but many of the Python devs I know would have their development Python separate from the system Python), etc. etc. -- most people would not want the hassle of upgrading every 6 months and I encounter many systems in universities that are still on Ubuntu 18.04 or CentOS7, because the 2 year cycle is too fast (it costs real money to upgrade).

So for any production system (and my undergrad and postgrad's systems) it's LTS (Pop!_OS or Mint), for at least one of my own, it's the short-term versions (Pop!_OS or Fedora).
The distro discussions are always the most opinionated üçø
> Stream is a better system

I feel like I've been pretty consistent saying that I have no problem with Stream. It's a great idea and I'm glad they created it. However, I don't know why they gave the name CentOS to Stream. I suppose your argument is that it carries the spirit of CentOS. The word spirit is appropriate here since CentOS is dead. 

Most importantly that nobody has addressed, Red Hat deprecated CentOS 8, a full 8 years prior to EOL. What's the point of an EOL if companies are fully willing to just deprecate products whenever they feel like it? 

>the project‚Äôs goal of furthering enterprise Linux innovation

That's surprising to me as I always thought CentOS preferred stability over innovation. Even the claim of bug-for-bug compatibility is about stability and predictability rather than innovation and rapid fixes.
Thanks for chiming in. I appreciate your insider viewpoint. Question though: How do your two statements align with each other? The first is saying that CentOS lifespans are determined entirely by Red Hat. The second statement however seems to abdicate responsibility for CentOS lifespans and EOL.
The only thing changed was rhel beta tests with CentOS and it isn't built downstream with only pubic tools

Where now rocky and alma are the same dynamic as CentOS Linux  and CentOS stream is just rhel beta
Neither should be used.  CentOS has never provided updates for a minor release after the next one came out.  The same is true of the new rebuilds.  In RHEL, minor versions have [independent overlapping lifecycles](https://access.redhat.com/articles/rhel-eus), allowing greater flexibility for when you move to the next minor release.  For example, you could still be getting security for RHEL 8.4, even though the current version is 8.6.  In a rebuild, minor releases don't serve a purpose other than batching up updates into a large update.  CentOS got rid of minor versions, in part to not confuse people about sticking with minor versions.  Check out [this video](https://youtu.be/tf_EkU3x2G0) by u/gordonmessmer for a more in depth explanation.

CentOS 7 can still be used as long as you're fully patched up to version 7.9, and will continue to get updates until 2024.
No problem.

> However, I don't know why they gave the name CentOS to Stream.

The name CentOS means Community Enterprise Operating System.  Thanks to the Stream changes, CentOS can actually accept contributions now, so the name fits better than ever before.

> That's surprising to me as I always thought CentOS preferred stability over innovation.

You're omitting the words "enterprise Linux" in front of that.  In other words, innovating within the scope of a stable enterprise platform.  That fundamentally shapes what that innovation looks like.  The leading edge of innovation still happens in Fedora.
Stream carries the CentOS name because it was created by the same group that made CentOS Linux. Different product from the same group, sort of like Fedora Linux vs Fedora Silverblue (not to draw a comparison between the wildly different Stream and Silverblue beyond the naming pattern)
What I'm saying is CentOS Linux never had any support commitment, which was the whole point of CentOS Linux. It was the free unsupported version. Our support commitments are a benefit of your paid subscription, not EOL dates in wiki pages.

Anyway, just use Alma or Rocky or Oracle Linux instead. All three are pushing updates faster than CentOS Linux ever did, so they're a better and safer experience for you, and they avoid confusing people into thinking they are supported by Red Hat, so it's better for us too. Win/win. It's unfortunate that this happened in a messy way, but at least we got to a good end result for everyone.
CentOS Stream is built in the public far more than CentOS Linux was.  It's also not a beta, as updates are delivered after they pass QA.
Read my original comment.
>Our support commitments are a benefit of your paid subscription, not EOL dates in wiki pages.

Yeah, this is probably the core issue that trips me up.   I definitely understand that paid subscriptions get stronger support. However, people are used to support commitments even on free software. Ubuntu LTS versions are great examples where people often build their tech stacks around LTS versions, even without paid subscriptions. Everyone viewed the CentOS EOL as a support commitment. And the ownership by Red Hat made the community belief in the commitment stronger. If Canonical killed an Ubuntu LTS release, I'd still be pretty upset despite the fact I don't have a paid subscription. 

Now the community has to go and reevaluate which EOL dates are actually real. It's not about free vs proprietary, gratis vs paid, nor is it even about the parent company. It's just a complicated case by case basis now. And that's frustrating.

Edit: also, clearly I won't be using CentOS stream. Although they have an EOL, clearly they might not stick to it, which would lead me to be vulnerable to security issues. As you've stated, CentOS is unsupported.
It is a beta as it is upstream of rhel, put it downstream and it won't be
Unless necessary, consider dropping the distro and developing the desktop environment alone, meant to be installed on any distro. Making a desktop is strictly easier than making a desktop *and* a distro, plus having it not tied to a distro will help adoption.
One thing I would like to say is, please contribute upstream as well, Cutefish used a bunch of things from KDE Frameworks but I didn't see much communication coming from them, so please talk with us as well!
1. Seeing as KDE were the ones who maintained QT 5 and they are preparing to move to the next version, how feasible would QT 6 port be? Doubt you folks could maintain a whole framework by yourself.

2. As many others said - a portable DE that will work on any distro would be much more appreciated than yet another distro. We don't need another ElementaryOS with an open DE that doesn't even work anywhere else.
If you're more comfortable with distros, how about you help create a Fedora Spin, an Ubuntu flavor, put it in the AUR, etc.

3. I think a lot of people appreciated how simple and elegant the DE looked, I'd very much like for this design to stick around after the fork.

4. Going with a *cute* theme, how about Cuddlefish or Bubblefish?

Anyways, good luck to you and the team!
If you really want, then make a desktop environment. Not a distro. We have enough distro in the linux world.
Developing a DE is like running a marathon, make sure you're prepared for it. And developing a distro to ship a DE is like running a marathon in a hazmat suit.
It's a shame. I was **really** looking forward to this desktop, so I really do hope you guys manage to do something with it.

1. From what I saw, it was pretty great already. I'd like the dock to be a little more customizable, or at least, have a few additions like a Trash icon, and a folder view like the MacOS dock. Also, it had no real settings for switching audio in/outputs. Definitely needed. Also virtual desktops.
2. Truthfully I don't care about the Operating System part of Cutefish. I just want the Desktop environment. Focus on the DE, the OS doesn't matter, and I assume, was only for them to show off the actual DE anyway.
3. Honestly can't remember. The things I "didn't" like were the things missing, see #1.
4. No suggestions currently.. Let your creativity run wild.

I have no programming knowledge, but I've already provided translations for everything in my own language, and will continue to do so, if this takes off. :)
This is nothing about Cutefish, but advice from someone who forked a closed project.

1. There are a lot of people saying they will help.  Almost none of them will actually help.  The "help" will be telling you what to code. :)
2. Right now there is a lot you want to do.  You will have less time than you think, and things will be harder than you thought.  So try and pare things down to the bare minimum, and expand from there.  The ideas in the thread about making the project just a DE for Debian, and perhaps Ubuntu with a PPA are not bad ideas.  Try and do too much and nothing will happen.
3. Be a politician.  You will be the face of the project so learning how to disagree nicely is key all the time.  And you will be constantly recruiting and begging for help.
4. Keep your soul out of it.  Good chance it will fail.  Potentially for the same reasons as Cutefish failed.  Be ok with that.  The project is not you.  And along those same notes, keep a work life balance, because the project will be work.  Just unpaid work!
well, I'm not even close to being "good" in C/C++, but I'd guess I could try to help a bit with some things, even just to learn.

Have I ever worked with QT? nope

Have I ever worked with a linux distro under the hood? Yeah not much

I could maybe just try to improve some code here and there, but not much, I think

[here](https://github.com/alba4k) is my github, in case you want to take a look
i suggest to develop more the cutefish's own theme, to ship by default only cutefish-style apps and no kde apps at all, but it will of course be possible to download them through the kde software store. as to gtk apps, they should have a windows theme to make they have the same border and buttons as kde ones or cutefish ones, to make everything more consistent and polished. :-) ;-)
When I first saw the review of CutefishOS I was excited about the project and it was sad to hear that it was discontinued.   
I'm glad that you are continuing this project. I have no knowledge to help you, but I can help you by using it and reporting some bugs. There are many people like me who can't help in development process but can help you by reporting bugs and suggesting new features. 

I think making a discord server will make it easier.
Like other people are suggesting, I would absolutely love a new desktop environment rather than an entire distro. That way it gives people the option of choosing whatever base distro they want and applying the DE to it.

From this fork I‚Äôd like to see what Cinnamon (IMO) fails to do, which is to offer something akin to KDE that is user friendly and simple to use (cinnamon just sucks and you might as well just use KDE because it‚Äôs better in every way possible), focusing on making it stable and easy to customise. So as long as it‚Äôs its own unique desktop environment that has its own place amongst all the other DEs and it‚Äôs stable, modern and pretty then I‚Äôm happy. Because at the moment my only 2 real choices (for serious work) for a DE is either GNOME or KDE, I highly dislike the workflow of gnome so I‚Äôm left with just KDE, I really like it but would love to see something that has that beautiful out-of-the-box simplistic experience of GNOME but in a KDE format if that makes sense.

As another note, whatever you end up doing, make sure it‚Äôs consistent with its own style and doesn‚Äôt end up Frankensteining apps like XFCE and how some look straight outta gnome and behave differently and others don‚Äôt etc.
1. User friendliness - Ubuntu and it's flavours are pretty user friendly already, so I guess it's a given. I'd prefer something minimal and aesthetic. Also, +1 for it being a DE, usable on any other distro (or at least Ubuntus)
2. I'd prefer it to be an Ubuntu flavour, so Ubuntu
3. Distro made for a DE - as others have mentioned, the main reason someone would install Cutefish is the DE, not the distro itself, so there should be something about it that makes it worth installing other than the DE
4. BubbleFish as someone else have mentioned sounds cute, +1 for this name
I know C, how can I help? And like many others I suggest developing just DE as well. Or you could do something like KDE Neon or GnomeOS if you want but that's for later.
Name: Bonito

It means both cute, and is a fish.
I agree with many other commenters about making the desktop environment distro-independent. A gentoo or FreeBSD spin with the Cutefish desktop would be excellent. I always liked the UI but not the underlying system.
I've never ever done anything like that but I would love to help! I study architecture, maybe I could try helping with UI/UX. Also I code for a hobby, I know C and Python but I think my knowledge is way too fundamental to actually help with forking and maintaining a distro.
About name: I hate fish
I am interested in your project and want to join the team! I have dmed you, hmu if iI am welcome in the team :)
I was waiting out for wayland support in cutefishos tbh. That's the one thing I'd request you to foresee. Also, please make sure to not tie the base down to a specific distribution. This can be done as a conscious effort when choosing technologies and toolchains which will work on a wide variety of distros. Pantheon for example has this problem of being tied down to elementaryos and even though they've made efforts to amend this by working with several distros to bring support, it's still not perfect. It should be easier to do this because afaik cutefishos once came as arch packages as well which was wonderful and will surely be helpful in making sure it stays the same ahead. For the name I'd suggest keeping the 'cute' part in the name as it originally represented the Qt stack on which it was built.
Many here have already said that you should just make a DE instead of a distro, saying that we already have too many distros, which is fair.

However, I'm pretty sure most Linux users don't install their DE independently from their distro. I think most people, especially beginners, look for distros that come prepackaged with a DE of their choice. Which is why I think it would be good to make a distro.

Which is also why I think it should be Debian or Ubuntu based. Arch users can just install it themselves (with maybe the exception of Manjaro users, but that can be fixed with a community varient). It's the Ubuntu users who most of the time don't install their DEs.
1.	Have you considered an OpenSUSE / Fedora base?
2.	I haven‚Äôt used Cutefish, but I would recommend making a graphical package manager that supports Flatpak. That way people won‚Äôt break your distro with random PPAs.
Inclusive for children, seniors and disabilities while being a nice introduction to IT learning --> minimalistic and supportive.

// I'd be delighted to give a hand in all things ergonomics, graphics and a little bit of coding if needed! \\


To my pov, what's generally missing in the computer world is a nice, elegant, simple, efficient and supportive way to adress children and seniors -- but also people with disabilities -- as not only regular, but also as empowered users.

Interestingly enough, CutefishOS keeps coming to mind for this as well as EndlessOS. Cutefish, by its minimalistic approach to design and UX, has a great potential to accomplish this kind of things, I believe.

I'm talking as both a father and as a Disabled Person Contact working in an IT school: people need to get a better understanding of the technologies they use, and people deserve to get a technology designed with their needs in mind.
Kids are curious, but they need the information to be sorted and the learning environment to be supportive. I'd love mine to have a computer suited for them, bearing just what they need to understand, plus a little bit more to be able to dig deeper when they feel like it, plus doors to cross in order to go pretty far when they get confident enough. I'd love them to have a nice, kid-sized hardware to! The closest I found is RPi hardware which is pretty neat, but Rapbian isn't that good as far as I'm concerned.

They are 6 and 7 yo and they know how to call the terminal, type Netflix and get their cartoons! ^^ So I dived a little bit deeper and we've got some fun commands / terminal games in the shell now :)

But I still feel the computer is quite complex for them.

Interestingly enough, I found that senior needs are somewhat similar: simplicity, readability, logic, sorted information, etc.

Needs of people with disabilities are more complex, but I think the minimalistic and supportive approach can only be better than any other.

‚ô° my goal in close future is to set up 3 hybrid pcs running Linux in order to let my kids and their grand parents (and me of course) share visio / gaming / homework time.

For now I'll be tinkering, but it would be nice to have an OS with families (and splitted ones too) in mind. :)
While I personally feel it'd be best as a standalone DE for any distro, developing it to be based on Fedora might be a smart move.
> 1. What would you like to see from a distro like CutefishOS? Any recommendations, improvements? Don't be afraid to ask for some major changes.

Port everything to [Maui](https://mauikit.org) and Qt6 instead of using custom cutefish widgets. Maui is already kinda similar to cutefish, and working together with an already existing project can be beneficial in the long term, look at LXDE and Razor-Qt

> 2. CutefishOS was using both Ubuntu and Debian as it's own base. I've also thought of Arch but I'm worried about stability and user friendliness, but it's not gone yet as an idea. Which one do you think would suit you better out of these three?

Just create a DE and not a distro, as other people said. Package everything in Debian unstable, AUR and Gentoo. Things will eventually trickle down to debian testing, stable, ubuntu and others downstream. Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary", so better not package it there, maybe try RPM Fusion or COPR.

> 3. Any particular things you don't like about CutefishOS? (Literally anything)

Too much fragmentation in the DE space. Everyone is trying to create new file managers, terminals, shells and a common set of apps. Working together with Maui will reduce this fragmentation and also help with Wayland compatibility and future maintenance.

> 4. Since this isn't really CutefishOS but rather a fork of it, I'd like to hear some name suggestions. Preferably not mentioning any other distro than CutefishOS.

Maui 3.0
We have already done this -- we've built a spin of OpenMandriva using the CuteFish desktop, and while we consider it a development snapshot, it's ready to use...  
[https://abf.openmandriva.org/platforms/cooker/products/43/product\_build\_lists/1162](https://abf.openmandriva.org/platforms/cooker/products/43/product_build_lists/1162)  
We could certainly use more people working on it.

OpenMandriva (including CuteFish spin) developers can be found at  
[https://matrix.to/#/#oma:matrix.org](https://matrix.to/#/#oma:matrix.org)
Just in case this website is still up : https://cutefish-ubuntu.github.io/download/
BonitOS
In my honest opinion I liked what cutefish was attempting to do.

I think if we can get a Mac os like theme on a distro with the thinking of stability in mind would help people cling to linux.

Sure it would be nice to use the de on other distros but I think what's seriously important is a polished environment with a reliable base is what a lot of people want.

Take this from a grain of salt. But I was using Linux as of late and started using OSX Currently and I just love how Macos does a lot of things, and the stability of macOS is great.

When it comes to a project like this I do think it requires the devs to understand what they are looking to do with it.

Are you aiming to make GNU/Linux usable to the average person?

If so how are you going to achieve this.

Cutefish was trying to achieve this by limiting stuff.

That or it was just not done yet.

Either way, I like the idea of a MacOS like distro and do think out of the many distros out there it would find a spot in distro ecosystem.

Even when it comes to all these random distros that achieve a specific goal.

With how linux is going I do think they will lose there purpose for being a distro but a Macos like distro shouldn't get hit by that.

Well this is my thoughts on it atleast.
Can‚Äôt wait to see a repo get put up. I‚Äôll definitely be watching this with great interest. I just wish I could be more help. But I‚Äôm more of a web developer than a desktop developer.
Bro you should name it Debintosh.  Also keep it Debian as it's more user friendly for beginners.
Make it into a fedora spin instead of it's own distro, would probably make it a lot easier to maintain and, for some people new to linux the newer fedora kernel might just save them some headache compared to the older debian kernel.

Also I don't know if there is a software store, but if there is ignore this and if there isn't then i suggest you make one.
I liked the look and feel of the os, but it had some visual glitches. It would be great if it stayed visually the same and more polished.
1. Browser in the dock

I  think I might contribute when I will know something else than python.
Although I am no Linux ‚Äúbackend‚Äù nor ‚Äúfrontend‚Äù expert in terms of actually contributing, I would still have some recommendations.

1. Keep it simple. Integrate apps in the theme well, as long as you can.
2. I think you should use Arch. Arch may not be that stable OS. However, installing different applications is a LOT easier. You don‚Äôt have to worry about PPAs like on Ubuntu. You just enable multilib, install an aur manager like paru or yay, which you can do at installation time so the user doesn‚Äôt have to worry about it. Plus, you could even write a fancy App store-like application (that actually works and doesn‚Äôt crash like KDE‚Äôs store). This would probbably already solve The Biggest Problem that a newbie would have with Linux: the fear of terminal. Most of us (or at least me) use the terminal 99% of the time for updating and downloading applications (on Arch.)
3. Make it work well without the OS, just the standalone DE. I tried Cutefish DE on arch, and it was quite buggy (this might‚Äôve been because it was still unstable) while on the CutefishOS, it seemed to work well.

Thank you for making an effort in revamping this project and I hope it succeeds :)
Name suggestion: CuddleFish
Dont make a new distro - maybe just have an iso with cutefish's DE installed

Keep it debian based. With maybe a version using debian sid.
I think you should try to find some nieche that current distros don't cover, or don't cover well. I've always thought Cutefish seemed cool but had to particular reason to try it out, because it didn't stand out really.

I don't know if this is feasible or even a good idea, but one option could be to base it on Void Linux and try to make a Void-based yet user friendly distro. That would, at the very least, be interesting enough that I personally would try it out.
Remove the Chinese spyware while you're at it
Do you need a beta tester for your DE/Distro fork?
I know some things about cpp and i want to learn qt so i am willing to help maybe for a small app first in order to learn more about qt and cpp
I have recently gotten into Linux. And I've been learning a lot (albeit, I still have a long way to go in understanding distros, desktop environments, apps, package managers, etc.).   


Anyway, I am wholeheartedly with you on this. I will donate to support this project. I would have supported CutefishOS but didn't know they had money problems, plus I very recently started learning about and getting into Linux.   


Furthermore, I agree that a desktop environment would be better that can be installed on any distro, but I see that you have challenges with that.  I can settle for a full-fledged distro if that's what you're making/forking.
I can contribute, but I will need some help and directions since my CPP is basic level and no experience with Qt/qml etc. 

Also can we explore to use Kwinft instead of Kwin for this project?
Please can you fix fractional scaling so that the scaling can be updated on-the-fly, without necessitating a logout/restart. This is a limitation of KDE on X rather than Qt.
I think every desktop needs to have a distro that is designed with it in mind even if is just for testing and trying it out. It would be nice to be able to use it on other distros, but I don't agree with a comments about leaving the distro behind. 

In regards to the base distro I would stick with ubuntu or debian, just one tho.

I haven't done much C++ and never used QT but I know my fair bit of C and I could help with Spanish and Portuguese translations as well as documentation if you guys need that. 

Here is my [github](https://github.com/migueldeoleiros)
Before you do any work, write tests. Too many projects lack proper testing.
We have so many debian and arch derivatives. I would love to see some more diversity. Why not base it on openSUSE, fedora or Void?
So, here's my full comment:

Also one question, I don't know if this is the offical CutefishOS, but it seems that this: [https://cutefish-ubuntu.github.io/](https://cutefish-ubuntu.github.io/) is Cutefish? It provides the orginal [Beta 0.8](https://cutefish-ubuntu.github.io/download/) download, but also with the Ubuntu download? I'm pretty sure this is the originla though: [https://github.com/cutefishos](https://github.com/cutefishos).

1. Cutefish was - and still is - a great distro, with many features and apps preinstalled, I really liked, but seeing that it's now dead, you could add a few more features while "reviving" the distro, like an updated theme for light and dark, and different color schemes like how Zorin OS does it.
2. I answered that here: [https://www.reddit.com/r/linux/comments/vwd0m8/comment/ifqhyu5/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/linux/comments/vwd0m8/comment/ifqhyu5/?utm_source=share&utm_medium=web2x&context=3)Quick summary, I'd stick with Ubuntu, for software availability and stability, though if you went the Debian route, you should use Testing or Sid as many other "Debian Spin-offs" did.
3. The only thing I really didn't like at the time of [Beta 0.8](https://cutefish-ubuntu.github.io/download/) was the some what minimal but off theme that it offered, and also something to add might be more support for Mac's as it would provide a very good alternative for Mac OS, and maybe a Chromebook keyboard layout too?
4. Name? Might be weird but the first thing that came to mind was if you're basing it off of Ubuntu, maybe Cubuntu? As in C(utefish)uBuntu, but that name would probably be a last resort, so maybe something interesting, like, Mango Linux? I really have no idea.

Also, wanted to address one more thing, if you want to make a simple website(clean and no bloaty stuff), you can hit me up on my Discord, as I can make HTML and CSS and very little JavaScript  needed(for performance and incase it's used in embed browsers): one of my HTML website templates can be seen here: [https://fyxk0v.mimo.run/index.html](https://fyxk0v.mimo.run/index.html), and my Discord is here: RootPSWD#2747, also I'm doing this for free because it's a side hobby, and because I have high hopes for this project, also if you can host it on either GitHub or GitLab that would be great.
A novice really here. What do you mean when you say you‚Äôre going to fork CutefishOS?
maybe ca it something with shark in the name
Definitely keep us updated. I've been poking around the open source community for a minute, and linux OS stuff seems like where I wanna start. I'm probably not useful enough to formally be part of the team, but I might be able to put enough time in to follow along with the work in progress, poke around and maybe throw a pull request (to be taken as a suggestion and no more) here and there (I think that's how github works?)
The one thing I feel compelled by is that this is the only Linux DE I've seen advertise a global menu. That's interesting if it's well integrated with the rest of the distribution and apps.
Nice to see a fellow crafty cutefish enthusiast! I've been trying to make the cutefish dock work properly with GNOME forever now. But anyway, my biggest issue with mainline cutefish is not being able to use a custom wallpaper. Limiting users to 6 predetermined wallpapers is just obnoxious.
Cutefish OS used to be laggy in my case. Screen tearing also happened often earlier. Other than that, it is that distro that provided the exotic feeling of having a Mac.
If you want an extra pair of hands, please send a pointer to the repo.
I know it's not the scope of the project you were wanting, but I'm going to echo others on here and vote you create a DE, rather than trying to maintain a whole distro.
The only thing I know about cutefishOS is the desktop environment. That was kinda it's whole "selling" point. I know in other comments you said you don't know much about the qt framework so instead of reviving a distro by making another distro why not push for cutefish DE for other distros. Like an Ubuntu cutefish flavor, Manjaro cutefish edition, or fedora cutefish spin. I think that doing this would get way more people to use it and possibly get a community of people that really like it.
Please call it octopussyOS
- Improve Vietnamese translation, because its broken
- Make it Ubuntu based, because the software is not too old (like Debian) and it have a big community.
- Read the first one
- What about `Coolfish`/`CoolfishOS`?
- Gitlab is better than GitHub for big projects

Also I have some experience with Debian's `live-build` (making isos), Arch Linux's `archiso` (again, isos), `deb` packages, Arch (`PKGBUILD`) packages (`.tar.zst` ones), and `rpm` packages, i can help you with the distro & packaging part of Cutefish.
I think a good title would be "Sushi" or "SquishFish", as they both maintain a similar spirit to the original name in my personal opinion.
I don't have much experience coding in C or C++ since I mainly code in Python at work, but I would love to help out on this project. :)
Please consider using Fedora as the base
Call the fork Cuttlefish
I personally would like to see a still well designed, but lighter window manager, maybe different versions for both Arch and Debian! Then keep in mind details like that the darkmode changes all apps. And idk if it is already working, but support for high refreshrate displays. I know it is a quite common struggle on kde.
Best to keep it user friendly. I tried to download and flash a CuteFish iso to my usb using BelenaEtcher, but it warned me that my MacBook might not recognize it as a bootable USB. 

I have a SanDisk Gen 3.1 USB and I use Win10 Bootcamp dual boot partition on my macbook.
Maybe consider sticking to ubuntu for its user friendlines/ comunity support
fish fish os  **final answer**
I would recommend using debian cause Ubuntu is a bloated mess and debian testing branch if possible
Call it NiceSharkOS
Maybe just try to make a Cutefish Fedora spin?
Could we please get the URL for the forked repository? I'm personally really excited to see Cutefish be brought back to life
1. Cutefish should be more customizable.
2. I would choose Ubuntu for a base but I think Fedora is a great option.
3. Saturn Linux or something like that in my opinion sounds pretty cool for a name.
I have no recollection of this OS, and I have yet to look it up, but if it is not already implemented, maybe an ISO for nvidea and one for everything else. Makes it easier for the end user. :)
Why though? What does CuteFish or your fork for that matter have that differentiates it from Ubuntu and the other more popular Ubuntu-based distros out there?

Sometimes, it's just better to let a project die a natural death.
>What would you like to see from a distro like CutefishOS? Any recommendations, improvements? Don't be afraid to ask for some major changes.

An absolute focus on ease of use and looks, like even your grandma could use it and a UI designer could confirm that it's beautiful as hell.

Can take notes from ZorinOS. This includes: Proprietary drivers installed by default, Latest WINE pre-configured, latest gaming apps in the repo, software store with snap, flatpak, AppImage and other repos pre-installed, Layouts for people coming from different systems like Mac and Windows.

>2. CutefishOS was using both Ubuntu and Debian as it's own base. I've also thought of Arch but I'm worried about stability and user friendliness, but it's not gone yet as an idea. Which one do you think would suit you better out of these three?

Definitely Debian/Ubuntu because it's the most popular distro, hence the ease-of-use and support. Many are asking it to be a DE but there's a real flaw in that argument. DEs are for Linux users, not most people. Most people can update their OS, not change their DE.

>3. Any particular things you don't like about CutefishOS? (Literally anything).

More focus on MacOS, instead of Windows users. Windows users are the majority, it's better to sell them an OS that looks like Mac and works like Windows than to sell a mac user a Mac like DE that works like Windows.

## I am ready to contribute whatever I can if you can get this project going. 

## Also, please change the name, the chinese to english translations were never perfect and sound weird. BlossomOS sounds like a cool name or maybe VioletOS? AuroraOS? BlissOS? IrisOS?
fedora for a base distro would be great
1 (and 3): I know Cutefish was still pre-release, but it always felt a bit empty as in looks, and a bit... "Chinese". Some spacings, icon themes etc all made it seem like a cheap knockoff of macOS instead of a more established design.

2: Make it Ubuntu, and add the DE as a PPA. This way, people can port the DE on other platforms too. And since Cutefish's whole idea was ease of use and user friendliness, Arch may not be the best imho.

3: I push the name CuteBeeOS. I think Cute is a way to say Qt anyways so no need to change it and the second word can be any animal with a short name and easy to draw siluette. If not, I push marbleOS. Fits the flat smooth theme it has and it can be called "As smooth as a marble" or something :P
I'd say:

\- I like all my distros Arch-Based. Rolling release does have good points.

\- Maybe make a \*good\* touchpad gesture-based navigation system.

\- Basically make something that is up-to-date, looks good (it already does look very good) and has a smooth desktop experience. Just focus on the experience if you really want to differentiate Cutefish from the crowd - the little details of how the UI responds to input, that is everything.

\- The above point doesn't mean make a "newb-friendly" distro. I'd say focus more on providing good, useful features as cleanly as possible. For the user-friendliness part, you can take the community's help (I can help with the docs too) to write good documentation (even illustrated in some parts) to help new users. I think it is a trap for distros to get stuck in making themselves friendly to new users (an example of this Nautilus hiding the ability to directly make an empty libreoffice document). If you write good docs and provide links to them in relevant places, it will be more than enough.

\- Finally, be clean, and minimal, and allow a kind of "meta configuration" (toml files are highly recommended). What I mean by this, is that it should have a system that is all-inclusive - toggles for the basic users, and config files for the power users (ik ik, config files are always there, but make at least the desktop config files more easily editable and their directory more structured for the power users to be able to tweak almost everything about the desktop). Provide window manager level configuration for those who can do it, and decent buttons and menus for those who don't want to bother with all that.

\---

I think there's a lack of a desktop that is lightweight, looks good, integrates well (the global menus are nice) and is infinitely customizable.

I think it would be a giant boon to the Linux community if we can have that. I am all for this project.

\---

Some extra points:

\- I agree with the prospect of just making a good desktop environment. You can definitely do that, but another (imo, great option) is that you can just ship an ISO with cutefish installed on mainline arch. No logos or whatever, just something that will help in adoption.

\- I want to contribute. I can write Python (C++ too, but will need some time to know my way around Qt), and... I can \*write\* (as I said earlier, I'd be interested in making the docs too. Want to get into technical writing so pls consider me. I don't have experience but hey isn't this the cool thing about FOSS? Anyone can contribute!)
What a waste of time and effort that was. You do much more good for everyone just contributing to Plasma.
Like many said, I wouldn't make a whole new distro, I'd take a the DE and work on it and make it distro-agnostic. I don't want to see another DE that I'd love to use, but it only works on a base that doesn't work for me. Pantheon is an example for me. I love that DE so much, but I just can't work with its old base (I know it's available on Arch, fedora and Geko, but it's broken on all of them), and to be brutally honest with no hate, its devs. I'd love a DE that gives me the elegance and smoothness of gnome mixed with the beauty and customizeability of plasma. 
As for helping you, not gonna lie and say that I have time and will help with maintenance and code, but I do have a spare laptop that I can use to help you test and report to github/gitlab, I'm pretty good at catching bugs and reporting them. 
Also, if possible, please use gitlab. FOSS needs to move away from github.

Edit: I think, I just think, that if you made a very good DE and it got good adoption by some major distros, you may also get help from those distros, too.
Can you reduce it to just a KDE theme? You mostly be cutting out code until there is a small amount you can maintain in the upstream.
What about calling it CuddlefishOS?
Imo don‚Äôt. It‚Äôs not a good base imo. A better option imho is to offer to help Josh Strobl w/ Budgie 11. That‚Äôs a much more serious undertaking that‚Äôll be every bit as good, if not better.
They just released a new iso today https://sourceforge.net/p/cutefish-ubuntu/activity/
it'd be cool to see a distro which can be based from any other distro, e.g you can choose if it's arch-based or debian-based in the installer for example
UglyfishOS dude, just do it
Name it CuddleFish
Are Ubuntu based desktops still good? I understand the hate behind Ubuntu but idk if that's just Canonical or Ubuntu itself.
I have never used cutefish before, but what I would want from any modern distro is to have modern packages, flatpak with flathub and I hope if you build it from the ground up it won't be Ubuntu based and won't have snaps
CuddlefishOS
And I'd love to have some arch mixed in but that would only be for pacman which can be installed manually anyways so I don't really see a point in arch in the fork
I'd personally stick with basing it off Ubuntu, just for the reason of Software availability, and also stability, the default repos have a lot of apps ready to install, even a working version of Wine 6, but if you went for Debian, I would use Testing or Sid for the latest updates possible.
I wonder how many people spend time  to re-invent the wheel ?  Are you changing Kernel ?  Are you inventing a new functionality, if yes, please go ahead.  

For common users, already enough choices available as regards to Linux distributions. 

I am of the opinion that, if the enough skill available, better to create some thing unique to all the Linux distribution.
beautiness won't save it.
I would encourage you to think realistically about user adoption and what's most likely to happen if you build the OS.
My complaint is that I don‚Äôt like the overly rounded design, I prefer softer/‚Äúmore straight ‚Äù borders (like the windows borders for example).

As others also mentioned, make an separate desktop environment not an full fledged distribution.
my opinion probably does not count much, but here I go:

1. use Ubuntu as a base, use their repos and do as least packaging on your own as possible
2. focus on the DE part, spent as little time as possible on the "distro" part
3. offer Flatpaks and Appimagelauncher out of the box, so ppl do not need to fiddle with packages
4. call it Cutebuntu
FIshOS maybe? Or MacFishOS XD
I would say stick with Ubuntu as a base, if nothing else other than the 3 mentioned. Preferably I'd like to see Fedora as a base instead, but I understand if that's not an option. 

Stick to standards, but don't use Nautilus as a file manager (or at least fix the issue with large data transfers not providing proper feedback to the user, for example, a data transfer of 32GB with thousands of pictures makes Nautilus freak out, says the job's done, but really it's still working on it in the background. While say PCmanFM has a popup windows and easily handles the job. I run into this daily as I work on peoples PC's, it's annoying and problematic).

 A better name is a must. It has to be something with a mascot, something edgy and cool (as corny as it sounds). I'm terrible at names, but I understand the necessity and requirements for success. Cutefish is definitely not appealing. Whatever it is, it has to draw attention and raise curiosity while at the same time representing what your distro is all about.

To me, in order to be competitive, the distro /DE MUST look good, clean and simple, minimalistic, transparency with blur. GNOME is the closest to what I've found that meets these requirements. Although power users that require more options should still have the ability, I think these options should be hidden under collapsible "Advanced" options.

That's about all I've got so far, best of luck to you and your team!
I would like changes to be made upstream (KDE). Add settings in KDE to make it look like CuteFish. 

One thing I don't like about CuteFish apps is that they seem to be just skins, just like Deepin, they don't respect visual changes.

A major change uh...How about kwin protocol/api/framework  to do this \[[1](https://jaanus.com/images/electron_default.png)\] -> \[[2](https://jaanus.com/images/electron_trafficlights_toolbar.png)\]
I think it should be named CuteBoi Linux
>4. Since this isn't really CutefishOS but rather a fork of it, I'd like to hear some name suggestions. Preferably not mentioning any other distro than CutefishOS.

hackOS
I can only do sound design stuff, but I really hope it works out for you guys. The name should be something shorter definitly, this could help for popularity as well.

A new good looking AND functioning OS would be so refreshing to see these days, I really hope it works out.
For a name maybe some other sea related creature maybe cutesharkOS or smth in style, also what does one need to work in such a project?
DE is preferable IMO but if you go the distro way then please do not base on Arch :) 

Debian or Ubuntu are just fine and fit the stable behaviour of Cutefish. (Personally I prefer Debian)
 i will give it a thumbs up  üëçüëçüëçüëçüëçüëç
How about MFac-Me, for My Free-and-open-source mAC for ME ?
Terminal themes would be nice. I don't understand why green is the default font color.
Do ist with amazin debian please, sir
First of all, a huge üëçüèª for taking this project!
Beta 0.8 was pretty stable in my computer, and the only important things I saw lacking are some important multi displays settings (such as placing the monitors in the right order).
Btw, if I want to contribute (I don‚Äôt have much time, but still, maybe I‚Äôll be able to help a little bit) what are the subjects I need to learn? I‚Äôm pretty familiar with c and cpp, but not to the level of beholding apps interfacing with the OS‚Ä¶
i'm not a dev, nor a big linux user, but please, remove the 1:1 icons from mac-os

i don't think they're any good, it have to be a little bit similar, but it's still linux.

and another thing, a different wallpaper for the dark mode
4. Since its a fork, maybe make the name kind of related to cutefish? Cats eat fish, maybe base off of that? Like idk, CatkatOS/CutecatOS? Also we could bundle the distro with some cute cat wallpapers\~\~
When I tried out CutefishOS, I found a bunch of bugs including, but not limited to UI, stability, etc.
So that would be your #1 priority. Next, the UI design honestly sucks, so you would want to redesign some of it(ex.settings in full screen really makes me throw up) and take a look at ui design flaws... there is a BUNCH... but you gotta do this whilist also maintaining that "cutefish feeling" which will make it unique. Besides that there is nothing more.. it is just Bugs, Ui, Stability, Uniqueness.. these will be your main concernes when developing it, so good luck with that. I'm really hoping that it will turn out great.
(BTW it would be really cool if you could make a discord for us cutefish supporters)
Can we get it running on debian unstable (Sid) or arch as a alternative bleeding edge distro build along with the LTS stable that it currently uses.
If you feel like managing that project, keep us updated, as  I was considering packaging Cutefish as a nixpkg (I'm trying NixOS lately). My Qt is a bit rusted (from my university days) and I'm a noob when it comes to rendering, window management, and other low-level stuff. But if I can be of any assistance (translation, minor bug fixes, GUI improvements), I'd love to participate. 

as for the name, I guess you should do some puns on fish. I don't have something fishy yet, but someone may.
I think Cuddlefish OS would be a good name change. It keeps the same cutsey fish theme, as well as being distinct enough to imply that it is a separate, but still related, project.

Other ideas: Crayfish OS, PistolShrimp OS, Cape OS
one thing that i think is important is that it works out of the box with Nvidia graphics cards. the only Linux distro i use is pop os because they give you an iso with the nvidia drivers already working.
Call it Q OS. Very simple, don't think it's been used yet either.
Hey, kinda late, but I have quite a lot of experience with archlinux, may be able to help packaging software/maintaining a repo if you want to go the arch-based route. I have some basic Qt experience, and know C++ quite well, I think I'd be able to help :) Please contact me if you ever go through with this
is the fork created already? i have a few bugs i have found in the ui and a few features i'd like to recomend.
using  manjaro cutefish it sad too see this 1 go   
 i do hope  you  will fork or continue
# Changes:

I think the philosophy of the DE is nice and sort of provides an alternative to the similar ElementaryOS.

High Level Navigation:

Something I've seen Elementary do that I haven't seen in Cutefish is mimic the expos√© view from macOS or activities view from GNOME - it's nice for multitasking imo and if cutefish doesn't already have it (haven't used it much, so sorry if it does already), having that in the new project would be great.  Something most other DEs and macOS have again that I haven't seen demoed is multi-desktop support.  It seems pretty essential in this age for multitasking efficiency and it would be nice to see a DE implement it well - including desktop preview thumbnails with the open windows in a desktop picker view and the ability to switch between desktops with either hotkeys or with touchpad gestures (3 finger swipe in macOS is nice, so that would be cool here too).  Keeping the dock static (if visible in the first place) at the bottom of the screen seems like it would be a better way to do handle it during desktop switching too than the one-per desktop way that feels kinda clunky to me in Elementary.  If users want dock icons to only show apps from their active desktop in addition to the pinned ones, just showing the dock animation for closing the apps from the last desktop and opening the apps from the becoming-active desktop during the desktop switching animation would be smooth the smooth way to handle this (again, imo).

Graphical Package Manager (GPM):

Maybe consider implementing the system-update feature into the Application Store/GPM, instead of a separate package, if that feature itself sticks around.  Personal preference:  I dislike having a separate GPM for my DE components and for my other system packages.  Given the distro-platform agnostic nature of a DE (see later notes on distro picking), it may be difficult to keep the GPM feature alive, but detecting the presence of and supporting pacman and apt as hooked-in backends for the GPM might be sufficient at a minimum (although flatpak - and AUR on Arch - optional add-ins would be welcome I think).  Having a section/tab dedicated to each backend package manager, plus one dedicated to DE components (although having those DE components still appear in the broader package manager searches would also be good I believe) and one dedicated to system updates when running on baselined/versioned distros would be my recommendation just given my preferences.  Maybe some users would say that having both in one (plus an OS updater in there too) would be breaking the Unix philosophy of simplicity, but I think this helps keep the UX simple.  If you view the "single purpose" from the Unix philosophy - not sure the point of this DE/Distro is really to embody the Unix philosophy anyways - for the GPM as "a program to manage system software" I don't think it breaks the idea too much anyways.

File Manager:

If the file manager doesn't already have it (again sorry for my lack of actual use-time here), mac users, in my experience, really love their commander file management view, so adding that would probably be a good idea there.  Another inspiration from Elementary:  having a quick shortcut reference would be good (they use the meta key to bring it up, but any trigger that seems intuitive would be good).  

Things done well not to change since they are so core to this DE already:

Global menus, the general design-language/appearance (leaning toward a very simple UX and a tastefully fancy UI), and Kwin WM (one less component to maintain) (also second the upsteam contribution suggestion made by others on Kwin)

A couple of other more obscure/advanced things to think about supporting on a more built-in-at-the DE level that might help with a little wider adoption:

Consider baking in support for domain-joined users. This is something that MacOS technically supports, and so does linux overall, but being able to join to a domain (Windows, OpenLDAP, freeIPA, IdM, etc.) right from the UI would take you a step beyond most distros and make the DE more inviting to maybe be adopted by enterprise.

Let users change their icon themes from the installed ones in the appearance settings.

Requires significant effort, but:  add more native apps.  Users may expect full-ish macOS feature parity - not necessarily the full-modern macOS, but at least with older versions. So, having native, DE-themed apps for calendar, todos, contacts, webcam, clock/alarms/timers, and maybe a messenger/phone integrated app (a la KDE Connect that would give users something kinda similar to iMessage) would all give users a more inviting experience without requiring them to download the baggage of other DE's when wanting to use their native apps, where yours has none.  Also, if implemented, please don't give the apps/packages for them weird names like Kalendar - its cute, but also weird and not that professional.

Easier-said-than-done and sort-of a "duh" thing, but keep power efficiency of code in mind.  I probably don't need to remind anyone of this, but there's a decent chance that users will want to run this software on their macbooks and while 1:1 parity with macOS in this regard might not be necessary, user's won't be very invited by needing to plug in their laptops every hour and a half.  Side note, having this available to use as a DE on Asahi just kinda makes sense imo - would be really cool to see.

Make sure you test with Wayland and X11 - Wayland is probably the future and supporting it (unlike other distros like XFCE - and don't get me wrong, I really like XFCE) will help the DE to stay relevant and usable going forward.

# Distro:

\+1 for making this a DE instead of a full-blown distro.  The modularity would be nice, although providing some image(s) with it pre-installed on another/some other distro(s) would be nice for new users, too. Maybe with a convincingly complete and stable DE, that responsibility for some of the images could be offloaded to other distro-maintainers like ArcoLinux, Manjaro, or any of the other various distros that ship with multiple DE options available OOTB.

# Dislikes:

I'll leave this brief and let other users talk more about it since I don't have that much experience using CutefishOS.  I think all of my dislikes are more-or-less inherent in my changes section.

# Name:

As far as the name goes, the fact that CutefishOS didn't ship with the fish shell always confused me, so my suggestion there is to either ship with fish if you decide to build a whole distro around this project, or drop the fish part of the name.

My suggestions (which will more-or-less wear the preference for moving to a DE and therefore not being able to dictate to users that they'll have fish preinstalled and therefore suggesting to drop the fish part of the name on their proverbial sleeves, but for most if DE is in the name, you could probably replace the DE with OS and they'd work just as well) are as follows:

(side note, I'm not reading other people's name suggestions prior to writing these in an intentional effort to not borrow their inspiration - any overlap in suggestions is coincidental and may indicate a good name nomination ü§∑)

Bad ones first - maybe technically nice for linux/geeky users, but I think not as good for the normie users:

Qt>O DE (pronounced cutefish still, but where the >O is supposed to look like a fish, without literally saying fish.  This one may not separate its name enough from CutefishOS)

QU73 DE (idk if macOS users get 1337, so this might not be a good candidate)

Better ones:

CuteDE (I think this is already a semi-official name for the old project, but could be a real-official name for the new one)

WhatAQt / WhatACute DE

PrettyQt / PrettyCute DE

QD (pronounced cutie and basically standing for Qt Desktop-environment - maybe kinda stepping on the toes of Lxqt with that one, but I don't think too badly)
I just wanna voice my interest in it as both a DE and a standalone distro, for both myself and my grandparents who are currently struggling with Gnome

I will try and contribute, but I don't think I'll be much help (as I am not very versed in this area of programming), but I will follow this with great interest
Would be nice to have a community group on telegram or discord to keep the community updated where they can discuss features and experiences
How does this compare to Deepin DE ?
Cutefish was a fork of a fork of a fork that seems to be made by the same person.

Are you sure your name is not Reven/Reion Wong?

They have a long history of forking this codebase, abandoning them and forking them again under a different name.
I wish you success in this endeavor üëç  
Following are the answers to your questions:  
1. CutefishOS got the attention because of its elegant UI (which is similar to macOS) and the simplicity principle that it adopted. My suggestions for the direction of development would be:  
	\- Focus on usability as MacOS has always targeted  
	\- You may not keep the customizability options like theming open given that you are having awesome defaults (macOS doesn't provide many customizations but is still loved). Having some restrictions helps the developers keep things simple and elegant on the code part as well.  
		\- I really appreciate the stability and "getting-things-done" principle that Gnome has adopted. Even though there has been a lot of criticism from the critics regarding their move to libadwaita, I believe it is the right thing for the development and stability that Gnome offers. Even though KDE offers practically infinite customizations, its stability has always been a cause of concern, and it's always better to:  
			"Do few things perfectly than doing everything suboptimally"  
	\- The best way to get ideas for features would be to see the good things that are offered by MacOS, KDE, Gnome, and Windows.  
	\- Lastly, one should look at Apple for quality of GUI & UX, Gnome/Fedora/MacOS for stability, KDE for features, and Apple in general for efficient/fast/smooth and easy to use software  
2. Regarding the OS base, I think Arch and Fedora both are great and have their own advantages.  
	\- I'd suggest going with Fedora. It has got quite stable over the years and is really user-friendly. And along with that, it comes with the perks of having the latest software available. Even though it is not a rolling release, it releases a major update every 6 months. So adapting would be a continuous process rather than a big wall to climb every 2 years (like ubuntu)  
	\- On the other hand, while Fedora has a scheduled release cycle, Arch's rolling-release system can be advantageous.  
3. Haven't used it enough to comment on this  
4. Anything would do given that the OS works :p
Fork name suggestion:  
CutefishOS > **CuddlefishOS**
dualboot option like ubuntu.
I really like to help if I can but I don't have much knowledge about distro maintaing in particular.I know C quite good.Also know C++ but never put much effort cause I hate OOP.i know JS but I don't think it will be any help in a linux distro.if I can help it will be my pleasure to contribute to a distro where beginners can switch.
Please include templates / Guidelines if someone wants to develop apps for cutefishos, it would make the whole system consistent and future proof :)
2) You can do something like MX Linux. It is based on plain Debian but it is still user friendly.

4) CutePhoenix because it is a revival ? Or something else that evoke the idea of revival.
I would also really like to help ..  i am a beginner programmer... if you are okay.. i would like to help you.. it would be a great learning experience for me too! Thank youüòÅ
> 2. ... Which one do you think would suit you better out of these three?

I would say **Ubuntu**. CutefishOS, given its polish and amazing looks is bound to appeal and attract more beginners than experienced users. And I think that being able to use Ubuntu resources and instructions would make it far easier for them.

> 3. Any particular things you don't like about CutefishOS? (Literally anything).

Yes, the fact that the close maximize and minimize buttons don't look like macOS's buttons. That is, on the left and colored. Such a missed opportunity if you ask me... üòî I would love to see that, or at least have the option to turn that on from system settings.

And also the fact that titlebar buttons in apps such as Firefox don't fit in next to the tabs like on Gnome, but are rather displayed on top of them as another line.
I think arch is just fine. If we look at manjare, for example, it's not instable and it's also user friendly. BTW, did you fork it yet?
Please call it CharmShark
Touchpad gestures would be amazing. Thanks for your initiative
PufferOS has a ring to it.
I don't have any experience on C++, but I can offer some help I guess
I don't know how much of a valid suggestion this is, but, I'd really like the global menu bar to be better used  


I like that you have... y'know... menus, but I'd also really like if the closing, minimizing and resizing buttons were connected to it once made full-size  


this is a problem I have with gnome which is that the bar on top really just feels like a waste of space no matter how much information you put into it, because it'll always be over the application headerbar instead of connecting to it  


I know this isn't too easy to do, since it would also depend on the software developers, but cutefish already has its own file manager and stuff so at least doing that to the softwares that are meant for cutefish would be a great start, and if nice standards are set then it'd make it viable for developers to maybe include it themselves? I don't know, I'm talking too much about something I know too little about, sorry
–°utefish os 1.0

Let it be based on 2 Ubuntu and Debian.

Switch to Qt 6

Make it in settings so you can customize it to look like Windows 11

Want more convenience

More settings

Good store

And wallpaper of nature
This post was referred to in this article: https://www.debugpoint.com/cutefish-os-development-halts/
\- Grab a distro that will be responsible for upstream updates/security. I vote Debian. Fedora Core comes to mind as well. If you're going to get a community behind you, then implement a back porting system to keep packages fresh.  
\- I also vote to build as an OS. Building just a DE will be a maintenance/support nightmare given the girth of distributions out there. Any obscure package or library change could break the DE for said distribution.  
\- I'm on the fence about Ubuntu base. My confidence in Canonical these days are wavering. Maybe a stripped down Ubuntu would suffice?  
My 2 cents anyways.
I really like the idea of it being arch based.
This is a great distro IMO and it's a shame that it was retired. It's still not possible to make it an official DE for all the major distros, or somehow revive this OS or scrap it to make a new one.
No 1 change should be the name of the distribution, since "cutefish os" isnt the best name imo...
I think ***CoolFishOS*** would be an easy name change
Responsive touchpad gestures is all i want , except for fedora they don't seem to work anywhere properly
CachyOs already gives the option for cutefish. I'd recommend using that to begin with. Having it in the aur repo's opens it up directly to other arch users as well.
Is there a github repository for this yet?
Making a new distro and not basing it off of arch seems to me megalulz but i guess Im an arch fanboi. Ran arch for work for a couple of years, a few months ago i decided i didnt run anything that requires Windows so i switched my daily driver at home, noob as i was back then i thought Ubuntu would be the best supported easily managed desktop distro‚Ä¶ boy was i wrong! Struggled with Ubuntu as my daily driver for almost 3months before going arch and now i dont actually have any issues. 

Also you shouldnt forget that ARCH LINUX IS THE FUTURE OF GAMING and gaming will be what pushes the desktop experience forward as a huge majority of working users just dont give a f and companies are already tied up in ms licensing
May I interest you in reviving a little OS known as Minix instead?
could you make a telegram chat for this?
Take a look at Fedora/RHEL honestly. The package manager is a bit iffy (I still don't understand the criteria that triggers a metadata refresh which is often infuriating. I literally sat there and watched it download 100mb of metadata the other day just to install nmap) still but they have a pretty solid suite of tools for creating your own images etc. And if you bundle RPMFusion it'll cover a lot of the rough edges Fedora can't cover.

OpenSUSE might be another good base to consider. It's rpm based but zypper is a much nicer experience than dnf/yum in my experience. Also, there's YaST.

Edit: a wild link appeared! https://fedoraproject.org/wiki/Remix#How_do_I_remix_Fedora?
I'd almost agree with you, but there are two limitations of doing so:

- 1. I know much, much better about how a Linux distro works, rather than the Qt framework (Used by cutefish). That's why I need a community to contribute on the desktop. That being said, I do know C++ and can still fix lots of bugs. I'm just more interested into creating a well tied desktop OS.

- 2. CutefishOS was already popular. Alot. A distro to take it's place would feel much better than just telling people to install the DE in their own distro. I've tried this myself, actually, so I'm speaking with experience.
I‚Äôm not sure why people think this is true but it is way harder to try to make a desktop environment that runs on any distro than making a desktop that runs on a single derivative distro. There are so many different versions of libraries with major API breaks etc across distros that you have to carry a lot of conditional compiles or just not have certain features to be fully cross distro

I love our downstreams like Fedora, Nix, Arch, etc but it takes a lot of effort on their part and our part to make Pantheon work for them and even after a couple years of working together there are still features that are pretty Ubuntu exclusive in elementary OS
100%

Forking, developing, releasing, and maintaining this as a generic desktop environment would make it more accessible to devs and end-users alike.

There‚Äôs nothing wrong with additionally releasing a *complete package* aka distribution, but the intention behind that distro should be sparing the not-so-linux-savvy end-user from installing any distro and putting the display server and desktop environment on there on their own.

The desktop environment, in my humble *opinion*, and in my *preference*, should be super easy to install and, by choice, come with a default configuration/setup which is **on par** with the distribution experience. There should not be any ridiculous hacking necessary to get the desktop environment running on any other distro, and there should not be any annoying bugs, missing apps/dependencies, et cetera ‚Ä¶

Fingers crossed for this project, and thank you for your efforts so far. This could help with general linux adoption, i.e. making it at least a little easier and more comfortable for *some* folks to switch to **and enjoy** linux.
1. Fish's widgets are drawn using a wrapper around Qt called FishUI. Upgrading to Qt6 is much, much easier than it seems, as the library calls are changing.
2. Never said the DE would not be portable, but a distro implementing it good enough is the primary aim (That's what CutefishOS did, they created a desktop and created a distro to use the desktop)
3. The design is not gonna change unless there's a better one recommended by someone.
4. Cuddlefish is already occupied by KDE. I think so. Idk it was a dependency and I often see it in my menu.
5. Thankss :))
Strictly speaking, Cutefish is the DE and CutefishOS was a Cutefish-centric distro.

It seems to me that the issues come down to distro-specific functionality (e.g., package manager integration) and the larger questions of what the developers want in an OS.

Ultimately, the new team's main focus needs to be the DE and any customizations to integrate with the chosen distro. 

One personal request: if you go the Ubuntu or Debian route, please create an appopriate "cutefish-desktop" metapackage and possibly a PPA. One of my pet peeves is when there is no clean way to install the new shiny on top of my old system. (Same goes for distros that change the theming of, say, Xubuntu by modifying the packages but keeping the old names.)
I second this. Also, as a BSD user, maybe make it OS-agnostic like Lumina. No Linux-specific dependencies like systemd, etc.
TBH we also have enogh DEs... :) however more choice is more betterer and the next big distro/DE is not among the ones that are already big today
[deleted]
Depends.. a Spin of an existing distro is just a Sprint imo & him forking cutefish would be more like a spin unless he got serious.. but still better off switching to Budgie imho.
What language do you speak?
Judging from the little bit of C code I've read, you can write C code.

Maybe we can include `albafetch` in the distro too, considering the effort you've put in the program and the fact you want to contribute.
Just wrote it down
No way I'm gonna use discord for anything related to the distro. I just saw a post where everybody in the comments despised it, me included.
I need more comments like this.

Anyways, as a consistency above all guy, I do feel the same way you do about looks. I'd assume great consistency comes with a really heavy price on the customizability. No middleground exists here, so either one or the other.
I actually prefer Cinammon whether you believe it or not. I semi-rice it to behave the way I want, and while, it's not perfect, KDE feels sluggish and buggy. Cinammon is boring, but at least stable.
Have you tried Trinity desktop?
\+1 for BubbleFish
C and the desktop can't go together. So I'd assume you can't help directly, apart from writing new portions for the distro. 

If you know C++ (Which is very easy to get into) and you can use Qt, then you can contribute.
BonitoOS, To the portuguese and spanish speakers in the world would sound great!
Love this name
The fork probably can't use the name CutefishOS anyway. It's trademarked. From the bottom of the distro's web page:

> [CutefishOS is a Trademark of CutefishOS Team.](https://cutefish-ubuntu.github.io/)

To use it, I think they would need the trademark owners to license it, transfer it, or abandon it.
That's basically Ultramarine in a nutshell
This comment should get more upvotes!  :)
>  Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary", so better not package it there

Literally what are you talking about. Do you know how many Fedora users use KDE?
> Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary", so better not package it there, maybe try RPM Fusion or COPR.

Fedora has an official KDE spin.
> Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary"

That the first time I heard someone said Fedora folks said that. I only heard that in the embedded developer community. Qt is dual licensed, the open source version is licensed under either GPL and/or LGPL depending on modules. To my understanding, those licenses are hard to comply for embedded systems. To comply with the licenses, the developer must provide a way for the user to replace the included Qt library with the one they compiled themselves, which is almost impossible in most embedded system. Therefore, a lot of embedded developers complain about the proprietary version Qt because that the only version they could use. The Qt Foundation also communicated very poorly about the subject which absolutely did not help. Desktop Linux users should not worry about Qt because they gonna use the L/GPL version anyway.
> Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary", so better not package it there, maybe try RPM Fusion or COPR.

That's wrong. Fedora uses Qt for [Media Writer](https://github.com/flathub/org.fedoraproject.MediaWriter/blob/master/org.fedoraproject.MediaWriter.json#L3). If they really hated Qt stuff, then they would've used another toolkit for Media Writer.
Fully agree with this. As a former mac user, I miss so many things about how mac did/does things. The linux ecosystem can, imo, still really benefit from an approach like that.

Personally I love that Linux allows users to customize and configure everything they want, but I prefer to have everything already preconfigured/customized to really suit most user's needs (what macOS excels at), while *still* having the freedom to do it myself if I want/need to (something macOS sucks at).

Also would love to see macOS like (colored) tittle bar buttons on the left.
idk if they have that in their code, but i'll check
There's none. Only stolen code.


Cutefish was a fork of an old project called CyberOS, which was also a fork of PandaOS.

All of these distros surprisingly seems to be made by the same person under different names.

Source: I know a cutefish/cyber dev, recruited them into the Ultramarine Linux team.
Probably, specifically with graphics cards.
LFS cuz why not
>Cubuntu

Not only does it sound the same as Kubuntu, but it's also a bad idea for Portuguese speakers.
The Cutefish devs seem to have a history of creating a project and when it becomes more popular they are abandoning it. That's why you see a bunch of Cutefish websites, all saying they're real. This is one of the reasons I'm pretty sceptical about doing this after all.
(Do not ask for sources; It's something I've seen people say around Reddit, not a fact).

I'll ignore the rest because it's a matter of development, and I'll have to think about the testing, contributions etc.

Still gonna think about the website, though.
To `Fork` something means take its source and use it as a base for something new
Unity had a global menu and it was great.
KDE has had a global menu for a long time now.
I fully agree
SquishFishOS is amazing
Best one yet
If and when I do this, I'll update the post. Or make a new one to let everybody know. Idk.

And yes, it's not a 100% guarantee. Already lots of problems testing on Arch Linux, VMs are a nightmare for Wayland so I can't add some pretty basic functionality, pretty bad code without documentation or comments, and a few hundred people each asking me to implement something other distros don't do right.
It will just add more fragmentation to already fragmented to hell and beyond linux based desktop environments...
[deleted]
>A better option imho is to offer to help Josh Strobl w/ Budgie 11. That‚Äôs a much more serious undertaking that‚Äôll be every bit as good, if not better.

Are there any updates to this? Some blog post with mockups or anything? I haven't heard about Budgie since the decision to abandon GTK.
*Shhh, don't let OP see this*
Not every outlandish idea needs to be implemented here.
With the current way things work, simply impossible.
Just Ubuntu, Linux Mint is bassd on Ubuntu but it doesn't sucks like Ubuntu
Personally I prefer arch based but I think it should be on Debian too cause it will be more user friendly.
PufferfishOS
It's also referred to the relevant video of Mental Outlaw
I'll probably make it a DE for all distros once I manage to somehow package it for those distros.
First three lines
Downvote me all you want or what ever... Steam moving from debian to arch should maybe at least make you consider the possibility, dont think Steam made the switch cuz they are Arch fanbois but what do i know...
Intel already revived it.
please god no I hate telegram
RHEL is too old for most software, and Fedora updates too fast, good luck catching up with it without forking every single package and make your own custom repos like Manjaro
Alternatively, what you can do is choose a base distro, and integrate with it, eventually providing custom repository for the DE, etc. 
Maintaining a whole distro implies running your own mirrors, building packages, etc.
That's a lot more work.

Those interested in the DE will port it to other distro, eventually. (With the current push to more standardization on the Linux Desktop, this is not that hard)
None of these are good reasons to keep what is essentially Debian with a nice DE as a whole distro. Maybe put your work into making the desktop easier to install instead?
> I've tried this myself, actually, so I'm speaking with experience.

Could you elaborate? What did you try it with?
I think something like endless OS would be nice, like: flatpak-first, OSTree, etc.
You'd be surprised at how easy it is to use Qt, with C++ or Python even. The documentation is second to none, it's cleanly done, feature packed, and very stable. Maintaining a standalone DE, or a whole distro, is not a small feat in any way for either, even if you're based on an existing one, such as Debian or Ubuntu. If the experience is the DE, then I'd encourage you to look more into maintaining the DE.

Either way, good luck with whichever you do.
[deleted]
Have ever considered making it a FlatPak? FlatPak was designed with packaging DEs and therefore already provides a lot of dependencies. This also would remove the dependency on a particular distro.
> Alot

oof
> Cuddlefish is already occupied by KDE. I think so. Idk it was a dependency and I often see it in my menu.

That's Cuttlefish, KDE icon viewer. Not exactly Cuddlefish, but could be confusing.
> Never said the DE would not be portable, but a distro implementing it good enough is the primary aim

Again, why not make a Fedora Spin and Ubuntu Flavor? You'd have a base that's maintained by a team much larger than yours. Don't reinvent the wheel.

> Cuddlefish is already occupied by KDE. I think so.

To be fair, a random distrohopper wouldn't know how a KDE icon viewer is called, but ok.
Just pipe the contents of this random webpage into sh and enter your password when asked

ezpz
that... sounds like a lot of extra work and very likely to break core implemented functionality.
you forget that this distro is for new users?
The number one and two most upvoted comments are both saying basically the same.

Personally, I don't really agree. I tend to pick a DE I like and then pick *the* distro that used that DE. If a DE doesn't have a "default distro" I am much less likely to try it.
Danish. :)
would be a nice touch, but I think it still needs a bit of work before it's actually ready (I'm mostly trying to get a way for the gpu to work without lspci, which is much slowet than I'd want, and the rest of the "TODO" section)
i am not a developer, just a user
btw, I think [vinceliuice](https://github.com/vinceliuice) provided the Cutefish theme. Maybe he'd still be willing to work with you guys?
Ohh lol, then I think it's a bad idea. Then, anywhere else where normal users like me can connect with developers and suggest features or report bugs?
what about a matrix server?
I am on the same boat. Can't help with code, but would help reporting bugs and suggesting ideas. A place to have discussions about the project would be nice. Perhaps Matrix or something like [Zorin Forums](http://forum.zorin.com/latest) would be nice.
> I'd assume great consistency comes with a really heavy price on the customizability. No middleground exists here, so either one or the other.

Why do you think that? And what exactly do you mean by consistency?

The way I see it there is no conflict at all between the two. Consistency is somewhat easier to achieve if you have no customizability, but is by no means mutually exclusive with it.

By consistency I mean that apps generally look and behave the same. The same standard shortcuts work in every app, they use consistent patterns in their design, the general theme is similar, etc.
Then I‚Äôd vote for consistency and stability over customisability, since KDE already does the opposite so it‚Äôd be a good position for a new DE. Are you gonna update this post with the GitHub repo if you end up doing that? I wanna stay up-to-date on this, I‚Äôd also love to contribute too but unfortunately I‚Äôm not a developer :/

Just out of curiosity, what programming languages do you need to know to work on a project like this?
I guess it comes down to personal experience then since I full on rocked Linux Mint Cinnamon for a couple of months and the entire DE crash a good couple times and had a few other bugs here and there. KDE on Arch has been the most reliable and stable DE experience I‚Äôve ever had, ironically enough.
I'm absolutely with you about KDE. I really want to like it, it has a lot of amazing features, but it really does feels sluggish and unpolished to me. 

When first trying Linux, I used Mint for around a year, and loved it. I think it's a great distro with a good enough DE
>KDE feels sluggish and buggy

Across my 3 laptops (Ryzen), KDE is indeed a bit buggier if you start to delve deep into it, but it performs buttery smooth (especially in Wayland session), whereas Cinnamon is noticeably sluggish, probably due to Muffin being an old fork of Mutter.
Just had a look it looks outdated af
No, I have no idea what that is
You do know that GTK is entirely C, right?
>C and the desktop can't go together.

Angry GNOME noises.
How can I contribute as a non-technical but tech-savvy/enthusiast?
It‚Äôs basically Fedora with a different DE in a nutshell. OP really should just separate the DE from the distro, CutefishOS has no reason to exist.
Heheh, thanks for your support:)
KDE spin user right here. Works great!
Yes, I do. There are more people in the fedora community who think Qt is proprietary than the number of KDE spin users. You can search "qt proprietary" on reddit, red hat bugzilla, searx and fedora forums if you don‚Äôt believe me.
Yes, that's because there are maintainers of the spin. Given the omission of KDE from RHEL 8, it is probably maintained by the community rather than RH employees. Given the change in open source Qt LTS releases, ~~expect a sudden announcement of dropping Qt from Fedora any time citing a misunderstanding of the relation between the Qt company and KDE free Qt Foundation~~, just like how CentOS 8 support was terminated 8 years before normal. Apparently Fedora has started talks to become a KDE patron, so no issues here
Fully agree, I always disliked the argument of gnome is like mac os. It kinda looks goofy even the current version. This is why I tend to like other things like awesomewm, etc.

Macos is just so well done and smooth to use.
Nvidia owner here, can help with that.
Hey mate I'd love to help in any way possible. I don't know C++ but maybe I could help with UX and testing or something? Anyway you should probably create a group on some messaging software. I've heard that Matrix is a good alternative to Discord.
if you make an arm64 build I'd love to test it with [Asahi](https://asahilinux.org/) whenever it's working on macOS Ventura
No package manager.
Yeah, good point.
I can already imagine what people would say.
Ok! I do have high hopes for this project, I've never particularly participated in a Distro fork, did you check out the website template?

Edit: I fixed the link.
For example, DragonFly BSD is a fork of FreeBSD
Aaaaah so it‚Äôs like, VSCodium is a fork of VSCode?
You mean the old Unity DE or the Ubuntu Unity distro with the new Unity? I only used Unity briefly in 2012 (?) and it was awful. It was poorly integrated with half of the apps still having the menus under the window bar and the thing crashed and lagged so much it gave me whiplash.

ADD: Not to mention it was ugly and almost impossible to be made to look good.
But they don't advertise it. I don't see it mentioned anywhere in their webpage. I'm literally hearing this for the first time from you.
I mean OP said he would consider Arch as a base distro. I don't see why a Fedora base would be any harder.
So did you see my whole comment?

https://www.reddit.com/r/linux/comments/vwd0m8/comment/ifqmbdd/?utm\_source=share&utm\_medium=web2x&context=3
Because steam used a image based system like Android, so there is little to no chance of it breaking, Arch is extremely dangerous (if not handled correctly) for newbies
Install the CutefishOS (and not only) on another distro. Trust me, it's a mess. The average user just wants a desktop to open Firefox or whatever on.
For open-source projects the license is literally just the GPL. IME as an API Qt is very well-designed.
> I've used Qt as the framework for a few projects, and personally I'd try to break away from it just from experience.
> 
> It's been awhile but last time I read up on their licensing terms alone I had a few things I wasn't a fan of.

You are not a fan of LGPL-3?
What would you prefer?
Depends on the license.
Also you can't seriously expect me to rewrite the Cutefish desktop from scratch, are you?
> It's been awhile but last time I read up on their licensing terms alone I had a few things I wasn't a fan of.

What, you're not a fan of GPL and LGPL?

Do you think you know better than KDE?
since when do DE's have flatpak versions lol
Even I confused it, definitely should not happen to some random distrohopper. Neve rmind legal issues, I don't really know if it's even legal.
Fedora is simply incompatible and the distro WILL be an Ubuntu flavor.
PPAs are often ran by organizations, and it‚Äôs much simpler for a smaller team than getting into official repos
Probably true, but hey, he's asking for suggestions after all.
Not sure why that's relevant to suggesting the DE be platform-agnostic.
Then you're limiting yourself arbitrarily. Also, that's not how that relationship works at all. It is, in fact, the complete opposite way around.
Clearly, you speak English too hah
If he's got the time. He's got so many themes already.
Snippet of my comment from some other thread:

> (...) I might just add that a lot of open-source communities / projects shift towards Matrix right now: [KDE](https://community.kde.org/Get_Involved#Start_Here.21), [GNOME](https://wiki.gnome.org/action/show/GettingInTouch/IRC?action=show&redirect=Community%2FGettingInTouch%2FIRC), [Fedora](https://docs.fedoraproject.org/en-US/project/join/), [Ansible](https://www.ansible.com/community?hsLang=en-us) etc.
If the choice would be [Matrix](https://matrix.org/), a bridge between Discord and Matrix could be used (https://github.com/matrix-org/matrix-appservice-discord). 

But nowadays, for example, Discourse or Slack are also used, which an average user should also be able to use. And in many cases you can also report bugs there, etc. And basically every Linux user should be able to create an issue on Github, for example. Github now also offers discussion forums.
There could be a perfect world where the amount of customizability would not limit us, but then, you have Linux, with 3 different UI toolkits, custom stylesheet support (See Chromium) and two different approaches on a UI desktop, by GNOME and KDE.

If I change my KDE theme, GTK won't follow, Qt apps not in v5 of the library are just going to ignore me, and some apps are simply gonna override the theme no matter what.

ElementaryOS takes the best approach to this but that's because it's strictness level is similar to Apple's. GTK, no custom themes, no Qt apps by default. We are on the Qt side for now, and that's already making the situation worse. 

And I haven't even said anything yet. Imagine the paragraphs I could write for libadwaita.
This specific project would need C++ with Qt and QML.
C++, QML, and if you talk about the distribution in general, C and probably some scripting language too.

>I‚Äôd also love to contribute too but unfortunately I‚Äôm not a developer :/

[https://www.reddit.com/r/linux/comments/vvyc8b/rest\_in\_peace\_cutefishos\_you\_were\_amazing/ifp1wgw/?context=3](https://www.reddit.com/r/linux/comments/vvyc8b/rest_in_peace_cutefishos_you_were_amazing/ifp1wgw/?context=3)

This is the original post, where I'm mentioning that you contributions don't really have to be all about programming. You'll see parts of the conversation in general too.

EDIT: Yes, I will add the GitHub repo and tell the people that are interested.
If you look into my posts I did a couple of months ago a ricing with Cinammon on vanilla arch on a weak laptop, never had a problem (but for stability reasons this now runs Linux Mint, and I still have no single issue with it)
I've tried a lot of distros, my "will work, no matter what" is Mint, my favorite "out-of-the-box" is Pop OS and my favorite for customizing is vanilla Arch.
Trinity is a fork of kde3 of course it would look outdated
And that's a reason against packaging it on Fedora because...?  


Every distro has users who are just plainly wrong about technical matters. Luckily you can just ignore those users for the most part.
I'm a Fedora maintainer, and I have never seen any of the misconceptions you are describing among the Fedora maintainer community.  There is zero chance that KDE or QT will be dropped from Fedora.  None.
> Given the change in open source Qt LTS releases, expect a sudden announcement of dropping Qt from Fedora any time

As in, they're going to not package Qt anymore? Or what would "dropping Qt" mean?
yes, I agree. though I must say that gnome has been improving a lot. Both in looks and simplicity. And out of the mainstream DEs, it is the closest to macOS imo (though admittedly it is still lightyears from it). You can get *way* prettier stuff with some WM, but from my limited experience it is not a full blown DE experience and it involves quite a fair amount of the user getting their "hands dirty" to get things working exactly as they want (not macOS like at all in this regard).

That is why I am still using gnome (Zorin OS). It is a balance of pretty enough, simple enough, "just works" enough for me...
Write your own. In Rust. Everything must be written in Rust.
i've managed to install rpm in lfs, and write packages for it too
Not yet, there is no point in the website if there isn't a fork in the first place
Just a quick question, do you have experience using BSD?
I wouldn't describe VSCodium as a fork, since it is not a completely independent piece of software. AFAIK the project just produces VSCode builds without proprietary components. If I understand it correctly, forked projects continue their lives as something disconnected from the projects they are based on.
It is shown on the wesbite: https://kde.org/plasma-desktop/, if you go to "Customizable" and click on "Panels", it shows a screenshot with a global menu.

Maybe that doesn't count as advertising. Plasma has many features, can't advertise them all I guess. It's pretty well-known among users though.
Id say your attitude is dangerous for newbies, contrary to most distros Arch has a kickass wiki that will have all necessary info to fix any problem you might eventually cause yourself :)

You learn a lot by trying,  its almost impoissible to break your system so bad you get dataloss , unless you are avtually trying, as long as you have a smart phone to browse the wiki you have all necessary tools to recover.  
Not sure what you are saying either... looks like debian is now Android?  
Im refering to this https://www.pcgamer.com/this-is-why-valve-is-switching-from-debian-to-arch-for-steam-decks-linux-os/
Personally in such a situation, I would fix whatever it was that is causing it to be broken on other distros. Most desktops do run on all distros, so it is a fixable problem. But I can understand if you think you can't fix that right now (you cited your inexperience with Qt).
It's not really widespread yet, but the Gnome devs are experimenting with running the DE in a flatpak.
> Fedora is simply incompatible

That sounds like a problem chief
I think i can make fedora work, wait a bit
I suspect because of the extra developement resources needed to both make it agnostic and not using dependecies that would make it easier, and the testing. As this is aimed at new users, they won't be going BSD for their first non-mac system, they'll go Linux, so the main focus should be on making that work as well as possible. Making it platform agnostic is a bonus, but one that may (or may not, I have too little experience in this specifically) be too much effort for the team
While I agree that it's a limitation, it's not arbitrary, as it serves at least some purpose.
Totally unrelated to your comment (which I totally agree with). Your name is hilarious Sir.
>And basically every Linux user should be able to create an issue on Github, for example. Github now also offers discussion forums.

Yeah. But it will be way easier to discuss with people in chat, like "What if it is implemented?", "What if this was changed to that?" etc. I think some small things are easier to discuss in chats rather than discussing in forums. It's just my opinion tho.   
Yeah, I can just create an issue on github and try to support the project that way too. Discord servers or any chatting platforms aren't the only way to contact devs, I just thought it was easier to connect with users using these platforms.
Good to know, thanks! :)
I just saw your post, it looks pretty clean, but would‚Äôve loved to seen more pictures. Never thought about ricing cinnamon lol
Not a reason to not package it, suggested to choose rpm fusion because they have a lower bar for entry
Not a reason to not package it, suggested to choose rpm fusion because they have a lower bar for entry
> misconceptions you are describing

https://linustechtips.com/topic/1177605-qt-plans-to-go-anti-open-source-because-of-coronavirus-and-need-for-profits-all-future-versions-possibly-delayed-by-12-months-for-open-source/

https://access.redhat.com/discussions/4861961

https://vegastack.com/blog/centos-8-end-of-life-in-2021/

Which of these are misconceptions? 

Anyway, KDE will probably stay since Fedora started talks to become a KDE patron. So no problem here
It doesn't mean anything.  Fedora isn't going to drop Qt.
Like how they dropped KDE from RHEL 8

https://access.redhat.com/discussions/4861961
Fork the Linux kernel and rewrite it in Rust
Good point.
Yeah, a bit. I'm learning FreeBSD right now. Always wanted to try something more close to the real UNIX‚ÄìI think that's the best way to learn UNIX-like systems. Today I finished migrating my web application from a VPS to a self-hosted FreeBSD installation. First impressions are quite good. FreeBSD doesn't support Docker, so I had to play around with Jails.
Oooooh got it. Thank you!!!
Yeah, it's hard to bring new users when you don't advertise your features. I've always been curious to try KDE but every piece I read about it starts with "if you want a windows like desktop" and that's just doesn't speak to me at all. I'm on Linux because I  *don't* want a windows like experience. That fades my curiosity, but I might try it sometime in the future.
https://www.reddit.com/r/Fedora/comments/qszbko/steam_os_will_have_an_immutable_file_system_like/?utm_medium=android_app&utm_source=share
Not all people know English, not all people update their system everyday, and not all people know how to recover data if their system doesn't boot anymore :)
I completely agree with you. Most of the time someone makes a distro it's just a themed version of something existing or a new DE but completely bound to their specific distro which then hurts adoption.

By far the most people won't just leave their favourite distribution for a specific DE, so if the DE isn't available for that distribution they just won't use it. If you want users, you make it distro-independent. There isn't any benefit to making a DE-specific distro anyway.
There isn't much to do. Most distros (Like Ubuntu) expect you to never change your desktop enviroment. You can install whatever you want in Ubuntu, but it will simply "Not feel right". 

Now that you're mentioning it, I could just maintain the DE alongside a flavor of Ubuntu. That still requires me to develop the desktop heavily tho for the flavor. Idk
Well, why not ship the whole distro altogether as a flatpak? /s
Do you have any links about that? Sounds like an interesting experiment.
No, I mean it. Cutefish has some pretty weird dependencies that Fedora does not provide afaik. Or maybe it does, but good luck finding them.
It's absolutely arbitrary. Which distro "belongs" to KDE? There's Neon, but that's just Ubuntu under the hood, and I find that Kubuntu does a better job in that niche. OpenSuse has been considered a "flagship" for KDE for the almost 20 years I've been using Linux, but its implementation doesn't differ from anyone else's all that much. Fedora also has a very good KDE spin which I would happily suggest to anyone looking for a good KDE distro. Or maybe you're into one of the BSDs instead. Which one of them belongs to KDE?

Gnome is even more arbitrary, because everyone uses it. Which distro "belongs" to gnome?

Sorry, but all your comment does is illustrate a lack of understanding about the ecosystem.
I thought that the whole reason people hate discord for development projects is that the support ends up being in a closed chat instead of a public forum. The support then becomes inaccessible and not indexable by search engines, so other people with the same problem are forced to join the chat service to get help. Whether it's matrix or Discord, the problem is the same.
Right now it looks better I think, maybe I'll post it again with more pictures, etc
The onboarding process is *very* similar.  I don't see how onboarding at rpmfusion is any easier than it is at Fedora.  The only good reason to package in rpmfusion is that Fedora policy would prohibit including the package, but rpmfusion policy does not.
> Which of these are misconceptions? 

"There are more people in the fedora community who think Qt is proprietary than the number of KDE spin users"

This one.  There are people who hold that misconception, but I do not know of any such people among the Fedora maintainers community.

"Fedora people apparently hate all Qt stuff as it‚Äôs "proprietary", so better not package it there"

This one, too.
Of course. Just wanted to see what doomsday scenario that commenter had cooked up.
bad bot.
Don‚Äôt really mean to bother. Its just been a half year since I started using Linux (still don‚Äôt know that much of difference between UNIX and Linux). All I know is that there‚Äôs FreeBSD, ClosedBSD, OpenBSD. Care to explain what they are? Also, what‚Äôs a VPS? And what web application are you talking about?
If you want to read a piece about KDE which talks about its unique features and strengths, I suggest https://www.dedoimedo.com/computers/plasma-desktop-awesome.html. It's almost a bit *too* glowing of a review, but it covers a lot of what makes KDE stand out.
Not all people are contributing to the thread :)))
This is basically how KDE Neon works. They start with the latest Ubuntu Server LTS, add the Neon apt repositories, and use `tasksel` - I‚Äôve converted more than one bare server install over to Neon
Yeah that's not true at all. It's trivial to change the desktop on Ubuntu and most other distros.

>Most distros (Like Ubuntu)

Pretty much just Ubuntu and its derivatives.
> Now that you're mentioning it, I could just maintain the DE alongside a flavor of Ubuntu. That still requires me to develop the desktop heavily tho for the flavor. Idk

I guess I'm a bit confused - ideally you'd _want_ the DE to be maintained, regardless of whether you have the fork be "actual base CutefishOS" or an Ubuntu/any distro base?

I never got a chance to try out CutefishOS, but wasn't the DE in quite a heavy beta state?
In my opinion you should for sure keep it a distro for many people that have less experience with Linux it will be a mess. Maybe if you've got time you can also try to maintain it as a DE. But that's just my opinion I think it will be confusing to most people with less experience in Linux.
How about a pitch to the Manjaro community forums too? I believe they managed to get Deepin working, practically every DE is available from them.
It would take some time to load the kernel /s
I'm curious; what are these weird dependencies?
Eeh. I'd like to point out that there were several COPRs building Cutefish.

I know, because I actually used one of them and ran it on a Fedora 35 install for a day or so, and I don't recall any major issues.

Not saying you should use Fedora, but just that it was possible.
[It‚Äôs actually very easy to find dependencies on Fedora.](https://linux-audit.com/determine-file-and-related-package/) Just get the files you need in cutefish then look for the corresponding packages with RPM and DNF on Fedora.
No, it's not arbitrary. It ensures good compatibility between DE and distro, and also helps when looking for help. It doesn't mean it's the only way to do things, and I'm not encouraging others to do it the way I've done it, I'm just trying to explain my reasoning. OP wanted honest opinions on how people reason, and I gave mine as an example.

As for the lack of flagship OS's for KDE and GNOME, that's a contributing factor to me not having used them very much. Good compatibility between Cinnamon and Mint was a big reason why I went with that distro for most of my setups.
Who needs to cook up reality?

https://access.redhat.com/discussions/4861961
Mmh (‚óû‚Ä∏‚óü)
Well, I use linux for more than 5 years and still don't know much hahaha. You just can google stuff on topics like this one but make sure you are taking info from multiple sources‚Äìeven more than two, since there're lots of people who spread misinformation. Shortly, Linux is a kernel and it's a clone of UNIX. People take the kernel and pack it with useful programs like core utilities, desktop environments etc‚Äìthese combinations of kernel and software around it are called distributions. Modern BSD systems are based on the BSD system, which is in turn was based on the original UNIX. BSD projects are not like Linux distributions. BSD kernels are separate as well. For example, FreeBSD project develops both kernel and the software around it.
Like you
I've gotten weird bugs and things not feeling right switching DEs.

https://support.system76.com/articles/desktop-environment/

Changing a theme as they suggest in troubleshooting there has helped me in the past, but if the DE you are switching to doesn't have one of those you are SOL without also changing the Username/home directory.
Fedora as well.
Don't know exactly because they've got completely different names on each distro. Something to do with xcb iirc
There's no such thing as a "flagship OS" for a DE unless it's cinnamon or Pantheon. That is very much not the norm. Your reasoning is not based on a valid hypothesis and serves a purpose only in your mind.

So yes, it's extremely arbitrary, and you're limiting yourself for a really bad reason. Do what you want, though, I guess.
RHEL is not Fedora. And even RHEL did not remove Qt, as far as I know.
Im actually posting quality content, linking to my refs etc.  
You just came here to argue, you dont even have a more reasonable argument then that you are too lazy to properly care for your system, please stay with debian, we dont need you :)
It's the same repo, no matter what flavour. Just don't install whatever additional DE you feel like.
If you need help with deps, you can contact me. Ima  developer and sometimes need to know package names for CI/CL systems so I can help you with that. Also, Ubuntu sucks and I would not recommend it for anything. That being said I would like to help regardless of using Ubuntu as a base.
Looks like you'd probably want `libxcb-devel` and a number of packages called `xcb-util-*-devel` (e.g. `xcb-util-keysyms-devel`, etc. etc.).
I'm willing to bet they're available in Fedora. XCB is an extremely common thing.
Let's consider a concrete example. A Linux beginner chose distro X and a DE called Y. They run into some issue, and since they're a beginner, they don't actually know if the issue is more closely related to the distro X or the DE Y (in fact, they might not even know that they should be asking themselves that question). So they chose to ask for help on, let's say, the subreddit for distro X. And as a response, they just get answers telling them "that's a Y issue, you should ask that elsewhere". If that person was a total beginner, this could be quite confusing.

It wasn't all that long ago that my knowledge of Linux was at the level where having just a one-stop place to look for help was a concrete benefit of using a tightly integrated distro and DE.
Not everyone is lazy, they have work to do, they don't have all day to fix their system :)
And by extremely common it means "everywhere that ships Xorg" :P
> they just get answers telling them "that's a Y issue, you should ask that elsewhere".

The type of user that can't figure out where to go after being told is not the type of user that should be on Linux. Or a computer, for that matter.
Nope the Wayland compositors mostly use parts of xcb as well
That's an r/gatekeeping comment if I ever saw one. Everyone's a beginner at some point, and not all help you receive online is particularly helpful. I don't think we should discourage people from experimenting or making their own experiences easier or better.
Being a beginner doesn't mean you can't learn. I want people who are actually willing to learn about the OS they're switching to rather than people who make unfounded assertions based on entirely incorrect information.

Why is "learning" a four letter word these days?
I also want people to learn, but I also accept that different people learn in different ways. If people find a way that makes things easier for them, avoiding issues, making it easier to find help or whatever, then that should be encouraged, and is certainly not a reason to never use computers.
Being told "go ask these people" is about as clear and obvious as you can get. If that's too much for someone, their opinion is worthless to me.
If everyone received helpful advice whenever they asked for help, such as explaining who or where they should ask, I might agree with you, at least to some extent. But that's not always the case. But that's besides the point - if somebody figures out a way to solve a problem in a way that means they require less help, that's generally a good thing

Furthermore, even if you don't care about the noobs' opinions, are you willing to make that decision for OP?
OP wants to make a whole distro just for one DE. If I could make decisions for him, this post wouldn't have happened.
What happened?
Damn, it had a really consistent UI ngl.
well rip

it was a pretty cool distro
It‚Äôs just the third project now the devs behind cutefish OS stopped developing after some cool working beta releases :(

Cool ideas, but nothing ever that came to fruition and could be built upon for years
I've unfortunately seen a lot of promising projects like this one eventually fall to the wayside.  Cutefish was a very interesting idea but I suspect that its development community was too small and that its members eventually moved on to other projects.  Oh well...
I don't know why they had to push Cutefish as some new DE and OS. It would have been better off as a custom script to configure a normal KDE Plasma installation as it would have been a heck of a lot easier to maintain.
I never noticed how much that activity graph looks like a heart rate monitor.
Thats really sad as i really enjoied using it
Well that really sucks.
For the people who are "sad" and claim they'll miss Cutefsih, DONATE to the devs if you are so sad to seem them halt development.
One of the best looking oses out there
We have a spin of OpenMandriva using the CuteFish desktop, and while we consider it a development snapshot, it's ready to use...

[https://abf.openmandriva.org/platforms/cooker/products/43/product\_build\_lists/1162](https://abf.openmandriva.org/platforms/cooker/products/43/product_build_lists/1162)

So if it's just CuteFish**OS** that's being stopped, it can be replaced quickly. Unfortunately we're a small team, and don't have the manpower to develop the desktop ourselves. If any cutefish developers are interested in continuing the desktop on another distro, please ping us. (Oh, and we've been around for decades without funding, so this won't suffer the same fate).

[https://matrix.to/#/#oma:matrix.org](https://matrix.to/#/#oma:matrix.org)
all of this is under the GPL license so it can probably be forked.

in fact cutefish is based on ubuntu and the last version is based on ubuntu 21.10 it will probably be very easy to make a version of cutefish based on 22.04 you can probably even use [the cubic iso tool](https://launchpad.net/cubic) to make it and package it.
Correct me if I'm wrong, but wasn't some of it closed source?
Thought Manjaro Cutefish Edition is a community maintained project. Did the development on it stop too?

https://liliputing.com/2021/06/manjaro-cutefish-edition-is-a-community-spin-with-a-macos-like-design-for-this-linux-distro.html
I cannot find any information about this anywhere.
The Dev was an asshole
Heard it first time, searched in distrowatch and didn't get any results. Was it based on ubuntu? I think that EVERY distro should ditch their ubuntu base and switch to debian or fedora as a base.
unexpected xd
Good thing I made a disk before it stopped shipping iso images
Not a big fan of UI like this but damn, this is sad. Hope same won't happen with elementary, I'm quite concerned about it's maintenance rn
I had a minor PR for its dock component and the maintainer merged it a week ago. I guess it's not fully dead. Enough people could revive it.
Oh no, I hope the DE advances.
It's one of the best looking desktops linux has had
Already dead?
I always find these graphs sad, like the heart beat literally going out slowly on a loved project.
What was so amazing about it? Never heard of it
Man I‚Äôve seen so many distros with great DEs just die, never to be seen from again. Does anyone still remember Semplice Linux?
Ff sake, was litterly looking to move to this today!
Currently useing Ubuntu budgie. I think
The development stopped indefinitely due to funding being cut short.
What were the others?
It was an OS for normal folks, not advanced linux users and their WM was based on Kwin but wasn't Kwin.
As a Garuda and Feren user, that's what I thought as well. KDE is fine for the most part, you just need to know how to hack it into what you like. Just take out that process and present the UX you want out of it- done properly enough and it's practically a different DE.

If there's something about KDE as the base project that they didn't like, then maybe as a fork of Deepin. For the love of god, unless you're THAT well-funded or driven, don't just create a new DE.
It *was* mostly based off of KDE Plasma.
I agree wholeheartedly, but shaming is never a good call to action.
CutefishOS wasn't even in beta so probably wouldn't have been a good idea to switch. The closest you can get it Deepin or Zorin or KDE with custom theme.
That's just sad :(
That's a shame CutefishOS was great a OS for those coming over from the macOS/iOS ecosystem. Hopefully someone makes a fork and continues development.
No way, really? It sucks, it had the best UI of all distros so far T\_T
Can it be forked?
See the flatlines on the charts,
Of the many little hearts,
That made CuteFishOS.
how do fund them?
The one right before cutefish was another DE looking quite similar (I.e. a macOS clone). I don‚Äòt remember the name sadly. Then there‚Äôs JingOS for tablets which they listed as ‚Äûfriends‚Äú and vice versa
Fedora is an OS for normal folks, Ubuntu too. Why wouldn't it have worked as a custom script for setting up plasma on those OS's?
As much as I like Linux, I don't think there is really a need for a "Linux for normal folks". If you are not comfortable running shell scripts and opening terminals, then Linux in its current state is not for you. I'd be surprised if someone who isn't comfortable running shell scripts could even figure out how to install Ubuntu-based distros on their PC.
Interesting. From my time using it there was very little I noticed that was different. It really just seemed like one big theme for KDE Plasma plus the panel was edited. Do you know what was different off the top of you head?
I did not shame anyone, I simply suggested a course of action that can actually produce results. Sad reactions will not provide any means to the developers to continue their work. I understand that people feeling frustrated or sad feel the urge to immediatelly express themselves in the fastest and easiest way possible (through a comment in a social media platform), but this will not make any difference in changing the conditions that caused the sadness. On the other hand, after their initial reaction, I urged them to see clearly and do something useful.
Ye, Deepin spies on you.
Someone already planning to, let‚Äôs see how it goes.

https://www.reddit.com/r/linux/comments/vwd0m8/i_am_about_to_fork_cutefishos_and_i_need_your_help/?utm_source=share&utm_medium=ios_app&utm_name=iossmf
DE from the US: Bland and boring theme and UI

DE from Asian countries (most notably China): Epic theme, Blur, animations, and pretty looks
I don't see why not?
Wanna step up?
I believe Jing was funding them
Deepin?

It looks more like windows now but it used to have a MacOS dock (you can still switch to it)
> Why wouldn't it have worked as a custom script for setting up plasma on those OS's?

That's like asking:
Ubuntu is just reskinned Debian, why wouldn't it have worked as a custom script for setting it up on Debian?
Disagree with this one. My little siblings, mother and grandfather are all using buntus, and they are more than comfortable with it without using shell scripts of any kind. They also never used a terminal either, GUI is more than enough for things they do - browsing the web, picture editing, office stuff and gaming. Also, they can't install neither Windows, nor Ubuntu, but I'll be honest, clean installing Ubuntu is literally the same as clean installing windows - you just click delete everything and you are done. There is nothing difficult about it.

I have no experience with cutefish, but I believe a "Linux for normal folks" will always be something needed, no matter how much Linux elitists are against the idea.
Bad take. The whole point of Linux desktop is accessibility, otherwise we'd be running TTYs. 

You don't win the world by excluding most of the population. Accessible technologies are what change the world, not the ones with elitist attitude and a promise to be stubborn.
Now that I look at it, perhaps saying "mostly" was inaccurate. The compositor/WM was just KWin with some plugins. Most apps (settings, file manager, terminal, screenshot, etc.) were custom, but built with their UI toolkit which was based on Qt Quick. The dock was not the KDE panel or Latte, but custom built in their UI toolkit. They used SDDM, but the lock screen was their own.
Not really, that was old news. The OS is probably fine but not for non-chinese people since the distro's english translations are awful.
But none of those things makes something a distro. What differentiates cutefish from other distros, what you described is a theme not a distro.
I remember installing red flag linux as a joke sometime around 2009. It was pretty funny when everything pretty much worked right out of the box including codecs and a lack of any sort of TOS. Life became so much easier when apparently the designers of that distro didn't think copyrights were something you needed to care about.
It's to lure you in so the Chinese government can spy on you.
No joke if we can get a few people to fork it I'm all in to become a dev

Edit: I'll try to contact the devs, and if the project is actually dead, I'm forking it.
I'm in!
It wasn‚Äôt deepin , that‚Äôs too big. It was a niche project Just like cutefish which I followed shortly

Edit: is deepin dead? Last time I heard they are still active?
And the answer, yes it'd work. Just not everyone would run that script so you'd probably burn ISOs of Debian _after_ running that script.... And then maybe rename it.... You catch my drift?

It could be a ready to install flavor of Fedora or Ubuntu easily; they don't need to bother with keeping the kernel and other softwares packaged and can focus solely on the UI.
Ubuntu is not just a reskinned debian. I'm not a Ubuntu user, but as far as I know, they actually maintain their packages and apply patches etc, so it's not just a skin on top of debian testing.
For sure, installing Ubuntu is probably even easier than Windows, as you don't have to put in any product codes and drivers are more frequently automatically installed.

But that said, did your family install Ubuntu on their computers themselves? Did they just happen to stumble upon it online? Or did they have someone like you to suggest it and install it for them?
Why does Linux need to "win the world"? Linux already won the server world and as time moves forward, we will probably see even less of Windows being used on servers.

For the casual person who doesn't care about their privacy and just wants to check email or what not, they will just stick with Windows. Fancy colors and teeming aren't going to convince a large chunk of the population to learn how to flash an ISO or find someone who knows how when the operating system the computer came with solves all their needs.
I agree, mostly. Though you are technically wrong because great looking consistent UX is way beyond what a theme can do. It's definitely what defines the whole desktop environment as well.

However when you're shopping for a _distro_ the bling should be among the least important factors, and it really should let you switch between DEs too. The package manager and the model it's based is the only important factor long term.
True, I should edit my comment.
You can contribute to the official repo for a while, see if they respond
Looks cool, feel free to dm me if you want any help with app development for it.
Cutefish is based on Ubuntu. The easiest way to keep this project alive would be to install Ubuntu, configure it to look like CuteFish, and then make your install a redistributable distro.

Basically, it's easier to ship a themed Ubuntu, than it is to maintain a completely separate OS repo.
the hero we need but dont deserve
What languages do I need to know to help?
Edit. Never mind. It is in the picture. Lol
Why not try this? We've already got some amazing people on reddit. 
I'm in...
I'll join u.

After you fork it send me the github link and I'll see what I can do.
Deepin is alive like never before, cutefish actually derived a lot of code from deepin.
I believe you are thinking about CyberOS, it was an arch-based distro with a DE very similar to Cutefish. In their [GitHub repo](https://github.com/cyberos) it says to go to CutefishOS since the project was abandoned.
Same with CutefishOS. It's not Plasma.
I have installed it for them, yes. 

See, that's the thing - if it was Windows, I would've had to download the ISO and install it for them as well.

Like, take my grandpa for example: he has no idea about operating systems or anything. I've replaced his winXP with Lubuntu, after he said his computer is broken (it was just slow, really), and all he noticed was the change in colors. There is a "start" menu, there is a firefox icon. He doesn't need anything else. If I told him it's not windows, it's linux, he would've no idea what I'm talking about.

As for my mother and siblings, they do know what Windows and Ubuntu Linux is. Or at least, for the most part. If they wanted to install either of those, they would ask me for help. Windows isn't that much better known among people who aren't really into computer stuff - most people who use windows doesn't use it because they prefer it over Linux. They use it because it came pre-installed with the computer.

This is why Linux - and "Linux for normal folks" should be promoted. The Linux community won't gain anything by being elitists, but the the world as a whole would gain a lot if people would properly learn about privacy and stuff, and why Windows - unless crucial for something - should be avoided in most case. And for that, we do need these Linux distros that are simple to install, set-up and use for normal daily stuff.
I feel like Package Manager and Release model is mostly done. I don't think there is any need to develop that again (for now).

If you want to release a distro based on "Best UI/UX, modern looks and the best user experience possible" this is 100% doable as a Desktop Environment. Not as a Gnome theme perhaps, but developing an entire new distro with everything that comes with maintaining a distro just for a new desktop is pretty dumb imho.

Cutefish as a Desktop would have been great. Release it as a DE with Add-on Repos for Ubuntu or Debian and most other distros will figure out porting it. Then, maybe, cook up an ISO that's more of an Ubuntu flavor, or Fedora spin with your DE preconfigured.

Unless your doing something like NixOS, Qubes or Fedora Silverblue, i'm not sure how we would substantially improve the standard Linux Distro we already have.
Yeah, can someone explain to me why you‚Äôd fork a project that‚Äôs not going anywhere (rather than forking a project that‚Äôs going the ‚Äòwrong‚Äô direction)?
You don't have to take every single part of the distro. That's going to be a hassle. What I mean by "fork" is take over the development Cutefish did (DE, Apps and other software), combine it with a distro base (eg. Debian) and use it's repositories for software.

Cutefish was after all just a different flavor of Ubuntu / Debian without many differences. We don't have to make an exact copy of it and keep maintaining it.
You actually don't even have to know programming languages. A distro is not just coding yk
I'll need to find a distro to base off of, either Debian, Ubuntu or Arch.
Why? Basically cutefish (like most distros) does not provide any way to create a new ISO. you have to do it manually or use a third party app which is distro-specific.
I only consider Arch because I'm currently running Arch and it's easier to work with rather than a VM.
Also known as Chinese spyware.
I think that was it. And some devs moved from the one to the other project iirc
Agreed, but I never actually tried Cutefish, so I can't say their reasons.

There are valid reason to try to innovate outside of the DE. For example you might want something added to the XDG desktop standards or GTK or some new protocol to get something working for Wayland. Then you might end up forking those repos and maintaing your own version of them because upsteam rejected your contribution etc.

I think the key here is to pick your battles if you want to get anywhere. Comply with the standards and make your DE use vanilla dependencies you didn't patch. Then people can actually install it on their systems.

Or don't build a DE at all. Pick a different battle on a lower layer and create something like new Wayland protocols, pipe wire that any DEs can use.
Because some people want to see this project succeed
Because you want to see it continue, and you have work to contribute to the project but upstream isn't listening / there. I've done it before for a developer who "stepped away from open source".

Also provides a single point to track all of those changes.
Then what is it? Because if its not just coding I'd love to partake in a fork
I mean I know that, but the whole post is about how development stopped on this distro. That's why I mentioned code specifically
Personally I like debian just because its really stable.

Just my two cents.
Would love to see cutefish arch based.

Right now I'm running the cutefish de on arch for my uni laptop.
please do an Arch version (apologize if it sound like a demand)
Do you have any evidence or just assuming bc is Chinese?
Artwork, scripting, custom builds of software are just some of many things a distro could provide without any coding. 
Now in this case, we are talking about a fork, so I'd imagine we do need maintainers and people who can make a change in the distro.
same here

>Do you have any evidence

Nope, just your typical reactionary idiot. It's open source ffs.
You trust the ccp?  I love the way deepin looks, but will never install it (or any Chinese Linux os).
Yeah true, I can only really do a little bit of python but I'd love to try some graphic design
Whatever I guess, I already own Huawei device like 50% of android users who use Chinese brands. I'm personally not a privacy diehard and I'm using deepin because it has the most realistic chance to be a mainstream desktop OS, which can finally bring some competition to the table.

Nonetheless, Deepin seems to be fairly transparent about what they're doing and it's open-source. If you're not satisfied with gnome or kde and low quality distributions, it's a good compromise.

Also they've even removed user experience program from the system, so data collection is really limited right now.
To be honest good graphic design is missing from the Linux space, I don't know why people underrate it so much.

Again, contributions can be pretty much anything.
I've found Zorin OS which has the same level of polish as Deepin, but not the spyware.  It's pretty good.  I would run deepin, but like I already stated, being chinese based is a no-go for me.
I've only done a little bit of GD but here's an [example](https://ibb.co/vzjpt9c) used the image for one of my album covers (original image is 3000x3000)
Not even close, ZorinOS is a well themed XFCE, but that's it. They have nowhere near the resources. Deepin has built an entire desktop from scratch and has modern app ecosystem around it. 

Also I'm still waiting for evidence that deepin is a spyware.
Zorin os is built on gnome.  I don't have any proof that deepin is spyware, but I DONT trust Chinese software.  That's why my OnePlus phone runs a custom rom, my tuya smart plugs have their shitty Chinese firmware ripped out and replaced with open source alternatives.  In short I don't like china.
Anytime the reviewer mentions the wallpaper like it's a revolutionary feature of the OS I click away

but yes I agree.
Most "reviews" I see on YouTube barely qualify as a virtual unboxing. Even supposed distributin comparisons just show moving the mouse around a VM with few quantifiable comparisons. Imagine a car review that showed the windows going up and down. I wish more would follow traditional tech sites (e.g., tomshardware) and do useful things like firing every distribution under evaulation up on representative hardware. Doesn't matter exactly *what* hardware so long as every distribution is compared on the *same* hardware. An old laptop with minimal CPU, disk & memory. A recent vintage midrange desktop, and yeah, your maxed out super box with 64GB RAM and 16 cores. Do some tricky installs. Run some benchmarks. The drive to fit everything into tidy 30 minute or less videos really kills the value of most of the video channels.
The flip side to this is how long a reviewer can run a distro. If a review is published over two or three weeks after the distro is released it's considered old news and out of date. 

Also if a reviewer is doing the review for work then they likely have a deadline (typically a week). They need to do all their testing and submit the article in under a week, giving at most about six days to run the OS.

Both of these factors make long-term testing very rare and usually only something amateurs who don't mind being a month or two behind release cycles can do.
absolutely.  But don't forget that the aim of the video is attracting viewers and monetize the channel :-)
Not to mention the distros, under the hood, are more than just the desktop environments. 
Lot of these Linux channels are pretty advanced too so it's mind boggling they don't cover what actually differentiates the os
The best review is to visit the distro's fora and read about the trouble people have with it.
That has been a major gripe for me with all these "Linux channels". I think Nick from the Linux Experiment does a decent job reviewing distros. To me, he broke the norm of "install a distro in a VM, show me the installed apps and wallpapers, done".
Techhut & Distrotube comes to mind cringe af
Sooner or later you‚Äôll realize that most reviews(especially on YouTube) are a waste of time and only serve as a way for someone else to make easy money. Generally speaking, market leader products nowadays are very similar in performance, quality, reliability, pricing, and design.
> There‚Äôs a lack of long term reviews, hardware compatibility reviews, and so on. The lack of long-term testing in particular is annoying; the warts usually come out then.

I agree it's a problem, but I can't see a reasonable way of solving. The hardware combinations is huge and, as you said, some problems only appear of extended usage.

Actually it's a problem for the general discussion of "Linux is good enough?": you have people saying "I'm using distro X for N years, no problems" and others saying that the same distro is broken from the start with similar hardware.

It's impossible to have a real perspective, only anecdotes without standard: a lot of users had small problems for a experienced user, they don't even remember of solving.
Ilike what Diolinux does. He installs the distro on real hardware, runs sinthetic, gaming, and productivity benchmarks, tries to install a few programs he uses, and talks about insteresting features. It's not possible to test a distro for a long term, because the 5 or 6 main distros are constantly updating, unless he makes each employee use a different distro (what may decrease productivity).

He used to do long tests, but doesn't anymore for this reason. He uses PopOS because that's a very popular distro newcomers may use.
Because most distros are just upstream but with the wallpaper and/or theme changed, so there is really nothing to review.
Agreed, give them a torture test, does it work out of the box on a laptop with an AMD A8 APU? Will it even load the live session or just a blank screen?
This guy does a "silent review" when he first buys a machine:

[https://www.youtube.com/watch?v=ITvmzWE8OdU](https://www.youtube.com/watch?v=ITvmzWE8OdU)

It's all about the wobbly windows...
Well, one of those is easy and one is hard.  That's the basic reason why things exist as they are.  But yes, I totally agree.
Indeed, how often do reviews report how well *upgrades from previous releases* work? After a few years of running Linux, that's become one of my most important criteria, and I don't think I've *ever* seen a reviewer look into it!

Granted, that's something which would actually require *work*, so I understand why reviewers skip it, but without it, the reviews are useless to me!
I agree emphatically and its unfixable in its present form because it takes time to discover the warts and interesting points. So unless your distro reviewer is committing to installing a distro and using it for a month you aren't ever going to get a useful review.

Instead you could do a MUCH better review by soliciting multiple users who had been using  that distro for 1+ year to write up their experience with it and synthesizing their experience into a long form article.

This wont get you  a fancy youtube video nor likes and subscribes but it would be useful.
Reviews are bullshit and a lot of the times are paid for. Everyone linux youtuber shits on Ubuntu for not being up to date and having the latest things, lack of wayland (till 22.04 came out). YADA YADA.   


Then before they finish the video, the suggest you use another distro, one that is based ON AN EVEN OLDER VERSION OF UBUNTU!!! insanity.   
Also Wayland is not there yet IMHO, 2/3 machines I installed it at I had a lot of issues with glitchy graphics and applications etc, and I have fairly new machines. Pop Os is Frankenbuntu and unless you plan to use it as is, you should stay away from it, since the moment you disable some of their extensions and add some popular gnome ones. Your desktop environment breaks. Just because something is an "interim" release it doesn't mean it has to be broken.
Totally agree, most distro reviews are something closer to reviews of theming + default applications, from someone who has spent between 1 hour, and 1 week at most with the distro. It's usually just first impressions and bullet points of what the broader community thinks of the distro.

 I find many/most distro reviews less than helpful, often actively misleading newer users as to what a distro is and what to pay to attention to.
You‚Äôre expecting too much from tech bloggers my dude
I agree. The things I care about are the software catalog, package management tools, how quickly they make patches available, and driver compatibility. I do use VMs to test these specific things out.
>I feel like most of the reviews on the Internet are useless

As are most distros?
Only you can really evaluate whether any distro will work for your use case. At its heart the Linux operating system is a learning experience. You will learn much more from personal experimentation than any review could ever give you.
Linux is just a kernel.  
A distro packages a bunch of utilities to make booting your hardware using said kernel easy and manageable.

Once you wrap your head around this, differentiating between distros becomes a LOT easier.  
As a linux user since 1999, the only difference between distros to me is their philosophy towards how recent or "stable" the package versions they ship are. This followed by packaging guidelines and other things like init systems. Most distro reviews are a joke, talking about look and feel.. you can make almost any distro "look" like any other, it's default appearance has little to do with what differentiates it from other distros.  


I started with Slackware linux, tried every distro imaginable for the next 10 years and have now settled with Arch linux.

You can now try every distro in your browser, no need to even spin up a VM - https://distrotest.net/index.php
Most posts I see about Linux read like vague copy pastes anyway‚Ä¶ agree with you totally.
I would rather they would stick to reviewing the crazy distros which actually do something different
I agree, most distro reviews are pointless. Most of them have the exact same software anyway. IMO what really matters is whether or not they ship up to date kernel and driver packages, which impacts hardware compatibility and which games you can play.
Honestly, I would level this complaint at _all_ product reviews. I do care if you had a good time for an hour, day week or whatever - but I also really want to know if things are still good in a month or a year. I think solving the general problem may be impossible though...
I agree with how underwhelming most reviews are, look at it in a VM, talk about the default programs, etc.  There usually isn't much hardware compatibility to discuss because for example if the review OS is based on Debian, it is likely to have the hardware issues that Debian has.  I think most people would get greater benefit from watching a discussion on the differences between Red Hat, Debian, OpenSuse, Arch, and the BSDs than a review of a new distro that is a combination of one of those base packages and a different DE. 
With all that said, they are great to raise awareness to new users.
100% agree. Also, 95% of all linux groups/forums consist mainly of "Rate my desktop"
Reviewers these days will put rando content nobody wants to hit the 20-30 min window. Honestly a ‚Äúlong term review‚Äù wording in the title of any review makes me click on it regardless of the product in most cases. Because you‚Äôre bound to see them point out things. Some big channels really don‚Äôt see the big picture.
Agreed. A distro will be coming out with a new release, and I'll head over to Youtube to see what's in it, and almost every one of them is just showing the install, some of the applications, and maybe clicking around. They never seem to really get at what I want to see, more of the nuts and bolts, more details about whatever specific DE that distro uses as its default (if it does), even things like "can I change the lock screen background, what would that be like?" would be immensely helpful.
The problem is not the reviews, it's the distros themselves. They have little to offer, no innovation, no in-house developed tool or feature.
https://distrowatch.com/
I wish there would be a channel focusing on the features under the hood; such as Fedora‚Äôs excellent integration and power management, s2idle support, btrfs, pipewire etc.
Or PopOS‚Äô systemd-boot manager, opensuse‚Äôs snapper and OBS, or any OS that implements TPM2 measured boot and encryption, FIDO2 integration, support of modern features like AMD SEV.. so far only phoronix does cover this properly but unfortunately their contributions are banned here.
Yes, they basically just boot it up and show you what apps they give you out of the box, and then spend the rest of the time reviewing the desktop environment, with no mention of what makes the distro different, or why you would want to use it over another.
>all the author does is fire up a live session, try to install it in a VM (or maybe a multiboot), and discuss the default programs ‚Äì which can be changed in 5 minutes.

Don't forget wallpaper.
yes and no, there is not that much difference from a distro to another is mostly just a matter of kernel and software version, unless is nixos
Yes youtube in particular is littered with people who have no business doing review videos.  Everybody wants to be a youtube star but so few of them are willing to do it right and put in the effort.
I don‚Äôt know about hardware compatibility, Linux is Linux.. if one distro doesn‚Äôt support something that another one does, it will probably support it in the next release. I feel that talking about hardware compatibility is moot.

As for long term I don‚Äôt know about that either.. if you use your machine just for emails and social media then day 1 will be the same as day 100. I‚Äôd much rather see reviews in the style of LTT: a bunch of preselected tasks and how easy it is to do them from an uninformed user POV, what kind of bugs show up etc.
It‚Äôs all about eyeballs, and these ‚Äúreviewers‚Äù just crank out video after video of useless information to generate revenue.

Do we REALLY need to watch someone installing an operating system and clicking around on a desktop environment?

Really?

How many times do we need to see the same thing?
in my opinion the best distro is the one you make the best for yourself
Absolutely agree. Wish I had the option to review, but I eventually gave up and had to try all by myself over time ü§¶‚Äç‚ôÄÔ∏èüòÇ
The thing about most Linux distributions is you can add Kernels, you can change the experience. You can add drivers. You can probably figure out what I daily drive BTW.

&#x200B;

I used all kinds of distributions for years. You will have to eventually add or subtract useful software at some  time. The best thing you can do is just buy a Linux laptop if it's such a headache. Tuxedo, Slimbook, System76, and other computers companies like Lenovo and HP that offer some laptops with Linux distributions pre-installed. Perhaps at the very least look up a machine that has good ratings for working with Linux distributions.

&#x200B;

If you are a hardware hacker, and put Linux on everything, then you should be writing the reviews.
Yes, they are almost completely useless. They never set up the network, or set up BTRFS... but at least we get a count of how many packages and an inventory of their wallpaper!
When it comes to content creators you do not pay for their work, you're the product (better your views) they sell to the advertising industry in order to make money. You're not their  customer.
Only partially. Due to the large number of distros, and the fact that many of them are geared toward specific users or purposes, it‚Äôs very difficult to have useful reviews for anything except the major players.  I suppose you could rank 1-10 or 1-5 in various applications like ‚Äúnew user‚Äù and ‚Äúenterprise server‚Äù but it would be tough to draw any useful conclusions beyond that. Best thing I have found is run out door myself on a vm for a few months and see if I like it.
Most Distro reviews assume that you already know older versions of said distribution so, they focus on what's new in the out of the box experience. As another user said, they are virtual unboxings of what is there on the default install.

Is there changes in the installation process, what is the kernel version, what default applications does it have and their versions, does it come with wayland or x11 by default, and what changes in the customization of the DE is there (Including wallpapers, that I agree is a funny thing to point out)

Hardware compatibility comes with kernel version and in some cases there are drivers that you have to manually install and are not available as easy, but that's something that can't be possibly listed on a review for every single hardware configuration there is (they give you the kernel version so you can do your research elsewhere).

Long term testing and usage most of the time come in other types of reviews like "used x distro for y amount of time and here are my impressions" or the performance test that some other people do with video rendering and the like to push what the computer can do, and that also wildly changes depending on hardware.

What package manager a distro have, the update frequency, and how far from most up to date versions of software it's available is also information that most of the time is not focused on because it is expected that you already know this information from elsewhere (the fact that debian has really old but stable versions, arch is bleeding edge, etc).

At the end of the day most linux distributions are almost identical, with the exception of the package manager, or the DE, and you can in most cases install everything you need and customize any distro exactly the way you want, so focusing on what is new on a new distribution release is the most relevant thing so you can have an idea of what you would need to do to get to your desired final configuration.
> Does anyone else agree?

Not really. Distributions are in a constant state of development with releases multiple times a year and with the massive range of requirements and hardware of Linux users it's not a realistic expectation for them to do a long enough review with a wide enough scope to keep everyone happy. By the time they've spent months doing all the testing and writing it up there's a new version X.y that's been released making some or all of the test data they had worthless.
I just feel like you can‚Äôt really review a distro well. You could rank distros for a specific laptop, or see which ones are best for gaming. You could set up a usecase and then test multiple distributions.

But if you take a distribution and try to cover how it behaves on different use-cases, you end up with way too many variables to ever make good conclusions. I don‚Äôt think any well maintained distribution even tries to satisfy all use cases. And if they do, they expect the users to self-select stuff like lighter DE for laptops.

Besides that we don‚Äôt have any new distributions. Distributions don‚Äôt pop out of nowhere. They have history and they evolve. So not only should you consider only a specific usecase, you probably should stick with the changes made in the latest release, or compare more broad categories like debian-based distros to arch-based distros. What individual version of EOS you use will matter *very little* for many important aspects.

So you need to not only define a use-case you review, but you also need to only draw comparisons to similar enough distributions.

The specific distribution and release actually only matters for those who are already using that distribution or similar distributions. They want very specific answers about their own usecase, not review of the pre-installed applications.

And new users should focus on wider questions: what they want to do with the OS, what hardware they have, and do they want rolling release or longer release cycles for stability. Once they figure out that, they should probably just pick the most popular one fitting that criteria. It‚Äôs probably the one with biggest development team. For them a review of a specific distribution is at best distraction, at worst perfect set up for disappointing linux-experience. What if the review is so old that the distro is now on life support from upstream releases? What if the review didn‚Äôt mention they shouldn‚Äôt be using even anything similar to that distro?
Couldn't agree more.
Most reviewers aren't going to take the time to do a full install on new hardware and test everything out. VM's are a way of life and while cursory, those reviews can give a potential user some insight to the distro that they might not otherwise have. That more thorough level of review you're looking for is the USER's job. No review is going to tell you if xyzLinux is right distro for your hardware; only you can verify that.
It is better when someone is reviewing something that has been out for a while, but there is such a "right here, right now" mentality in anything to do with the media that you don't often see this, even in a niche segment like desktop Linux. By the time any issues have come to light, everyone has moved on.

There are YouTubers who sporadically review stuff when they feel like it and will often review "mature" distro releases, but half of them don't get their facts straight. There is one with a review of PCLinuxOS who said it was based on Debian, when it is actually a fork of Mandrake which has been independent for 15 years. I commented on this and the reviewer deleted my comment. So I guess it is the only Debian-based Linux in existence that uses RPM packages...

I will read and/or watch distro reviews just to see someone kick the tires on something, but if I have actual interest in a distro for some reason, I'll download a copy and install it on real hardware. It's very rare to find a review that will give you an adequate picture of what a distro is all about.

And, as many others have said many times before, the desktop environment/window manager used is really more important than the distro, anyway. But it's even harder to find a reliable review of a DE/WM because of everyone's personal preferences. Too many discussions come down to "KDE sucks" or "GNOME sucks" when neither one does (or XFCE, Cinnamon, Budgie, etc.). They just work differently.

Honestly, coming to a place like this is a person's best bet most of the time. I advise about Linux to people here and on Quora and the first thing I ask is what they are using now and what they do with their computer. There is no "one size fits all." My wife and I don't even run the same distro, because what is best for her is not the same as for me.
In defence of reviewers. Most distros are so similar that there is almost nothing beyond fine grained nonsense to report on and the ACTUAL key details that differ distros apart only come out after months of use.

One example: the old distro Crunchbang. Its where I started with Linux truly. But technically it wasn't all that. Just debian with preinstalled Openbox and some fiddly things - what made that distro truly made it PERFECT was the forums that surrounded it (people who said "Oh you CAN do that, its tricky though so back up all your stuff, here is what you do!") and that is hard to report on as a reviewer. 

Then it comes down to finetuning and different distros work differently well on different hardware sometimes, so you cant go in to too much detail there either.
Agree
I'd rather see reviews on new and useful software than distro reviews.
I have found some reviews particularly helpful, but at the same time encountered issues installing to check it out myself. I simply state it in the comment section, so others are aware that sometimes all that pretty stuff may not work on your system. But there will be one that would fit. Sometimes you may have to test a few before you find it, don't force it‚Ä¶ the right one won't complicate your life.
I get what you mean but for better or worse, many times those things are pretty much the only thing there is to review. There are SO MANY debian or god forbid ubuntu derivatives that just change the wallpaper and the default programs or at best change the DE (god I really hate DE distros with the exception of the ones made by or for the DE devs).
Unpopular opinion: Ubuntu is the worst of them all
yeah, it's annoying. I wish there was a video focussing on package managers, software availability and ease of installation of different Distros. the Desktop Environment, and programs can be installed anywhere really.
Tyler‚Äôs Tech on YouTube does a nice review by going through the set up and distro to highlight what makes a distro he‚Äôs reviewing distinct.
"It feels snappier in this release." Says every distro review every, not realizing there have been no changes to improve the speed.
There are some 200+ distributions, that is probably the biggest issue for new comers. Unless this is whittled down to a hand full, it is unlikely that Linux desktop will see any higher adoption numbers. Sure reviewers can than review the pros and cons of distributions but most people don't care. They will continue using the default Windows OS and rather than deal with what is a very fragmented Linux desktop.
I mean, it's true but shouldn't be surprising. For the most part desktop environment and packaging is the only real difference between distros. For most folks into "distro hoping" it's mostly about finding a DE with pretty defaults. I think the people who actually care about package management and internals aren't going to watch distro review videos and it's hard to make that engaging.
Although hypothetically viewing an informative, proper review of a distro can be helpful, the best thing you can do is simply establish your own impression of the OS by loading into a VM and test it whenever you have sufficient time. 

I was exorbitantly thorough everytime picking an OS, reading everything. However, this doesn‚Äôt necessarily help you drawing a conclusion
There isn't much difference between distros tbh. What is the point on doing a review about mint and another one about ubuntu or zorin if underneath they are the same? The default apps are the different factor, and honestly it's what most people care about
When I look at these reviews, I keep in mind they are aimed at some hypothetic noob who could by chance choose Linux rather than Windows or MacOS, but indeed, for someone who has been using Linux for 25 years or more, it's rather dissuasive, especially since distros often put forward gadget applications that are their own and that we often know are not worth much (e.g. Hypnotix on Mint).

It's true that it would be interesting to have a page listing the compatibility of hardware and especially pointing out incompatibilities. But Linux has to survive in a competitive commercial world where the custom is to boast about one's alleged advantages and to hide the problems.
I have no problems running linux long term. It just works. However, sometimes an update can go bad and dork things up.
100% agree.  For someone like me who wants to try Linux again, this is what I look for and can rarely find.
I mean distro reviews in general are kind of pointless imo. At some point every Linux user notices that distro doesn't really matter, all it comes down to is ease of installation, release cycles and the package manager/the repositories almost everything else will/can be customized in the long run anyway. And, correct me if I'm wrong, but hardware compatibility mostly comes down to the underlying kernel. 

I think the main reason so many people/sites do distro reviews is that they get a lot of clicks because almost anyone starting out with Linux will start by searching "best Linux distro" or "Linux distro review/comparison". 

Imo pick a package manager and release cycle you like and than try out different desktop environments / window managers, see what apps work for you. 

For hardware compatibility I've only once had problems when I built a new PC earlier this year with a 12th gen intel processor which wasn't supported by the kernel Debian shipped, so I swapped to an arch based distro (arcolinux) which shipped the most recent kernel.

Also you'll learn the most about what distro suits you by just distro hopping for a while and troubleshooting on your own without blindly following some advice/tutorial you've found somewhere.

Edit: typo
It is really hard to do a long term review because Linux distributions are not too different from each other.

I would settle for benchmarks and package statistics of dnf vs apt.
Yeah they could have 2 reviews. One to showcase how the distro comes by default and a second part that TEACHES the user some tricks. In Feodra for example, you have to add that Flathub repository and 2 DNF repositories:

    sudo dnf -yq install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm
    sudo dnf -yq install https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm
    flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo
    flatpak remote-delete fedora

Without this, you don't get access to many apps you should have access to. This should be in every review, otherwise, many users will be like "wtf why are so many things missing?"
yeah i was looking for my first distro a few weeks ago.  Most of the reviews are worthless because it's what yu said, install it and default apps.  I did find a review of gecko and opensuse that were a review after 3 weeks of use.  Wuch is is a lot more help full
Not really. 99% of Distros are either Debian or Arch, thats it. Distros are defined by the default programs and configs.
there are many distributions and most are forks of ubuntu and debian. Therefore, what some offer is only a new desktop theme and some small modification of libraries. So you can expect something different if you compare fedora with ubuntu or fedora kde with kubuntu vs arch kde vs kde neon (neon is based on ubuntu but implements faster changes as a beta), even opensuse kde features and differs from others, as well as the clear linux gnome for all the work behind it.
Be the change you want to see
When the testing consists of firing up neofetch, I tend to tune out.
After years of distrohopping, I've come to the realisation that the best way to pick a distro is to search "*distro name* default wallpaper" and just pick the one that has the best one.
DistroTube in a nutshell.
like seriously, we have had this option for decades and you still comment on what shitty wallpaper the distro includes? who cares about them?
"The Linux Wallpaper", the useless sequel to "The Yellow Wallpaper".

Bargain Bin both. Just try it out.
Arch has the best default wallpaper!
That brought up a frustrated memory. I had started watching a linux cert training series and early on the presenter was showing how to change the wallpaper. After a few minutes they said something like "But I'm not sure if this is on the certification test". I closed the video at that. Sure, a quick how-to is ok but to present it in a training course and not even know if the material you are covering is relevant at all? Get bent.
Yeah, or if they praise it's default theme as a feature over another distro and I'm like..."You can easily recreate that, wtf?"

I feel like most of them are just posers anyways without any real value to their reviews.

I mean, on the surface it's all just visual differences between the distros. But it's only once you've used one for a while, that the you discover where the differences lie. The devil is in the details.

But these are things that you just won't discover if you just looked at a distro for 5 minutes.
I used to follow a guy in YouTube that always talked about iphone colors like it was a revolutionary feature. YouTube reviews can be a huge waste of time.
Doing those types of reviews is a lot of work and you need to know what you are talking about.  That's why most reviews look nothing like that.
Agreed. There‚Äôs absolutely no point in saying ‚Äúthe desktop felt smooth and responsive‚Äù when they were only running like 5 apps on a modern PC. One thing I appreciate about Dedoimedo reviews is that Igor actually tests out distributions on older laptops, and the results can be quite eye-opening.
> really kills the value of most of the video channels

To you and me, yes. To the channel? No. The money is made before you realise that the "Real Life Review" is just another "virtual unboxing" (I like that term).
But as long as the kernel is linux the computers would basically work all the same. Then it's taking in account the small differences between specific DEs. In the end we get to the topic of what makes a distro truly different to even matter existing.

I don't think showing different distros on a range of hardware would matter at all.
This is why it‚Äôs so important for distros to send out press releases ahead of time with an embargo date. Whenever we release a new version of elementary OS, we try to give press at least a week heads up and send them a press kit that includes our release blog post, logos and screenshots, and a summary with just the major highlights and most important messaging for that release
This is why most "professional" reviews are useless for the most part, and this doesn't only apply to distros, but also phones, tvs, or any other gizmo you can think of.
They only want to rush out as many reviews as possible to maximize profit, and this is especially true for youtube "reviews". They gey all their shit for free, and sponsor deals too. Have you ever seen a youtuber say something negative about a product? I sure haven't. They can't (or won't) risk not getting any more free stuff from whatever manufacturer they speak badly of.

When you read a 'Real life review' on the xda-developers site, it's often quite different than what you read on some high-profile tech-site, since the latter tend to favour how fast you can scroll a web page without the screen flickering, pointless benchmark scores, and wether you can max the graphics in candycrush.
Why are you approaching this from the perspective that distros are like new gadgets which have to be reviewed when they‚Äôre hot? An OS has to last a long time. It seems perfectly reasonable to me to review a distro for e.g. 1 month. Doesn‚Äôt matter if it‚Äôs a rolling release distro or a fixed release. It‚Äôs the long-term experience that tells you all about borked updates, broken dependencies, DKMS shenanigans, QA, regressions, how quickly the distro packages upstream software, and so on.
In that case, is it really a "review" or just an "unboxing"? Again, if I'm looking at cars, a road test is going to be much more meaningful than a video showing the window controls. I can only glean so much by watching a review fire up neofetch.
This. I remember when Ubuntu 21.xx came out last year and the internet was flooded with reviews of the ‚ÄúUbuntu release‚Äù that were _exclusively_ reviews of GNOME 40.  
Really, the main things that differentiate distros in my mind are package managers and repos, security updates, and release schedule.
This.
Nick's are the only reviews I watch these days, and occasionally Tylerstech. He is also a bare metal guy.
Even the YouTuber I respect most (to remain nameless) did a "performance comparison" booting distributions up as VMs. Everything has to fit the 20-30 minute format I guess.
+ Tyler's Tech. That's their king.
I can only think of The Linux Experiment actually doing long-term reviews of distros and that is only because his main content is not really reviewing distros, and so he uses the distros daily for work to make other content. And because he **uses** his PC for work and does not just tinker with it, he's only had long-term reviews of elementaryOS, Manjaro and Fedora.
Some things are consistent though. Debian Stable is bad on new laptops because it ships old kernels. Fedora and OpenSUSE take extra effort to setup Nvidia drivers (and Broadcom etc.) Some desktop environments have well-known regressions that users might not be aware of, e.g. MTP in Dolphin.
Sounds like what I'm looking for. I'll look for Diolinux. Thanks!
I think there are a few more distinguishing factors that should be noted:

* **Default configuration and installer abstraction** (does it pre-configure btrfs with snapshots? Can you enable full-disk encryption with one click? Does it automate snapshots properly? Does it give you those quality-of-life features abstracted away?) Is it a fully manual distro for advanced users who gives you white paper on how to set up your environment, or does the distro provide a ready-made ISO with all the right dependencies installed and services enabled? Are these ISOs provided for more than one desktop environment or does the distro prefer one DE? Does it have any distinguishing quality of life features not necessarily enabled by upstream? (say: automatic updates with automatic btrfs snapshots, auto-detection and setup of hardware that requires out-of-tree modules or lack thereof, etc etc) Does it offer any utility to tweak the distro further via a GUI (like YaST?) How is the battery life with the default configuration? Does the configuration of things like kernel parameters, sysctl, Intel GPU drivers etc etc deviate at all from upstream to favour performance / battery life in any way with further setup pre-applied (like thermald, non-default i915 parameters etc)? What init system is being used? What bootloader is being used?
* **Default security configuration**: what isolation technologies are enabled? AppArmor, SELinux or none? Does the system run a Wayland session by default? Does the installer offer an easy LUKS setup? Does the distro support secure boot and TPM 2? ...If it's not a fully manual distro, does it enable a firewall, right? What firewall is it using? How easy is it to use and configure rules for?
* **Software provided by the distro:** how large are the repos? Is there a distinction between free and nonfree repos? How are packages split? Can you opt out of downloading development bits of a package or is it all bundled together? Can you easily install debug symbols? Is there any integration with gdb to help you with that? How about obscure packages, is there any way for the community to contribute unofficial packages? (AUR / copr / openSUSE Build Service). Is any external package manager for GUI set up? (snap, flatpak) and does it have a distro-specific repo enabled as well as flathub? How is the package manager (performance, robustness, usability, features, failsafes)?
* **Distro design choices**: how are releases handled? How fresh or old are the packages? What are the quality assurance policies? How reliable can you expect a system installed with this distro to be? How up to date do you expect the software you're running to be compared to upstream?
* **Distro quality**: how is the distro maintained? Is security taken seriously? How quickly are CVE's resolved? How quickly are bug reports addressed? How often is the distro known to fuck up, how did they handle fuckups in the past? How bad were they, on a scale between a rough upgrade that slipped through and bricked some systems requiring users to boot from the previous btrfs snapshot and selling user data to Amazon without explicit consent? Does the distro make any attempt to keep up with modern technologies? Is the distro known for breaking down over major upgrades or is it known to manage just fine? Is it an amateur distro maintained by a bunch of people and is in reality just a tweaked version of another Linux distro or is it maintained by an organized community and an established distro?

These are all things that matter and do not necessarily come directly from upstream, I like to think of a distro as all the upstream projects that make up a functional Linux desktop taken and glued all together. They are not all glued together the same way, and the glue you use can make or break an user experience for a certain demographic.
As u/bobstro said. Only true to a certain degree.

In practice the useability of a Linux distribution depends very much on the use-case (which is a very subjective thing).

Based on the ootb experience I very much would prefer Fedora, but then there come the out-of-distribution problems. Since most research software in my field is developed on Ubuntu, it is just a major pain to get a Fedora system working for me. I know how to do it, since I work on CentOS on our cluster, but it's not something I want to have my students go through (wouldn't be a good first impression of Linux).

I do numerical simulations in engineering and of the 5 software frameworks I use only 1 is packaged for Fedora/Redhat. So on Ubuntu (or Pop!_OS, which is my current workhorse), it takes me 10 minutes to get going from a fresh install. On Fedora it takes me 15 minutes to download all the different sources, one to two hours to circumvent the library incompatibilities, and then around 12-15 hours of compilation time. The first time (finding those incompatibilities and the workarounds) took me 3 days from install to working production system.

An edge-case for sure, but there are many use cases with similar restrictions.

For balance: setting up a beowulf cluster takes me around 2 hours on a Redhat clone and it would most likely take me days on anything Ubuntu. So there are massive differences between distributions.

Even between variants of the same distribution: Ubuntu Desktop and Ubuntu Server have completely different network-management systems. So the old "just install a DE on a server distro" is going to lead to a really bad experience when you follow that idea on a laptop.

And speaking of laptops: something that no VM install review can tell you is how does it work on real h/w. In particular laptops. Out of the box support for Optimus? Battery life? Currently the two distributions that I'd let near my laptops (HP Zbooks) are Pop!_OS and Fedora. Ubuntu has (out of the box) 2h less battery life or so.

The differences in the distributions are more than skin-deep. When you look at things like kernel, scheduler setup, firmware-updates, xorg/wayland configs, power management, etc. - that's where you will find the differences. And then there's the systemd vs init differences when you go into the more niche distributions.

All of these things are completely ignored in many of these youtube reviews. Heck, in the time it takes me to watch these, I can download the ISO and fire it up in VM myself and start looking at what it takes to get my workflow set up.
Not to argue, but that's only true to an extent (IMO). Sure, once you have the OS running, they are very (very very) similar these days. But testing things like installation on older or hardware constrained systems, installing & running complicated apps (even if just games), and other "real world" comparisons would be a lot more valuable in terms of watching something. *Getting the system to that stable state* is of interest to a lot of people. Hell, a baseline of upgrading from the previous version to the current on typical hardware would be useful.
I'd like to see:

* Installation on an old laptop with limited CPU, memory, and disk.
* A "typical" mid-range desktop of the sort lots of people install on when they get sick of Windows.
* Sure, a 64GB system with 16 cores.

Come up with a variety of baseline scenarios -- just so long as the hardware remains the same for each generation of "reviews" -- and compare real-world user challenges rather than showing us trim.
Oh definitely. I remember that Mint didn‚Äôt support upgrades for a long time.
Sure but it would be nice to at least know some basics about how package management on X distro works, if the kernel is compatible with my hardware, how up-to-date are the main packages, what the update cadence is, and so on. It would save me a lot of time.
yup big agree. It seems like most desktop distros exist because of the combinatorial explosion of DEs x  package managers. I feel like only a handful of distros are novel enough to warrant existing, most are just shipping a fancy theme. Kinda unfortunate since the extreme fragmentation makes developers lives that much harder.
For me, it's all about native package availability, how recent the packages are, how easy it is to run a window manager on it, ease of installation, good support. This left me with ArcoLinux, and I'm probably not moving on from it ever.
[deleted]
I feel that used to be the case when there were a lot of copycat Ubuntu derivatives around, but they seem to have dropped off in popularity recently. The Ubuntu derivatives that are still around do seem to offer something. Mint will give you Flatpak instead of snap and Cinnamon instead of Gnome. PopOS has systemd-based full disk encryption (not mentioned enough IMO).
h/w compatibility is about more than driver support. It's about configuration, and that will differ massively between distributions. Slightly different scheduler settings can make a huge difference in, e.g., responsiveness vs performance, power management, suspend/resume behaviour, battery life.

Some distribution work well on laptops out-of-the-box, others don't.
Different distros ship different kernels, and sometimes they can be very out of date, hence hardware compatibility is an issue. Also, some distros definitely do a better job managing Nvidia drivers, and proprietary drivers in general.
> LTT

lol, dude is nothing but clickbait
> As for long term I don‚Äôt know about that either.. if you use your machine just for emails and social media then day 1 will be the same as day 100.

Not really though... Linux used to be known for it's stability and uptime. Not anymore. Many distributions won't last 100 days without dozens of major upgrades and possibilities for failure.

Linux is Linux, but distributions are distributions ... some are focused on stability, many are not.
Depends on the creator. If a creator has a Patreon account or something similar so their audience are a main source of income then the creator has incentive to make what people want to see, not what will feed the click machine. But if a channel is views/clicks only for income then, yeah, it's about getting eyeballs not providing what the audience needs.
All distros being close to the the same isn't even close to being true unless you are looking at 17 different derivatives of ubuntu.
You mean, not 3 videos on tiling window managers?
technical concerns aren't the only difference.  Vision and governance are important too.

Fedora tries to adopt new stuff regularly quickly and find it's ok to break things at least temprarily in support of that. For example wayland by default, and adotping pipewire and also wireplumber.

Some distros take their stances on Free software very seriously, while others don't.

Debian has a social contract AND a constitution. 

Others have a BDFL (benevolent dictator for life)

Others are more freewheeling. 

Those things matter too.
Here you go: https://angry-penguin.blogspot.com/2022/06/the-problem-with-manjaroarch.html
Neofetch performance is important!
If I see neofetch at all, I tune out.
My first distro was freespire back in 2007 *only* because it was the prettiest I saw. Not much else to go off of before you get started, really.
> the best way to pick a distro is to search "*distro name* default wallpaper" and just pick the one that has the best one.

That's a really dumb strategy -- if everyone followed it, we'd all be using Hannah Montana Linux!
Frankly this is probably as effective as any other method.
From the first distro I tried I only rememeber the wallpaper being very green as typical of a Suse, even before it was opensuse and Lilo was the best bootloader available everyone was using.

Then I also don't remember the walpapers from the Knopix distros I used.

But anyway, the internet barely worked at that time.
This dude is probably the worst Linux YouTuber out there.

If all you‚Äôre doing is showing off the defaults that some distro placed on top of stock GNOME or stock KDE, then what the fuck are you even wasting the viewer‚Äôs time for?

Put the distro on actual hardware and run some simple benchmarks, at the very least. How does the system perform at common tasks? How does the system behave when you ask it to perform basic functions, like playing media with proprietary codecs? Are there any limitations with the out-of-the-box experience and, if yes, what workarounds should an average user expect to perform? That sort of thing.

Good lord, I can‚Äôt stand this dude‚Äôs laziness. I guess when you make money pumping out videos you can‚Äôt afford to sit there and do some prep work or homework before you start recording, but man does this make for a horrible viewing experience.

DT is the Linux equivalent of an ‚Äúunboxer‚Äù. I can‚Äôt stand this kind of shallow, worthless content.
I definitely don't watch any of DTs distro reviews, but I find some of his software and customization videos pretty useful.
ikr? the man has cool scripting videos and such, but his reviews are so low effort it's just never worth watching (he always goes through the install and then shows which default apps are installed and that's it).
I prefer Gentoo's
I questioned this is a similar thread a while back. It comes down to: not everyone wants to, or is up for, tweaking umpteen dot files, etc.  So a distro that looks nice to them out of the box is more likely to be interesting and useful. If it means more Linux users, it's for the better.
A more useful review is comparing package managers, DEs and rolling vs stable since those really the only differences between distros for the majority of users. The mixed and confused messaging on a what distro even is doesn't help. It's all Linux.
Tbh I use Linux everyday and I don‚Äôt even know what I‚Äôd talk about in a distro review. The only notable differences between them is usually the package manager.
You are correct. To truly review a distro, you need to be more thorough. On some distros you need to read the documentation. I've seen Distrotube review a distro and not really know what he's doing because he wasn't properly prepared.
I like Dedoimedo for the same reason. We don't always have the same priorities or experiences, but we have a similar view on how a review should be put together.
> There‚Äôs absolutely no point in saying ‚Äúthe desktop felt smooth and responsive‚Äù when they were only running like 5 apps on a modern PC.

If it *wasn't* smooth and responsive in modern hardware I'd be fucking worried. That should be the default state of any OS, not something worth mentioning at all.
I'll give it a look. Glad there are still "in depth" (old-school hacker) reviewers.
What about using the same VM specs? That would help standardize the hardware and with a synthetic load (like Geekbench) it might change things.
But the kernels *are not the same* between distros. Nor are drivers.That's exactly why I'd want to see a "review" to include a couple of common installation and upgrade scenarios. Give would be users the benefit of the reviewer's experience. 

If reviews don't show installation challenges and only show cosmetic differences, then call them an unboxing or walk around. Rather than dumbing down our expectations, it would be better if content creators quit changing the meaning of words to fit their clickbait titles (IMO).

If you don't consider installation and upgrade challenges worthwhile, and differences between distributions to be cosmetic, what do you consider a "review"? Honest question, not intending to argue.
Agreed, that is helpful. But apart from elementary OS, the only project I can think of which does this is openSUSE for their Leap releases. Of the other 400-ish actively maintained distros out there, virtually none of them provide sneak peeks for reviewers.

Even then, that just gives the reviewer a week (or so) to test the software. It sounds like the OP wants a month or more of trial time to sake out the bugs.
[deleted]
Is a beta ISO included so they can do actual testing?
I love your distro. I don't use it myself but elementary os is my first recommendation to new comers to linux. I use arch btw :D
So they should just use the beta version then
> Have you ever seen a youtuber say something negative about a product? I sure haven't.

I think our YouTube bubbles must be very different.  My tech reviewer channels are full of people with negative things to say about products, who make it extremely clear when something is an independent review versus when it's a "sponsored showcase" (and explicitly not a review).
Can't say I ever watched reviews on YouTube for this reason. It's not a good platform for learning about things beyond the surface level.

You seem to be conflating YouTube "reviews", which are typically PR bits for products, with actual reviews where a journalist gets or purchases a product to honestly review. You're not likely to find those on YouTube very often.
I'm not, the audience is. People complain, a lot, if a review is of a distro that has been out more than a few weeks. It's usually not worth it to review something that's been out an entire month as readers will consider it outdated.

Also, as I already pointed out, professional reviewers are usually on a weekly schedule so reviewing anything for a month is impossible. It would be nice to run something for a month to get more information about it, but it's nicer to be able to pay rent. If people want in depth, long-term reviews then they need to be able to willing to pay for them. Right now almost all professionals are on a week or bi-weekly schedule because that is what the vast majority of audiences demand.
If someone is just installing and running neofetch then, yeah, that's more of an unboxing. A review should cover more about what the system is, how it works, and who might benefit from it, any obvious problems. Which is what I try to do, but there is always more which could be said. Time constraints though are always a problem.

Going back to your example with car reviews. Cars are much the same. Car reviewers usually get a day or a week with a car to test drive it. They don't get to take a new car for a couple of months to really sake it out, drive it in different seasons, etc. They have to get across as much information as possible from a few days of usage. Distro reviews are the same. Car reviews can't tell you how long the brakes last or whether the door will start sticking in 18 months.
Exactly. Most distros are fairly similar under the hood. They all ship different flavours of the same set of apps, sometimes with a unique DE. All of those things can be replaced or tweaked for user preference. What really matters is the philosophy and direction of the distro as a project, and those things don't tend to change too much between releases.
That's why I don't distrohop anymore. At the end it's only choosing the package manager/repos, if it's rolling or LTS, some custom packages that are preinstalled and default DE. That's all.

Maybe it's better to just choose between debian or arch based, choose the distro that has the default packages you like and stop distrohopping. (at least when you start using linux)
He does an excellent job at showing off why each distro he reviews is distinct.
> Fedora and OpenSUSE take extra effort to setup Nvidia drivers (and Broadcom etc.)

In my opinion, another consequence of the short term review, is the disproportionate importance given to the installation and initial setup process. It's important, really important for beginners, but it's only the experience of the first day of use. 

The update to a new major version, for fixed release distros, is way more important from the perspective of the user that don't want to reinstall everything, specially from an LTS to another.
I've forgotten to mention his videos are in Portuguese only
You should start reviewing distros following that script :-)

I'd watch/read that (read preferably, but possible against the trend).
All of this is awesome, but I think the hardest part and most ephermal part of distributions are the magic sauce that happens when you bring all of this together - the glue of the glue.

For example, one can go over lots and lots of interesting technical design decisions between fedora and arch, but the common user is still going to really have trouble understanding what using each distribution is like if you just go through the technical details. Whereas describing arch as a "hobbyist-focused distro focused on upstream minimality with a large community build system" and fedora as a "distro focused on professional usability, security and presentation with a focus on bold projects to improve the linux desktop" may be more useful?

 My phrasing still isn't useful, there is a lot to unpack and clearly you're smarter than I am. But i hope my point comes through? I think its vitally important to go through all of the details but then also step back and discuss broad philosophy, direction, community and your own subjective feelings. 

That last point I think is especially important. I feel like too many reviewers focus on trying to be objective - whereas I find myself preferring the ones who describe their subjective viewpoints clearly. Its why the only linux channel I can stand is the one with the inordinately buff man who is obsessed with elementaryOS's lack of desktop icons. I actually dont agree with him on design taste, but I dont mind because I can understand where he is coming from and why!
Dude, obviously not all downstream distros are useless. I was just making a jape about the countless arch and ubuntu based distros that contribute very little besides adding a wallpaper.
Btw, I guess I‚Äôm going off-topic from the original post, but I still wish that things were a little more organized. There are legitimate reasons why you would have a different distro for a different use case like desktop and server flavors, but there are still stuff that makes no sense for it to be fragmented like when you mentioned ubuntu desktop and popos. I‚Äôve only worked on strictly RHEL servers so I don‚Äôt have any experience to comment on the server space. But, for example, those research software shouldn‚Äôt only have an rpm version or only a deb, etc. And then there is the battery life thing with nvidia. So yeah, it‚Äôs really not that simple.
Do you know if that power consumption difference change with a tlp setup? I genuinely cant think why two different distros should have such a high difference in power consumption if you're running similar software - other than the default power profile being more focused on performance (whatever that means) vs battery life. 

Fedora does use power-profiles-daemon instead of tlp, but I assumed they did similar things under the hood. If tlp doesn't work for you maybe it would be best to use power-profiles-daemon on ubuntu? That would hopefully give you the battery life of fedora while keeping the convenience of ubuntu. And the setup there is annoying but certainly easier than fedora for your purposes.
Unless you use very common hardware (even if you use very common hardware) it's unlikely the reviewer is using the same hardware combination you are. There really isn't a reasonable way to test all of the thousands of components and millions of combinations of hardware on the market.

As for stuff like how up to date the packages are and the release cycle, you might be better served browsing DistroWatch than reading reviews for that kind of information.
If someone is finding themselves mainly bouncing between distros that are all in the Ubuntu ecosystem a similar process is building up what you want out of a desktop environment starting from the ubuntu-server install.
>	Different distros ship different kernels, and sometimes they can be very out of date

Yeah sure but eventually they all catch up, and when new major versions are released they usually ship a relatively new (max 1-2y) kernel. If you have bleeding edge hardware it‚Äôs much simpler to check which Linux version supports it and check what the distro ships, it makes much more sense than to take an arbitrary set of devices on each reviewed distro and go ‚Äúhmm yep works‚Äù.

As for proprietary drivers - that‚Äôs not the distro‚Äôs hardware compatibility, it‚Äôs ease of use. If a distro makes it painful to install nvidia drivers they are still compatible, just hard / annoying. So what you want to see reviewed is not hardware compatibility, but ease of performing a certain task - installing proprietary drivers.
Hard disagree. The Linux community lives in a bubble for what ux is concerned, his videos about the Linux challenge were a big wake up call. I don‚Äôt follow him or anything, I haven‚Äôt seen 99.9% of his videos, but I watched those two with big interest.
>Linux used to be known for it's stability and uptime. Not anymore. Many distributions won't last 100 days without dozens of major upgrades and possibilities for failure.

LOL what?  Linux in general is very much still known for it's uptime.   Hell, it's gotten even better these days as minor version glibc updates very rarely tank the whole OS. 

>Many distributions won't last 100 days without dozens of major upgrades and possibilities for failure.

Outside of rolling release, what distros are forcing major updates during a release cycle?   You should be regularly updating  your software and 100 days of no updates, just means you're a security issue waiting to happen.
In that case the audience pay for their work, which is not the case I mentioned.
Fair point. But to be clear I am not claiming there aren't differences just that they are slight AF in most cases (there are a lot of distros out there) - OR that the value of them is less somehow.

Some gal or guy or kid might be doing one as we speak and we SHOULD applaud that. If nothing else imagine the skills they are learning, even if its just "ubuntu + something else"? That skill, that comes from a community allowing experimentation for the sake of experimentation - will benefit us all in the future.

So just to be clear, I am in NO WAY saying "many distros bad" or for that matter "all distros are the same".... just "most" and thats not bad in and of itself either :)
Heh..No thanks.
A fellow arch user i see
It seems like yesterday that you could walk into Best Buy or Fry's Electronics and pick a distribution based on the pretty box ... only to realize it was a year out of date. Ah, the SIMTEL CDs were fun.
My first distro was MEPIS Linux which had awesome wallpaper. That might have been the very last release ever made of MEPIS. But it won me over to Debian and my first impression of Debian being rock solid never changed.
Lmao my grandfather bought linspire around that time
You're not?
[deleted]
I feel like he's self aware and just doing it for the memes half the time. I mean who tf can go full time Youtube with his audience size, can afford an office space, a ballin PC setup, and an even more ballin audio rack?
And honestly, I wish that'd be actually talked about.

Oh, this distro is *bloated*?  Cool buzzword brah.  The fuck are you talking about?  What actually is installed that you think has *ANY* tangible impact on the computer's operation when the app isn't running?

Talk about the package manager.  Talk about the repos, how many packages and how up to date are they?  Are you constantly having to compile from source without so much as an AUR helper because the stuff you want isn't available?  How often are you finding major issues in important packages in the repos as compared to other distros, how's the quality control there?  What is the update process like?  Don't you fucking *dare* say stability without explaining exactly what you mean, desktop users do not typically care about packages remaining the exact same so that their shitty scripts don't break, when you say "stability" to a general audience they think you're talking about reliability as in not crashing or having bad bugs in the DE; if you want to talk about those things, back up your claims with some numbers or specific examples that you didn't find on other distros.

Default DE layout - fair game.  A lot of users don't want to go through the process of changing out the DE or doing everything they can to customize it, if a distro has a quality DE configuration and a good list of default apps that's worth mentioning with some praise, but put that into the context that it is easy for more experienced users to ignore that and just swap shit out.

GUI tools, are they decent?  Are there prompts that might help make it easier to maintain the distro, does it handle Nvidia bullshit with those prompts well or does it even need to query you about the Nvidia shit before fixing it for you?

The kernel, does it make any notable tweaks to the kernel that might make it better suited for desktop use or playing games through Proton?  How out of date is the kernel, and by extension how much of a pain in the ass is it to run the distro on newer hardware?

What's the documentation like?  Are we talking Arch Wiki?  What about the community support?  Are you going to get flamed for asking questions, or not get a response?

Is the team behind the distro trustworthy?  Is there some sort of scandal where you might have reason to be suspicious of what software they're putting on your computer?

And, perhaps most importantly, how well does the distro actually serve the niche it is setting out to serve?  You cannot condemn Garuda for being far too bloated to run on a netbook, but you *can* criticize it for not being ideal for mid-to-high end gaming machines.  Debian the desktop needs to be judged by different critiera than Debian the server.  Gentoo not coming with a ton of GUI tools and precompiled binaries is why people would be interested in Gentoo.
I wouldn't, either. This is why when I had a blog in the 00s, I focused on new underlying piping instead of "new features of Ubuntu 6.06" or something. Free desktop specs that were likely to be widely adopted, Vala or Mono and how it might impact application development, etc. There was always something to talk about if you read a few email lists.
I stopped reading Dedoimedo reviews over a decade ago.  I don't remember exactly why.  But it might have been because the reviews were written from point of view of a Windows user who was clueless about switching to Linux.  So they were not "old school hacker" to me.  Maybe they have changed now but after seeing the negative comments here:
https://www.reddit.com/r/Fedora/comments/jtbkv1/fedora_33_review_by_dedoimedo_i_dont_know_about/
I decided I couldn't stomach even checking out the review.
Honestly, I don‚Äôt really care about benchmarks: firstly, there‚Äôs Phoronix; and secondly, benchmarks are measures of throughput, and don‚Äôt always correspond to latency. For example, Clear Linux might do really well on benchmarks, but it doesn‚Äôt take into account the fact that the desktop animations lag, that some apps take a long time to load, or even how long package management tasks take to complete.
That's not remotely representative though. VMs still suck at virtual GPUs for example and a real one will perform 10x better, be less buggy, support more features,  etc.
I‚Äôm not sure it would be realistic to try to give someone a month of lead time since we do monthly stable release updates. So by the time the reviewer was done, they would have a fresh round of bug fixes and new features ready to install. It‚Äôs probably not really worth including an issue that it would take a month of testing to find in your review, but that‚Äôs just like my opinion. You could start reviewing features maybe during early access and then spend that week in RC trying to break it? I‚Äôm not sure what the best solution could be there. It might be viable for a distro with a much longer freeze process to have that long of a lead time
Doesn't matter for rolling release distros, if there aren't separate releases then the reviewer isn't time constrained in the same way
Not a beta image no, but a release candidate image yes
I use arch, but typically recommend Pop
> I use arch btw

Good Bot :)

^---  
^I'm ^also ^a ^bot. ^I'm ^running ^on ^Arch ^btw.  
^[Explanation](https://www.reddit.com/r/linuxmasterrace/comments/v9thbo/whenever_someone_says_i_use_arch_btw_respond_with/)
‚ù§Ô∏è
No, when we make a press release we send out a release candidate image so it is either the same or nearly the same image as the final release
I need some better tech tubers to follow. May I ask for some recommendations?
Yeah, I did a poor choice of words, sorry about that. I have never intentionaly seeked out such reviews, but one does stumble upon them every once in a while. But yes, what you describe there is the majority of "reviews" found on youtube; pr  and money milking.
Sorry, I don't (think) I know your channel, but I'm interested. When doing a "comparison" or "benchmark", I'd really appreciate seeing it done on live hardware. Again, pick 2 or 3 old machines, stuff a reasonably sized drive in 'em, and carve out a bunch of partitions. Hell, even observing how tricky installing onto an existing drive is useful information! When the new version comes out, do an in-place upgrade. True, it won't show every problem but it can be very revealing. Some distrubutions worked fine on my old Nvidia card, and others failed completely for example. Knowing that a distrubution *requires* current hardware is good info for us linux types that insist on reusing perfectly good hardware. 

Using the car analogy, you can't run it for 6 months, but you can test acceleration, mileage, on and off road handling. More than just opening the hood and turning on the stereo in other words.

This isn't meant personally. I'm just grumbling about the prevalence of channels that over-promise on what they deliver. Hoping this is just a friendly discussion.
Yes! That is something I was thinking as well. In general, the things that differentiate distros can often be described in general terms, and don't change much. For instance, Arch is always going to have newer packages than Debian. That was true of Debian 9 and is still true of Debian 11.

Release-specific 'reviews' are somewhat narrowly useful to users of the distro in deciding if they want to upgrade to the new release. However I **rarely** see reviews aimed at this population.
I would caveat that defaults matter a lot *for distros designed to be beginner friendly* and otherwise probably worth a section when a distro like Fedora works to improve first user experience. But beyond that, the underlying stuff you mentioned matters the most.
The arch conundrum lol. An annoying day of setup but really easy mantainence.
Thank you! I'm finishing my degree so probably not now, but when I get more free time I think it would be help put together a nice **serious** comparative review of the most important distros available

EDIT: I also agree long-form written text with comparative tables is the optimal format for this kind of thing. The differences between distros that matter are a bit too complex to discuss over video and not lose the attention of the audience immediately. Showing off a different eye-candy theme over video is way more enticing than talking about the security benefits of using SELinux, Firewalld and Wayland on your setup
There‚Äôs a little-known YouTuber called Egee who‚Äôs been doing this for years.

His channel has very few views because he‚Äôs just a disembodied voice who never comes on the camera, but his distro reviews are outstanding.
> and clearly you're smarter than I am.

No

Aside from this I agree, I get it. Like, the nuances that make up a distro do create consequences that influence this or that use case more, and this needs to be expressed at a higher level when you're stating pretty much where the project is going and what it aims to prioritize. Sadly fanboyism doesn't help with this: every choice negates or compromises another since it's a short blanket and you can't have it all, but not all users accept it. For example, I have seen many Arch users who just don't seem to want to hear about the fact that the distro they're using does not prioritise stability and QA just because they personally haven't had a problem with it (yet, because don't worry, spend enough years on Arch and you'll have that rough update, it's just a thing that is bound to happen with a system like this and it's nobody's fault) since it takes some maturity and emotional detachment to realize the system you chose may not necessarily excel for anybody's use case‚Ä¶ or even for yours, people make suboptimal choices in life all the time and the distro you run can be one of them, but fear of realising this scenario is playing out drives people to went to discuss and justify their own choice to the world.

With that out of the way, yes! Users should be given descriptions that make sense like this one, but more technical users should be able to look at an in-depth comparative who tells them why the description says certain things, to convince them that it's not bullshit but it's the consequence of s series of design decisions.
> Dude, obviously not all downstream distros are useless

I took "upstream" as in "upstream kernel" or "upstream Gnome"

> about the countless arch and ubuntu based distros that contribute very little besides adding a wallpaper

I don't even look at those :-)
>  those research software shouldn‚Äôt only have an rpm version or only a deb

True. But who's going to pay for that? They do have tar-balls of the sources, so I can always compile. Best solution is most likely going to be a (transparent, which Docker isn't) container format. Currently playing with distrobox/toolbox.
Of course you can change the settings and adjust power profiles and CPU governor settings.

My point was that there are differences between the distributions when it comes to defaults and usability, which would be interesting to know in a review, other than themes and wall paper.

Btw, we are talking 8 vs 10h. Not 2 vs 4.
You don't have to test everything, but some common baselines would be useful. Given the cheap price of storage these days, I'd think a "professional" YouTuber (influencer?) could carve out partitions to boot the actual OS rather than a VM and run some baseline scenarios. Do this on an old resource limited laptop, and a mid-range desktop and you've provided some really useful data rather than a walk-around tour. Keep the previous version partitions and do an upgrade from an older version. Things like that.

I agree that once the OS is up and running, there's not a lot of visual difference between systems. Even package management -- the reason I wound up on Debian back in the mid 1990s -- is pretty smooth on most distributions these days.
It's exactly that "catching up" that I'm looking for the self-proclaimed experts to highlight. Sure, a distribution may *eventually* be as good as any other, but how will my install go *today*?

As far as buying new hardware, I agree 100% that you want to stick to fully supported hardware, but may (most?) linux users are making do with older hardware that can't handle MacOS or Windows well, but works well with Linux.
>  The Linux community lives in a bubble for what ux is concerned

No, no they don't. It's more like developer resources are thin, and people don't contribute.  People really really like using free software, but they don't actually want to help it out. So many people don't debug a problem or submit patches.   It's always just give me my free windoze and stfu!

There are no 1000 eyes reviewing code, it's more like the 1-2 devs working on a project, and the occasional random bug report forcing them to go back and review that code.
> LOL what? Linux in general is very much still known for it's uptime. 

 "In general".... because 99% of Linux installations are on server racks in data centers running solid, server distributions. That's not the topic of this discussion.

> Outside of rolling release....

Yeah, outside of those... but those are some of the most popular distributions. And others, like Fedora, aren't rolling releases but are still extremely aggressive in pushing out upgrades. 

 The point is that for the overwhelming majority of desktop users, day 1 will *not* be the same as day 100.
Mandriva Linux on sale at a Walmart near you. Ahh the good old days lol!
Burn the heretic!
No, I got a special dispensation from the Dear Leader to use Red Star Linux.
Me too!
> Oh, this distro is bloated? Cool buzzword brah. The fuck are you talking about? What actually is installed that you think has ANY tangible impact on the computer's operation when the app isn't running?


But it won't install on my 8" floppy drive !
Yes, so GPU pass through is still an option.
I agree, it is not realistic at all to ask developers to publish media for reviewers a month in advance. I'm not suggesting that at all. I'm pointing out that the OP is looking for months of testing time before a review is written and that isn't going to happen - it's not a suitable timeline for the developers, the reviews, or the audience.
Yeah ok that‚Äôs iust a naming thing then but great that it is included.
Pop feels heavy. IMO :

- 1st : Arch 
- 2nd : Fedora
- 3rd : OpenSuse

Absolutely new to linux ? Elementary OS it is.
Yeah, that‚Äôs what I was talking about. Different name, same idea. Ubuntu, Fedora, and openSUSE Leap all have release candidates that (most of the time) end up being the final release but they sit there before the release time. Reviewers should use those to stay ahead.
For hardware, Gamers Nexus. [https://www.youtube.com/c/GamersNexus](https://www.youtube.com/c/GamersNexus) Steve is not shy to tell what he thinks and backs it up with data. He also regularly goes out and buys stuff with own money so he isn't only relying on manufacturers sending him everything.
I don't have a channel, I think video is a poor medium for providing useful technical information. I mostly write for DistroWatch with occasional publications on other tech platforms. You might have seen some of my past stuff in BSD Magazine, for example.

I agree though about running distros on physical hardware, seeing how it performs, seeing if it needs alternative drivers, etc.
It'd be less frustrating if people were more up front that they were just reviewing the DE.  Yeah, the DE's extremely important, that's 99% of the computer to most people.  But I can get that DE anywhere, tell me why Fedora may be better or worse than Ubuntu without mentiong anything about GNOME that isn't a package version and release date, for the sake of comparing how fast each gets updates.
I completely agree. I think it's important to separate uncriticality from subjectivity. One can have their own viewpoints and express them but do so without blindly dismissing other people's arguments. 

I think there is a tendency to underappreciate design decisions in a certain way. For example, in arch, people will talk about how fast pacman with the implication that it's somehow programmed better than other package managers - which is a silly assumption! When in reality pacman is faster than dnf because it does less and because arch prioritizes a simple packaging philosophy over a more robust and stable one. 

Most of these choices that distros make are trade-offs, even the ones lots of people disagree with, like snaps, are not done out of the blue. And even if you hate snaps you have to take the reasoning and design decisions behind them seriously and make a good-faith effort to actually engage with them. Maybe a distro makes suboptimal decisions but they rarely make suboptimal decisions for no reasons. 

The implication that there is nothing you prefer, that you don't have specific use cases or idiosyncratic preferences or just things you find cool - the idea that you don't have a viewpoint is bad for a reviewer imo. I want to know where reviewers stand so I can understand where they're coming from. 

You're right in that there is a weird defensiveness when it comes people's distro choices though. I use arch right now and I completely agree with you that it does not prioritze stability or QA in any way, even though I've not had much trouble with it. And arch is very clear  that it does not prioritize these things, it sacrifices them in order to do other things it prioritizes higher. Yet when you talk to a lot of people who use arch they'll defend it tooth and nail as the most stable distribution, when that clearly flies in the face of all reason.

 I really don't get why so many people are boosters for they're distro tbh? I feel like I'm always the opposite lol, hypercritical grass-is-always-greener sort. Maybe because I grew up in a culture of semi-comedic self-hatred. When I got to my local diner I don't talk about how good the coffee is, I usually say "this coffee is shit but I love it anyway". Which is exactly how I feel both about arch in specific and about linux as a whole lmao.
It is quite difficult (without special hardware) to record the installation on bare-metal. While OBS can easily record a second screen where a VM-installation is happening.
Yeah, that is what professional reviewers do. You're describing what most of us do. If you're watching YouTube videos you're likely getting the quick-n-dirty first impressions rather than an in-depth review because that's where the views are and it's what the media is geared toward.
Catching up = shipping the newer kernel 

The kernel is where the drivers live, and whether a device is supported or not depends only (simplifying) on the kernel version.

If you have old hardware, 99% of the time, it either works on all distros or it works on none. It‚Äôs very unlikely that support for an 8 year old device gets added now. It happens, but very very rarely.

That‚Äôs why I‚Äôm saying unless you have bleeding edge hardware, the compatibility will be exactly the same no matter what distro you choose. The reviews should focus on something else.
Well said.  For all the rah-rah cheerleading and neofetch screenshots I see on reddit rarely if ever do I see a post bragging about a pull request or something.
>	No, no they don‚Äôt. It‚Äôs more like developer resources are thin

That‚Äôs not it though. We have gotten used to things that for us make sense and we take for granted while outsiders may find unacceptable, that‚Äôs what being in the bubble means. We have become complacent, the systems work well enough for us and we don‚Äôt look much further.

Again, see the LTT controversy. Should users really be trusted to know better than to nuke their systems? On windows you have to work extra hard (barring bugs) to brick your installation, on Linux you just have to copypaste the wrong command from whatever Japanese blog post you found - which is what you will end up doing when you ‚Äújust want to‚Äù do something simple (see for example his attempt to ‚Äúsign a pdf‚Äù).

Linus was trying to install steam and assumed that the system wouldn‚Äôt self brick just because he told it to. That‚Äôs an example of the bubble: for the general public that is **not** acceptable, however there is no significant effort to change this not because of available manpower but because of ideology - the same ideology that will probably make you disagree with my earlier statement about being unacceptable.

I‚Äôm quoting LTT a lot because like him or not he is a prime example of what happens when an otherwise tech savvy individual attempts to daily drive Linux without expert support. Imagine how worse it would be for much more inexperienced users.
Was Mandriva also for sale physical? I think I remember seing Mandrake before the merge with Conectiva.
Aye!! Time to get pitchforks ladies!
Happy cake day
At that point might as well run real hardware and have a spare Ssd in your machine for it.
No it‚Äôs not just a naming thing. Beta images are built from the unstable daily release channel and get pre-release updates. They are targeted at developers and known to be unstable.

Our release candidate images are built from the stable release channel and only get release updates. ‚ÄúRelease candidate‚Äù means that as long as we don‚Äôt find any major last minute issues, this image can be uploaded to the CDN for mass redistribution.
Pop and elementary are comparable in terms of resource usage iirc. Pop is generally easier for new users though, particularly those with Nvidia GPUs. Also more features (pop-specific features, not just preinstalled stuff) for both beginners and power users.
Please don't recommend Elementary for new users. Getting software is a hard enough task on it by default that new users might just go away.

Also, it kinds tries to be a bit oversmart with copying the wifi passwords from the live session to the final install and one of my beginner friend's wifi which worked on the live session just didn't connect after install. That new user just never booted into Elementary again.

You and I could make Elementary do what we want after a few tweaks (I've daily driven it for ~2 years), but let's not rec Elementary for new people just because its beautiful. For new people, its just a sandbox to play in with limited apps and functionality.
I just made [another comment](https://www.reddit.com/r/linux/comments/vvsc20/distro_reviews_could_be_more_useful/iflwfk1/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3) that beta and RC are very much not the same thing
Much appreciated! I'll add him to my subs.
I understand your point, I am the same way. I think the defensiveness comes  from insecurity and how people handle it. If a person truly is completely happy with their setup they don't need to convince themselves or others of why it's the best choice for them, no further action is needed. If you aren't happy with your setup it might be worth branching out and trying new things, which requires effort. After you settled down on a distro it can be legitimately draining to find out that distro no longer fits your preferences and you should do the work to move to another, take the time to adjust again etc. Much easier to just dismiss it.

As for the grass is only greener on the other side thing I think it stems from the fact that the desktop is not mature yet, even though it's developing at an unprecedented pace. I personally love Linux, but - I will be lynched for implying this - I think the desktop side still lagging behind Windows and macOS in a few important areas and that, most importantly, when the community advertises reasons to switch to Linux on the desktop, they focus on some false / frivolous ones that are easier to understand while leaving out reasons that I think are much more objective and compelling reasons to switch. The thing is, it's all a compromise and there is no single de-facto desktop configuration (distro and DE) that is objectively a better choice. For example, my specific use case and preferences are technically covered on Linux, but those features exist across various desktop environments, there is no single one that has them all. As for distros it's easier, Fedora or Arch are pretty much the sweet spot for me. But even then each of them has its own critical points that stick out like a sore thumb in daily use and make your life legitimately hard, be it Fedora's lack of packages or Arch with the obviously untested upgrades that tend to break things on more delicate setups, they create moments of frustration that make you wish you were using the other distro instead. With the pace projects are progressing at, though, this might soon be a thing of the past.
I'm definitely griping at the YouTube crowd for the most part. I'm just surprised when some of the more popular and (apparently) viewer funded channels take the lazy way out and still call it a "review". 

Damn kids.
Yeah ok I know there‚Äôs a difference between release candidate and beta but I was just typing quickly and didn‚Äôt think too much of it. But I meant just something to test.
I suppose you're right. I've not used it in a long while. But people usually like it for its mac-like UI. Haha. Fedora it is then !
I see, I guess it‚Äôs a different process with each distro. In Fedora, the beta images are built with (soon to be) next releases stable channels. When you install the beta Fedora image, you are essentially installing an early release of it because they share the exact same repositories, so the difference between the beta and release is very minimal. I guess reviewers should ask the distro maintainers how they do the releases to know what‚Äôs up ahead.
I agree with you there. People put a lot of very odd reasons to switch and are not very good at describing what's actually alluring about using linux. I'm not sure if linux will ever become 'mainstream' tbh. I think it can certainly become more user-friendly and more open to newcomers, generally easy to use, but honestly some amount of jank seems fairly unavoidable. Linux with mainstream appeal invariably becomes something like chrome OS or android, just by the nature of what those design decisions entail. 

I do think that there is a large middle ground between something a few weird elitists use and something that has mainstream appeal though. I'd be happy if desktop linux could get to and stay at 6-7% of the laptop market. Enough where you could reasonably get a work laptop with linux but probably not enough for most tech support to care about you lol.  

The only case where I see linux actually becoming mainstream in the pc market is when most people stop using laptops and desktops altogether, which may or may not happen lol. A lot of problems of desktop linux can be solved, certainly, and a lot of them have been solved. But linux users are a cantankerous bunch with a cantankerous collection of operating systems. I've never been convinced that new people getting in are any less or will become any less cantankerous lol. 

A side anecdote, because I'm just rambling now:

I remember when my apartment's ethernet broke. I had to call in tech support who kept on asking me what my ethernet ip address was. I kept on trying to explain that the DHCP failed so I did not have one, but he kept on insisting that I tell him - I even gave him a screenshot of the "ip a" command! After almost an hour of going back and forth he asked me if I'm mac or pc and after I answered he transferred me to level 2 tech support. Level 2 tech support immediately told me that there was an ethernet switch installed by my landlord in a screwed-in hatch underneath my washing machine, which is all I needed to fix the issue. I learned later that mac and windows show a default ip address when DHCP fails whereas networkmanager doesn't show anything at all. I've been thinking about how desktop linux could have solved this sort of problem, and I'm not really sure it could! Sure, networkmanager could give a default ip address if dhcp fails and a static IP is not set, but what about the hundred other programs that can manage your internet? And even if they did all have this behavior as default, what happens when a tech support person asks you "mac or pc?". Are they going to be able to do tech support for all of the popular DEs and distros - that cost exponential scales up for very little benefit! Perhaps one distro and DE could rule them all, but like I said earlier, desktop linux really doesn't seem to attract people who like conformity, and that seems as true today as it did decades ago.
Yeah, I'd never turn to YouTube for a review. It's just not a good medium for detailed information. Plus it's hard to search through a video to confirm a specific piece of information.
Linux Mint has been the de facto #1 recommended distribution for newbies for many, many years. And it still is today, as far as I‚Äôm concerned.

It‚Äôs everything good about Ubuntu, with all the bad Canonical decisions removed, and a bunch of smart defaults and QOL improvements, all running on a polished desktop environment that‚Äôs a good middle ground between Gnome‚Äôs simplicity and KDE‚Äôs familiarity.

Honestly, I think anyone recommending a distro other than Mint is just setting up that person for frustration. You can just install Mint and configure/customize NOTHING, and it will work flawlessly for a newbie just like that out of the box. You cannot say the same for most other distros.
Yeah we but a big warning header on our early access page that the images are known unstable and please don‚Äôt write reviews of beta releases etc
Btw, they have a different branch for beta (in traditional sense) called Fedora Rawhide. This one uses unstable channels and is essentially a rolling release (it doesn‚Äôt resemble any stable release).
YouTube works for the final results I think. A final summary and biased impressions are fine, with a link to the tabulated data. I've been away from Linux (except for the Raspberry Pi) for a few years, and am really disappointed to see the distro wars still being perpetuated with little factual information to back positions up.
Yeah, that makes sense.
Okay yeah so then rawhide would basically be our early access :)
OBS studio
OBS is awesome for this but I'd be interested to see how to use ffmpeg for more advanced operations, ie, compositing an ffmpeg stream with the "real" webcam stream to feed to the virtual webcam.
I am lacking imagination this morning.  It's interesting but what is the use case?
Exactly. Just add your webcam as a source if you want to do some color correction. Add multiple scenes if you want to showcase something else.
Took me all of 5 minutes with no reading to work this out after curiosity bit.  


OBS is great and presents as another "camera" input
Doesn't that just use v4l2loopback as well?
OBS is basically a nice real-time compositor. I regularly use it to add information and sometimes videos to my webcam feed.

ffmpeg and v4l2loopback are building blocks... And OBS is a nice customizable building.
Probably to make someone requiring it (teacher, boss, exam proctor) think the user is actively engaged and not faffing about.
It's handy for streaming video in conferencing software when you don't have a physical web cam, or authenticating with service which expect a webcam photo of you, or displaying a random image instead of yourself when on a conference call.
Happy cake day
Yes
As far as exam proctoring goes, there has been a large push to automate it so that a real person doesn't have to actively watch the feed.

That being said, if you submitted some non-live feed to them (could be a still image, or one with some jitter) -- I believe the backend software would be smart enough to detect that it is not a live feed (such as the movement is "pixel-perfect" or there is some detectable pattern to it, and not to mention the fact that it could also detect that someone hasn't blinked for a really long time).

Er go, using this for exam proctoring is likely not going to work.
Good point, thanks for the clarification. As I haven't had to take an online exam in almost 2 decades, I am admittedly a bit rusty on the ins and outs of it all.
> I think it's a bad idea to present them a UI that is kinda-like-what-they're-used-to-but-not-really and only make things more confusing.

I have heard this a few times now, but have never seen research on it. Is there any?

Anyway, arguments can be made just as well saying that UIs radically different from what they're used to are going to be jarring, and having some elements of familiarity helps orient them in a new environment. I think it balances out, and in the end you can just recommend either. They'll be mostly fine on either option.

Far more important is to ensure that the distro is easy to install and maintain, takes a "just works" over a "DIY" approach, and will have app and hardware compatibility on that distro.
Hard disagree. We just bought my boyfriend a new PC last month and he wanted to try Linux for the first time so I put Manjaro with KDE Plasma for him. He loves Linux! The man just wants to play his games and is not interested in deep-diving into the OS straight away but he was still able to overcome some minor obstacles he faced. We had zero issues with the OS installation, all hardware worked out of the box, Steam was installed, and the whole process took us maybe half an hour. He even regrets not doing the transition sooner. 

We as a community should embrace that Linux allows for vastly different use cases for vastly different users and that is a positive. I am a tinkerer and enjoy my i3 environment that I have build over hundreds of hours over the years but I would never recommend that to someone who just came from Windows (unless they have that inclination of course). I was actually very impressed with KDE Plasma and how full-fledged and sleek it is. 

So when we are talking about **intuitive** design, like OP did, we should take into account the view point of the user we are advising. Intuition is something that builds with time and if you say to someone that has used Windows for decades that the UI is badly designed you would be wrong because you are prescribing your bias about how things should be and creating artificial barriers to them.
Suggesting/recommending something is not a bad idea, forcing it is.
The biggest problems when transferring generally na√Øve users to Linux, at least as I've seen it, is not UI related, nor is it console command related (though that usually doesn't help).

The issue, as I've seen it, is that when things don't work, they ***REALLY*** don't work. This is ironically what turns them away from other OSes as well, but it's generally a huge deal breaker when a user goes to launch their FavoriteApp.exe and WINE throws up 20 errors and then quits. It also doesn't please them when things that ought to be easy, like logging in, suddenly don't work because SDDM/LightDM/whateverDM doesn't support a touchscreen keyboard well, out of the box, and without modifying an (to them) obscure config file.

I could go on, but generally, the problems are the same as switching someone from XP to Vista, or Vista to 7 or 7 to 8, etc. etc. People get used to things working as they expect, but then as legacy stuff is deprecated, or their old apps lose support, they read this as their OS breaking.
> Assuming newcomers will face difficulties no matter what (having to adapt to new alternative programs, having maybe some hardware issues, etc) I think it's a bad idea to present them a UI that is kinda-like-what-they're-used-to-but-not-really and only make things more confusing.

Alternatively, if you present a UI they are used to then they can focus on the internals of the OS itself and not spend 2 hours figuring out how to make minimise work or open new instances of an application.


> Having something more carefully designed and intuitive (like gnome 40+, imo) is more beneficial. It's a hurdle to learn something from scratch but at least they have a way to mentally separate the UI from windows.

Is there a meaningful reason to separate it? And while you say intuitive (which from my own experience and struggle in GNOME, it was not), I say more mobile-OS-like.


Frankly the answer is this: For some people approach A works, for some approach B works. But we have to recommend something, and we make that on a case-by-case basis generally by knowing what they like or by what they've told us. Saying we should only do A/B or, in this case, shouldn't do A/B under this premise is objectively wrong.
Meh. I've seen good results recommending Mint to folks wanting to transition away from Windows.

So far, a few ended up switching to another distro (Ubuntu, Fedora), only one went back to Windows, and the rest all love Mint with an intensity which borders on ludicrous.

I'd never run Mint myself, I'm Slackware forever, but it sure seems to work for them. Why not go with what works?
No, it's not a bad idea. Why are you trying to make things as difficult as possible for someone to switch? This isn't the 90's or early 2000's where we had to compile things to even get sound. 

I've been using linux for 20 years and TBH I don't get all this look like Windows UI that has to get rehashed every other week. 

There's only so many ways the UI can look, so everyone is limited. Windows looks like KDE since they "borrowed" a lot of the look from them years ago. 

How about everyone just use what they're comfortable with and like and everyone else do what they like. I find bars on the top and in particular on the side annoying. If you like them, great but not everyone else has to like the same thing. Why do we have top bars, why are we trying to copy Apple? 

I used KDE for years,and now I'm using Cinnamon.. Why? Because it is what I like and that's what works for me.
Why make it harder for people to transition than it already is?

This gatekeeping BS needs to stop so we can get more users using Linux, even if it looks like a Windows setup. 

Maybe it is up to each individuals preference? I personally despise what Gnome has become with their need to reinvent the wheel every release. Ive only used a Linux OS since day 1 of Mint 8, including Fedora 17,19,32 and then Manjaro and Endeavor but all of them with KDE or xfce because I like simple things that work. I like having a toolbar and a minimize button and I assure you that does not make me a Windows fan in any way.

Edit: read u/Conscious-Yam8277 comment as says it better than I can.
If you‚Äôre able to manage to install Linux, there‚Äôs no way *any* popular DE is going to give you any trouble. The difference overall is negligible

I‚Äôm glad I started on gnome and eventually returned because it‚Äôs unique yet simple
> stuff that has been there for 30yrs

Stuff that took decades to settle on its appearance, too, from an era where the main target market had no prior experience to ease the learning process. An era where technical challenges meant that the engineers and designers had more weight than the management, so "what looks good on the projector" (hell, no projectors, just overhead slides of sketched mockups!) did not overrule calculated, researched, and iterated design language.

You can see it in the current visual stylings of the past decade and a half, which eschew colour (except for the occasional vibrant action button that demands all attention with its full saturation), shadows, and outlines, all three of which your brain has learned, from the real world, to use when separating interactive objects in a space. Where old UI designs relied on intuition in turn learned from physical sight, real-world metaphors distilled into abstract shapes that still retain just enough of their original essence to be quickly grokked, new UIs are cargo-cult imitations of imitations of imitations. One notable thing is that early tablet UIs (windows 8 notably) stopped using top, or top-left lighting because it might be placed flat on a table. Yet that lack of shadow was mindlessly ported back to laptops and desktops where the monitor *is* oriented vertically, *because* flat panels had become trendy. But without that lighting cue to add depth, they had to compensate, create ever more dead whitespace to pad regions with, using a lesser tool to separate information clearly.
Gnome 2 was good, Gnome 3 sucks. Unity sucks. 

When someone asks me for a first distro to try I always recommend Mint.
I think gnome is the poster child of "different does not necessarily mean good"
No, it‚Äôs not a bad idea. 

A bad idea is to suggest new users to try something completely different, they will get frustrated and eventually end up going back to their previous system.
The idea is to make the learning curve as simple as possible. If it works like they are accustomed to, that makes the switch even more simple. No one with an IQ above 80 is going to freak out using Plasma or Cinnamon if they are used to Windows.

If someone is coming from Windows and wants to switch to Linux, I hand them Mint Cinnamon. If they are coming from macOS, I hand them Elementary.

If they get the hang of Linux and then want to try a different workflow, there are tons of options. But if you take someone who has been using Windows for 20 years and hand them stock GNOME, they are going to get frustrated and go right back to Windows.

You're a GNOME fan, obviously, and you talk about using Ubuntu as a teen. You were young and not set in your ways. Someone who is older may have been using Windows for longer than you had been alive. You think someone like my wife, who was 57 when she switched to Linux, is going to want to learn GNOME just to use her computer? Hell, she didn't even like Windows 10! But I gave her Cinnamon and she's used it happily ever since.
I would disagree, at least to some extent. People used to Windows seem to like Cinnamon's familiarity while also being at least reasonably customizable if/when you decide you want to change things up. 

As for what we should recommend to newcomers, I think we should try to adapt our advice based on their needs, at least to some extent. If they're used to Windows and want something familiar, Cinnamon/Mint is a great choice. If they actually want to try something new, GNOME is a great choice.
The first time I ever sat down to eat chicken noodle soup after eating a fair amount of chicken and rice soup over the years was almost certainly a non-event. I certainly don't remember it. It was chicken broth and a starch and some chicken, maybe a few veg thrown in. Switching from Campbell's to Progresso might mean bigger carrot chunks or a different broth, but it's a very similar experience.

The first time I tried pho, I spent a surprising (though brief) amount of time worrying: was I going to eat it properly? was it going to taste good? was I going to commit a cultural error by spooning with the wrong hand?

I happen to like trying new things, so those speed bumps are an expected part of trying something new. But if I were introducing my elderly father to pho, I would start with a chicken broth variety to establish touch points.

If I were introducing my wife to Linux, I would show her gnome because I know she likes that basic workflow and is a confident computer user. If I were introducing my elderly and computer-averse father to Linux, he would get Mint (or possibly Fedora with KDE). My child is already using Fedora/gnome and it's a non-issue because he's learned it at the same time he's been learning macos, iOS, and Windows in various locations.

Put another way, new users, just like old users, come in all kinds of varieties, and the first thing you need to do to get someone comfortable with something new is to know what they want and what they don't want, and while some generalization is reasonable when discussing a mass topic like "getting new users to use linux" linux offers the opportunity to tailor the experience to the user like macos and windows cannot.

Having said that, I try to avoid selling (most) users on "linux can be whatever you want!" because that will panic all but the most adventurous potential users.
I mean, people who make the shift to MacOS have to deal with a similar jarring experience UI/UX-wise and they're mostly fine. It's mostly the fact that Linux isn't an 'established' OS that causes complaints.
I agree with your opinion to a certain extent.
It depends entirely on the user. My mother won't care if she can't do the three finger salute, neither will she try to install Wine to run videogames, she just wants a task bar, a "start" menu, a video player and a browser. Telling her "no you need to get used to GNOME" is, well, not really productive.
Nothing wrong with choosing a gui with a 'start' like menu and min-max-close on the right

Nothing on Linux really replicates Windows anyway. The light familiarity is just a launch point for adaptation
The problem of windows is not the UI (although they are trying in some of the more modern versions).

Also, your good memories of gnome 2 come from the fact it was an amazing DE, that unfortunately was killed for the sake of change.
Found the GNOME dev.
This post makes no sense. "Don't let the users be more comfortable that's confusing! Make Linux as alien to them as possible!".

Like good job bro, now they think they hate Linux when its just the DE.
i use gnome, i like it but i wouldn't recommend it to people that want to try linux because it's too different and they won't have a good time with linux
Suggesting new users to use Gnome is a dumb idea - there fixed it for you
I wouldn't subject a *friend* to gnome, but skinning somehting to look like Mac/Win fits counter-productive badplan, I think.
TBH I would personally recommend a Linux distro that blatantly copies Windows' GUI down to the last minute detail, because it would mean a newcomer would be able to easily and smoothly transition from that to whatever they want to enjoy their own computer instead of Microsoft's.  Unfortunately, such a distro doesn't exist, because they all assert some opinionated differences (understandably).  If such a distro existed, though, it would be an awesome gateway.
gnome is a bad example, you don't have even a minimize botton per default... disgusting
It can be a difficult transition if they don't prepare beforehand by looking up alternatives to their software or the fundamental differences between Linux and Windows, so why not just make it even harder? Heck, why recommend Gnome, when we can go even further? Just plop them in front of a TTY, tell them "DWM is probably good" and leave. They are learning a brand new system which involves difficulties, so giving them anything that sort of resembles what they might know would just be confusing.

Sarcasm aside, it limits variables when they switch over, so you know what parts of Linux might actually bother them and which parts can be changed. It also provides a feeling of familiarity when someone knows generally how to do things, and might have to find the nuances rather than learning something completely new. And finally, I prefer KDE over Gnome and a good experience with Cinnamon, so why wouldn't I recommend them?
Newcomers to Linux fall IMO into two categories: those who are curious to try an alternative OS and those who are convinced from us, Linux users, to try Linux. For the first category the UI should be no problem, they are curious and eager to learn new stuff and they can learn to use the unnecessarily complicated but beautiful Gnome. For the second group I would recommend KDE or Cinnamon (more Windows-like), because I think Gnome would scary them away.
I use KDE people who get installs from me get KDE unless they ask for something else. Gnome is not intuitive, in fact it gets everything backwards.
I don't agree. It's easier to try something that is similar to Windows because even Windows-like distros are different than Windows.  


Trying something which is hard for a beginner might discourage from going deeper into Linux.
I have mixed feelings about this. 

On one hand I can see where you're coming from. 

But on the other hand, if you look at the Steam Deck as an example of a successful [somewhat] mainstream product that's shipping a Linux desktop, I have personally seen a lot of positive reactions to the KDE Plasma desktop mode. I think it's been really good to minimize the friction that Deck users have to face when computing in a pretty radically different hardware/software environment from what they are used to.

Plasma isn't my desktop of choice, and I've never been in love with the Windows UI paradigms, but I think it was a good call for giving Deck users a *somewhat* comfortable desktop environment.
You like different; good for you.

Most folks don't. They want something that feels familiar; ergo -- a Windows-like interface.

Just because it is a "dumb idea" for you; doesn't make it a dumb idea.
If someone cannot understand a different UI, recommending an entirely different OS is probably not a great idea. It takes a minimal amount of computer understanding to learn a new UI.

How many Windows users have switched to macOS?
It≈õ a very interesting point, but i guess people are very lazy like older ones, for exemple i change my mom to linux mint, and she dont care, she justs uses web browser and thats it, so the OS dont really matter.
How many times I told myself not to poke nose in such disputes? :-))

There are almost 2 left... Gnome and KDE. KDE replicates Windows ideas, but way too overloaded with different settings. 

Gnome inherits simplified "mobile device" UI. 

PopOS and that Deepin from China are something new and branched from mainstream ideas. 

Even if you dramatically want  the Win look&feel you won't have much success with XFCE, it might be close, but not the blend of Win.
It feels like a lot of people are misinterpreting what your saying as an attack on other WMs or as being super pro gnome and poopooing others.

That said I get what youre saying that we should be shy about recommending the different WM experience because that might very well be a plus for some reason. Sure enough I also got into linux because of those old compiz eye candy videos on linux showing all the cool looking stuff you could do and I fell in love with the stability, the speed, and the security of linux coming from windows.

Likewise I have also seen those people who are like "well I can hook up my family with linux and just make it look like xp,7, 10, osx etc and nobody will notice(until they try to run windows or mac software and get mad). Yeah those cases are dumb too. You cant trick your relatives into using linux because they will be mad when their windows stuff doesnt work. 

That said I think youre underselling cinnamon and kde quite a bit. Firstly dont forget that cinnamon started out as a gnome 3 fork so a lot of the hot corner, present windows and workspace view was cooked into the wm from the start.  Likewise while it isnt the focal point of the workflow kde also has hot corners, and touchpad gestures to present windows and workspaces and they even recently introduced a very gnome like overview mode cooked in. KDE gives you lots of customizability while evolving from a more traditional space.

I think its a little reductive to put down the "windows like" WMs as being windows clones when they still offer tons of features and customization and in fact in many ways can offer a more powerful and eye catching experience than gnome can.
Agreed. The people making these recommendations are generally lijux users stuck to the windows metaphor and using plasma or xfce or mate or whatever.
The windows clones like KDE and mint are a crutch tbh. Classic desktop experience my a**. They just look like the crappy windows UI
I agree 100%
Wrong approach.

**There are several kind of users, and some of them can't jump into the open source / free source ship right away.**

I have used some distros that are not Windowze UI that are cool, and I hate how slow are Windowze PCs, or how I can still use older hardware, ( 32 bits machines, anyone ? ), but ...

**Some of us have to use windowze, at work even if we don't want to, and new users may felt confused with the different OS / UI combos.**

I personally have my job's Windowze laptop running open source software with my employeer' required commercial software.

( I enjoy doing diagrams and graphic design with Libre Office Draw )

Having both non Windowze UI and compatible UI are necessarily.

I remember a story posted on Reddit long ago, where some guy / gal had an linux PC with a Windowze style UI.

Some old lady coworker ask him to use the PC, to print some PDF files, while he was elsewhere doing something else.

Later he realized he forgot his PC had Linux. The lady didn't have any problem cause the UI distro was similar.
I can see giving users a somewhat similar experience but most people understand different systems work differently yeah. The new comer's main gripe with Linux is that some of their weekly or daily tasks are GUI driven on Windows but for GNU/Linux it's a series of 10 commands each of which you have to google. 

Most normal people will adapt to things not being completely 1:1 all the time.
Most of the time they ask for something kinda looking like windows so...
Anyway, I always recommend mint to new users because it comes with everything a basic user should need. I could recommend something with gnome but I really dislike gnome so I don't know...
It depends on the individual. Mom and pop users that basically log in, clock the browser icon and go to facebook etc will not notice either way.

The bigger rough spot will be "power users" that make frequent use of keyboard shortcuts and dig around the settings. For them anything but 1:1 equivalence of either Windows or MacOS will be unbearable.
What you're referring to is called "germane load" in cognitive load theory in education, and you're right being able to draw on the familiar is absolutely demonstrably helpful.
I agree with OP. I have tried Linux a dozen times throughout my life and never liked it enough to stick with it. My last install was Pop with the Gnome desktop and I love it. I‚Äôve been using it as my daily driver ever since, which is about 6 months now. Now if I could just stop myself from typing bash commands without knowing what they do, maybe I could stop breaking parts of the OS. Lol
Agree.  Some folks want something 'familiar enough' to get going.  Mint / Zorin etc do a great job of providing that IMO.  Plus they're good distro's in their own right.
Lol, I'm not forcing anyone, just saying my two cents since every time switching to Linux is recommended I ONLY read comments about cinnamon or KDE
Absolutely, you said it better than my original comment!
Yes, an EndeavourOS KDE user!


Saying "Windows" is not allowed btw. (I'm joking you aren't violating sub rules but rather Linux community "rules"


Say "Windwoes", "Windoze" or "Winblows" lol
How dare you hate on Unity like that /s

But yeah valid, I hated Gnome 3 initially and liked 2 and Unity, but then grew to like 3, but then the more locked down nature of that still pushes me away so *shrug*

>Hell, she didn't even like Windows 10!

Exactly, let's put it like this: if someone is so done with windows that they're open to switching OS I think they won't be disappointed in trying something new and fresh
Sending someone who's used to macOS over to elementary is a great way to frustrate them lol.
I am doing this switch now at work. I am mostly fine, but would be more productive in a step with a standard keyboard which I am used to, instead of the mac keyboard. The same for window management, I found a way to organize them, but with windows it would be easier to add new windows on my setup. And I could restart the PC more often, without losing time with the setup.
It depends on who the person is.

If it's someone who is working a 60 hour a week job and has no time to figure things out and wants things to just work (like the LTT videos), then familiarity is important.

But if it is someone curious about trying something new and has time to explore, it wont be an issue.

Anecdotally I once installed gnome for a student who had experienced a hard drive failure and needed something quick. She was actually pretty pleased with how it worked. It did the job until she got a replacement laptop.
I think gnome would be more intuitive for people who aren't used to windows that much as it's more mobile like where most people certainly have experience in using phones.
GNOME has a classic mode which is is a bunch of plugins to add a menubar and a more WIMP user interface. Makes a good transition point from Windows or Mac.
There is also the uncanny valley effect. If it gets so close but slightly different, it will be more infuriating when things dont work the same way.

"I got a new webcam and the cd doesnt install the drivers!"

If you know it is a different system that works differently you.wont expect things to work the same as often and depending on your mindset you may be more willing to research or find how to do what you need.
At least ZorinOS gets pretty close. It even use WINE or install from repository when you try to open an .EXE
Having a minimize button in Gnome would barely make any sense. If you have 2 monitors, you learn to appreciate virtual desktops. Or, that's at least what it was like for me. If you had... let's say, 6 monitors, you wouldn't really need a minimize button, would you?

And virtual desktops are like virtual monitors, meaning more space on your desktop. And, minimizing windows is much more confusing than having them on some virtual desktop, since they are hidden when minimized. And minimizing an application, then opening another one on top of it, then having to minimize that and so on... is just awful.

I don't think the concept of virutal desktops is too hard to grasp in Gnome, since it kinda forces you to use figure them out, especially with the missing minimize button. Imagine a world where you don't need alt+tab... it's amazing, and I've been living in it since I started using linux on KDE Plasma. I prefer Gnome tho.

About the missing maximize button... you can maximize windows by double clicking the header bar or by dragging it up. In my opinion, it's pretty intuitive.

Really though, even the close button could be removed... it would be a bit of an inconvenience having to enter overview to close apps tho lol

Anyways, I think useless buttons are pretty disgusting, when you can get rid of them just fine, while also forcing users to learn a better way of managing their desktops. IMO everyone wins in the end.
I'm not saying gnome is the be all end all.

I recently switched to it (on Fedora) and it works so amazingly well on a laptop. I've been using Linux for well over 10 years and never really used workspaces. Now that the gestures are so effortless and the overview is so central when opening applications or searching anything I'm just naturally pushed to organise things that way.

Also it sounds dumb but there's no use in minimizing if the desktop is empty üòÖ
Ok that's a good example, but I think it's a bit of a different scenario, where people are brought to Linux as a "side effect" of buying a product that has a very specific prime purpose than just being a Linux device just for the sake of it.

I might have phrased the original post poorly (I'm also not a native speaker) but what I was talking about were those discussion where people are *actively* asking recommendations on what distros to try because they are tired of Windows and/or want something new, yet people mostly recommend distros with windows-like UI
Disagree. Most of learning the ins and outs of the OS is optional. Knowing how to interact with the UI for basic computer usage is not.
Yeah, and even worse, having a similar UI is setting the user up for failure when things don't work like they'd expect. It superficially looks the same, but doesn't work the same, and that's an issue.

I'm not saying things should be different just because, but I don't think the argument that "it looks like something they're used to" is a good thing either, because fundamentally it isn't something they're used to either way.
I switched from Windows 10 to Linux Mint at age 25 while working partially as a developer and sysadmin. 

I don't think you can generalise the reasoning down to anything near as simplistic.
They're not clones. Windows actually copied a lot of KDE ideas.
There are also the Marilyns of the world who will F R E A K if anything at all is unfamiliar or changes. An icon on the desktop not being in its place will generate a helpdesk ticket.
> For them anything but 1:1 equivalence of either Windows or MacOS will be unbearable.

I don't know if that's true generally speaking. Usually the people not willing to learn "what is over _here_ on Windows is done _this_ way on Linux" are usually just difficult people who aren't going to be happy with anything you present them with unless it's just straight up a Windows ISO with a valid license key (because that's what it would ultimately take).

Most people are alright with things being a bit different as long as they're not so different as to be incomprehensible.
That's great. But I know many people who did not like GNOME when they started out with Linux. They were happy with more traditional-by-default interfaces.

My point is that you can't generalize either recommendation as a "dumb idea" as OP does.
Nobody is accusing you of enforcing it. It was Just stated that it‚Äôs bad to enforce.
I think people recommend what they like or have used and if people like cinnamon or KDE they will recommend it to others as well. If you are seeing more comments recommending cinnamon or KDE that means more people like and use them.
I haven't used cinnamon much. It wasn't compatible with by hodgepodge of different resolution monitors, but KDE plasma is fantastic. I would recommend it to anyone, Windows user or otherwise.
Thank You, I like having a minimize button as well... I can't stand that hover feature and see 5 different boxes pop up on my browser.. First thing I shut off. 

I want simple, easy and it just works. I'm trying to work, not constantly re-invent the wheel.
no

stop

saying that shit has been universally considered immature for quite a while now
Why are you like this?
To be fair, I gave Gnome 3 a fair shake several times over the years to see how it has progressed. I started with Gnome 1.x back on Redhat 6. Those were the good times, when misconfiguring X11R6 could set your monitor on fire... Memories...
What if they don't want "new and fresh," but "reliable and doesn't spy on me"?

I couldn't care less about "new and fresh." Neither does Kay. We're middle-aged people who use computers a lot and are perfectly happy with the traditional desktop paradigm. We wanted something that worked and didn't spy on us. Hello, Tux.

Different people want different things. You know what's great about Linux? Whatever it is you want, someone has it to offer. There is no "right" or "wrong" approach and no one is "dumb" because they choose differently from another.
Well, do you know any better alternatives I can recommend?
That's why in my hypothetical ideal gateway distro, it would interact identically, without any flaws like that.
That's sort of my initial point, but people here think I am a gnome crusader or something
Then take away any mobile OS except Windows Mobile from Windows users.
So that means that KDE and mint don't look like crappy windows but windows looks like crappy KDE and mint?
You can get minimize in gnome.      
         
!>As long as you installed gnome-tweaks<!    
I didn't communicate this really well.     
I shouldn't need a tweeks app and extensions to make a desktop usable.
Nope. I will keep calling windows names wether you like it or not, because I hate that pos
Seems like a /s dropped from my comment.
> in my hypothetical ideal gateway distro, it would interact identically, without any flaws like that.

What will you call this distro, 'Lawsuit'?

Even if you could actually create something that will behave identically, which is not guaranteed, lawsuits would swiftly approach because look and *feel* are copyrightable things.  Behaviors and interfaces are patented.
Some may have been burnt by trying to convince users unwilling to learn a new system.

When the user has time and more importantly, interest,  it's not as big of an issue.
A mobile OS is different in that you can barely learn anything about the OS unless you're a developer. The only learning curve to consider is the UI.
I don't like Gnome.. Never have.... 

Yes, as long as you install Gnome tweaks because the devs go out their way to remove anything useful that people might actually want.
As long as you are cool with not being taken seriously; go right ahead.
https://i.kym-cdn.com/photos/images/newsfeed/000/738/025/db0.jpg
https://osgameclones.com

As long as it's reimplemeted from scratch and no assets are distributed, it's fine.  See fheroes2, openttd, civone, Open Syobon Action, CroftEngine, etc.
Which is exactly the point.
I get ya there, but I do really like how the MacOS desktop behaves. Gnome tics some of those similarities ootb and Ive never really looked into pantheon.       
If MacOS was arch, I'd never look back.
And I'm not going to take you seriously.
Lol sthu
Agreed, I want a macOS-like GUI. I got away from Windows and would like to keep it that way as much as possible. (Except for work... which MS clearly has a monopoly on business computing and nobody bats an eye)

Ubuntu 16 is super mac-like, and there's no real modern replacement for it. The global menu bar was axed and very few seem to want it back. Elementary OS is always suggested as a Mac-like distro and although the visual appearance resembles it, the fundamental use of it is not.
You may find [Ubuntu Unity](https://ubuntuunity.org/) to be of interest
Is this actually any good? I still take the so-called "Linux community consensus" with an enormous dump truck load of salt because of how incredibly wrong they were about Unity. I would love nothing more than for Unity to beat viable desktop environment for Linux again.
I tried it, and didn't like it, it felt very clunky to me, but I never used the original unity desktop, so I don't know if it's worse than it used to be.
You‚Äôre the actual libreboot dev? 
That‚Äôs worth one of my boxes trying libreboot. 
Very cool. Thanks. This‚Äôll teach me more about BIOS and grub.
Isn't it extremely dangerous to remove the microcode updates from the CPU?

Does this mean you leave the CPU open to all the transient execution attacks, where the recommended mitigation is microcode updates?
Are there any modern laptops in the world that can run Libreboot?
Hi Leah, did this issue you raised ever get resolved?

https://web.archive.org/web/20161207233755/https://libreboot.org/gnu/
Stallman loves you.
I don't mean to crap all over your efforts, but this still supports only Core Duo hardware from like 15 years ago? What real-world applications are there for this?

edit: wtf people, how's this offensive? this is akin to producing PPC software *today*, that's the time period. asking why this exists outside of r/retrobattlestations isn't offensive and wasn't intended to be.
I await the day I can use it
What's the difference between grub and libreboot?
I didn't know that software like libreboot, able to give me back control of a boot which is more and more locked to prevent customization, already existed, but I already felt the need for it, and I might use it when I can be pretty sure to do it without damage.

As for the security argument, it is constantly used to make us accumulate crapware and incessant updates that are so many modifications of our system out of our control, and I prefer in the end to trust the seriousness of FOSS, which is more disinterested and leaves me more in control of my decisions.
Seems cool. Wish it worked on more than a handful of old computers.
[deleted]
Stallman will smite you leah!
Awesome news! Keep up the good work!
Yeah, I actually wrote about that here: [https://libreboot.org/news/policy.html](https://libreboot.org/news/policy.html) 

I have a fork of Libreboot that I maintain, that includes microcode by default (including on libreboot-supported devices): [https://osboot.org/news/policy.html](https://osboot.org/news/policy.html) 

I'm planning to merge osboot into Libreboot, and make Libreboot build images per current osboot policy, on the next Libreboot release; current Libreboot policy will be adhered to if using a special parameter when building. This merger is actually described in the release announcement:

https://libreboot.org/news/libreboot20220710.html#planned-osbootlibreboot-merger
It depends.

As I always say, 
Don't cloud the issue with facts.
Libreboot is a coreboot distribution specifically for devices that can run without blobs, no modern CPUs will boot without blobs.

A few new laptops are sold with coreboot, Purism, System76 and Star Labs sell them. Also, every Chromebook runs coreboot.
I made peace with the FSF years ago. Let it rest.
Plenty you can still do on such hardware, in fact [libreboot.org](https://libreboot.org) runs on said hardware. The server is literally a thinkpad. I compiled that libreboot release from today on a ThinkPad X200 Tablet, which I still personally use for all my daily computing. Also there's osboot. See: [https://osboot.org/](https://osboot.org/) \- supports newer hardware, and can support anything from coreboot, because of different policy.

Read this: [https://libreboot.org/news/libreboot20220710.html#planned-osbootlibreboot-merger](https://libreboot.org/news/libreboot20220710.html#planned-osbootlibreboot-merger) 

The next Libreboot release will support a lot more hardware. I did this release today to complete a year-long project; getting the first stable release of Libreboot after many years. I had 2 testing releases last year that had issues, but I'm happy with the release I did today.

The next release will be better.
grub is a bootloader

Libreboot is a Free as in Freedom BIOS
one is grub, and one contains grub
[Why use Libreboot?](https://libreboot.org/#why-use-libreboot)
It makes a lot of sense to let the user and their individual threat model decide if they need the microcode updates or not.
Reading the referenced links, what are your thoughts on the 3mdeb and dasharo efforts with the msi z690 motherboard?
Oh.
I get it now. Why are people down voting an obvious honest question?
Yeah they do good work. I want to integrate some of it into my project. I'm happy that they exist. The more coreboot devs, the merrier.
People suck lol
Thanks for the feedback. I‚Äôve been on the fence as not all opinions are supportive (when are they), but as someone that actually contributes to the effort, your opinion carries much more weight for me.

Edit: on the fence and about purchasing.
Yeah.
I'm using Dasharo with the Z690, it works very well for me.

It doesn't have all the overclocking settings and fan control, is that the features you are missing?
I‚Äôve not purchased one yet, however, if/when I do I‚Äôm unsure if I‚Äôd purchase an unlocked cpu or not (cost dependent). I tend to UV my ryzen systems to keep energy/heat (and fan noise) down. I‚Äôm not really fussed about squeezing every last drop of performance out of my machines by burning more energy :)

Edit: Thanks for the feedback - this was supposed to be the first sentence.
I was looking at system 76 for my next laptop. Now I'm looking framework.
Two of them are the 5700U... Which is actually Zen 2, not Zen 3. It's basically a 4000U mobile chip. The one we actually want is 5800U. Or even better, one of the amazing 6000 series chips, which basically has TB4 support.

Anyway, for me the best option was Framework, as they've finally updated to 12th gen. Just waiting for that to ship, annoyingly. Hopefully Linux support didn't regress or anything.
hp dev one has sparked my interest
Sidenote : that open source ai translation tool seems very nice
I wonder when we will start seeing these in stores for people to play around with
I personally can't justify the price of any of these when I can get an equal spec Windows laptop for much less and just throw Linux on it.  I have been on the fence for a System76 Thelio, but again, I've only ever bought one prebuilt desktop ever.  I just buy the parts and build them because it's way cheaper and works just as well, or better since it has exactly what I need.

I bought an ASUS ZenBook on sale for $1500 (CAD), regularly $1700.

For an equal laptop, 14" with  i7, same RAM and SSD size:

Framework : $3115

System76 : $2466

Threw Linux on my ZenBook and it works perfectly.  My favorite laptop so far actually.
Not that I want to rain on the parade (I love the idea of more and more preinstalled Linux machines being for sale), Slimbook and System76 are still just using white labeled Clevo (and other) laptops as far as I'm aware, and Tuxedo does not have a good reputation at all. 

I personally am excited to see more come from Framework, who is truly innovating in the hardware space like almost no other OEM is. I hope they also embrace more interesting software like Core Boot. 

The RISC laptop looks interesting, but their "preorder" page looks extremely suspicious to me...like at best, it looks super amateur hour and at worst it makes me wonder how legitimate it all is.
What‚Äôs up with all the crypto garbage on the risc-v machine?
The Tuxedo one looks fantastic, though it's still powered by DC with just one USB-C port, which is rather annoying.  Anything I should know about them?
getting better\*
Surprised they didn't mention the StarFighter 4k laptop by StarLabs
None of them will ship in my country. Order will get delayed and eventually get canceled - customer service is horrible in my country. Amazon or other local shipping stores don't have linux laptops
The Slimbook and the Pulse are effectively the same TongFang ODM hardware offered by two different vendors (and since I own a Pulse gen 1, lemme tell ya, the keyboard is utter trash and won't last, which is why I'm typing this on an HP Dev One) .  I wouldn't bet on that Roma laptop shipping anytime soon if ever.

Lemur is legit though ;)
> ROMA is Web3-friendly

The fuck does that even mean, also, just no, eat shit with your NFT laptop
Get a grip.  All laptops are Linux laptops.  MS insist on being included but you simply wipe its arse off your new toy.

If your distro can't do secure boot then go in the BIOS and turn it off and crack on.  It's pretty rare these days that hardware doesn't just work.  I speak as someone who endured the nonsense of NDISwrapper on only one device.

New hardware inevitably delivers new and exciting opportunities for driver disaster but generally Linux is a first class citizen these days (or at least allowed into carriage B provided it only smokes menthol fags and puffs out the window, whilst looking a bit dangerous)

I run an IT company and insist on Linux for me and the boss (wife).  Every now and then we buy new lappies.  HP, Dell, Lenovo, whatevs - they always work.

Just before the second lock down (UK), I grabbed an old laptop off the shelf and slapped Mint on it.  She still uses it - it insists on simply working and granddad updates it every few weeks (ISO27000 and Cyber Essentials+)
Until someone makes a good 2-in-1 linux laptop, I'll have to stick with getting windows licenses, I'm afraid. My X1 Yoga is probably good for another couple of years, I'm guessing.
Linux is the future
Honestly, this should be done long time ago tho. Idk about drivers support issues for linux nowadays, but they will provide more value for many manufacturer there since they don't need to spend much moneys for windows key and many paid softwares
All of these are wonderful! But I‚Äôm most excited about the new Linux laptops from big OEMs like the Lenovo with Fedora and HP with Ubuntu ones. I just hope one day they sell these on stores instead of online only.

Edit: it‚Äôs HP with PopOS. It‚Äôs called Dev One
Cool, now I wish I could afford one of those laptops :,V
I just want my star labs laptop to show up after months and months of waiting :(
The lemur pro was backordered for forever when I needed a new laptop so I went with the macair. 

This new one is tempting, but pretty hard to justify dropping another $1200+ on a laptop when the m1 air is still crushing.
Every single laptop Linus MacBooks are Linux laptops
Same. I have an XPS 13 dev edition, which I thought was as small and thin as I could get until I looked at the Framework and saw it's almost the exact same dimensions. I'm trading in the XPS for one as soon as I can.
I'm using pop os on a framework laptop.

It's good, but could be a lot better. Battery life is not great. The battery drains when sleep or shut down. And the screen resolution is not 4k, or 1080p. It's in the middle, which forces you to either use fractional scaling, or use native resolution for the ui and scale up the font which is awkward.

I think a framework laptop with AMD Apu and 4k display is the ultimate laptop.
Really looking forward to having Framework laptops available in my region.

Freedom to customize my laptop has always been the large negative for me when buying them vs a desktop.
I was looking at moving away from my MBP M1, system76 was ticking all the boxes until I confirmed that their displays are still 46% NTSC gamut. Framework is much better in that regard, just waiting for real world experience with the 12th gen.
I came really close to buying a framework laptop, but I heard the bezels wobble a fair amount. So I ended up going with a similarly priced Thinkpad with better specs and casing.
Even the big brands have trouble getting 6000 series Ryzen in bulk right now. So do not expect much from the smaller ones that are basically rebadging ODMs.
It‚Äôs very nice and runs other distros well too. The only things added in Pop OS are a hwmon driver and some opt-in telemetry for HP. Code is available to build for other distros. I built the telemetry on Fedora because I want them to make more Linux laptops.

My only complaint is a matte screen would have been better, and fonts can look blurry (I think it is poor optimization for highdpi). I did an rma for blurry fonts and my replacement seems to do a better job. On the RMA form they describe it as ‚ÄúElitebook 845 w/ PopOS‚Äù and it is significantly cheaper than the equivalent 845.
Yeah I saw that too, natural translation is a perfect fit for ai.
I hope so too, but there needs to be demands for these, and there is few because people don‚Äôt know about Linux. This all sounds very familiar‚Ä¶
> The RISC laptop looks interesting, but their "preorder" page looks extremely suspicious to me...like at best, it looks super amateur hour

Yeah dodgy little wordpress site, that they appear to be hosting from a 2400bps modem or something.  

Literally taking minutes to load at the moment.
The RISC-V Laptop is a collaboration between two not-so-popular Chinese companies. Will have to see what they come up with.

What do you think about the MNT Reform?
From what I‚Äôve seen on their social media/press releases, System76 is moving towards more in-house builds. I‚Äôm a happy customer and someone who‚Äôs running Pop! OS on two laptops sourced from System76 that are, in fact, generic builds. What you‚Äôre missing is that System76 went through the hassle of finding the right combination of hardware that just works. When it comes to a laptop, I‚Äôll happily pay them to find compatible hardware, especially when it comes to hibernation/suspension. Even if you _could_ buy the overall kit for less, do you want to take the chance that something critical like power management or WiFi causes kernel panics or data loss? That‚Äôs where the value is added, in my opinion - they test the hardware so I don‚Äôt have to.
What's wrong with Tuxedo?
The somewhat sad and yet somewhat amazing thing to me is that if you are just looking for a laptop to surf the web, do document editing, watch videos. About the cheapest "clean" experience you can get brand new that will probably last years is one of the M1 Macbook Airs. 

I grew in an environment where it did not matter if I liked Apple or not, they were just not in the budget. But these days it seems more and more like anything new that is not held together with duck tape and baling wire so to speak costs just as much if not more.

I think these native "Linux" laptops need to lean into the types of things that Framework is doing. It is something really tangible they are providing that is genuinely better than off the self Macbook or traditional Windows OEM sellers can or will be willing to provide.
Wow, it looks really bad. Booo
Right? The site looked dodgy by itself, but the NFT/web3 shite really pushed it over the "I need to stay far away from this" edge for me.
I'm also considering buying from them, and the only negative thing I've heard is that assembly/delivery times can be slow.
Thanks for filling that in, now I can see why you're
I covered that in the previous issue
[deleted]
Did it autocorrect "minus" to "Linus"? That says something.
Macbooks from 2015 or before that don't have Nvidia graphics run great with Pop OS in my experience.
Shoot. I've been going in circles on this - initially was looking at Lemur Pro, but have read too many things about quality seeming low for the price. So then I thought Framework, but I was under impression there were shipping/availability/support issues for Australia. So I started looking at XPS.

Now I see your comment. :)

I'd love to hear from an XPS user why the are switching to Framework.

EDIT: Ugh, nevermind. Can't get in Australia anyway.
What's the resolution on the XPS vs framework? That's the main reason I stick with Thinkpad atm, after using a MBP for years at work I can't go back to 1080p
I'm interested in one thing, how rich are you that you would just dumb your pretty new/functional XPS dev edition to buy a new laptop?
Try just font DPI scaling. Literally no performance cost compared to fractional scaling.
I'm using Fedora on my Framework laptop. The new beta BIOS 3.09 fixes the battery drain issue. So far I haven't had any issues using 150% fractional scaling in GNOME on Wayland; 200% scaling with 4K resolution (keeping the tall aspect ratio) would be great however.

I'd be interested in an ARM or RISC-V processor Framework.
Honestly, the only laptops that seem to be able to sleep correctly and not drain anything are MacBooks. Nothing else seems to implement it correctly. I can literally leave my MacBook in sleep mode and come back 12-14 hours later and only 1% is drained.
Ad sleep battery drain. That thing is buggy on windows also (to my knowledge, and not only on Frameworks) and one of the very new kernels should handle it correctly. Not sure if 5.19 or 5.20 though.
What kernel you running?
You want modern hardware gonna have to run a modern kernel...
And also, the aluminum is too soft and depending on the position it bends causing a click on the trackpad. Which can be annoying
Yea. We'll have 7000 series laptops before 6000 can matter.

Actually, that might be the strategy in some ways.
On the thinkpad sub there was  video yesterday about a new one with a 6600 amd apu and it is a freaking beast it seems.
Opt in telemetry, I can live with. I detest MSFT and APPL for not doing it opt in.
It was about time hp pushed the marketing a bit for the linux part, since I remember many years ago elitebooks and the like had the option to order with no distro or some other known ones like suse and redhat.
the fact the RISC laptop comes with an NFT is a huge red flag for me
More in-house? Currently none of their laptops are in-house so this could be interesting when it happens.
> From what I‚Äôve seen on their social media/press releases, System76 is moving towards more in-house builds.

And I applaud that! I hope to see more progress soon. 

> What you‚Äôre missing is that System76 went through the hassle of finding the right combination of hardware that just works. When it comes to a laptop, I‚Äôll happily pay them to find compatible hardware, especially when it comes to hibernation/suspension.

Honest opinion; these days, unless you're absolutely bleeding edge, and especially if gaming isn't a huge concern, you really can't go wrong with hardware. 

I bought, without any research whatsoever, a Lenovo Thinkbook 13s Gen 2 AMD. $650 (on sale) for a 8c/16t 4800U, 16 GB RAM, 512 GB SSD, and a 16:10 screen. 

I did no research on its Linux compatibility whatsoever, and Arch booted up just fine on it and every single feature works great. 

I've installed Linux on quite a few laptops from Dell, Samsung, Asus, and Lenovo, and I can't actually remember when the last time was that I had to do something special or found out some component didn't work. I realize this wasn't *always* the case, and I remember what it was like 5 or 6 years ago. 

But these days, things with kernel 5.17+ seem to run just fine on all the newer machines without too many issues. 

My opinion is that the whole "they test the hardware so that I don't have to" bit is a *bit* over exaggerated as a tangible benefit. 

Although, to be fair, I've been using Linux now for more than a decade and I'm used to doing a bit of extra work to get things working, so maybe I'm not quite the target audience.
I bought an InfinityBook Pro 14 gen 6 2 weeks ago, they shipped it after 4 days. That was with a Swedish keyboard and some extra ram and ssd storage included, that is to say not your off the shelf model. So far so good, no issues at all.
r/linux is so fun
Ah, my bad. It was probably there that I heard about it! Thanks
It‚Äôs not ‚Äújust‚Äù a Clevo - it‚Äôs them doing the legwork and finding a particular combination that works well in a Linux environment.

We‚Äôve probably all had devices that _almost_ worked, save one or two features. System76 does a decent job of finding the right combination of parts in that Clevo that saves time and frustration finding out the hard way that some critical piece of hardware is unstable in a Linux environment.
Clevo sellers are non existent in my country. People only care about mobile phones, so laptops are approx 200 usd more expensive.
Haha yes it did
Yeah, the international pricing and shipping situation for the Framework isn't great.

It'd cost over double my ASUS laptop with similar specs. It's not completely crazy, but still too expensive to justify.
I've found dells ro be perfectly fine Linux laptops.   I run kubuntu 22.04 on my work issued Precision 5530 mobile workstation and the only thing that doesn't work is the fingerprint reader, which I never used anyway.
Framework is 2256x1504, 3:2 aspect ratio
For the XPS you have 1080P IPS or 4K OLED option.
I am also interested.  I'm using a Dell Inspiron 15 with an Intel Core i3 running Parrot OS that was designed for Windows 8, but I am rich on the inside. Smiles!
The XPS was given to me by my old job. Probably going to sell it to help pay for the Framework.
Forget about the money aspect. Unless they're planning on doing something with the old laptop (donation, for example) it's just wasteful.
What is the optimum font scaling that you recommend? 2x is too big. Everything between 1 and 2 seems not crispy clear.
I actually use font DPI scaling as well on Plasma, even though it doesn't recommend it. Fractional scaling causes inconsistency in line sizing--that is, in rows full of 1px lines, some of them will be 2px instead--which irritates the *hell* out of me.
I'm waiting for the bios to become available on fwdup. They said it will take a few days.
I have a Latitude for work, and it's a roll of the dice whether it will go to sleep and stay asleep when I shut the lid. Also has a fun issue where it won't sleep and kick the fans back and forth between 0% and 100% every few seconds if I shut the lid with a dock plugged in.
I have a Dell XPS 15 running Pop!_OS that I left in sleep mode for like 3 weeks and it only lost ~10% battery. There are sleep states you can change somewhere for laptops. I believe default is S2 (idle), with S3 (deep) being the preferred and what I set mine to. It might just take a few seconds longer to fully resume from sleep.
Not a kernel issue. The modular peripherals don‚Äôt sleep, currently.
Pop OS uses a pretty new Kernel. Also, this issue exists on 11th gen CPUs on Framework. They said they tried to mitigate it using some bios updates and also the new 12th gen is supposed to be better.
I'm curious about this.

1) Why does it come with an NFT? Which purpose?  
2) Why is that a red flag?
Offering an NFT isn't just a red flag,  it's an automatic deal breaker. I almost like it because it makes choices easier: NFT? automatic nope.
They are going to develop and build their own laptop and they had an internal goal of having a prototype done in 2021 I think, but the pandemic and supply chain issues really set them back. They basically had to shelve it for a while but they've probably started moving on it again.
Well, it's almost impossible for anyone entering the market like system76 to have volume and capital power enough to be able to custom order and use things to even be possible to be sold at a reasonable price. It's the pain any company trying to enter the hardware market has to suffer. Now these many years later they finally have the name, the branding, the marketing and the user base to be able to achieve more custom hardware. And it seems they have been pushing into that direction by giving proofs like making custom keyboard.
> Honest opinion; these days, unless you're absolutely bleeding edge, and especially if gaming isn't a huge concern, you really can't go wrong with hardware.

Not necessarily. There was a case recently on here where a formerly well regarded specific laptop model, that worked excellently on Linux, had an undocumented h/w change (different WiFi-chip) without changing model numbers etc. Don't remember the details. So unless the laptop is developed with Linux in mind (Dell XPS, HP EliteBook, HP ZBook, Lenovo Thinkpad - not sure about the ThinkBook) you can never be sure that you don't end up with a weird RealTek chip in there somewhere.

As long as you stick with the pro-series or higher-tier series of the respective manufacturers, the chance that the work on Linux is high. Not so much on consumer grade laptops.

Re generic laptops from China: yes, it may seem that System76 is taking advantage, but they check the compatibility, etc. and spec the laptopts they resell in detail. So if you found out which Clevo the S76 system is based on, you'd have a good chance that one or the other component is NOT exactly the same and run into trouble. So there is a good reason for the markup.

If you are expert enough and go through the extended communication with a Chinese expert, to find out what exactly is in the system you want to buy, then you may end up with a cheaper, just as good system. But that doesn't mean that anybody can just go on AliExpress and order a system that will work.
You got lucky. The T14 Gen 2 AMD does have compatibility issues, with the trackpad no less, and also sound. Real disappointment.
Things are usually fucked for wifi and Bluetooth chips, bios with weird stufd and broken acpi tables. Or bios settings that can't be changed or hostile to linux. Sleeping problems. Some years ago webcams were very very bad. Also almost every laptop has a unique configuration of sound chips and how they are configured and there's too many quirks added to every single kernel version related on how to handle and work around stuff in laptops constantly being found out. It's very rare to be able to have a proper machine working without having to add some flags to the kernel command line until stuff is properly fixed in the kernel for the specific machines. 

So, yes, in general things kinda work and boot in most hardware but is very far from not having problems or for things to be working properly, kinda working is not always proper.

I've always had problems and have used mostly good known to work with linux machines: dells, thinkpads and some years ago asus which generally weren't known to have much problems with linux.
I'm also getting one with a Swedish keyboard, which is why I'm going with them over System 76 (oh, that and import taxes), kompis.

Did you stick with Tuxedo OS or did you switch to something else? If so,  have you noticed any difference in battery life etc?
I'm very scared to buy anything tech related from a company that doesn't have some kind of physical store or presence in my country. How does one deal with warranty problems and stuff like that?
np!
What Jeremy Soler said in an interview is, that they are actually co-developing the machines they sell with Clevo. So they are not "generic Clevo machines", but really have some System76 engineering on them. Clevo then sells those designs to everybody else...
Yeah, fingerprint support is still being worked on for KDE Plasma. Though, I believe fingerprint support will finally arrive in Plasma 5.26 (Mid-October 2022).
I honestly thought the dell I have would be a much better machine but it has so many problems in the wifi and Bluetooth and gets so freaking hot thay I think I'll avoid them. I have for work the xps from like 2 years ago maybe and had previously a thinkpad extreme gen 2 I think and that handled linux much better.
Neat thanks, thought I remember it being FHD
I can't have two laptops?
S2 and S3 has been considered legacy for a while, and fully died with Tiger Lake at least on Intel. S0ix is the new standard and is somewhat more complex but in theory better.
You‚Äôre lucky in that case, every laptop I‚Äôve had using Linux and using the different sleep states (I literally tried everything) still sucked in sleep mode. Either they wouldn‚Äôt wake up or they would drain excessive battery. I‚Äôve even had a Dell XPS 15, the 2020 model and it was the same issue. Even using a thinkpad x1 carbon 7th gen was the same thing.

At the end of the day I guess if it works for you then great! But for me and many others it doesn‚Äôt. For me what works is an M1 Pro MacBook Pro. I‚Äôve pretty much given up using Linux on my laptops. I really don‚Äôt want to tweak things anymore, I just want it to work.
They have a bunch of crypto buzzwords, the NFT is another piece of crypto-hype, there is no purpose. That is a red flag.
NFTs are useless pyramid/ponzi schemes who only operate as unregulated digital assets with value determined by how much the person to buy it from you is willing to pay based on avg market rates or subjective value they give it

It's just weird to sell a novel piece of hardware (a RISC-V laptop) and also highlight "oh it supports web3 and comes with an NFT" as if we didn't just watch most or all of web3 crash and burn over the last couple months when some of the bigger institutional investors pulled out and caused a prolonged selloff 

As for your first question: I have no idea why or for what purpose the NFT serves versus a serial number (especially when Blockchain would be less efficient to any other distributed database with serial numbers and user info)
Yep, completely understand. My comment was on the language used.

More in-house. I was checking that there wasn't one that I had missed.
Same reason I chose them :)

So far I've been using Tuxedo OS and I find it really responsive and well made. The pre-configured version is good and their included software does the job. It is my first KDE Plasma system and I'm happy with it for now, the only drawback being the lack of Flathub but that is easily fixed.

I haven't yet tested the battery fully since I've been mostly at home but what I've seen has been ok, somewhere close to their promised specs. Perhaps a bit less but not by much. The only strange thing I've noticed is an unusual amount of idle ram usage, mainly related to KDE.

Lycka till med k√∂pet!
I haven't used it myself so I don't know how well they handle warranty claims but their website has quite a lot of information regarding that.

It is always more of a risk buying online from other countries than in physical stores but there are no alternatives for a niche product like this.

I suppose it would be easier for you to make a warranty claim from an EU country or at least one not too far away from Germany considering shipping costs and the like.
That's good to hear, though I'm not a big fan.  Anything that can log me into the laptop while unconcious (or unwilling) is a hard pass.  Same reason I don't use face or fingerprint ID on my phone.
I avoid the consumer laptops...their business laptops are a little more expensive, but way better.   I have a Precision 5530 "Mobile Workstation"   I want to say it's in the neighborhood of 5 years old, just replaced the fans in it (they started sounding like a freight-train) but one call to dell and had the new fans in 48 hours.  It was awesome. :)
You guys have laptops??
Hail Apple!
I know, but what is the excuse that they use to sell the NFT
Give me a working product that I don‚Äôt have to change any settings and tweak and then I‚Äôll go to that one!

I don‚Äôt particularly have brand loyalty, if you make a better product, I‚Äôll use that one. If it‚Äôs BSD or Linux or Windows, I don‚Äôt care.

I generally see them as tools to get work done nowadays. I don‚Äôt really care what it is. If I wanna sit down and program that‚Äôs all I care about in that moment. I don‚Äôt want to tweak something to get better battery life such as tlp or whatever that new hotness is.
It is a promotion for people who pre-order.
Lol, that sucks
Offering pre-orders itself is a predatory practice, as well.  It's been shown to be exploited repeatedly and people still pre-order, leading to more exploitation because evil companies know it'll work.
Performance-wise, it was identical on my 1650 MaxQ. Both for gaming, rendering and CUDA computation.

However I had two issues which I did not investigate further (yet) but disappeared exactly when I switched back to the closed source drivers.

1. No power management out of the box: the GPU seemed to be powered even when I'm using integrated graphics. This resulted in much shorter battery life

2. The system couldn't resume after suspension, even when the Nvidia card was not in use. Any attempt would result in kernel panic.

I experienced these issues on the latest kernels, both mainline and lts.
Not complaining though, it was made clear that GeForce cards support is still in alpha stage for this driver, and apparently I'm one of the very few who had problems.
I plan to report an issue on GitHub when I have time to switch again to nvidia-open to document better everything.
In my experience it has been generally identical to proprietary with very minor differences.
I switched from a 5600XT to a 3080 shortly after the release. I was relieved that it was actually a pretty simple swap. I just installed nvidia-open-dkms before putting in the card and it seemingly just works. I have not gotten Wayland to work but I figure that's a bit of a lost cause at the current moment. I haven't used their proprietary driver in a long while so I cant speak to how it compares but for an "alpha" quality driver it's worked as well if not better than I would have expected for NVIDIA on Linux.

One funny quirk I will say I ran into was Lutris could not detect what type of drivers I was running so it would crash. I manually patched Lutris from a merge on their GitHub and am waiting for the update to hit the arch repos.
I've got a GTX 660 and 970, so it doesn't exist.
When I gave it a try maybe a month back, Gsync wasn't working and at the time I didn't felt like troubleshooting, and switched back. Otherwise everything else seemed fine. Performance seemed about the same too.
I've noticed general performance improvements when using a newer kernel (I'm running xanmod simply due to the fact that it had an easily installable PPA on Ubuntu and I figured it would be pretty close to mainline anyhow in my use case). 

I've got Wayland working but it's only really functional with gnome. I really dislike the user interface and the DE isn't as lean as I'd prefer but it's very smooth. I just wish KDE was more compatible with my hardware. 

The new drivers seem to have improved performance pretty significantly on my machine. I'm running a 2060 on Ubuntu 22.04. 

There are still some bugs. Until VERY recently gnome still had a lot of UI glitches. Gamescope also is pretty hit or miss. I got some AAA games working just fine with it under proton but my native build of mario 64 isn't rendering the textures properly. 

I would highly recommend upgrading to the new drivers if you can though. It IS better but they're not exactly stable. I wish I had AMD hardware to compare with but I feel like Nvidia on Linux is quickly catching up. It might take a few more years but eventually I hope we'll have comparable performance to Windows.
Just as a side note regarding the *opennes* of their new driver everybody should take a second and read the following, since it‚Äôs a far cry from real open source kernel space drivers like AMD's: https://mobile.twitter.com/marcan42/status/1524615058688724992
noticed absolutely nothing the only reason i use my Nvidia GPU is to game but unfortunately windows is giving better performance
Does it support the GTX 950M?
Would it support a GTX 1060? I think I heard it would only work on 3000+ series, so I haven't looked into it.
It was a miserable experience for me and made me use windows for years, but maybe it got better now idk, my new laptop has a ryzen apu so I'm free of Nvidia's bs
Appreciate the time you took to write everything, I don't have a laptop and had no idea the power management had some issues. Thankfully, I've seen only improvements for me. It's hopefully going to just get better and better from here.
Interesting, I personally haven't used Lutris as I've only bought games from steam since moving full time to Linux. But defo good to know that its already patched and on its way.

About the wayland issue, I also have had some issues still but with the open drivers at least we know some day it will happen.

The nvidia-settings also could see some modern changes hopefully (as much as I love the CRT monitors picture at the top, they just don't fit anymore!)
I think the Lutris issue has been fixed on Lutris' master for a while, so you could just install lutris-git from the AUR, no need to patch manually
Do you have the nvidia drm kernel parameter set? Wayland does not work without it.
*[Sad 1050 Ti noises.]*
Not sure why you're getting down voted. This is important because anyone with a 9 or 10 series GPU is screwed. Nouveau cannot use anything in this driver to improve compatibility with those cards
Not sure, don't have the card to test unfortunately
[https://developer.nvidia.com/blog/nvidia-releases-open-source-gpu-kernel-modules/](https://developer.nvidia.com/blog/nvidia-releases-open-source-gpu-kernel-modules/)

You can check at the bottom of this page
I think you are thinking of nouveau drivers (non-official), this is not that. This is Nvidia's official Semi open source kernel module
Is it the 1060 that starts the support?
Well, I guess that some people got the impression that I was senselessly bashing NVIDIA and shilling for AMD, which is actually not the case. And it‚Äôs cool if this *"open"* driver works better for people than the previous one, but we should get the facts straight, and imho this driver kinda seems like a pr-stunt to improve their image when it comes to open source software. And the resulting predicament that the users of older cards find themselves to be in - which you‚Äôre rightfully pointing out - is something that wouldn‚Äôt happen with a real open source kernel space driver, because what they‚Äôve thrown out is entirely **useless** when it comes to the collaborative and supportive aspects of *open source*, and while Intel and AMD are far from perfect, the community can at least to some degree work with their drivers.
My laptop has one, and I'd really like to use them, but I'm not sure if it's supported
Oh the new one, yeah I don't have a new card compatible with that, hope it gets better but I may not buy an Nvidia card anymore
It's only for Turing and newer, so the 16 series and newer.
Ah damn, only Turing or later :(
Found this, I think its the correct page but if you scroll down to FAQ it says "which gpus are supported by nvidia open" https://developer.nvidia.com/blog/nvidia-releases-open-source-gpu-kernel-modules/
Great, pixel 3 stops getting updated soon, seems a waste of a good phone
3a invited to the party?
hopefully this is usable when my current plan is done
It would be interesting to see how it performs. I was excited by Linux on the pine for software security update reasons. However they seem to follow the same batch image update mechanism. As oppose to an apt style inctemental one.

Though I have seen Gentoo. So is there incremental options?
Gonna say it. What you‚Äôre referring to as Linux is actually GNU/Linux. 

This is actually somewhat important of a clarification considering android already runs linux. You‚Äôre replacing Android/Linux with GNU/Linux.
Yeah, Linux is great for extending the life of old devices.
The 3 doesn't receive updates anymore, I don't know when it stopped. The 3a, in May, stopped receiving updates too.
Who the fuck is running stock rom on a Pixel? Pixel 3 is going to be continually supported by LineageOS and many other AOSP roms for a loooonngg time. Hell, the original pixel is on Android 11 and the pixel 2 is on the latest version of Android 12.
I think it's specific to the Snapdragon 845 SOC. That phone processor was in a lot of popular budget phones like the POCO F1 and OnePlus 6 so it got a lot of development focus. You can run actual Windows 10 ARM on those phones because Microsoft's ARM Windows tablet also has an SD845 based SOC and you can run many Linux distros on those phones such as postmarketos.
I mean 3a is the best supported Linux phone rn, at least for UBports
That one only has similarity with the 3 in its name pretty much. The hardware is quite different. There are some people working on it, but idk if they made any significant progress.
Please realise that the PinePhone can run any number of distributions. Not sure which one you used too but most of them update using a regular package manager. Mobian even using apt, but for example postmarketOS via Alpine's apk.
piene64 dont produce software for their devices, they leave that to the community. I'm not sure which distribution you're referring to here, you can go try out Mobian which use debian as a base if that's what you're interested in.

From a performance standpoint, sdm845 currently outperform the PinePhone Pro by a not insubstantial margin - although there are a few bugs and stuff still to be resolved to reach feature parity.

On top of that, in a head-to-head of Android running on top of Mainline vs downstream lineageos on the OnePlus 6, mainline is still about 30% behind in geekbench - so there's still performance to gain.
actually, if we're going there... I'm running postmarketOS on this device, which is based on Alpine and thus uses musl lib. so it is in fact not GNU/Linux...

the distinction here is actually between downstream vender kernel forks (see https://not.mainline.space) and upstream Linux, personally I wouldn't say Android devices run "Linux" as the kernel they run often includes code and features which would never be accepted into upstream
It doesn't really extend the life of an old device without device support code from the manufacturer, you are still running outdated firmware and drivers that are vulnerable to publicly known vulnerabilities.

Using such a phone should not be encouraged outside of tinkering, and definitely not as a daily driver.
yes, but it won't get vendor updates anymore
well ... Linux 4.9 (which the pixel 3 runs) will stop receiving security patches in a few years, even ignoring the fact that most of the attack surface is in the proprietary vendor blobs which definitely aren't getting patches anymore, a custom ROM doesn't get you very much
Tons of Normies do, I run stock OS on my pixel 1 so I can use work and banking apps
I haven‚Äôt run a custom ROM in years. It‚Äôs just far too much hassle these days and breaks too many features.
*Sobs in Razer Phone 2*

EDIT: because mine had one of the faulty USB ports
Only for UBports, because they use Halium/libhybris which enables them to use Android's proprietary userland drivers to enable hardware. For distributions like postmarketOS which don't use such hacks and only care about mainline, it's just as useless as most other devices.
It's not even listed under devices?
Okay, cool. I thought I saw an updater downloading a mobian image but maybe it was a version upgrade as oppose to rolling. Android updates should really be done the same way for all devices at the same time.
Ah, good to know.
>and drivers

The SD845 is mostly mainlined at this point. The kernel and drivers are not out of date, and the kernel is usually *more* up to date than the kernels of brand new Android phones
not sure you've been following here... we're running upstream Linux, 5.19-rc, with ~100 patches to support a bunch of Snapdragon 845 devices.

firmware is still stuck yeah, but generally has a much smaller attack surface
Won't they work with microg or opengapps?
But let‚Äôs be honest, postmarketOS would be even more broken
i daily a custom ROM on my Poco F3 and ngl it's been a surprisingly smooth experience, I run into some bugs sometimes but it's nothing too severe
At this point I'm sure almost all of us do. Mine somehow still is in okay enough shape but I know where it's going.

Shame because I don't really know what I want to upgrade to. I was hoping Linux phones would've been a little more mature at this point, but alas Android it will probably be.
What about the baseband firmware or firmware for other components on the device (WiFi, Bluetooth, etc)? Often these are not always something you can write FOSS alternatives for and may even be legally problematic. I'm not sure how I would feel about using a device that runs the latest Linux kernel but has outdated baseband firmware.
Just because the driver is mainlined doesn't mean that it receives attention from the original developers.

There is a lot of undermaintained drivers in Linux that almost certainly have an abundance of unknown vulnerabilities that nobody cares about.

If nobody cares about discovering vulnerabilities in those drivers they will simply stay unfixed and fly under the radar.

And this doesn't even include all drivers, since many drivers for Android devices are maintained out of tree for usually one specific LTS release of Linux.
All updates matter, attackers will simply choose the path of least resistance when firmware bugs remain unfixed.
No, some of them look for the presence of things like magisk and whether the bootloader is unlocked. It depends on how much of a jerk they want to be about it and how actively they're trying to protect you from doing whatever.
Without a doubt. If I were using an Android phone I‚Äôd probably want GrapheneOS but they drop support about as quick as Google do.
Aren‚Äôt banking apps and Google pay basically unusable though?
I've been daily driving an OP7T since my Razer's port falied, but have been in the same place pretty much since. Waterproofing isn't *as* esoteric of a requirement as it used to be, but display output + BL unlock is still an obnoxiously hard juxtaposition to find, never mind that at this point I'm just itching to see USB4 support start cropping up on mobile...

I *was* considering a Nothing before they confirmed they're skipping the US market, or an OV1 before it got confirmed as a crypto phone, but in the interim I'm thinking about just pulling the trigger on a Pixel 6(a?) so that I can farm the OnePlus off as a project device.
Outdated firmware is a real issue but judging by how many updates my phone got during its life time I sadly don't think there is too large of a difference between "supported" and unsupported devices.
The baseband firmware on the Pixel 3 just got updated this month...
> Just because the driver is mainlined doesn't mean that it receives attention from the original developers.

Definitely. However if someone (almost never the original developers) takes the drivers, makes them fitting for inclusion upstream, possibly even rewrites them for that and goes through the Linux review process, that's very often gonna be more attention from capable developers than most drivers for Android devices ever sees.

> And this doesn't even include all drivers, since many drivers for Android devices are maintained out of tree for usually one specific LTS release of Linux.

When people say that a device is mainlined that means that exactly that is not true; you don't need any out of tree modules. This is the case for my OnePlus 6 for example, which also uses the SD845.
this argument doesn't really make sense, functional drivers used every day by hundreds/thousands of people tend not to need much attention - they just work. Finding and fixing vulnerabilities is not something which most vendors invest a whole lot into, the upstream community do, however.
yep, it's a sad truth, please do join the rest of us in making more people aware of this and demanding that vendors give us true ownership of devices once they go EOL.

Given the alternative is landfill, and the risk for most users is realistically not that large im comfortable with it. properly understanding the risks is important, fearmongering about outdated software in a thread about developers trying to *fix* outdated software is just dumb.
Nah sometimes it works out of the box with no root (i don't know how often, i never use unrooted), and with Magisk you can follow some steps to pass safetynet (it's pinned in /r/magisk)
It doesn't fix outdated software/firmware. It only provides an option to run an OS that is less secure than Android with incomplete security patches on an EOL device.

And they should make this clear instead of advertising it as an extension of the device's support cycle, which it is not.
I don't think it's stated or even implied anywhere that we (postmarketOS i assume is what you're referring to) in any way attempt to offer support even close to the software support from the vendor. much of our work flies in the face of what vendors want.
sure by default postmarketOS or another linux flavor is less secure than android but it can be hardened to a much much higher level
Almost nobody does that (and no distro does it OOTB), and those that do know that it has terrible UX and requires a lot of manual work to figure out how to properly sandbox apps that do not expect to be sandboxed, and work around bugs caused by system hardening.
The Pogoplug / Sheevaplug is what kickstarted my interest in a small low powered device to run Linux at home. I did eventually get one but it stayed in the original box. I already had a full desktop PC running Linux but the idea of a small low powered device to allow me to do the same was pretty cool. Eventually Rpis came along and took that spot and was a bit more capable.
by the way, what's this keyboard ? looks nice
Yes i had one.. No idea where it has gotten to.  I do remember seeing that Base just the other day in a box of Junk.
NSLU2, Dockstar, Pogoplug, RasPi, then FreeNAS.   Got my first lesaons in JTAG debugging via parallel port and learning that serial ports can be something other than 5v and 12v.  They were fun little projects, I used a Dockstar as a bitcoin miner controller
Yep didn‚Äôt they have a nickname of a slug?
I didn't. What are they? They look like neat devices :P
I just stumbled on my old one!
That thing was rock solid.
I used it as a terminal server, with a multi-port USB to serial dongle attached to it.
Oh yes, the slug! I had two!

Also, it was called NSL**U**2 -- hence the nickname slug. I still have them somewhere.
The company I worked for in the early 2000s bought a couple hundred of them and we made "usb dongle charging stations" from them. Maintenance crew used the usb dongles to access slot machines and install software to them.

Such nice lightweight devices well suited for that task.
NSLU2, WRT54, SheevaPlug and Fonera. Oh, those were interesting times.
I ran, and still have a gumstix Linux computer, it's literally the size of a stick of gum, they came out in about 2007-2008, it was a neat little thing made to be mounted onto a mother board that you designed/built, but also had a little case you could snap onto it.
Ah‚Ä¶ the clock drift‚Ä¶
nslug!
Yup its in a drawer
Gosh those are things I don't miss. Slugs, Dockstars, Pogoplugs and the lot. I still have some of them kicking around in a box (probably with my original RPi. The RPi was such a massive leap ahead by just being easy to flash and update.
What keyboard is that?
My first machine that was running Linux.
Oh the old days! :) how i loved these!
I've never even seen this before. Before PI I just used old PCs.
Anyone have some resource for that thing i wanna learn about it
I knew of them, but the rpi with 256MB? of RAM was the first buy. I switched to Odroid as they had USB3 about a decade before rpi offered it. rpi is still missing fast crypto.
I missed the boat on these mostly because i was too young. But I got my hands on two of them for free a few years ago and I've been wanting to try to do something with them. Not exactly the latest and greatest tech and honestly no idea where i'd start but they're interesting.
oh man, unslug was amazing on that thing.

somewhere out there in a drawer somewhere I have an old KangarooPC too.
I had a Seagate GoFlex which could be easily re-flashed to a simple little Debian system. I had it running for several years before I got my first Raspberry Pi.
I mean, "put a Debian on it" have existed before and after NSL2. I had Dlink DNS-313.
The slug got me a job making cellular gateways. I owe my career to that lil dude!
I remember these types of installs from back in the day. I wanted to try it, but never got around to it. There was some $50 linksys print server appliance that was capable of running Debian, iirc.
ixp4xx
DDWRT was my thing.
So nope. üòî
I have a pink Pogoplug and a blue one in storage. I had Arch Linux running on at least one of them, maybe both.
Oh yeah! I had one, overclocked, running debian. My first NAS. Next was a dream plug.
Paging u/sail4sea what keyboard ya got homie?
I still have üòÅ
I used one with mpd and a cheap usb audio adapter for music playback on my stereo. Not an audiophile setup but worked well. The slug's supplied network cable had a ferrite core on it and it REALLY needed it. With every other network cable I got noise on the audio. I still have that ferrite core in my network drawer.

Also hosted an IRC bouncer on it, thinking about that gives me another wave of nostalgia.
Sure thing, it's still in a box
I used to have one. I still do. But I used to, too.
What kind of keyboard is that?
Pogoplug for me.
I had an NSLU2.  It was fun.  Eventually is was useful enough and, at the same time both time consuming and underpowered that I eventually went commercial and bought a DiskStation.
Oh yes, these were interesting little things.  Sadly it was replaced by a Sheevaplug... then a Beagleboard... then the Raspberry Pi.

Now I have a NAS in the form of the Argon40 Eon that's Pi powered.
I love your keyboard! Do you remember where you got it?
Me too! Its power module melted like a marshmallow. Bit alarming! I replaced it's power with a old routers adaptor and it run a few more years. It's what got me into Debian. My first ARM Linux device, though I had played with the idea of putting Linux on my old RiscPC.
I got the SheevaPlug when I was a kid for Christmas one year and it was one of those things that took my knowledge of Linux up a couple of notches very quickly. Not having any GUI forced me to really learn how to manage a computer with just the command line.

I eventually used it as a VPN so I could get around the Internet filter they had at my high school to watch YouTube videos.
Also got my start on a pogo plug, and within a year, I was homelabbing with multiple Pi‚Äôs, pfSense, you name it
I loved my Sheevaplug, ran Arch on it for a long time I think. It was great to have something that didn‚Äôt take much electricity and I felt better about leaving it on all the time. I only just got my first Rpi.
Sounds like you are talking about phones
I got one at the thrift store a few weeks back. Here's a picture.

https://drive.google.com/file/d/1GZLLUOOfhJe3qh\_Dkt\_hL1QKGIqnQtQ8/view?usp=sharing
Me too! I got a free ping pogoplug from my job at a popular electronics retailer and instantly installed linux on it to turn it into a really crappy but fun mini-server and nas.
It‚Äôs a CM Storm Quickfire XT with a custom controller board that runs QMK.  So all the Unicode symbols can be typed.  The keycaps are GMK Space Cadet.
Unfortunately the NSL2 is obsolete.  They had it running until stretch.
The Dockstar was amazing for its time, and had a very compelling price. Real gigabit Ethernet (something the first RPis didn't even have), a good amount of RAM, and datasheets for the SoC publicly available. I even ran bare metal applications on it, loaded via JTAG, or used the GPIOs from Linux for bitbanging.

Before that I used various OpenWRT supported devices like the WRT54G or the Fonera AP for it (which you could get very cheaply).
Yes it was the Slug.  I learned to solder on this thing.  I learn to host a Linux server on it as well.   I ran a mail server with hoard if you believe that.   Disappointed the Raspberry pi never had a piezo speaker like this thing.
It was a consumer NAS device that had enough firmware space to run a Linux boot loader and you could run Debian on it.   It was a fun device and I‚Äôm sad it is obsolete.
I‚Äôve never found something that quite matched the original WRT54G, it was only a few years ago that I replaced it. The legacy still lives on in a lot of devices though.
The god damn gumstix :) haha. I fought so hard to get a tool chain working to cross compile for it from my PowerBook G4 running OSX 10.2 or something.
It‚Äôs a CM Storm Quickfire XT with a custom controller board that runs QMK.  So all the Unicode symbols can be typed.  The keycaps are GMK Space Cadet.
It‚Äôs a CM Storm Quickfire XT with a custom controller board that runs QMK.  So all the Unicode symbols can be typed.  The keycaps are GMK Space Cadet.
I explained it in other comments.
Oh I didn't know about the argon40 Eon. I'll have to get myself one
Keyboard: ebay
Keyboard controller: 1up keyboards
Keycaps: drop.com
I also got started with a Sheevaplug, the power supply melting/dying was a common issue, happened to me too (might be due to running a HDD off the USB port). Changed the power supply to an external 5V one and still works (although just in a drawer these days).
A link on how to do that is somehow [still up](https://www.arm.linux.org.uk/machines/riscpc/installing.php) on linux.org.uk
What are the symbols below the letter keys on the keyboard? The symbols on the "QWER" keys are either mathematical symbols or organ pedalling marks.
I also had some  Mini PC that booted and ran linux from a CD Drive, with a tiny bit of storage onboard.   The 'Think NIC' a $200 linux system.    It was kind of obsolete when it was made.    This was when the idea of a 'live cd' was a new cutting edge.

I have had so many weird gizmos and gadgets over the years..
I love OpenWrt routers.
Pi is largely diy. If you want a piezo speaker, plug it into a gpio.
Looks like openwrt has updated through 2019 for the device if you want to run something a little more modern.
Yeah it took some work to get it running, I remember it being a pita.
Yeah, it's a nice piece of kit.  Takes two 3.5" drives, two 2.5" drives, and a USB adapter for an SSD (but be careful!) plus the MicroSD card which it extends out for you.  Got it on the Kickstarter.
Thanks!
Snap! The USB claimed to be able do 750mA, which was the same as the disk, after some reading I read it can be 1A or over during spin up. Mine still works too, I just got no job it's worth it doing.
Be pointless though. I really should donate it to some museum rather than keep it in the loft.
They are mathematical symbols.  I wrote firmware so they actually type those symbols in Unicode from the keyboard if I want them to.  They also type Greek letters in Unicode.
Here's a review of it: https://strom.com/awards/218.html

Unfortunately does not share its specs
True, but there is a Unix beep command that you just can‚Äôt run from a raspberry pi even with putting a piezo speaker on the gpio
Actually, I'm almost certain you can. It may require rebuilding the kernel but I believe you can modify the RPi's device tree to assign a "pc speaker" to a gpio pin. Not that the amount of effort it might take would be remotely worth it lol.
I have actually been meaning to investigate this exact thing. I can beep my speaker using a python script but I'd love to have all the native pc speaker calls also work. I'll try to get to this in the next few weeks.
Would be worth it if once built you shared it. Could send rPi dosbox or FreeDOS builds through the roof lol
I second this.  Please share.  I can solder a piezo speaker but I want this to work natively.
I actually installed this by chance yesterday. It was strange seeing most sites on 11.3 still and then apt update saying I needed to wait an hour before the release file would be valid.
Not bad, but incomplete.

If you're sshing through a bastion to another host, like `A --> B --> C`, and your session on C hangs, ` ENTER ~ .` will take you all the way back to `A`. If you just want to go back to `B`, `ENTER ~ ~ .` will do it.
fun fact: `ssh-copy-id` won't work with windows (i.e. if you want to upload your public key to windows host). You need to do it manually using scp or something. If your remote windows user belongs to Administrators group, then you need to append your key to `C:\ProgramData\ssh\administrators_authorized_keys` instead of default location.

Oh, also looks like win32-openssh ignores debug levels when writing logs to windows event log (which it does by default).

Thanks Microsoft for such useful improvements, it makes administration so much more easy and obvious (lol)
I prefer tar over ssh vs scp, but whatever

Ex: https://www.cyberciti.biz/faq/howto-use-tar-command-through-network-over-ssh-session/
scp -3 ...
Do you have any SSH resources for beginners?
How to start from scratch?
Also is there a guide for TLS?
Agent forwarding is a giant security mess. You should be encouraging people to use `ProxyJump` instead.
It‚Äôs huge, more a cheatbook
Good stuff but missing ssh multiplexing, dynamic forwarding, and x-forwarding.
I thought only the sftp protocol was deprecated, not the tool (which could still use sftp being the scenes. Am I wrong?
Wait....you mean I don't have to wait like an idiot until it says broken pipe????
Thanks! Have since updated the article with it!
[deleted]
I'm not familiar with these topics (the submitted link is not my article).

These books might help:

* [Linux Bible](https://www.wiley.com/en-us/Linux+Bible%2C+10th+Edition-p-9781119578895) ‚Äî basic operations, server management, administration, automated deployment, etc
* [How Linux Works: What Every Superuser Should Know](https://nostarch.com/howlinuxworks3) ‚Äî booting, device drivers, networking, development tools, effective shell scripts, etc
* [UNIX and Linux System Administration Handbook](https://www.oreilly.com/library/view/unix-and-linux/9780134278308/) ‚Äî definitive guide to installing, configuring and maintaining any Unix or Linux system
Thanks for the suggestions! Will update the article with it.
As far as I know, when you use scp it will use sftp behind the scenes. You can use -O to fall back to the old scp protocol which is useful when the remote end cannot handle sftp.
I used to open another terminal, and kill hung ssh sessions. üò¨
Where's my cut?
because it's Microsoft who maintains this fork and ships windows with this config
I do have Linux bible and I am at scripting,while
Do not have how Linux works and Unix Linux sysadmi
Thanks for sharing
I just reboot the vm lol
ü§ñ
Because that's impossible on a technical level. Each distro compiles all it's packages slightly differently. Thus it would be impossible for you to depend on anything from inside the distro in this proposed LUR 

Think of it this way: imagine trying to make a package definition that will work for both Debian and Fedora. Debian's latest stable release (11) is using GNOME 3.38 packages from ~2 years ago. Fedora's latest stable release (36), meanwhile, is shipping GNOME 42 packages from ~2 months ago. How would you, as someone maintaining a package that depends on some GNOME libraries in the LUR, respond to this? The 2 month old library has removed old API from the 2 year old library, but the 2 year old library doesn't have the replacement yet (because it's two years old). So if you depend on the old library, your code breaks with the new one, and if you use the new one, your code breaks with the old one. It's like this with every other package you'd depend on (libc, compiler, build system, etc)

I'm not even touching on the differences other than version. Maybe Debian disables some features that Fedora doesn't. Maybe arch configures its init system in a way that Ubuntu doesn't. Good luck finding a consistent set of packages you can depend on! And this gets exponentially worse with each distro you add on

Oh and there's also the legal issue of licensing. Every distro has different rules, and some packages are weird versions or forks or whatever to make the licenses work out. The "same" package on two different distros might be two different diverging forks of something that got relicensed. Or, even worse, one distro might have a fork because the new license wasn't acceptable to them, but another distro is using the original project under a new license, so you end up in a situation where it's legal for you to use it as a dependency in, say, Debian but not in Fedora. Good luck figuring that one out!
There is something similar to AUR for Debian (https://mpr.makedeb.org). However, it currently has far fewer packages.

>Why don't the aur developers just release it universally as lur

Why should they? AUR is an offer for Arch Linux or distributions based on Arch. Nothing more.

But basically PKGBUILD files are just bash scripts. It should therefore be feasible to use them under several distributions. It just needs someone to make the effort. And that's where I see the problem.
You could run your distro of choice and then install an Arch container in [Distrobox](https://www.techrepublic.com/article/how-to-quickly-deploy-a-linux-distribution-with-gui-applications-via-a-container/) (docker/podman), which would allow you to install and run packages from the AUR. You can also export applications from your Distrobox containers to make them appear to be local to your base installation.
Request missing apps on Flathub forums. Some things can't be packaged by Flatpak, but many apps can. There are users (such as myself) who have packaged everything we use, and genuinely don't know of other apps that can be packaged. If someone makes a request, we can package it too.

Same thing goes for reporting packages that are out of date. Please report the package so the maintainer knows!
The AUR is just packages build script provided by user, you know.
And actually, I think there is probably already some projects to convert from a PKGBUILD to a .deb package if you search hard enough...
pkgsrc. Nix & Guix. Linuxbrew.

Once you go cross distro you effectively have to not use any distro libraries since you have no guarantee of a base to build from.

Be the change you want to see. I look forward to seeing your cross distro patches to the AUR.
Opensuse's OBS and Fedora's COPR are pretty similar to AUR.
Most distributions have larger repositories so don't need an AUR. Though, if you really want to dive into third-party repositories, I mean, there is Pacstall for Debian, Copr for Fedora, PPAs for Ubuntu, the community build system for openSUSE, the SlackBuilds for Slackware. 

Plus Nix and pkgsrc are cross-distro and massive.

Basically no one else needs the AUR because there are lots of other, sometimes more extensive, options.
openSUSE Build Service

https://build.opensuse.org/

https://en.wikipedia.org/wiki/Open_Build_Service
Nixpkgs has a more extensive and more up to date repo than aur AND it‚Äôs cross distro compatible with even works on MacOS
>Are there NO other similar repos as large as the aur

IIRC Nix has the largest repo out of all distros. Don't quote me on that though.

Oh and you can use Nix on top on any distro other than NixOS.
Just go full nix or guix, it's the end game :P
My biggest *complaint* about Arch is the AUR. I don't see the AUR as a good thing. Installing packages slapped together by some random users doesn't appeal to me at all; it really smacks of a Windows mentality, where users install stuff from god knows where.

I'll take my chances with my distro's official package repositories. I don't mind waiting a few days or whatever to get the latest version of a package if it means it was vetted and packaged (with signing) by official maintainers.
You want large amount of up to date packages ?, use Suse tumbleweed.


Otherwise use debian, for insane amount of outdated but security-patched and well tested packages.


Then there's NixOS I know nothing about except that its package count is big and constantly growing.


Take your pick.
Nix has (i) more packages than AUR, (ii) is available for all Linuxes.
Nixos has a huge number of packages available and you can build the ones you don't have. The aur never really worked for me, it was hit or miss whether a package worked or not. Nixos is much more reliable, due to it's ability to build almost any version of any package.
- https://mpr.makedeb.org/
- https://nixos.org/download.html#nix-install-linux
> Flatpak's repo is REALLY small.

Based on what metric? Flathub has *a lot* of apps. If you check it by sheer package numbers, then the AUR is obviously going to win because it operates on *package* scope--which means all manner of modules that affect not just parts of graphical apps, but also system components--rather than *application* scope.
AUR is already helpful when making packages for other distributions: It is simple, accessible via the web without downloading a huge tarball and then hunting in there. Converting from PKGBUILD to say Flatpak for Flathub is often not that hard. Go and add what you feel is missing.
You could try using distros based on Arch, such as manjaro.
Every single major distro has an alternative to the AUR

Fedora has COPR 

Opensuse has the Opensuse build system (OBS)

Debian has backports (officially, but it has pacstall and the MPR unofficially)

Ubuntu has ppa's (officailly, but it has pacstall and MPR unofficially)

Each one has their own alternative, with upsides and downsides. I could list them all, but that sounds like a lot of work, so go research it yourself.

I absolutely despise the way Arch Linux and the AUR have consumed discussions, causing people to skip over perfectly fine distros, instead going for a distro that may not be right for them.
Arch uses AUR, Slackware uses Slackbuild, but many major distro don't have one. They provides bunches of pre-compiled packages, and I guess they just wanted their users to use git for more apps. Many tools that is universal make build easier btw
> Because that's impossible on a technical level

That's untrue as shown by flatpak, nix, guix, homebrew (formerly linuxbrew), appimage and the like. You just have to dismiss the system library versions (and therefore your distribution's package manager) and bundle your own in one way or another. There are obviously some downsides (increased space usage, some potential for incompatibilities when it comes to state, etc) but it is often worth it.
[deleted]
I mean, dpkg is on the AUR.
>My biggest complaint about Arch is the AUR. I don't see the AUR as a good thing. Installing packages slapped together by some random users doesn't appeal to me at all; it really smacks of a Windows mentality, where users install stuff from god knows where.

I'll be frank; the alternative is literally the windows mentality of blindly building/downloading things from websites in order to get them.

You'll never have a complete enough distro package repository to not need something like the AUR, and the fact Arch is the only one who seriously pursues such a thing is mind blowing to me.
> I don't see the AUR as a good thing. Installing packages slapped together by some random users doesn't appeal to me at all; it really smacks of a Windows mentality, where users install stuff from god knows where.

In the AUR, only recipes are offered, so to speak, on the basis of which the packages are created. These can therefore be checked very easily. Therefore, your comparison with Windows does not fit in my opinion.

>I'll take my chances with my distro's official package repositories. I don't mind waiting a few days or whatever to get the latest version of a package if it means it was vetted and packaged (with signing) by official maintainers.

The AUR is not about getting a package there earlier than via the official package sources. At least not primarily. The AUR contains many recipes for packages that are not available in the official package sources. And some of them can never be there for legal reasons. Or these are packages with modified configurations than those available in the official package sources. And so on.
You know what really great about the AUR?

Don't like it? Don't use it. And its still there for those who want it.

There is being secure and then there's being impracticaly secure.

Using computer software has always been about trust and to live in the real world some level has to be given.

You trust your distros maintainers but have to actually read all the source of all their packages?

Sure you could say the AUR is "less safe" but by how much really?

Using something from the AUR is miles safer than randomly downloading stuff off the web.
You know, many packages in arch community repo were first in the AUR. With votes and the pkgstats (it's an opt-in telemetry that you can install on your machine which send the list of packages you have installed), Archlinux TU can gauge community interest and adopt an aur packages and move it in community.

So using packages in the AUR (after vetting it yourself) and voting on it is also a way to participate in arch
I think you underestimate a repository that is auditable by the public even if most people are consumers rather than auditors and editors. Someone once tried to add malware to AUR https://www.bleepingcomputer.com/news/security/malware-found-in-arch-linux-aur-package-repository/ needless to say it was quickly discovered.
I'm actually in mostly the same boat as you, but I do trust Flathub in addition to my distro repo because it has a more limited, explicit scope while also serving a wider userbase and having backing from numerous upstreams.
What do you do if you want a piece of software that isn't available and will never be available in our distro's repo?
And to add to this, how often do you really need the latest version of some package? I mean *really.* I personally has hit this obstacle one single time during my 10+ years of using Debian. That's why Debian/Fedora is the perfect distro combo if you have more than one computer.
I was on Solus for a long time where the repo is very limited, I now just manually build from source anything I find :P

Atleast now I know alot more about the insides of linux :D
The downside to using Nix on other distros is that anything involving OpenGL is a headache, and the [hack around it](https://github.com/guibou/nixGL) has some awkward edge cases.
However, in my opinion, the .nix files are more complex than the PKBUILD files. Therefore, even though I think nix is interesting, I would currently continue to stick with the PKBUILD files.
I still don't really understand what Nix is, its relation to NixOS, etc.
Nix also has the [NUR](https://github.com/nix-community/NUR).  I have never needed to use it, while when I used arch I had many AUR packages.
> My ONLY reason to use arch is the aur.

I suspect he is already using Arch at the moment. So switching to a distribution based on Arch would be of little use.
Right I'm well aware. But that's far removed from how the AUR works, and the OP was asking why not make an AUR that works cross-distro. I explained why not. 

I wasn't saying cross-distro distribution is impossible. I was saying cross-distro packages that make use of system libraries, like the AUR, are impossible.
That's not how it works.
Only the development version, the stable release is in the repos.
Better yet, it is on Arch's [official repositories](https://archlinux.org/packages/?q=dpkg). Althought I wouldn't use it on an Arch install.
> the fact Arch is the only one who seriously pursues such a thing is mind blowing to me.

Gentoo allows anyone to create and add Portage overlays. [The ones listed here](https://overlays.gentoo.org/) can be added to your system using `eselect repository enable [repo name]` and then syncing the ebuilds. Then there's [GURU](https://wiki.gentoo.org/wiki/Project:GURU) which is the official Gentoo repo/overlay for user submitted and maintained packages. Sometimes packages from `::guru` find their way into `::gentoo` as well.

The main difference between AUR and overlays is that the packages are treated as equals by Portage. No need to install a new package manager, since they integrate seamlessly into your system.
> I'll be frank; the alternative is literally the windows mentality of blindly building/downloading things from websites in order to get them.

However, I fear that many users of AUR also do not check the PKGBUILD files before installation or before an update and thus install blindly.
>and the fact Arch is the only one who seriously pursues such a thing is mind blowing to me.

Don't you think there is a reason for that?
Another reason for the AUR is to provide "packages" for things that aren't really packages, binaries, or even libraries. Plenty of AUR packages are themes and the like.
>In the AUR, only recipes are offered

There are binaries of both free and proprietary programs in the AUR, so not "only recipes".
I doubt most "normal" Linux users have the expertise to vet packages themselves; there are lots of ways to compromise a system. Putting that expectation on users just makes Arch's package security seem even worse than I suspected.
The question that seemingly nobody dares to ask is why does a pdf reader has any default permission except reading the pdf file it is asked to display. Any other permission (printer, internet, write specific files for an export function...) should be given only when needed on a case by case basis by the user. 

I kinda think of taking the best aspects of Android and Flatpak, combining them into a secure permission concept that is both easy to use and does not restrict the users freedom to give their trusted programs all permissions they need.

That would be an additional layer of protection both against malicious packages and also bugs that could be exploited by a malicious pdf file.
> And to add to this, how often do you really need the latest version of some package? I mean really.

I would say it depends on the respective use case and the respective package. Let's take the static website generator Hugo as an example. In new versions, bugs are often fixed and there are often new useful features. Or existing features are improved. Does one therefore really need the new version? No, but it doesn't necessarily make sense to continue using the old version.

>That's why Debian/Fedora is the perfect distro combo if you have more than one computer.

That may be the case for you. No problem. For others, however, it is not the best solution.
That's true, forgot about this one.
Complexity is not a bad thing per se. I could make the argument that the AUR/PKBUILD's apparent simplicity is actually a limitation in the sense that it attracts people who don't know what they're doing writing semi-working packages that break every release.
In a nutshell: Nix is pacman/apt, NixOS is Arch/Debian.

Except that Nix can work outside of NixOS and use weird configuration files instead of just `install`/`remove` sub commands.

Pros: 
 
  - whatever package you've installed just work and is automagically configured following the instructions you gave in the aforementioned configuration files;
  - you can copy/paste these files across computers and replicate configurations, from ‚Ä∂have these packages installed‚Ä≥ to ‚Ä∂install nginx with foo and bar settings and modules, and have it proxy these 5 web services, with PostgreSQL and MongoDB backing them with such and such settings‚Ä≥ through ‚Ä∂I have these files with my very own config, and whenever I get a new laptop, I just launch them, make myself a coffee, and come back to FFox/Syncthing/Emacs/git/KDE/... set up as they should be and I can start working right now‚Ä≥.

Cons:

  - there is a big fat learning curve;
  - Nix has to run as root, so no ‚Ä∂install in my `$HOME`‚Ä≥.
Nix is a very weird build system and a package manager. The language in which you write the "build scripts" for this build system is confusingly also called Nix. Think ABS+pacman if we're talking about Nix-the-package-manager, and PGKBUILD if we're talking about Nix-the-language.

nixpkgs is a (huge) collection of package descriptions for the Nix package manager/build system. Think AUR.

cache.nixos.org is the "binary cache" of pre-built packages from nixpkgs, so that Nix can transparently download them instead of you having to build them locally a-la AUR. There's no alternative for this in Arch land, since this is quite a unique feature of Nix.

NixOS is a Linux distribution based on the Nix package manager, and it is configured entirely and declaratively using configuration files written in the Nix language. Think Arch itself, although _very_ different in many ways.
> I was saying cross-distro packages that make use of system libraries, like the AUR, are impossible.

But they didn't mention using system libraries.

You are right that we can't change the system (for example replacing the window manager with its git version or change a network config) with a cross-distro AUR but most of the time we install additional stuff with AUR that could just be shipped with the Flatpak method. OP mentioned Flatpak so probably this is what they mean and being able to change the system is an assumption by you.
Well, it's not _too_ far-removed: essentially, if we stuff the official Arch repos into AUR, it becomes possible to install stuff from AUR (let's say in a chroot or similar) on almost any Linux distribution, since now it's a self-contained repository. Not like that makes much sense given we already have nixpkgs and the like, but still, it could technically be done.
They are right instead, we can have a Flatpak equivalent of AUR i.e. building scripts uploaded somewhere by other users and the result is a Flatpak package that is then installed. OSTree solves every conflict between dependencies versions and since it stores only the diff between different versions of files it won't use much storage.
Right. Therefore, the package is not intended for this purpose. You can use it under Arch, for example, to create a package for Debian.
Yes. Yes they do. And you won't prevent people from blindly installing programs those people want. So the AUR at least attempts to mitigate risk to people.
I can make a reason for anything. Doesn't make it sensible.
Themes can be delivered in packages. Arch distributes some in their official repositories.
Makepkg should also be used for such things and thus a package will be created.
Directly in the AUR? I doubt it. There are offers in the AUR that end with -bin. For example, trililum-bin. But if you look at their PKBUILD file, you should notice that the binary file is downloaded via a different URL (for example https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=trilium-bin). 

But these are, for one thing, exceptions in terms of the masses. And for another, the PKBUILD file describes what is downloaded and how these files are installed. It is therefore still an easily verifiable recipe. Or instruction. Or whatever you want to call it. 

And yes, I would only use such offers if the URL from which the binary file is downloaded matches that of the official project. Otherwise I would rather use the normal version without -bin.
Well, the expectation in Arch is that you know what you're doing.
And AUR are not official packages, so if you can't inspect the build script and the resulting package, of course don't use it

There is some problem with arch packages per se, but not the AUR. The package databases are not signed currently, the process for doing it correctly has not yet been decided.
> I doubt most "normal" Linux users have the expertise to vet packages themselves; 

The AUR does not offer ready-made packages, but only instructions on the basis of which the packages are created. These are much easier to check than, for example, ready-made packages offered via a PPA from Ubuntu. For example https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=ttf-iosevka-term.

>Putting that expectation on users just makes Arch's package security seem even worse than I suspected.

AUR is not an official package source. The wiki clearly warns against AUR (https://wiki.archlinux.org/title/Arch_User_Repository). And Arch has a specific target group. If someone is not part of this target group but still uses Arch or AUR, then they have to live with the consequences.

And no, this has nothing to do with arrogance or gatekeeping. But just because I passed the driving test, I don't get into a racing car and drive it in a Nascar race. And if I do, I have to live with the consequences. Which in the worst case are fatal.

One simply has to accept that there are certain things one should stay away from. It doesn't always have to be Arch. There are so many other good distributions that everyone can find the one that suits them, so that existing distributions do not have to adapt to individual users.
Small things like static site generators are usually not linked to a lot of dynamic libraries. Therefore you could probably just build it from source.
Basically, I agree with you. 

But AUR, for example, is not an official offer. So the user is responsible for what he installs. And this is where I see the advantage of the PKBUILD files. Because of their simplicity, they are easy to control.

But apart from that, I think I understand the .nix files I looked at yesterday. But because they are more complex, I wouldn't want to create one myself even if I could. I do, however, enjoy working with PKBUILD files.

So yes, everything has its advantages and disadvantages.
>Nix has to run as root

Are there any plans or possibilities of remedying this?
> there is a big fat learning curve;

Absolutely true. I would go as far as saying that unless you already have plenty Linux experience and are ready to invest some serious time into learning, it's better to avoid the Nix ecosystem for now because of how involved it is (people are working on improving documentation and making it user-friendly, though). Otherwise it is absolutely amazing and you will not look back after understanding what it is.

> Nix has to run as root, so no ‚Ä∂install in my $HOME‚Ä≥.

There are many ways to not "run as root". For example, see: https://matthewbauer.us/blog/static-nix.html
I'm not talking about changing the system. I'm specifically responding to this question:

 > Why don't the aur developers just release it universally as lur

In other words, "why can't the AUR developers just make their packages work on other distros". I answered that question: with the way the AUR works, that is impossible on a technical level, for the reasons I described

I happen to agree that Flatpak covers a good amount of AUR use-cases. But that's just not the question I was answering
The moment you're involving chroots, the whole interaction becomes much more complicated and you're basically duplicating Flatpak and the like. It's no longer "traditional packages that get installed into your system" like the AUR, and it's now just containers like Flatpak/Snap/Docker/Toolbox/Nspawn/Distrobox/etc. It cannot, by definition, be as flexible as the AUR is with regards to what it can do to your system; it's not a 1:1 replacement, even if it covers many of the use-cases of the AUR

All I was trying to do is explain to OP why you can't just magically transport one distro's individual build scripts to another distro (with the implication that you're not just shipping the entirety of the first distro along with it), because they seemed frustrated that nobody is doing that
If you have a flatpak definition you can just upload it to flathub tho
>I can make a reason for anything. That doesn't make it sensible.

The AUR is not a metaphysical being, its existence (or lack thereof in other distributions), its weaknesses and its convenience can be rationally explained. There is a big caveat on the AUR website for various reasons, ranging from the quality of the packages to their security. Not all Arch developers are in favour of the AUR and it is not considered an official part of the distribution, which excludes all of its content from Arch's reproducibles builds efforts and other features like hardening of packages and other guidelines that the official packages have. It has literally no quality standards, which means that the quality and maintainability of its recipes vary widely (there are currently 21440 PKGBUILDs that have never been updated).

It exists in a distribution like Arch because its repositories are smaller compared to other distributions, it is managed by a comparatively small team and package building is much easier than in other distributions, among other reasons.

It is very popular among Arch users because of how convenient it is to use and the ikea effect that come with it. Most people prefer this criterion over security or quality. Most users don't know (or don't care) about the latent dangers in a free-for-all repository, but there is a history of precedents about them, people simply choose to ignore them.
>But these are, for one thing, exceptions in terms of the masses.

A tendency doesn't invalidate their existence.

> It is therefore still an easily verifiable recipe. 

Sure, but not an easily verifiable binary, into which malware can be injected at compile time (sometimes by a third party without the knowledge of the distributing developer), making inspection much more difficult than if it was only source code.

**Edit:** There are currently 3978 "*-bin" packages in the AUR, so not an insignificant number.
Maybe I don't understand your post. But the point was whether you always need the latest version or not. And I used an arbitrary example to explain why the latest version can make sense. How you install this version is in this context not important. 

Whereby I would never compile software simply like that and thus bypass the package management. And especially in such a case, the simplicity of Arch's package management is a blessing. Usually it is enough to simply update the current version of a package and the checksum of the archive in question in the PKBUILD file. After that, you simply need to run "makepkg -cirs PKGBUILD -noconfirm", for example, and you have the new version installed.
Nix does not need to run as root. As far as I know, it never has. It does need a user added to the proper group, but using root is a hacky workaround to that requirement.
Nix is mainly used in two ways, to configure the system, and to act as a build system (fancy makefile). It does need root access to configure the system if that system is NixOS. 

In other OSs, nix is mainly meant to act as a build system, though user configuration is possible through a program called home-manager. Root access is not needed when used this way.

This is just like a normal distro, you need root access to perform system wide changes.
Sure: https://matthewbauer.us/blog/static-nix.html
Not that I know of :/
As I said in the other reply you could figure out what OP meant instead of taking their question literally
Maybe we shouldn't bloat FlatHub with themes, extensions, versions compiled from master and all the stuff you usually find conveniently in AUR
>The AUR is not a metaphysical being, its existence (or lack thereof in other distributions), its weaknesses and its convenience can be rationally explained.

I think you entirely dodged the point here. My point is that just because there is a reason for something does not mean it is one sensible. Note that sensible is a very subjective term. But more specifically, sensible to the average user.

> There is a big caveat on the AUR website for various reasons, ranging from the quality of the packages to their security.

Those caveats are not something I think that would warrant entire restriction from any official or semi-official distribution. Plenty of packages are relative poor in quality or security in official package repos for every major distro.

>It exists in a distribution like Arch because its repositories are smaller compared to other distributions

I disagree that this is the sole, or even chief, reason the AUR exists. A lot of people use the AUR for its convenience, not because the Arch official repositories lack breadth. A good example is github releases. The official package repos would never host a direct github download and install script. Another example is simply .zip style libraries like Proton-GE, where the user is either never actually expected to look at the source code for the end product specifically, or the source code is unavailable. Then we get into the experimental programs territory as well. Linux kernel build scripts. Etc.

>It is very popular among Arch users because of how convenient it is to use and the ikea effect that come with it. Most people prefer this criterion over security or quality.

Yes, and Linux is all about providing such freedom. Moreover, we cannot expect or hope to become anything more than a niche platform without embracing user choice. If users want to be secure, they have the choice and can openly do so. We can have both. Hell, that's Arch does, both.

>Most users don't know (or don't care) about the latent dangers in a free-for-all repository, but there is a history of precedents about them, people simply choose to ignore them.

No amount of boxing them into a repo will change that.
Themes don‚Äôt get displayed on Flathub, they just get installed when flatpak detects your system theme. Same would go with extensions.

As for a master branch thing, flatpak could adopt snap‚Äôs channel system, where you could choose to use a stable version, stable candidate, beta, or edge version. I think developers could also define their own channels if that isn‚Äôt good enough.
>I think you entirely dodged the point here.

Reasons and rationality are not dominated by 'sensibility' and are not mutually exclusive. Reasons should be what inform a sensible judgement. If not by reason, then what informs sensibility? Is it an irrational criterion, then?

>Note that sensible is a very subjective term. But more specifically, sensible to the average user.

That is a very objective way to frame something that is supposed to be "subjective", i.e., something is perceived similarly by an average quantity of users.

>Those caveats are not something I think that would warrant entire restriction from any official or semi-official distribution. Plenty of packages are relative poor in quality or security in official package repos for every major distro.

Not really, there are distributions that have standards for packages in their official repositories that the AUR lacks. For example, Arch's official repositories are subject to [reproducible builds](https://wiki.archlinux.org/title/Reproducible_builds) and [hardening features for binaries](https://shibumi.dev/posts/hardening-executables/). I don't see how AUR packages are equatable to this, and saying "Plenty of packages are relative poor in quality or security in official package repos for every major distro." seems lax and a fallacy.

>I disagree that this is the sole, or even chief, reason the AUR exists. A lot of people use the AUR for its convenience, not because the Arch official repositories lack breadth.

I didn't say it was the sole or the chief, but one reason. Many of the packages in the AUR are not in the official repositories, so there is an evident value in their availability.

>The official package repos would never host a direct github download and install script.

Why do you think this is not the case?

>Yes, and Linux is all about providing such freedom. 

With freedom, there are costs to pay. Freedom is never free of consequences.

> Moreover, we cannot expect or hope to become anything more than a niche platform without embracing user choice.

MS Windows and macOS are the empirical refutation of that.

> If users want to be secure, they have the choice and can openly do so. We can have both.

You can have one or the other, but not both.

>No amount of boxing them into a repo will change that.

It won't be 100% bulletproof, but there are measures that can be taken to ameliorate the risks.
That already exists in Flatpak. Apps can have different arbitrary release channels. If you install the Flathub beta repo, a lot of apps have a `beta` or `master` or `canary` or whatever variant you can install
> displayed on Flathub

That's not the point, FlatHub have not infinite computation power.

> get installed when flatpak detects your system theme. Same would go with extensions.

Are you talking about GTK themes? GNOME extensions? I'm talking about everything, I don't even use GNOME. If you install a certain app, how do you choose to install an extension for it if it's not displayed in FlatHub, specially in immutable OS like Silverblue and Kinoite? Who said I can/want to install a theme/extension system-wide so that Flatpak detects it and install its version?

> As for a master branch thing, flatpak could adopt snap‚Äôs channel system, where you could choose to use a stable version, stable candidate, beta, or edge version.

This is already the case for beta channel but how can you think they can compile every single master branch of every FOSS project everytime there is a new commit? The point of example-git entries on AUR is that you conveniently build the project locally when needed with your computation power.

Did you really use AUR in the first place?
>Reasons and rationality are not dominated by 'sensibility' and are not mutually exclusive.

No. No they are not. But my entire point was;

>I can make a reason for anything. ***Doesn't make it sensible.***

The fact that reasoning and rationality are not mutually exclusive does not also mean that the two are mutually inclusive. People regularly provide invalid reasons for a variety of things. The entire point I've made is that people want this and they ought to be allowed to use their computer the way they want.

The fact that people oppose actively and openly providing that for fear of security ignorance is being tacitly ignorant that the sort of people who use the AUR are either fully aware of the security ramifications and have made what they find to be appropriate precautions, or do not care.

No amount of not providing something like an AUR will prevent someone from opening a massive security hole in their system to download BigBootyBitches.totallynotavirus.sh

>That is a very objective way to frame something that is supposed to be "subjective", i.e., something is perceived similarly by an average quantity of users.

Something subjective can be statistically objectively measured. Even if you dislike that fact. The average desktop user cares far less about security than you are implying. This even applies to the average Linux user. Know how I know? The fact that people aren't running like hell to Wayland is a great example of that. X.org is a know, quantifiable, and obvious security risk to any system it is actively used on in place of Wayland.

>Not really, there are distributions that have standards for packages in their official repositories that the AUR lacks.

You are intentionally ignoring the point, and entirely misrepresenting the statement. I am stating that such caveats, I believe, do not warrant entire exile from official or semi-official distribution. I am not disputing that such exile happens or exists.

>For example, Arch's official repositories are subject to reproducible builds and hardening features for binaries.

Arch is not a good example because the AUR is a semi-official distribution method.

>seems lax and a fallacy.

It is not lax, and is in fact proof evident by any and all instances of vulnerabilities making it through to any user of a distribution from an application that is distributed through official means. It is also not a fallacy. My argument is not, "Official distributions are also bad, so this stuff is okay." My argument is, "If this stuff is bad ***because*** of it's poor quality or security, then official distributions are evident not solutions since they suffer from the same issue." Also, dismissing an argument whole-sale simply because you find or claim it to be fallacious is, itself, [fallacious reasoning](https://effectiviology.com/fallacy-fallacy/). If you insist I format this in a non-fallacious logically (near) perfect manner, then fine:

If, as affirmed by you, the reason that unofficial or semi-official distributions are worse than official distributions is quality control and security screening, it is evident that official distributions are, just as well, subject to quality control and security screening flaws. Thus the argument is insufficient or inapplicable as-is. Either we must fall to the slippery slope of how much quality/security is acceptable, or we must accept that quality control and security control are logically valid reasons to not consider semi-official distribution methods such as the AUR.

>I didn't say it was the sole or the chief, but one reason.

I would suggest it's an irrelevantly minor reason.

>Why do you think this is not the case?

I should clarify, they would never host a direct github download and/or install script for anything that was directly or indirectly associated with the distribution. This immediately makes many projects impossible to host on the official repos.

>With freedom, there are costs to pay. Freedom is never free of consequences.

I agree. However, my entire point here is that freedom ought be the utmost goal, rather than security. Security can be pursued on an individual basis far better than it ever could be on a repo basis. And that's entirely ignoring the fact that people should frankly consider anything outside of their LAN to be adversarial.

>MS Windows and macOS are the empirical refutation of that.

Do you think that MS Windows or macOS became what they are without user choice? Windows wasn't always restricting user freedoms. It used to be just as embracing of user choice as Linux is today. It has slowly lost popularity, and only maintains its position due to general inertia. And macOS ***IS*** niche. This is all ignoring that Linux will never be as good or better than either in anything other than user choice for the general consumer.

>You can have one or the other, but not both.

You absolutely can. Having access to something like the AUR does not necessitate you use this, and this is the part that irks me the most. It's not like distros can't just say, "Provided with limited screening." Or something to that affect. The AUR makes very clear that that is a perfectly fine method of communicated the reduced security of the repo.

>It won't be 100% bulletproof, but there are measures that can be taken to ameliorate the risks.

If the amelioration of the risks comes at the cost of people seeking more insecure methods, it has not ameliorated the costs at all, but rather inflamed the issue.

Let's take one of my favorite pet issues in regard to this. The entire purpose of a repo is to prevent people from downloading random binaries from random sites on the internet. But if someone wants to download something that isn't in the repo, what recourse do they have? Well in Arch, maybe it'll be on the AUR, which while definitely less secure than the main repos, is far from just some random site on the internet. But on any distro without such a community sourced repo with low barrier to entry, they will have to download it from that random site on the internet. And perhaps even run random, untrustable code on their machine blindly.
See my other reply
It‚Äôs not as integrated as on snap though. If you go to the Flathub download page for an app, there is no option to switch channels there. You also can‚Äôt switch channels from Discover or Gnome Software.

>Are you talking about GTK themes

Yes. Each one needs to be packaged for Flatpak and put on Flatbub. Or else your flatpaks won't use your themes.

>GNOME extensions

No. Those shouldn't be managed by your package manager either, especially on an image based system

GNOME extensions are out of scope for Flatpak. They're not separate executables, they can't be sandboxed, etc. They're managed separately. There's actually nice Flatpak apps to install/update them though (at least in development iirc). But they are not themselves flatpaks 

>If you install a certain app, how do you choose to install an extension for it if it's not displayed in FlatHub, specially in immutable OS like Silverblue and Kinoite?

Idk about KDE discover, but in GNOME software apps that support extensions will have check boxes on the app details page. If you select some, it'll install those extensions for you. It's not used all that commonly.

For things like development SDKs, compilers, etc you'll likely need to run a command to install them. Or, if your IDE is aware of Flatpak (like gnome-builder is), they'll magically get installed by your IDE if needed

Things like video codecs, gtk themes, video drivers, etc are all installed automatically by Flatpak when possible (usually during an update). Things like icon themes and fonts are just pulled from the host

>Who said I can/want to install a theme/extension system-wide so that Flatpak detects it and install its version?

You don't have to. Flatpak reads your local gtk-theme setting and installs the appropriate theme so your Flatpaks have the right theme. Of course the prerequisite is that the theme is packaged for Flatpak.

>but how can you think they can compile every single master branch of every FOSS project everytime there is a new commit?

Flathub doesn't. But each project is free to do so. GNOME, for instance, has a nightly repo where they rebuild all of their packages (the SDK, all their apps, etc) from the master branch daily. So you can install stable GNOME apps from Flathub, unstable GNOME apps from flathub-beta, nightly/git/master branch GNOME apps from their nightly repo

A lot of open source projects also build Flatpaks in their CI pipeline, so you can just click & download the latest git commit (or even an unmerged merge/pull request) and install it for testing

Some specific platform things, like mesa-git, can be enabled by setting the right environment variables to force it on. You cannot install arbitrary versions of libraries in the Platforms because that kinda breaks the point of the Platforms. But you shouldn't need to anyway. If anything you can just install a newer version of the whole platform; at least for GNOME git master builds exist

>The point of example-git entries on AUR is that you conveniently build the project locally when needed with your computation power.

Flatpak allows you to do that too. It's geared more towards development and less towards end-users, but the functionality is there. You can pull down a flatpak manifest and build a flatpak from it locally relatively easily (at least compared to compiling things usually). And again IDEs are helpful here: in gnome-builder, you can git clone a project, hit the run button, it'll automatically install dependencies, build the Flatpak, and run it all with one button press.

The experience is closer to mkpkg (download the Flatpak manifest, run a command to build a package, then another command to install it) than to yay or whatever other aur helpers exist, but I suppose there's no technical reason a helper couldn't be written. Again, this is assuming the app publisher isn't making nightly builds, which they may well be

Of course Flatpak isn't and can't be a 1:1 replacement for the AUR. Its behavior is somewhat different, and so are its goals. But it covers many of the use-cases the AUR seems to be trying to cover. At least for apps.
I am sorry, but your argument is not logically coherent. Also, the AUR is not "semi-official", but [completely unofficial](https://wiki.archlinux.org/title/Arch_User_Repository).
You can from GNOME software. They appear in a drop down in the top-right corner of the window. You can't _switch_ channels because Flatpak just lets you parallel-install them: so you can install the development version of an app next to the stable version, launch both, etc. The snap store app was (is?) just a fork of gnome-software, and I know at least at some point they used the same drop down to select channels

Idk anything about Discover. And yes seems like you can't do it from the Flathub webui. A lot of the time alternative channels aren't in Flathub proper though (they're either in flathub-beta, or in a project-specific nightly repo, or they're installed from a flatpak bundle you download from an unmerged MR, or whatever). It's not used quite the same way but it's pretty close.
> Yes. Each one needs to be packaged for Flatpak and put on Flatbub. Or else your flatpaks won't use your themes.

And who said "GTK" themes? Assumptions...

> You don't have to. Flatpak reads your local gtk-theme setting

Again, what about other kinds of themes? And even if it can read the setting, how do you set a theme if it's not already installed? The mechanism you mentioned is a patch for GTK themes that are already installed with another method.

> Flatpak allows you to do that too. It's geared more towards development and less towards end-users, but the functionality is there. You can pull down a flatpak manifest and build a flatpak from it locally relatively easily (at least compared to compiling things usually).

I know it, and wouldn't a command line tool and a repo of recipes like AUR's convenient to do so?

`flat-ur install x`

`flat-ur update`

etc

This is what OP was talking about, they can't say it in the right way but you guys could figure it out.
My argument is logical, repeating the claim doesn't make it anymore true. Also, the AUR absolutely is semiofficial. It's got official Arch branding, uses official Arch servers. Even has some official Arch vetting! The only way in which it isn't official is that all of its contents are provided as-is and not in official capacity from Arch.

If the AUR isn't semiofficial, nothing is. I think given this point it's not worth pursuing this discussion with you anymore as you evidently don't want to engage in good faith.

>And who said "GTK" themes? Assumptions...

Just talking about things I know. I don't use any Qt apps (or themes in general for that matter), and I really don't know how their theming system works. I'm pretty sure it does support them somehow, but that's all I can say

>And even if it can read the setting, how do you set a theme if it's not already installed? The mechanism you mentioned is a patch for GTK themes that are already installed with another method.

The fact that you can set the gtk-theme setting to be an arbitrary string aside, when would you try to set a theme that isn't installed outside of Flatpak? You will have GTK apps that live outside the Flatpak, and anything you install through flatpak is isolated to Flatpak, so what do you expect to happen in this case? Themes are something that flows from the host down into Flatpak to work properly, or else you get Flatpak apps that look different from host apps.

>wouldn't a command line tool and a repo of recipes like AUR's convenient to do so

Meh I don't think so. The app developer is most likely building Flatpaks in CI anyway, so they can publish a nightly repo. If not, you can install builds from artifacts they generate in CI. And for the apps that do neither, you can just build it yourself from the manifest pretty easily. If you really want unstable versions just for the hell of it, you should probably be using a beta release and not git releases of apps. And the most useful usecase for -git components, testing latest parts of system libraries like mesa, etc, either has special magic support in Flatpak or is completely unsupported because of technical limitations. So I'm not sure I see the benefit to duplicating the AUR's behavior here
> The fact that you can set the gtk-theme setting to be an arbitrary string aside

Are you suggesting to manually change a string instead of listing themes in a repo/FlatHub/software center/some GUI?

> when would you try to set a theme that isn't installed outside of Flatpak

I'm talking about themes in general, not GTK nor application themes but everything you can find in AUR and that can be distributed in a distro-agnostic way (with Flatpak or something else). Forget about this GTK theme mechanism because it has nothing to do with my point.

> The app developer is most likely building Flatpaks in CI anyway, so they can publish a nightly repo. If not, you can install builds from artifacts they generate in CI. And for the apps that do neither, you can just build it yourself from the manifest pretty easily.

And what if devs are not committed to nightly builds and one of them ask you to try their latest commit to see if they fixed a bug you have? With AUR it's like installing/updating a package.

You are thinking about large projecs with CI but the AUR is full of recipes to build hobby projects hosted on GitHub and almost forgot. Instead of looking at their README for building and installing instruction you have instead an entry in a package manager, you don't even need a web browser.

Sometimes on AUR you can install a -git version that has changes important for you but the dev won't make a new release for months or even they abandon the project.

Flatpak and FlatHub are a new thing and projects using them are mostly fresh, but what will happen in the future of niche projects?
https://lutris.net/games/roblox/
Have you tried [Bottle](https://usebottles.com/) ?
Fedora is the only distro I had problems setting up grapejuice on. My problem was 32 bit wine, I recommend that your friend uses a Debian based distro first because fedora can kinda be a hassle ito getting certain packages.
About the automatic package updates, why didn't you, er, I mean, "your friend" install the [dnf-automatic](https://docs.fedoraproject.org/en-US/quick-docs/autoupdates/) package? For many good reasons not everyone wants that feature, so it is reasonable of Fedora to make it opt-in rather than opt-out. When moving to a new system reading the documentation is always a good thing to do before complaining. That said, perhaps "your friend" is better off with Windows.
Your friend should have never switched to GNU/Linux because it simply cannot cover her needs.

The community is doing the job the companies should do and it is doing that for free. Your friend expects things to work the same way like on windows but that is not possible because most of the companies do not offer active GNU/Linux support, especially for desktop.

I vote with my wallet because I do want to help GNU/Linux adoption. I do not buy products from companies that do not support my operating system. I search for alternatives or I rethink my workflow. I am willing to learn, and I enjoy doing that, but most people don't care. There is nothing wrong with it, everybody has different priorities in life. Hence, they should stick with what works for them. 

What people like your friend fail to understand is that GNU/Linux is not a Windows 'alternative'. It is not a clone. It is a different operating system. Since it is a different operating system it works differently and since it has a small percentage of desktop users it is not supported by companies like Windows is. 

It is a chicken and egg problem. I get it. GNU/Linux has not enough users so the companies do not support it. But I would prefer that the users to stay on Windows, rather than switch and then spread the voice how things are broken on GNU/Linux, since they often do not understand the whole picture of the community doing the job, completely free of charge, that the companies themselves are supposed to do.
How not to get people to use linux: 1) Use Fedora as your starting distro 2) Try to get the kind of people who play Roblox to use Linux
Interesting. Aren't Ubuntu und Ubuntu-bases distributions particularly newb-focused, though?

Edit: What's your problem, all? Care to explain?
Linux will never be that 100% beginner friendly.  As long as they want some crappy app on non Linux, things there will always be friction or another excuse.  The goalposts are always moving.   This month its roadbox, next month its some new platform or android game.

I have found it to be a waste of time to suggest or even give it to anyone.    Just use it as your superpower and forget everyone else.d
I think they already 'switched' back to windows, suggestions at this point provide almost no usefulness.
What's wrong with this comment? It provides a partial solution and a reasonable explanation why this isn't the default on Fedora. And yes, people talked into Linux by others are not likely to stay on it, because they are sceptical from the beginning and feel vindicated with every small thing they expect to work differently, because Windows.
Why are you quoting "your friend"? 

It's not to them to install packages like that. It's up to the distro to create the right experience for beginners.

If you use flatpaks, I believe they self update flatpaks from Software preferences. Mine seems to have set that up automatically.
Egg comes first. Egg always comes first with chicken and egg problems.

Windows 11 is more broken than Fedora 36. I am not paying more to get less with Windows. I would rather get my RHCE and move everyone else over. 

I can get support form companies so I don't care if it s the same companies that are selling me the hardware.
If they bought a laptop from say System76, they'd be able to call up their customer service and get help that way.
Why are you gatekeeping an OS lol
Not focused. Friendly
Upvoted. I don't think most questions should be downvoted.
The downvoters will soon come to realise my hot take above as reality and not a negative.
As long as I don't have to use Windows I actually don't care what other people use. I just think it is profoundly unfair to the individuals who out of their own time and passion support users without even being paid. The least the new users could do is to inform themselves more about the system before switching.
For things like Roblox, Photoshop or whatever runs exclusively on Windows?

I don't think so. I think they would help with the hardware and operating system issues and not with the programs you want to run in the userland.

Heck, I don't even know if they would help you with devices that do not have the drivers in the kernel.

And this is not because they are not a good company. The variety of the issues is so broad and it would not be fair to place on their shoulders the job other companies are supposed to do.
Yes, very important distinction. Lots of 'noob-friendly' distro users that have been using Linux for 20 years. There is no such thing as a progression from 'easy distro' to 'hard distro', since, outside of Android and ChromeOS, you can be doing hard things in any Linux distribution.
Are they mutually exclusive? I don't understand the reason you (or anyone for that matter) downvotes this simple and correct statement.
I pretty much agree with you with one caveat. I would rather use Windows than Mac OS X if those were my only two options.
And I didn't dispute this in any way. In fact, I didn't even talk about this aspect at all.

I simply stated the fact that Ubuntu is a "newb-focused" distribution (it started as "Linux for Human Beings") and I did so in the OP's very own words.

I don't get you sometimes, r/linux.
Saying that Ubuntu is noob *focused* basically implies that only noobs use it which is obviously not true. Saying that ubuntu is noob *friendly* doesn't say that it's for noobs, only that it's easy to use.
I have used MacOS exclusively for 8 months before getting fed up with it. I would pick any BSD over it any time of the day if I could not run GNU/Linux.
The AUR Ubuntu never had

Pacstall is the AUR Ubuntu wishes it had. It takes the concept of AUR and puts a spin on it, making it easier to install programs without scouring github repos and the likes.
Pamac makes it pretty easy too. Is this like pamac?
Sorry, was just saving folks the click.
All good. I wasn't criticizing. I'm actually curious how this compares to pamac
Pamac is a GUI package manager that aggregates multiple sources such as the AUR, flatpak, snap, and pacman. We do not do this, therefore I don't think it's a fair comparison
>Pamac is a GUI package manager that aggregates multiple sources such as the AUR, flatpak, snap, and pacman.

Pamac is also available as a CLI version (https://wiki.manjaro.org/index.php/Pamac#Installing_Software). As far as I know, you have to install plug-ins for snap and flatpak to be supported. As far as I know, only Appstream is supported out of the box, besides the official package sources and AUR, if I'm not mistaken (https://wiki.manjaro.org/index.php/Pamac#Overview). And Pacman is not a source but the package management that Arch uses as standard.
So it's yay?
Oh, I didn't know it was also available as a CLI. I'll look into that
The number of people, even on high enthusiast ecosystems like Linux, who does voltage tweaks are very few. Wayland and good FOSS drivers are frankly more important from both a market, user and my personal perspective for viability. Saying this as someone who has held a world record in a sub-category on 3DMark for about a year, so I am certainly not against manual tweaking. Not saying they shouldn't do it, I am just saying that claiming it's critical for their market viability on Linux is a exaggerated. That said it's not really an one or the other scenario.
So, I'm curious... I'm an enterprise developer, I never game with my machines. I built a new daily driver a little while ago, and got a 16 core zen3, and gobs of ram, and fast ssds. It's generally quite capable, but at the time graphics cards were Richland expensive, so I just kept my 10 year old card, I think it's a gtx 1080 iirc. 

My question is would I even notice it if I dropped a modern GPU in there?
Have you tried GWE (GreenWithEnvy)?
I'm using `nvidia-smi -pl 130` limit power to 130w. Seem pretty ok
in the past I've used a combo of greenwithenvy (for fans) and nvidia-smi (not voltage but power limit)

the last drivers they worked for me was on 460.67, and newer versions didn't work. not sure if the current ones work, I stopped trying ;_;
https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks#Overclocking_and_cooling
Isn't tweaking graphics cards with software (like afterburner on windows) mostly snake oil anyway? Everything is managed by the card's firmware, and when "overclocking" or "adjusting voltages" etc you are in reality just making minute changes which are still well within specific limits hardcoded into the firmware. More like allways driving your car in 4th gear and never change to 5th, sure you get slightly better acceleration, but fuel consumption goes up too.
Was this recently removed?

I used to be able to that easily in the Nvidia control panel far more easily than with AMD 3rd party tools back in early 2010s.
I have a newish notebook with 64GB of RAM and Nvidia 1650  intending to run some games on Linux but life got in the way and it's now just a bunch of unweidly VMs and containers - so will this GWE work and does it have any significant impact on my "gaming notebook"?
>Nvidia needs to implement voltage control so that undervolting is possible, even more with the new generation of gpu's being more power hungry.

It's already possible. Just install GreenWithEnvy or gwe and you have complete control of your GPU.

&#x200B;

>Currently this and some specific games not supporting easy anti cheats linux port are the largest blockers for me daily driving linux instead of windows. (gamepass is another reason to dual boot I guess, hopefully it will come to steam before win 10 is EOL)

Not related to your post and I guess it's a rant. Okay. Known issue.
I'd say its more important to add a proper control panel like the one in windows without losing the current features.
Depends on if anything you do can leverage the GPU for hardware acceleration. 3D modeling/rendering, video encoding, crypto-whatsits (coins, nfts), etc.
A 1080 is 6 years old, not 10.
Honestly, other than a lower idle power draw, probably not a great deal, no. 
Your card supports 4K 60Hz monitors, so no issue there either. And your card is more than powerful enough for basically any GPU-accelerated tasks any normal person is likely to do. Video codec support won't be an issue either.

Unless you went with an AMD card for the better Wayland support/not having to install Nvidia drivers.

I wouldn't say it's worth upgrading if you're currently having no issues tbh
I want to power limit my card too. What is your card and how did it impact performance? Did your temperatures drop?
You can lower a 3080/3090 power consumption by like 100-150w without losing performance, so no.
Seems to have been removed since pascal - 2015, 10-series nvidia gpu‚Äôs.
GWE doesn't allow changing voltages, only power limits, which does nothing for coil whine or efficiency.

&#x200B;

As for the other part, yes a small rant.
I do use OpenScad for 3d modeling, but the last time I looked into it, it was cpu bound. I think the only thing that might benefit is chrome, thanks for the insight
I'm wrong on the card, it's a GTX 980, and it was not 10 years, but it has been a while.
I have gtx 1070 max power is 180w. Temperature drop 6-10 C. The performance basically the same
So why isn't this the default setting if there's no loss in performance?
To my understanding, you can't control voltages on many NVidia cards directly at all, unless the AIB manufacturer adds such functionality themselves, making it their special sauce, not NVidia's.
You would notice if your desktop is running in 4k and you are gaming at 4k‚Ä¶otherwise not really
not all chips can use as low a voltage, there‚Äôs binning which selects better chips for more expensive sku‚Äôs, while disabling parts of the chips for lower ones(3060-ti from 3070 etc). the voltage they are shipped with basically guarantees that they are stabile.

and as some cards are power constrained you can even get more performance by undervolting.
Nvidia have to ship cards which will run reliably and stably out of the box. If you're willing to effectively do a ton of QA testing on your own card to find its limits you can usually get quite a lot of performance out of it that it wouldn't make commercial sense for a vendor to provide in bulk. I believe this is particularly true of Nvidia's 30xx series, which had to be manufactured with a particularly temperamental process due to an industry-wide squeeze on capacity.
Because that would lower the amount of cards Nvidia/AMD could sell, because not *every* card could do that. Especially not after years of use. Imagine if 80% of 3090s could operate fine with a 280W TDP, that'd be 20% of cards that they could no longer sell at $1500+, they'd have to use that die for a 3080, which they can only sell for $700+. Nvidia, knowing that (sadly, IMO) most buyers don't give two shits about power consumption, would rather set a higher voltage and sell the cards at a much higher price.

It also necessitates greater testing on their end, which costs money.

It's not dissimilar with CPUs. Undervolting Alder Lake, for example, takes it from being quite inefficient, to being completely respectable.

AMD used to be notorious for pumping waaaay higher than necessary voltage into their cards. I used to be able to reduce power consumption by ~25%, and the card would actually be *faster* due to it not hitting temperature or power limits.
You can absolutely change voltages, you just can't go past a hard limit. You can underfoot all day though.
Just running a desktop at 4K is absolutely fine in my experience (as of last year when I did it) on a Radeon 7850. A 980 would have no issues.

Gaming, sure, but they explicitly said they don't game.
Most people when the hop distros and want to keep their stuff, just take their /home and whatever data directories (movies, steam/game library, whatever) with them on a new install.

I'm not sure what's there to prove in the exercise though. If you took everything down to just the kernel, only the necessary modules, if any, and the barest of GNU toolchain you could put whatever was compatible with it.

Might as well use LFS docs and try to build something from source.
Ubuntu to Debian? Yes, given sufficient effort.  Google actually did this for all its Linux workstations and laptops several years ago. But they had a dedicated team to develop a one-off script to do the conversion.  It was pretty impressive‚Ä¶ basically, you just kick off the script before going home and you come back the next day and it's Debian.

Arch to Debian? Would be much harder due to different package managers.
Technically yea, this could be done. Packages are basically archive files with some useful metadata.  With enough removed and bootstrapped to replace, it should be more or less the same

It's just... Beyond user data in /home not much from before would be left. You'd either remove or replace pretty much everything - binary/library/config/manual

It's like that philosophy question about replacing parts on a boat.  Is it the same installation \[boat\] if you replace everything?  How about if you then take the original parts to reassemble *that*?

Edit: There are quite a few practical challenges though. Ordering for example.

A lot needs python - often the package manager itself.

There will be a delicate time and way to swap from old to new to continue the conversion process
> doing all this while keeping the original gnu/linux part completely intact

and what would that be exactly? you are going to replace everything, including the kernel and the entire userland binaries/configurations
Hmm... yes, it *could* be possible in theory... but I don't think you'll find much in how to do it, so I don't think it can be done in practice.

For example, Arch uses pacman as the package manager. Debian uses deb. The package manager describes, for a given package, what files need to be installed, where the files should be placed, what configurations to the system need to be made (do I need to update an existing configuration file? Or place new ones? The users or groups need to be defined?). In order to convert from pacman to deb, there would need to be a translation of sorts to describe how these things are mapped over, and that's assuming that they even agree on how the outcome of the configuration should be (which may not be true!). So doing a conversion of package managers would require doing something challenging, like, evaluating all the installed files (and configurations) for the given Arch installation, identifying which Debian packages would contain those files, determining what changes to the installed files (and configurations) that would need to be made to make them conform with Debian, then replacing the package management database with that information. This would be hard to do.

It would be so much easier to take an existing Arch installation, create a separate set of partitions and create a chroot'ed environment and install Debian there, then swap out the main system then it would to do some sort of conversion. So you don't need to shutdown a distribution to install another distribution (short of a reboot), but you aren't technically converting it either. You're doing a parallel install of the new distribution next to an existing distribution and swapping it out with a reboot.

The thing to keep in mind about Linux distributions is that they don't do a good job of distinguishing system packages from user packages... so system packages (like the Linux kernel, the GNU core utilities, and so on) are indistinguishable from user packages (like LibreOffice, Chromium, FireFox). Things like FlatPak, AppImage, and Snaps are trying to... sort of, fix this, but without considering things like this, all the programs on a typical Linux distribution are going to appear like a huge collection of everything and they're coupled, so you can't safely replace one with another, this is why the distribution model is so big... one of the roles of the distribution is to make sure that all the various packages, wether they're system or user, work together. This is also why third parties sometimes struggle with providing software and why it's typically preferable for third parties to work with distributions to get their software packaged specifically for the distribution.
Well... it's possible to do, both theoretically and practically, although it's not really worth the effort, and you'll probably end up with a few components not working well enough if you miss something during the process.

You would need to have full knowledge of both the distro you are on now, and the target distro, to slowly swap files and configurations and gradually migrate. You would also need a lot of free time xd
Bedrock Linux?
As a general thought exercise, if you are genuinely curious, you can look into the Linux From Scratch Project:

https://www.linuxfromscratch.org/

You are not knowledgeable about Linux by your own admission, but if you want to get an inkling about the idea and what would be required, this would be a good place to start. If you run into a topic you don‚Äôt understand, Google it and then continue.

If nothing else, it‚Äôs 100% free knowledge, which is never a bad thing to have, even if you never dig into it deeply.
I've done it before and it's not worth the effort. Unless you want to do it as a learning exercise.
because debian and arch dont use the same package manager theres no way to tell debian anything left over from arch is still there.

It is, however, possible to unpack a minimal system into ram (tmpfs), switch root into that so that nothing on your normal root filesystem is being used anymore, gut that original root and replace it with a different distribution, switch root into the original root again and be running a different distribution without even rebooting (still running the kernel from the old distribution). You'd ideally do this with physical access from the console, but could in theory do it over ssh but its more risky.
>I wonder if it's possible to convert a Linux install to another distro

Using linuxfromscratch.org you can build your own distro from an existing OS.  
You can also check if MX Linux fits your project, as it features a snapshot tool. It does a system snapshot saved to ISO that can be used for system recovery. https://mxlinux.org/wiki/help-files/help-mx-save-system-to-iso-snapshot/
Moving from Arch to Parabola or Artix isn‚Äôt too difficult although they have the same package manager.

There‚Äôs no technical limitation, only practical.

See also: linux from scratch
allways
Could you do it? Probably, it's gonna be annoying and probably wont work completely right afterwards.

Should you  do it? I mean, if you want to. Sure. But it probably wont be a pleasant experience.
Is it possible? Yes, in the same sense that summiting mount Everest is possible.

If you are an experienced distro developer / packager, you could, but the juice isn't worth the squeeze. Backup /home and reinstall from scratch.
bedrock linux
Yup absolutely, though some distros are much easier than others. I've used this project a few times: https://github.com/jeaye/nixos-in-place to install nixos when I couldn't access the physical machine. It works surprisingly well. Thanks to the way nix is structured you install the entirety of nixos in the same filesystem and then switch to it. I imagine most FHS distros are going to be much harder since you're going to be in an intermediate broken state with lots of conflicting files for a bit while everything is switched over.
Yes it is completely possible.

But not suggested anyway.
You can boot to a live USB or ssh in, then reinstall over the running system using something like debootstrap, and finally reboot. I've done that at least three times in the past twenty years for various reasons.
Yes, but there are better uses for your time
You can, but it is messy. You run a bootstrap in a directory and then make grub load it.
I mean have done from Ubuntu --> PopOS, not sure if possible from Debian --> Ubuntu or further than that but... Wasn't there a Bedrock Linux or something made for this concept?
> Arch to Debian? Would be much harder due to different package managers.

dpkg is in the Arch repos, not like it's a huge problem

https://archlinux.org/packages/community/x86_64/dpkg/
>It's like that philosophy question about replacing parts on a boat.  Is it the same installation/boat if  you replace everything?

[https://en.wikipedia.org/wiki/Ship\_of\_Theseus](https://en.wikipedia.org/wiki/Ship_of_Theseus)
Running server thousands of kms away that was broken such that it wouldn't survive a reboot. That was my reason for reinstalling over a running system.
Oh my, that sounds really fun actually! I may install this on my install just for the novelty of using dpkg on arch. Whole new levels of "I use arch btw"
That's it!  Thank you for sharing
I'm glad they're cleaning up their store. Imagine if something like the debian repo just had duplicates or old repacked packages strewn about. At the end of the day, a software store isn't unlike a central repo these days (especially since distribution is directly tied into it).
That actually makes more sense, but why didn't they word it more precisely when they first announced the new policy?
The bigger question is why are they being accepted so easily.

&#x200B;

Microsoft store isnt even as big as playstore or something, it shouldnt have made it in the first place.
Stop supporting Microsoft. They are not here for our benefits.
If I write a decent open-source image editor and release it under the GPL; it \*is\* anyone's right to package up my software and sell it on the Windows market place (so long as they release any code changes publicly). Microsoft will blanketly call these guys "scammers".

Just like it is my right to sell it myself. We will have two competing products on the store. Their release might be more popular or me as the author might be more popular. Both bits of software are valid and legal.

Because... if I didn't release it on WinStore (perhaps I prioritize other platforms), and no-one else can; suddenly the software is not available to users on the WinStore platform and a commercial vendor gains a foothold potentially with inferior, more costly software. I would rather someone else pushes my free software (and profits) than my users having no access to it just because they were dumb enough to tie themselves down to WinStore.

What Microsoft is doing is still unacceptable.

&#x200B;

>was designed to catch scammers that lazily repackage FOSS software for $$, probably needs an exception for the copyright owners.

Read the LICENSE file. That is all that is necessary. No exception is needed. This is not how free software works.
>old repacked packages strewn about

We call those derivate distributions
Microsoft doesn't excel at wording things.
I honestly read initially it was only for those selling **others'** apps. I'm very critical with Microsoft lots of times, but I don't think this was malintended
Perhaps it was a trial balloon.  If there was little outrage, it might've become active policy as written.
This. The intention is not relevant - only the wording.
Probably couldn't even think of someone at the same time offering their product for free and also selling it.
Because money. I'm sure they've now determined that they're at risk of litigation, so to protect the shareholder's investments they've elected to revise their policies. Publicly traded corporations have an obligation to maximize return to shareholders, so that drives every decision. This isn't a moral judgement, just a fact of modern day capitalism.
>If I write a decent open-source image editor and release it under the GPL; it \*is\* anyone's right to package up my software and sell it on the Windows market place (so long as they release any code changes publicly). Microsoft will blanketly call these guys "scammers"...

Because regardless of the license, if they are being used with the intent of scamming people, they should be flagged as such.

And it's Microsoft's right to decide what to allow in their store. Why are we pretending that the scamming is actually not happening? We all know it is. So long as they have mechanisms in place to differentiate the legitimate from the copycats... Wake me up when there's actually any substance to this FUD and any FOSS developer is actually being hurt by the new policies...
Yes, I agree. From a legal perspective, there's nothing wrong with someone taking a GPL licensed program, changing literally nothing, and selling it. That's not where the GPL controls come into play. The GPL controls come into play around access to source code and stuff like that, meaning I can't distribute the binary without providing a reasonable means to access the source code.

That being said, Microsoft is within their rights to restrict the sale of these products on their store *as a matter of policy*. If you review their [policy](https://docs.microsoft.com/en-us/windows/uwp/publish/store-policies?irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=(ir__jhzhxwya2ckf60iwftqik9tzl32xvrvjc3vylwun00)(7593)(1243925)(kXQk6.ivFEQ-oESgYYWwpc5azTwRBVLSnw)()&irclickid=_jhzhxwya2ckf60iwftqik9tzl32xvrvjc3vylwun00#108-financial-transactions?ranMID=24542&ranEAID=kXQk6*ivFEQ&ranSiteID=kXQk6.ivFEQ-oESgYYWwpc5azTwRBVLSnw&epi=kXQk6.ivFEQ-oESgYYWwpc5azTwRBVLSnw) you will find that they have many restrictions that do not fall within the realm of illegal, yet they're forbidden. Section 11.7 forbids "Adult Content" and section 11.9 forbids "Excessive Profanity and Inappropriate Content", these are just two examples of what can be restricted on their store. Before someone says "yeah, but that might be illegal somewhere!" I would direct you to section 11.10 "Country/Region Specific Requirements" that basically forbids anything targeted to a country/region where said content would be illegal, so there's no need to blanket forbid "Adult Content" or "Excessive Profanity and Inappropriate Content" across *the entire store* just because it's illegal in *some places*. I know this isn't an unlimited right that Microsoft has with their store, but I'm just pointing out that just as with any other store, they can place restrictions on what they allow or do not allow to be sold. Not every decision they make is driven by the letter of the law.
This is correct.

**Permission to sell open source software does not belong exclusively to the developers of said software, however unethical you might think it to be, this is permitted by the license the developers have chosen.**

They could just as easy have chosen a different licence, e.g: non-commercial, or e.g: GPL with selling exceptions (a common practice).

This myth that you cannot sell open source / free software needs to die already.
Funny how you're downvoted for stating facts about the GPL and Free Software. Oddly enough, I bought my first CD of Red Hat off Ebay sometimes in 1997 - 1999 (RH6). How was that any different or scamming?
Exactly, it‚Äôs one of the advantages of free software and GPL. Everyone, including the developer and the user, have the freedom to do whatever they want with it, and they can‚Äôt ‚Äústeal it‚Äù or ‚Äúscam‚Äù people because the license requires you to share your code.
copyright vs ‚Ñ¢ vs copyleft
Spicy lmao
You could say it's not their power point.
> Microsoft doesn't excel at things.

Fixed that for you.
Yeah. If you followed the windows store since they announced opening it up to non-uwp apps, one of the main issues people had with it was the amount of spam apps, and how difficult it was to find the official distributions. 

It was pretty clear to me that this policy was a direct response in an attempt to fix that. I didn't assume they were trying to remove all FOSS apps - it makes no sense why they would want to do that, and they had a clear spam problem to address, so of course the policy is probably because of the latter.
That's not the wording I first saw, so maybe some journalist was probably deliberately stirring shit.
[paint.net](https://pain.net)  does it, plenty of apps do this they have free opensource versions for example anki the popular spaced repetition app charges for ios app alone while android and desktop web are free.
> This isn't a moral judgement, just a fact of modern day capitalism.

I dunno, I judge that this fact of capitalism leads people to immoral decision-making and immoral business practices.
**Microsoft absolutely has the right to dis/allow whatever they wish on their store.** And I'm sure there are people misrepresenting apps, maybe pretending to be the developer, and trying to trick people into giving them money. **But simply distributing open source software you didn't write is not inherently a scam.**

If you compile Free Software and distribute it for a fee, people are paying for a service. They're paying for the time and effort you spent compiling it; potentially offering tech support or a user guide; or maybe providing a bugfix or a feature that isn't in the official release. If the program isn't already available in the store, some people may enjoy the convenience of discovering & installing it from a central location instead of hunting for it elsewhere on the internet *(like you can from your distro's package manager)*.

This was more common when fewer devs offered binaries or installers, computers & internet connections were slower (if you even had internet access), and it could be time consuming to download & compile it yourself. Third parties often sold CDs of open source software & bundles of software through the mail and sometimes offered documentation or support that the developers didn't provide. It was also common for magazines to provide distros or other software on CDs with the purchase of each monthly issue.

Many companies have sold their Linux Distributions including the open source binaries from other developers that they compiled to run on their distro. RedHat still sells RHEL and promises 24/7 customer support with the distribution. SUSE does the same thing with SLES.

On the flip-side, Xchat used to sell official windows binaries direct from the developer. And you were free to compile the source yourself at no cost but your own time. This lead to unofficial builds being distributed by 3rd parties at no charge. The developers knew this but were giving the users an opportunity to compensate them and the peace of mind that came with using official binaries rather than unofficial ones that could have been maliciously altered by a 3rd party.


**GNU even publicly outlines, how Free GPL software may be sold:** https://www.gnu.org/philosophy/selling.en.html
> "Actually, we encourage people who redistribute free software to charge as much as they wish or can. If a license does not permit users to make copies and sell them, it is a nonfree license."
> 
> "Free programs are sometimes distributed gratis, and sometimes for a substantial price. **Often the same program is available in both ways from different places.** The program is free regardless of the price, because users have freedom in using it."
> 
> **"So if you are redistributing copies of free software, you might as well charge a substantial fee and make some money. *Redistributing free software is a good and legitimate activity; if you do it, you might as well make a profit from it.*"**
> 
> "Strictly speaking, ‚Äúselling‚Äù means trading goods for money. Selling a copy of a free program is legitimate, and we encourage it."

They also encourage 3rd party distributors to donate a portion of their profits back to the development of the project they're distributing (or to the FSF, lol) to *"advance the world of free software"*.

If you're honest and open about what you're providing, I fail to see how it's immoral or a scam; It's a service.
Absolutely Microsoft can limit what they sell in their own stores. But it is still completely unacceptable from a moral and competitive standpoint. They can "clarify" as much as they want but they are still being a bunch of anti-competitive pricks and lying to consumers.

To be honest, this whole locked down AppStore concept is criminal. Only a fool would use them.
Indeed; now it is getting to the point that \*you\* as an individual can't make money on free software. Only the overlord gods such as Google, Microsoft and Apple can.

i.e Selling a port of the open-source OpenSSH in WinStore would be "banned" via Microsoft's criminal ruling and yet they happily sell it as part of Windows...

Yes, Microsoft are criminals but the only people to really blame are the naive consumers who ignorantly suck on Microsoft's ~~coc~~WinStore.
Indeed. Same with your downvotes. It is a bit ridiculous how many do not understand free software... They must just enjoy being milked like a bunch of twits.

Possibly we need a more weaponizable license than the GPL. Perhaps similar to the old Open-Motif license stating that the software must not be run on non-free operating systems. At least then Microsoft can't be the \*only\* one to benefit from open-source such as OpenSSH, and the entirety of Linux.

Something like this: http://www.opengroup.org/openmotif/license/
>Everyone, including the developer and the user, have the freedom to do whatever they want with it, and they can‚Äôt ‚Äústeal it‚Äù or ‚Äúscam‚Äù people because the license requires you to share your code.

But does that mean that Microsoft has to allow such scamming practices in their store?
No, sad truth. It's why I'd never recommend hopping onto any hip debian / ubuntu derivate unless there's a serious, sizeable team of maintainers behind it
Word
They just don't have the right outlook for it.
They really should word things so the meaning is more accessible.
But surely they must have Access to competent communications People.
Woosh
Don't attribute malice what can easily be explained by incompetence?
the point is that every sufficiently large corporation will throw morals to the side if it means more return for shareholders. it's fruitless to cast moral judgements on the corporations when the economic system they exist in necessitates that they behave in such a manner.
> If you‚Äôre honest and open

That‚Äôs the thing; they often aren‚Äôt, and try to trick people into thinking that they made these things themselves or that there‚Äôs no way to get it for free
Because so many people don't understand the concept behind free software. They are just idiot consumers that are there to be milked in any way possible by scammers (specifically Microsoft and their other anti-competitive cronies).
>Absolutely Microsoft can limit what they sell in their own stores. But it is still completely unacceptable from a moral and competitive standpoint. 

I don't know... when you get into what is moral and immoral you're walking into subjective territory and describing what people thinks is right or wrong. You could argue that Microsoft would not be a fair judge of what they should or should not allow in their store because of who they are, their potential conflicts of interest and such, and that would be fair to say, it definitely aligns with the "competitive" concerns but I'm unsure if it aligns with the "moral" concerns. But how can you not also argue that if someone takes a piece of code and, essentially, republishes it for a profit and does *literally* nothing more, how is that morally correct? It satisfies the "competitive" concern, after all, allow for anyone to sell anything. If someone repackages GIMP for $50 and someone else comes along and repackages GIMP for $45, so be it! Again, the "competitive" concern, valid. "Moral" concerns, though? I don't think so. I would argue that this policy change is very much a "moral" stand.

Now, that being said, I am interested in seeing how they are going to enforce this... I have no idea how the Microsoft Store works in this regard, is this going to be reviewed when the seller tries to publish an update, kind of like how the Apple App Store does it? Or is there going to be a hotline where people can notify Microsoft of their concerns and Microsoft would investigate this? I don't disagree that this could go in a really bad way.

>They can "clarify" as much as they want but they are still being a bunch of anti-competitive pricks and lying to consumers.

I assume you're describing Microsoft and not necessarily the Microsoft Store. I don't disagree with this statement. I also know they were definitely a lot worse in the past, but they have, it seems, mellowed some over the decades. Just don't forget that they're an organization and can change, and not everything they do is bad or wrong. I didn't read this policy clarification as them trying to save face as Microsoft the corporation but them trying to clarify the stand as the Microsoft App Store.

>To be honest, this whole locked down AppStore concept is criminal. Only a fool would use them.

I, too, am not a fan of using these app stores as a concept. I don't think it's been established that it's criminal or anything of that nature.

I would argue that, at the end of the day, the intention behind these policies is to try and create a friendly and welcoming environment for the store, something where sellers and customers will meet. If they allowed for 5 different people to all repackage functionality identical versions of GIMP and various prices it would be incredibly confusing for the customer. If they allowed for price gouging to exist then consumers would be scared off "Hey! Don't shop at the Microsoft Store, you might get ripped off!". It's the same reason they don't allow porn or other stuff. The whole scenario is more complicated with lots of different inputs/outputs. People are weird. 

I might have a different opinion on the whole matter if the app store was required.

>Possibly we need a more weaponizable license than the GPL.

I don't know how that would work out in a court, but the Fuck Around And Find Out License (FAFOL) says, like the JSON license, that the software is not to be used for evil. Unlike JSON it adds "as determined by the primary contributors to the software". I think that's interesting.
What scamming practices? Did you read what I wrote? The license makes it so they CAN‚ÄôT steal because they have to share the code back. Them going the Apple route of banning free software doesn‚Äôt do anything except hurt the developers from distributing the software on a popular platform.

Windows is getting closed down every year, and, soon enough, Microsoft will make it so that the Windows store is the only way to get software. So it‚Äôs scummy for them to just ban free software like this.
No. "AppStores" attract scammers and should be shut down.

That is the only "reasonable" solution for Microsoft to protect you consumers.
I see no reason to use anything but Debian. The debian-derivatives just break things.
Perfect
Bingo
but when you say "I want to change the economic system in which we exist", people treat you like a madman and/or a radical.
Agreed. The fact that there are so many downvotes for those who understand benefits of the GPL shows that a \*lot\* of idiots with "poor people" mentality are flooding into Linux in recent years.

This isn't about making / disallowing quick cash (and so what if people do). This is about freedom to carry on doing what we enjoy with computers.

It is the whole "AppStore" concept that attracts scammers; Microsoft should shut themselves down ;)
yeah, we got the US federal government to thank for that. invade/coup every country that tries to establish a proper democratic economic system and then project all of capitalism's faults onto communism in school curriculum.
Disable secure boot, install Linux, install own key, re-enable secure boot... this has been SOP for a number of years now.

The difference with Microsoft's Pluton is that this procedure invalidates keys in TPM, which you may or may not care about.
Lenovo in particular has been making it hard to get to a boot menu at startup, they expect people to use Windows to get to the UEFI BIOS setup. On two of mine you have to hold FN while pressing the power button to get to a BIOS boot menu. So try that if you're having trouble getting one.
99/100 machines I ever used needed me to play in the bios/uefi to change the boot order so that I could boot from a Linux USB stick. (Heck before I was booting off USB the same was true for booting off CD).

Given that, I don't see how the extra step to change the secure-boot settings is a big deal.
UEFI et al really do make one pine for the simplicity of the BIOS.

And not the first time Lenovo has screwed up. I seem to recall they at one point had a UEFI that only booted Linux if it was labeled as Red Hat Enterprise Linux.
Something tells me the EU is not going to be happy about that. It is already in the works to force providers to allow other app stores on smartphones, for example. I think the idea of a chip that prevents other operating systems from booting on PC will fall flat on its face. At least if there is no way to turn this off completely.
Hopefully the Linux model will be for sale later this year.  See https://psref.lenovo.com/Product/ThinkPad_Z13_Gen_1?tab=spec
and
https://youtu.be/3weDwYFAFco?t=968
As others have pointed out, this is nothing new. There's always some sort of dirty tactic to force users to stay with Windows, like using raid controllers that aren't supported in linux, even when you're not running a raid config, secure boot, etc...

Here they've opted to not whitelist MS's own CA that it signs for third party boot loaders by default.

However, there is a BIOS option to allow it. https://download.lenovo.com/pccbbs/mobiles\_pdf/Enable\_Secure\_Boot\_for\_Linux\_Secured-core\_PCs.pdf
Ok so there are two flip sides to this:
1)you usually need to disable quick startup in windows 10/11.  In earlier versions of linux usb installers you may have needed to disable secure boot in bios/uefi yes it is true, but with newer versions of installers and particularly fedora and all its spins you don't need to disable the secure boot in bios/uefi.
2)you need to be able to boot from a different drive/external drive. I recommend installing fedora silverblue on an external nvme drive. for the least painpoints with uefi boot menus.

With respect to booting a different device, you need a boot menu.
https://download.lenovo.com/pccbbs/mobiles_pdf/z13_z16_gen1_linux_ug.pdf
Page 27 and 28

Enter the UEFI BIOS menu

Restart the computer. When the logo screen is displayed, press F1 to enter the UEFI BIOS menu.
Note: If you have set the supervisor password, enter the correct password when prompted. You also can
press Enter to skip the password prompt and enter the UEFI BIOS menu. However, you cannot change the
system configurations that are protected by the supervisor password.
Navigate in the UEFI BIOS interface

Change the startup sequence
1. Restart the computer. When the logo screen is displayed, press F1.
2. Select Startup ‚ûô Boot. Then, press Enter. The default device order list is displayed.
Note: No bootable device is displayed if the computer cannot start from any devices or the operating
system cannot be found.
3. Set the startup sequence as desired.
4. Press F10 to save the changes and exit.

To change the startup sequence temporarily:
1. Restart the computer. When the logo screen is displayed, press F12.
2. Select the device that you want the computer to start from and press Enter.

One last thing.  If you can't make your bios appear with f1, then try with a little "Novo button" pin hole somewhere on the laptop and poke a paper clip in there.  The bios and boot menu will show up there.

I hope this helps anybody confused about this and wish them well.
Yes, Palladium has come, in form of Pluton (Xbox security chip)

Is the literal boiling frog, and we are about to be fully cooked.
From the thread:

>This is exactly what we opponents of the so-called "Secure Boot" have been warning against all this time. Restricted Boot is by design not a security technology, it is a vendor lock-in technology (as also evidenced by the need to get your bootloader signed by Microsoft in the first place, and then they sign it with a different key from their own so that vendors can do exactly what Lenovo is now doing). Your (your and some other GNU/Linux developers') pro-"Secure Boot" attitude is what has lead to this.

IMHO, that's what this is about. Stop promoting "security" before it is too late.
very curious to see the outcome to this, as i want a Z13 for Tumbleweed...

laptops have been like that for the past 5 years , this is nothing new

usually you just need to disable secure boot etc to boot linux
In my opinion, this is story is completely overblown. Secure boot is enabled by default. Woohoo, big deal. Disable it and install Linux and live happily ever after.
Can someone explain what Lenovo would get out of this? Seems like quite the restriction to limit devices to only booting Windows by default
Yea I encountered a laptop with that intel RST tech. You gotta disable certain things in order to use linux. Torvalds Called out intel about running something high on the memory or something I forget the article from a few years back so I knew the gloves were gonna come off at some point. Try switching to AMD processors they‚Äôre more Linux compatible. I‚Äôd tell you how to override the firmware but doesn‚Äôt sound like your very experienced. Took some tweaking on my end but I got to flash my new OS onto a blank hard drive. Plus I don‚Äôt do anything for free.
we need a Linux manufacturer for laptops and phone
RedHat or Suse doing that would be awesome.. but well probably i should keep dreaming


and yes, i know people can say " is not his business" and??? was not the business for apple neither ans they ended doing that
There's usually an F key to bring up a boot menu.
When you put a Linux iso in and boot it you can delete the windows
Non-story written by someone who doesn't know what the fuck they're on about.
I am glad my dell supported linux out of the box, they are expensive but they work so!
Oh sh*t. Does this apply to the z16? I just ordered it for work ...
> If this post is offtopic, sorry, please delete it ...

No problem at all, IMHO this is newsworthy and topical.
Couldn't pay me to buy a Lenovo - this is just another reason.
Burn em
What can you expect when Microsoft puts a security chip in them?
Wow didn't Microsoft learn anything from Apples failures. They made their system too proprietary and limited the number of people who could write software for their system, especially when you have a very capable competitor, for Apple is was Microsoft and for Microsoft it is going to be Linux. It almost seems like they trying to get out of the consumer market.
Windows consoles.
First boot into any USB OS eith F12, download it and it will probably ship with Grub.
>  install own key

you can't do that with Pluton (Xbox security chip Microsoft is asking OEMs to add)
i bricked my lenovo t14 twice when trying to tinker with secure boot keys. it turns out you are not allowed to remove microsoft keys even when you install your own. second time lenovo refused to do repair and charged me for motherboard replacement...

never again lenovo.
Is secure boot actually useful? Heard there're some risks with installing your own  in case you lose the key or something, and a bit unclear on what the benefits are.
I've actually had one laptop that wouldn't let me disable secure boot. It did let me delete the keys though which force disabled secure boot, but there are a handful of manufacturers out there who try to make it difficult.
Its not been necessary to do this for years for Ubuntu, Mint, Debian, Red Hat, Fedora, and (open)suse so I wouldn't call disabling secure boot a standard part of installing Linux for the majority of Linux users
I think you missed the point. Matthew's post is suggesting that integrity of the boot procedure is already protected by TPM measurements. An attack attempt that invalidates the official MS signing key is trivially detectable, so it's unnecessary to ban 3rd party secure boot signing key (by default).

I don't think this is directly related to Pluton either. If a laptop with TPM & secure boot has Windows pre-installed, and seals some secret (e.g. Bitlocker keys) inside the TPM, then booting something else WILL invalidate keys in TPM. That's how it's supposed to work for quite some time. You could also configure it to boot Linux and auto-unlock LUKS with TPM, in which situation booting Windows will invalidate TPM keys the same way. But Lenovo's doing now is (abruptly) restricting boot option to Windows only by default, instead of using standard UEFI security.
Don't work with Lenovo Legion. You can disable secure boot, and it still won't boot from USB, even if you specifically tell it to boot from USB.

You can install an OS which has a Windows installer which you can run from within Windows, but that requires you to activate your Windows, so you can't get a refund from Microsoft for it.
You need to turn off secure boot for some distros anyway.

It even asked me to turn off secure boot for windows 11 official iso.
It's designed with one goal. Make your computer their computer.
Legacy BIOS was hardly simple. It was a collection of arcane hackery and workarounds for issues dating back to the original IBM PC.
Yep, the best solution is for Linux to come pre-installed on computers!
Thanks for the link. Looks great.   
But for some reason, I can't see display refresh rate information.

>Is the literal boiling frog, and we are about to be fully cooked.

Last time this was posted everyone here was mad and afraid.  

But the unfortunate case is that Linux is still nowhere near a majority, and so most customers don't know and don't care about this. Therefore companies can do this with zero consequence.  
This isn't the community's fault.
We'll own nothing, and we'll be happy. :-(
Pluton is literally just a TPM
You don't necessarily need to use Microsoft's key to use Linux with Secure Boot. If the Linux community had embraced Secure Boot, which it never really has, they would have produced guides and simpler ways to enroll a user key and easily sign bootloaders and kernel with them, putting the focus back onto the hardware manufacturers which don't support that. A system with a proper Secure Boot setup is undeniably better than a system which doesn't have it.

Instead, the Linux kernel still doesn't even have proper hybernation support with Secure Boot under the kernel lockdown mode, because not enough people care about that.
Up vote for underrated SUSE
Vote with your wallet, corporations never change if it doesn't mean profit loss.
No, usually your Linux distro would just import the key to boot with secure boot. Now, by default, it can‚Äôt do that, adding one more hassle to install Linux.
> laptops have been like that for the past 5 years , this is nothing new

More like 9 (since Windows 8), but now they added an Xbox security chip(Pluton) that only accepts Microsoft keys.
Indeed, it's not exactly a new issue that secure boot doesn't always work properly with Linux. In fact there's hardly any laptop out there that works without 0 issues on Linux. There is (almost?) no Windows laptop that works 100% without issues on Linux.
Secure boot works out-of-the-box with Linux.
That‚Äôs not the problem. If secure boot was just enabled by default (like in most computers) your Linux distro will just import its key to boot, but this change makes it so that only Microsoft keys are allowed by default.
I agree, this story is misleading in its headline, as may of already been pointed üëâ out, linux will still boot on these laptops, the issue is Microsoft is/has dissable 3d part certificates and this stops linux using secure boot, but what is quickly glazed over    is you can go in to the BIOS and enable 3d party support and all is good in the world again. 

So I guess  ü§∑  a headline of lenovo implement a new Bios feature to help secure window, would not get the attention the author was looking for...
Money from Microsoft either directly or in terms of favorable treatment as a partner.
My laptop came with Intel RST enabled from the factory. linux wouldn't see the internal NVMe. Trying to disable RST caused Windows to stop booting.

Solution was to do a special dance with Safe Mode that will get Windows to self-repair the boot setup with RST disabled. Only THEN I could install Linux.
I bought my laptop from System76 (the Pangolin) and it's been more than fantastic https://system76.com/
Nope, not anymore.  
You know, safety!!
this is amd's security chip.
Oh. Can secure boot (or equivalent thereof) still be disabled, at least?
Perhaps with Pluton you will be able to blow a fuse in order to disable boot signature verification?  This is the kind of thing you can do with unlocked phones that have signed bootloaders.
Incorrect, to a degree

You can still very much assert control over the platform by installing your own platform key and KEK pair in UEFI

UEFI secure boot and Pluton/HSP secure boot (the latter primarily verifies the UEFI firmware itself, as well as the chip‚Äôs own firmware, the former is what‚Äôs being discussed here) are independent and unrelated

That said Lenovo screwed up big time by placing the 3rd party UEFI CA key in dbx by default.
[deleted]
Uhm yes you can, lol. Any proof?
You don't need to remove Microsoft/Red Hat keys to put in your own.
Lenovo has a rescue image you can download that fixes it. It basically reimages your machine with a fresh copy of windows and replaces the keys. I thought I bricked mine as well after I had installed Linux and decided to go back to windows. The other issue is that the laptop will not recognize the drive at all, their rescue image fixes that at the same time.
Problem is likely that some of the BIOS components are signed only with those keys, so removing them makes the system unbootable.
Yes, Secure Boot is useful.

Secure Boot does help mitigate evil maid attacks, but it's not limited to those. Secure Boot helps protect your firmware and kernel from malware infection via any source, which is important because malware that gains kernel access is nearly impossible to detect (though it can usually be eliminated by wiping the drive and reinstalling), and malware that gains firmware access is both nearly impossible to detect and nearly impossible to remove.

A lot of people look at Secure Boot as protecting the pre-boot environment, as if it is a brief event.  It isn't.  In addition to the OS you interact with on a modern x86 system, there are (at least) two and a half other operating systems running at all times, with *more* control over the system than your primary OS:

https://www.youtube.com/watch?v=iffTJ1vPCSo

Secure Boot's purpose isn't to protect the system you interact with from malware, so much as it is to protect your kernel *and the lower-level operating systems* from malware.  Rootkits that embed themselves in firmware are becoming more common, and they are nearly impossible to remove without specialized equipment. Secure Boot is one of the recommended mitigations:

https://usa.kaspersky.com/about/press-releases/2022_kaspersky-uncovers-third-known-firmware-bootkit

To expand on that a bit:

Once malware gets on your system, the malware is likely to begin execution in your user context. The POSIX multi-user design prevents malware from modifying the system outside what your user has permission to modify, unless it can leverage another exploit to get root. And that's where Secure Boot comes in, because in a legacy design, root is the highest level of access, and nothing prevents malware from modifying the kernel or the system firmware from there. Secure Boot adds another level of separation, protecting the system firmware and the kernel from modification by malware.

Imagine that malware manages to gain access to a system, and further is able to use a local exploit to get root access.  Maybe it joins a botnet at that point.  It's probably going to take extra steps in order to persist (which is to say that it'll save itself to a file or download a file to execute in the future after a system reboot, and it'll modify the boot process to execute that file).  Now, unless it takes additional steps, it's detectable.  You can use "ps" to see it in the process list, or "ls" to see its files on disk.  

Many types of malware will take additional steps to hide themselves.  The easy way to do that would be to modify "ps" and "ls" so that they no longer show the malware in their output.  Simple, right?  But what if you use "find" to look at files, or "top" to look at processes?  What if you apply updates and overwrite the modified tools?  A more complete hiding effort involves loading a kernel module to that the kernel itself no longer tells user-space about the malware's files, processes, or network traffic!  Now when the operator runs "ls /" or "find /", the malware's kernel module filters the responses to readdir(), and never includes files that contain the malware.

A modular kernel like Linux inherently allows loading software that can operate at a very low level, and can prevent anti-virus software from discovering and removing the malware.

Linux Secure Boot systems with kernel lockdown will not allow modules to load unless they are signed, and that makes it very difficult if not impossible for an attacker to load a kernel module that can hide malware.  Malware can still modify user-space tools directly, to try to hide itself, but it's much *much* easier to overcome that to determine if a system is infected or not.

An example malware module can be found here: https://github.com/mncoppola/suterusu

And a series of posts describing how all of this works (in rather a *lot* of technical detail) is available here: https://xcellerator.github.io/categories/linux/ (starting with [post 1](https://xcellerator.github.io/posts/linux_rootkits_01/) and proceeding for 9 total posts)
It protects you against an "evil maid" attack, where you lose physical control of your device for a while during which they install e.g. a keylogger to capture your LUKS password. Other than that, not much use.

And if your BIOS has a known "master password" ([most do](https://www.derdour.fr/bios/)), then anyone can disable secure boot, boot an unsigned live distro, install their own keys, and finally install the keylogger anyway. I'm not a big fan of it.
If you're installing an off the shelf distro then those are most likely signed already, yes.
Works fine on my Legion 5?
currently running archlinux on my Legion 5i. Had no troubles installing it, brightness works, sound works, wifi works, everything works. dunno what issues you're having.
It's literally not though and y'all need to chill with the hysterics lol. If they wanted complete control, it wouldn't be UEFI at all. ie, the Switch does not use UEFI, PlayStation omits UEFI, all Android phones use UEFI but some don't allow enrolling custom keys, Apple's iPhone/Mac bootloaders are not UEFI (though, Macs allow booting third-party kernels securely).

UEFI was designed to give users the *option* of having all boot components verified, and in general provide some baseline drivers for peripherals that can be used in OS setup/safe boot. It's a good option to have in a lot of cases, it's just that vendors suck at implementing it correctly.

The thing to actually look out for is if they start suggesting eFuses for downgrade prevention and tying components together, that stuff is a nightmare and only leads to e-waste.
The X1's with Fedora pre-install are slick af
Like that'll ever happen
Fight against.
> A system with a proper Secure Boot setup is undeniably better than a system which doesn't have it.

Why?

Like, legit question, why?

The signed boot shim from Ubuntu and Fedora still reads from a grub/whatever config to point to what to actually boot. You can trivially inject your own kernel to an unencrypted disk, modify the bootloader config, and backdoor a system.

A system with an encrypted disk has no such vulnerability and no need for a bootloader lock like Secure Boot.

I've put this to the test for many years in a row at Defcon, which is arguably the single most hostile "you're gonna get hacked" environment on the planet. It's been easy to defeat Securebooted linux installs while it's been impossible to defeat basic opsec installs without secureboot.

Nobody there thinks secureboot is relevant as a security measure, and few if any actually use secureboot, and following proper local opsec, remain secure in spite of.
SUSEpiciously underrated
Laptops that ship with Linux don‚Äôt support Linux 100%?
> There is (almost?) no Windows laptop that works 100% without issues on Linux.

Thinkpads.

Since the Fedora/RH devs are mostly using thinkpads, they fix and upstream their own problem solves. Fedora and Lenovo have been working together for a year or two as well, and that code is upstreamed.

My X1 Nano has true 100% compatibility with Linux. Including the fingerprint reader, WWAN etc.

But my T60, X230, T440p, W550s, T480, X1C5, all also enjoy 99% or 100% support (some finger readers don't work, but that is the only limit)
My Framework laptop works perfectly with Linux, I hear Thinkpads are pretty good as well.
What are you talking about?  Every single laptop I've ever owned (10+) was pre-installed with Windows 100%.  On every single one Linux ran perfectly without any issues whatsoever.
>Secure boot works out-of-the-box with Linux.

as others said its subject to the distro
Not with an ARM laptop ü§£
But not with all distros, like the glorious EndeavourOS.
https://download.lenovo.com/pccbbs/mobiles_pdf/Enable_Secure_Boot_for_Linux_Secured-core_PCs.pdf
Microsoft offers free bootloader shim signing under their own key to some linux distros (Ubuntu, RHEL, etc)
Whatever works breh
You can get all kinds of Linux preinstalls online. but what is needed is a high street presence ala Apple's stores.
sadly no amd6000cpu+6000igpu but thanks because Lenovo don't have it neither
so i will keep looking in system76 and lenovo
Oh. I got a msi through https://www.hidevolution.com/

Last year
I was able to use flash drive and install artix on the hard drive removing windows entirely
In the only laptop I've yet to touch with Pluton, an X13s from lenovo, both secureboot and pluton can be disabled in BIOS.

"For now".
Sure it can be ‚Äúdisabled‚Äù /s

I don‚Äôt actually know, but I couldn‚Äôt trust any firmware straight from MS.
there should be an option to go back to legacy BIOS(remember we're still talking about Microsoft, backwards compatibility is in their blood), but who knows.
That would be terrible if we had to burn a fuse to run linux
Pluton is literally just a TPM
Yes, but removing them shouldn't brick a device.
well, yes. but the point of secure boot is to make sure that when you type your hard drive encryption password on your hardware you can be sure that you are only giving it to the software you yourself installed and approved. if anyone can download a bootable usb and overwrite that software on your machine, what's the point of secure boot at all?
that was one of the advices support had before sending the laptop in. but bricked means bricked. it would not get to the stage where usb or any other interface was working on the boot.
yes, that was precisely it
What a great comment, thank you. i learned a lot from this!
Those are signed with ms "3rd party" key, that key is not present in those Lenovo ThinkPads, that's why it become notice these days, but anyway, nobody should ask Microsoft for permission to install their software
Don't remember which Legion I have, it's a few months old. Didn't work.
I can get it running no problem, the problem is booting the install media from USB or PXE. Disabled UEFI and secure boot, told it to boot from USB (or PXE), and it still refused to boot. Checked both boot media variants on other machines. Had to run the install from within Windows, and was dead scared while doing it, as a failure to boot would likely be a brick.

It's not the first machine I install. Probably not even my 100th. I've been doing stuff like this for almost 40 years. I know what I'm doing. This one just didn't want to work.
Lmao fr, I can‚Äôt with this sub, the tinfoil is just so think so often üòÇ UEFI was *sorely* needed to address BIOS shortcomings
Try to configure it to boot windos and knoppix without having to hack the bios and reboot 3 times for each changeover. This is designed to make linux hard and for no other purpose.
Sorry I'm confused, what will never happen?
> The signed boot shim from Ubuntu and Fedora still reads from a grub/whatever config to point to what to actually boot.

Note how I didn't talk or consider the signed shim method anywhere. Instead I talked about rolling your own keys.

>I've put this to the test for many years in a row at Defcon, which is arguably the single most hostile "you're gonna get hacked" environment on the planet.

You've then probably seen dozens of practices that aren't commonplace because they regard scenarios that normal people simply don't need to care about. Features that can be used by a wide user base are good.
I've been using System 76 machines at home since 2015 or so. They now ship with coreboot and support linux seamlessly since that is all they ship with. 

They are not for the bargain basement shoppers, but the price is fair for a low volume seller.
Not my XPS 9310.
The issue here, of course, being supporting the very company that is responsible for the problem in the original post.
Ymmv, what laptops did you try?
You should have read the article: the distro they used is signe by Microsoft 3rd Party UEFI CA key. Default secure boot should boot anything signed with those key.
Yeah, but most major Linux distros like Ubuntu, Fedora, and openSUSE work with secure boot. If you‚Äôre using something else then you probably already know to disable secure boot. Now new users are expected to mess with the UEFI just to boot Linux even on user friendly distros.
I think you meant glorious Arch Linux.
Yes, that‚Äôs just what I was saying. Previously 3rd party signed keys would work out of the box. Now you have to enable it manually in the UEFI, adding another step to install Linux which is bad for new users.
Yes, these are the Microsoft 3rd party keys. They used to work just fine OOTB before, but now they don‚Äôt (at least with Lenovo).
totally agree but look like noone with capacity to do that is interested
Sure, but the laptops discussed in the original post won't boot that way due to security features.
I hope it stays that way, because none of the more open-source laptop manufacturers make a good small form factor gaming laptop. And I need something like that for college because I'm not gonna haul around a giant 17 inch 20 pound desktop with a screen attached.
Once Pluton gets widely adopted, online services will make it a requirement. Probably games, too. So when that day comes, although you (might) still be allowed to switch Pluton off, said online services will refuse to serve you if you do.

If you think I am joking, look up NGSCB or Paladium. They have been cooking this up since 2003. Pluton being the final piece of their puzzle.

The open PC, what remains of it, has five years left, at most. The only thing holding them back now is adoption. That's why Microsoft has to convince all the normies to trash their still-pretty-good Skylake machines and anything older.
I doubt that very much, the latest Intel chips don‚Äôt support it.
Legacy BIOS cannot address modern storage. Can't go back, Mr. Reacher.
It's not terrible, you just choose a menu option to do it.

See https://lineageos.org/engineering/Qualcomm-Firmware/ for some details on how it works for one vendor (look for "QFUSE").
Except you can't put them back in.

They're hard-coded for a reason.
Except Microsoft and Red Hat won't be signing bootloaders that steal credentials...
As i understand it, the major limitation was related to MBR.

But it would have been fully possible to bolt a new partitioning scheme on top of the existing BIOS and keep on trucking (like had been done before when moving from say manual to automatic HDD configuration).

Never mind that at least consumer drives have not really moved past the 1TB mark, instead replacing high capacity spinning rust for lower capacity, but faster,  SSDs.

Nah, most of UEFI belongs in a rack rather than a desktop (except maybe a office desktop, but then those are returning to "terminals" for ease of maintenance).
No one cares about linux users
Check who I replied to
An encrypted disk is better, simpler security for 99% of people but windows doesn't force it because it isn't convenient for vendor lock in. The vast majority of people anyway don't need to worry about physical attacks. I would actially even argue the opposite - lax physical security is good for most people because they care more about recovering their daya if they forget a password or do soemthing wrong than they do someone stealing their laptop for their data
Their support is first class.

----

I have one of their desktops. I had a problem with a PCIe card and the chassis.

Long story short, there was no dicking around. They took my report and photos seriously from the get-go. They obtained one of the cards in question and they fixed the design going forward and shipped me a custom machined PCI card bracket (at no cost to me) to replace the one on the card, that let it seat correctly as a sort of physical "patch."

----

Another time, I stuffed up the UEFI flash, messing up the OEM DMI data. They worked with me over weeks, sending me firmware packages to flash trying to solve the problem. Eventually fixed the issue (that I caused, and I had told them so too).

----

I'd not expected that level of care at all.
What‚Äôs wrong with it? (Also, I didn‚Äôt even knew they sold Linux versions of it)
Buy used off ebay. Not one cent supports. The META for thinkpad usage is buying them off-lease in the 3-4 year old range. You get laptops that sold for 2000 for 200, that will hold up for many more years due to MILSPEC build quality, and perfect linux support.

Also, Lenovo does sell, direct, Ubuntu and Fedora pre-load laptops, that would be ethically clean to purchase.
Mainly asus, lenovo, hp, dell
To have your laptop certified Designed for Windows or whatever it is, there are two secureboot certificates relevant...

The one that Microsoft uses only to sign its own code, and the one that it uses to sign other people's code. Guess which one is in-fact *optional* for the requirement... (and in fact disallowed for any ARM laptop lol)
[deleted]
I think I knew I was talking about. At least EndeavourOS makes NVIDIA less of a pain.
Personally I just disable secure boot. Who needs that $hit anyway?
If you can't enable one option in the BIOS, you should probably be using a Chromebook.
No microsoft will sign stuff 1st party.

The ubuntu shim for example can boot on any secureboot laptop without needing to import a key.
Have you checked out the Eluktronics Max 17?  I am thrilled with how portable it is for a 17" laptop with some serious specs.  And the performance is excellent as well.

I hear what you are saying about smaller form factor, but as someone who does a mix of gaming, programming, and spreadsheets, I can't work on anything but a 17" at this point.
System76 and others (i.e. dell) have really good hardware already and come preloaded with Linux out of the box. They're not common, or cheap, but they'll run you about the same as a gaming laptop and will be able to do the same and more.

Edit: Wanted to add tensorlabs for those that don't know about it.

https://lambdalabs.com/deep-learning/laptops/tensorbook
> And I need something like that for college because I'm not gonna haul around a giant 17 inch 20 pound desktop with a screen attached.

Uh, why are you bringing a gaming computer at all? Arthur taught us that *Having fun isn't hard / When you've got a library card!~*
Tbh, I would go the ultrabook + egpu route nowadays. Then stuff like framework is an option.
And those are games I won't play.

I'm already fine with giving up some games I really like to stay on Linux (although I do play them on my Xbox). If devs start getting on this trend, I just simply won't play their games.

Hopefully Valve can put their foot in the door if the Deck is enough of a success and makes SteamOS more popular.
Duke Nukem 3D, Doom, and Unreal Tournament will always be available and just get more fun every day.  Future freedom is retro and self service.
So what you're saying is we have 5 years to break Pluton
This explains recent Windows 10 notifications telling users their hardware is out of date but doesn't support W11. It's priming users to ditch perfectly working hardware and buy new stuff with their shit chip and anti-competition BS.
Macs. They can't get rid of or coerce Apple into picking this up, so it'll never be able to hit that critical mass.
or we'll end up with dual cpu motherboards and hypervisors with a full 2nd cpu passthrough
Welp, really?

Guess I will stick with my old thinkpad
A cheap LSI SAS card running 6Gb/s over pcie flashed to IT mode takes care of that minuscule hurdle.
Burning a fuse on a PC would certainly hurt it's market value and make it worth less than if a fuse hadn't been blown
that's not how it works. you can install and boot any distro becauase your bootloader (typically grub2) is signed, not your kernel. the thing that boots is not the signed kernel... unlike with secure boot setup using your own keys, when you have to sign the kernel every time you update.

as long as my computer boots standard grub anyone can load anything they want with it.
The shim is signed by them isn't it? So anybody can plug a pen and boot a distro with secure boot enabled and ms keys on.

Unless I'm wrong since I never used or turned secure boot on anu machine I own.
Proprietary driver for the fingerprint scanner that only worked in Ubuntu 20.04LTS.
This. 

I bought my P50 a couple of years ago and it runs Fedora beautifully. 

Best computer I've ever owned.
How dare you! It's **the** meme os!

I use arch btw.
I don't think there is any difference in pain level. Just

`# pacman -S nvidia-dkms nvidia-utils`

And you're done
People who care about the Evil Maid attack. Journalists, dissidents, etc. And people who just want their system to become a brick if it gets stolen.
Secure boot is a security standard developed by members of the PC industry to help make sure that a device boots using only software that is trusted by the Original Equipment Manufacturer (OEM). When the PC starts, the firmware checks the signature of each piece of boot software, including UEFI firmware drivers (also known as Option ROMs), EFI applications, and the operating system. If the signatures are valid, the PC boots, and the firmware gives control to the operating system.

The OEM can use instructions from the firmware manufacturer to create Secure boot keys and to store them in the PC firmware. When you add UEFI drivers, you'll also need to make sure these are signed and included in the Secure Boot database.

Secure Boot is just letting you know that the operating system has not been tampered with in any way, shape, or form
Until the day when secure boot will be mandatory
That‚Äôs the wrong attitude. Some people don‚Äôt want to deal with finding the right key combination to enter the bios. Then figure out where the option is for their computer because they all have different UI and naming schemes. Not to mention a lot have bios locks either by an admin, or just forgetting the password.

It should just work out of the box like it used to before.
Yes, and so does Fedora‚Äôs shim, and so does many other distros. Those are signed with Microsoft‚Äôs 3rd party keys which used to work out of the box, but now, with Lenovo, you need to enable it through UEFI.
I almost bought an Eluktronics laptop, but didn‚Äôt. I couldn‚Äôt find a lot of positive customer reviews and I was afraid of spending a lot of money on something that may or may not break in a few years.

Please feel free to private message me with any adverse experiences you have or don‚Äôt have. Maybe my next laptop will be an Eluktronics.
My problem with Dell is that build quality isn't always the best. Though I appreciate how I can do bios updates through Linux on my Dell laptop
I'm on the pop\_os and S76 subs and it seems like there's people complaining about issues with their laptops every single week. Be it weird keyboard config, bad wifi drivers, temp, battery-life, weight, etc. and while the I suppose the argument is valid that we shouldn't expect their stuff to be *as good* as major manufacturers, I'm not paying optioned-out macbook prices for something with HP quality.
Because I need the processing power for CAD and some programming stuff I do, as well as gaming.
*yawn*
DO NOT get me wrong. I'm sure that's someone's vision. But the nanosecond that "open PC" becomes a market differentiator, and a tactical sales advantage, the OEMs will push back. There was -- briefly -- a similar concern and problem when secure boot came out. And I suspect the new computers are a replay of that whole thing, again. But likewise again, I just don't see it lasting.
Confirmed, just collected (nearly a gigabyte!) of custom Duke3d levels to play.
Macs already have their own Secure Enclave chip. Apple already restricts what you can do if you touch any of the security settings (such as SIP, you can no longer use iOS Apps).
Apple has their own. The T2 chip that also serves to hamstring repair-ability so you have to take it to an Apple store, where they will invariable tell you it's either *very* expensive to fix, and you won't get you data back, or you need to buy a new device, and you won't get you data back.
Certainly the server ones. No idea about the consumer chips in laptops.
Any 64bit CPU doesn't support booting from BIOS due to boot block size I/O requirements and long vs. protected mode, so it's been EFI or UEFI under the covers specifically because of that.  What you're thinking of is Compatibility Support Mode, or CSM.  Most OEMs don't really ship hardware that does this anymore, but they've shipped desktop/laptop devices with EFI/UEFI since the very first amd64 CPU (and Intel's Itanium before that, but consumers probably weren't buying super-expensive server-only platforms even back then :) ).  It was really intended to bridge the gap where most major OSes didn't support EFI-native booting when CPUs that required it were first sold to consumers and businesses in mainstream devices, so it's not surprising that this support has essentially disappeared with devices that ship with support for Windows 8 or newer, or Linux, etc, as there aren't really any mainstream OSes that don't support native UEFI or EFI booting at this point.

Edit: downvote all you like, but it's still a fact that a 64bit CPU *requires* EFI or UEFI to boot and can present "BIOS" to an OS via CSM.
Yea? And?
> I use arch btw

Good Bot :)

^---  
^I'm ^also ^a ^bot. ^I'm ^running ^on ^Arch ^btw.  
^[Explanation](https://www.reddit.com/r/linuxmasterrace/comments/v9thbo/whenever_someone_says_i_use_arch_btw_respond_with/)
Also don't forget `nvidia-drm.modeset=1` kernel parameter :)
Pain.
And the evil maid can‚Äôt turn off secure boot?  Few people set firmware passwords.  Or the evil maid could use Ubuntu.

Theoretically it prevents rootkits, but I don‚Äôt see the benefit for Linux since iirc only grub is signed.
I'm a big proponent of people trying Linux. I think it has gotten to a point that most people can use it easily. But for *any* OS that is more open in terms of what you can do with it (including Windows), being able to do a basic Google search and follow directions when something goes wrong is an essential skill. 

If enabling one option in a BIOS prevents you from installing Linux, I imagine you'd have a lot of other problems. Like how to set your default browser in Windows, how to unblock a game you just installed from your antivirus, how to manage your files when your disk fills up with downloads you never delete, and so on.

Frankly, changing one BIOS option and installing Linux is probably easier than running Windows these days.

So I am very serious when I say that if the thing that prevents you from installing Linux is changing one option in the BIOS, Windows, OSX, and Linux all are probably a bit too complicated.
As long as microsofts keys are preloaded, anything signed by them will pass verification.

There is no "microsoft 3rd party key" it's just signed by "microsoft". As far as UEFI cares it's the same authority as the stock windows loader.
I haven't had a single issue.  Price and spec ratio was unmatched when I bought mine and never looked back.  I love the lack of bloat and low-key design aesthetics.  Holding up better than my Asus, and I have been a diehard Asus fan for the last 10+ years.
With dell you have to basically focus on their Latitude lineup that most closely tries to copy thinkpad designs (nipple mouse, rugged chassis)
Look up the HP Dev One

It's an Elitebook chassis (so extremely rugged business grade) and has AMD hardware, good specs, and a price lower than a macbook air, and is designed for and ships with linux.
always gotta remember too that people don't post when things are going well as often as they do when things are going poorly.
If we're talking downsides, the thing I don't like about S76 is how they'll use intel, nvidia, and other proprietary components that are usually a pain to get to work unless you know what you're doing. As far as PopOS goes, I'm not really a fan. You can always throw another distro on there though.
Amen brother.
Can you share? :D
What do you mean? The BIOS boot block is part of the OS-facing side of the BIOS, and you can totally boot a 64-bit CPU from it. From the OS point of view all that changes between UEFI and BIOS is how the OS/bootloader interacts with hardware before it loads drivers and how the CPU is initially configured in terms of mode and interrupt handling. If it wasn't possible to boot a 64 bit CPU from it then 64 bit OSs would not be able to run under CSM.

When the CPU initially powers on it begins executing a fixed address in physical memory that is mapped to a ROM by the motherboard, so a boot block doesn't even enter into the picture. You can read the Intel manual, volume 3 chapter 9, for information on initial processor state after reset: it starts up in 16-bit real mode with paging disabled and executes the instruction at 0xFFFFFFF0 in physical memory space.

In fact across the 3000 something pages of the Intel manual UEFI only comes up as a side note in microcode updates.
>Edit: downvote all you like, but it's still a fact that a 64bit CPU requires EFI or UEFI to boot and can present "BIOS" to an OS via CSM.

You are absolutely and utterly wrong. XP-era systems were 64-bit capable but did not support UEFI *in any way*. XP itself, even the x64 version, did not support booting from UEFI *at all*. It required BIOS or CSM.

Seriously, it was less than 20 years ago. Are we at the point were history can be rewritten like this and people here don't realize it?

>but they've shipped desktop/laptop devices with EFI/UEFI since the very first amd64 CPU

This is outright ridiculous. The first x64 CPUs were AMD and UEFI was not even a thing back then.

The first widely distributed EFI (not even UEFI) based firmware was the Apple Intel Macs, and both AMD and Intel had already shipped several generations of x64 CPUs before then. Heck, amd64 is from 1999!, while the first Intel Macs were from 2006. It would take at least *a decade* more until UEFI became prevalent on non-Apple PCs, during the Sandy Bridge era.

64 bit systems with BIOS just boot in 16 bit real mode,  exactly like 32 bit systems, and it is the responsibility of the 64 bit OS to enter/leave 64 bit as necessary, much in the same way it was responsability of a 32-bit protected mode OS to enter/leave protected mode.
You made me look at one of my systems: It does not support uefi. It's a dell optiplex mt 780 with this cpu:

https://ark.intel.com/content/www/us/en/ark/products/40478/intel-pentium-processor-e5400-2m-cache-2-70-ghz-800-mhz-fsb.html
I'm running Linux on a 64 bits CPU with BIOS only (T400)
Pain.
>And the evil maid can‚Äôt turn off secure boot? Few people set firmware passwords.

Anybody who cares about evil maid will have a firmware password.

>  Or the evil maid could use Ubuntu.

Evil maid can't do that if their target uses their own keys.
> Like how to set your default browser in Windows, how to unblock a game you just installed from your antivirus, how to manage your files when your disk fills up with downloads you never delete, and so on.

These are all predictable tasks that you can find an exact answer online. With computers BIOSs, it‚Äôs much more fragmented and it‚Äôd be especially hard for more obscure computers

> Frankly, changing one BIOS option and installing Linux is probably easier than running Windows these days.

You can‚Äôt compare a task to an operating system that does multiple other tasks. Using windows to browse youtube is easier than going into the BIOS, but using windows to debug software isn‚Äôt.

I kinda agree with you that if you‚Äôre not good enough with computers, you shouldn‚Äôt switch your OS, but we should close the gap so even people who can‚Äôt do those tasks will be able to use Linux.
>As long as microsofts keys are preloaded, anything signed by them will pass verification.

There are two microsoft authority keys, one used to sign Windows and the other one(refered as the 3rd party key) is used to to sign stuff people requested and Microsoft approved like Fedora's shim. To pass Microsoft's Logo certification you had to bundle at least both of those keys, but apparently now they mandate the third party key to not be installed by default.

>There is no "microsoft 3rd party key" it's just signed by "microsoft".

The only thing signed by "microsoft" is Windows under the name "Microsoft Windows Production PCA". Everything else approved by Microsoft is signed under the name "Microsoft 3rd Party UEFI CA". The latter certificate is missing by default preventing Linux on to boot on the laptop with Secure Boot on.
Thank you for your response. It's a relatively unknown brand. Finding a lot of positive or any user reviews was kind of hard at the time that I was shopping for a laptop.
Yea AMD is much better for Linux
I said "I/O" specifically and I'm not talking about loading the OS boot block, I'm talking about loading the EFI/UEFI boot block from flash - updated my original post to be more clear for those that come across this later.  One other thing I didn't touch on to add to that, there's another reason why a 64bit CPU booting in 64bit mode is always running an EFI or UEFI - a 64bit CPU when running in 64bit long mode (what it is doing to bootstrap itself and then load the EFI OS to start the machine) cannot load 16bit code, and thus a 64bit CPU following spec today is not going to be capable of initially starting and running BIOS because BIOS is 16bit.  Once the CPU has switched to protected mode, which it *can* and does do once the UEFI has loaded so it can load drivers and other code and then boot an OS, it can load and run 16bit code, and this is how CSMs work - 32bit protected mode can load 16bit code and thus present "BIOS" to the device, even if it's not actually running BIOS.  Note that the reasons behind why a CPU loads in 64bit long mode and then boots the system isn't because it could not possibly do this, but because the EFI/UEFI spec created initially by Intel is designed to run in real or long mode, and not set up all of the IDT and it's dependencies on boot that BIOS would do, and only create it's structures in a reduced/simplified format and then let the EFI OS handle that to make booting quicker/easier, supposedly.  To make an amd64/x86-64 or ia64 CPU be able to bootstrap in something other than long mode and load 16bit code directly on boot, you'd need a re-write of the UEFI spec, and a good reason to do this - and especially that last part, it just doesn't exist in the business sense.  Also, why rewrite BIOS to be 32 or 64bit when such an OS (EFI/UEFI) already exists and is an open spec and is fully supported by every 64bit CPU out there, and once loaded it can emulate BIOS anyway with CSM?  So, every 64bit CPU amd64/x86-64/ia64 machine you see that's booting "BIOS", is doing so via CSM because the CPU will not run 16bit code on bootstrap, and BIOS is 16bit and thus your device is booting CSM, even if the OEM doesn't give you a way to switch it on or off as in the Optiplex 780 (which, why Dell?  but that's a Q for another day).

Also, iirc, Intel Ice Lake was the first CPU to ship with UEFI class 3 support, meaning Ice Lake and newer Intel CPU devices are not going to have CSM support at all unless the OEM writes a custom UEFI based on the EDK that adds it back, and again, you'd need a good business reason to do this, and I cannot think of a good one.  I am unsure if AMD has removed the capability of CSM from their CPUs as of yet, and since I don't work with those I just don't know and the interwebs don't have any real clues that I would trust either.
When xp was around there was not even any dreams of even replacing bios as a concept. And when uefi appeared there were so many broken ones deployed to users that it took years to have it reasonably working well.

I miss using Lilo as the bootloader when it was standard instead of the now mainstream grub. Good times. All bios. All XP era.
> dell optiplex mt 780

It's a "legacy boot only" device for some reason, but again, it's doing that via CSM.  The Pentium E5400 is absolutely a 64bit CPU and absolutely requires larger than a 512byte boot block to start.  The Dell firmware doesn't support exposing UEFI to the OS booting on hardware, but it's still EFI.

For fun, see that someone created a replacement EFI for that hardware to boot Hackintosh onto, supposedly:
https://github.com/osx86-ijb/Dell-Optiplex-780-OC-EFI-Catalina
No, you aren't, I guarantee it's still EFI under the covers - 64bit CPUs have boot block I/O size requirements larger than BIOS' 512byte boot block.  Also, I'm guessing the reason vendors are switching to MS only by default is the [hole in the boot vulnerability](https://msrc.microsoft.com/update-guide/vulnerability/ADV200011), ironically exposed via a limitation or vulnerability (depending on how you think of it) in GRUB.
Realistically, we need more computers to ship with Linux. I'm hoping the Steam Deck keeps pushing things in that direction.
> it can load and run 16bit code, and this is how CSMs work - 32bit protected mode can load 16bit code and thus present "BIOS" to the device, even if it's not actually running BIOS.

This is a distinct processor mode - check out section 2.2 in vol 3 of the Intel software developer's manual. In protected mode the processor supports virtual 8086 mode through a task flag. It's not the mode the processor is in when control is handed to a BIOS bootloader and is different enough to be incompatible with a BIOS bootloader (it uses the protected mode IDT instead of the real-mode IVT, EFLAGS is set differently, CR0 is different, etc). It's instead in real-address mode.

> One other thing I didn't touch on to add to that, there's another reason why a 64bit CPU booting in 64bit mode is always running an EFI or UEFI - a 64bit CPU when running in 64bit long mode (what it is doing to bootstrap itself and then load the EFI OS to start the machine) cannot load 16bit code, and thus a 64bit CPU following spec today is not going to be capable of initially starting and running BIOS because BIOS is 16bit

It's also in real-address mode after a reset - again see section 9.1.1 in the manual - not protected mode or 64-bit long mode. It additionally starts with interrupts disabled and an empty IVT. The code it begins to execute at 0xFFFFFFF0 has to have a stub putting it into 64-bit long mode to then bootstrap itself like that. But it starts in 16-bit real-address mode and the firmware doesn't need to ever leave this mode if it doesn't want to. Section 9.7 directly calls this out:

> If the processor is to remain in real-address mode, software must then load additional operating-system or executive code modules and data structures to allow reliable execution of application programs in real-address mode.

I think I'm maybe getting confused because you're ascribing things to the CPU that are actually done in the chipset/firmware? UEFI and the CSM aren't specifications directly supported by the processor. They're specifications for how the platform firmware is supposed to work and interface with with an operating system. But I see nothing preventing a system with a 64-bit x86 processor exclusively using real-address mode during initialization and handing off to the OS in that state. Though there is no economic reason to do that, that's not a limitation of the processor itself.
The CPU will start in legacy mode and default to real mode. It will happily jump to 0xffff:0000 (alias 0xffff0) and execute the long jump that's encoded there. 

https://en.wikipedia.org/wiki/X86-64

BTW, you can use `dd if=/dev/mem bs=1 skip=$((0xffff5)) count=8;echo;` to see the BIOS date at that location.
> For fun, see that someone created a replacement EFI

The doesn't prove your point - you can do this to basically anything because UEFI is just a firmware specification. You can replace a native BIOS with something that fits the specification if you go through the trouble of writing replacement firmware.
neat
Alright everyone has a different taste for different things. This one is not on my favorites list, but really guys? OP actually recorded and made some effort into showing HOW to accomplish what he intended. Downvoting it just bc you don't like it doesn't help anyone. We can do better!
Nice guide. Free software is, among other things, about customization and combining things you like with other things you like.
I want it to look like classic Orange Ubuntu which I think was around 9.10.
Ty
This is really useful. Unrelated but I wish there was a way in gnome 40 to use vertical workspaces. I might be the only freak that liked it and prefer it that way.
Ohh my gosh pls no..
[deleted]
why would anyonelike how ubuntu looks especially the current gnome made to look like unity.

Like sidebar made sense with unity global menu and pixel savers.

But now ubuntu got to be the ugliest implementation of gnome with some of the most unappealing colors.

Like the old orange ubuntu had a style to it but now uggh

People like ubuntu for the whole stability and package.
Just reboot it once in a while for no reason and setup a cron job to email your log files to Mark Shuttleworth every day. That should do it.
I threw up in my mouth a little. What a terrible idea.
Is there a way to get the dock on the left hand side like Ubuntu? I have not used much GNOME (or Ubuntu) as of late, but it appears to me that this matches Ubuntu in appearance but not in functionality.
Except for the short support time, Ubuntu 10.10 was the greatest (and last) GNOME 2 release. But 10.04 LTS was good too.

Of course, Ubuntu MATE took inspiration from GNOME 2 and continued adding features. I believe you can get an orange look with their 22.04 LTS release if you want.
I'm into it, I've always liked Ubuntu's aesthetic. And colors. But it's all subjective/personal preference.
Distro hopping because of the default aesthetic is stupid. You can more or less make any distro look however you want.
Customizing the theme in Ubuntu shouldn't be harder than switching to a different package management system, different security mechanism, and different release/support cycle.
I have pretty much the same opinion, but other people can have different tastes.
Think about it if a group of like a hundred people would actually set this up xD
I agree with you. Ubuntu 10.10 was the greatest Gnome 2 release. I would even say it was my favorite release last decade.
Classic Ubuntu look (think Hardy Heron) was absolutely iconic. Modern Ubuntu is different, but still kind of catchy.
[deleted]
Getting vanilla GNOME theme and style on Ubuntu is as simple as running this command:

`sudo apt install gnome-session`

Then log out. Choose your name on the login screen. Click the gear to change your session to GNOME. Finish logging in.

It's the same source packages as regular Ubuntu so my opinion is that you would still have full security support.
You are all over the place here ngl. First of all you are complaining about having to put in work to change the default aesthetic... I mean ok, this is Linux. Putting in work to customise it to how you want is kind of the point.

You then say your wanted a leading edge distro, which is why you.... Choose the long term support version? Ubuntu has a more up to date version, you didn't have to pick LTS. Theres a place for rolling, intermediate and stable releases. Screaming about the LTS being out dated is very silly. There a reason why for tumbleweed there's leap, for fedora there's CentOS, Debian has like three different version of varying stability (the default being far more "out dated" than Ubuntu).

I think there are valid reasons for preferring fedora over Ubuntu, but it really seems like you are blaming the distro for your own lack of research going in.
FYI Plasma has this built-in
I think I could find a use for this.

Do you plan to add ability to limit script execution to specific apps?
A simple way to limit the script to specific apps is using the `app_name` property. For example, this hook will only log notifications from Firefox:

    {
        "hooks": [{
            "shell": true,
            "command": "if test \"$1\" = \"Firefox\"; then echo -e \"$2\n$3\n\" >> ~/firefox-notifications.log; fi",
            "arguments": [["app_name"], ["summary"], ["body"]]
        }]
    }

Additionally, access to the full D-Bus message (which *should* give access to the notifier's PID) is planned.
This one is very nice, it also has a lightweight desktop version using tauri.
[Matrix](https://matrix.org) is an open network for secure, decentralized communication.
Very weird font rendering on the Flatpak.
> No more clutter

> Electron

LOL
Uoooh?
I wish so many apps didn't use javascript, webapps are bloaty.
Doesn't actually use electron, according to their twitter they use tauri, which - from what little I've seen - is way less bad.
> No more clutter
>
> You will see what's necessary to give you a better visual feedback.

What does the statement, when quoted in full, have to do with Electron? Nothing I would say, as it only refers to the display within the client. But apparently once again every means is right when it comes to Electron. And is Electron used at all?
Still not support native look and feel
If you're looking for a native look and feel, I would suggest the following:

Qt / KDE - [NeoChat](https://apps.kde.org/neochat/)

GTK / GNOME - [Fractal](https://gitlab.gnome.org/GNOME/fractal)
Nheko-reborn is also good
Nheko is probably the best option right now since its supports encryption, which is sadly quite rare for matrix clients.
Wasn't aware of this one, thanks!
Pretty sure Fractal Nightly also supports E2E, but I could be wrong
TLDR; Firefox now has a cold start launch time that is about 50% faster than what it used to be. This was achieved by Firefox now only copying a single language pack rather than all languages, the Gnome and GTK runtimes now use LZO compression algorithm rather than XZ compression algorithm (this also speeds up all other snaps which use these runtimes). They are also exploring precaching and a kernel configuration that makes SquashFS use multithreading; this was already being used in Fedora, which made snaps launch faster there.
With the Steam Deck becoming a pretty popular consumer device that uses Flatpak by default, I think Snap really needs a compelling reason to exist. Right now it's the less popular, slower, more closed alternative.
It's nice they are working on improving snap performance, but most people are not gonna care unless it gets similar or same performance as native/flatpak packages.
They are trying very hard. Best wishes. It is good for linux.
Still don't care. Snaps are crap.
how long until they finally get rid of that crap and move to Flatpak. It's about time!
Canonical fucked up pushing snaps when they hadn't been tested fully and the little speed increases that happen over time still don't compare to a standard .deb install.  Fix yo shit Ubuntu.
The best way to improve Snap performance is to delete snap
Keku just use flatpack and contribute to it
Snap is terrible. It will always be inferior to Flatpak/native. I will always dislike it because Canonical is making a conscious choice to go against the industry standard, which is Flatpak.
snaps are crap
no snaps please
Is expressvpn and similar plugins still totally borked in the snap vs native?
Can‚Äôt believe it took them this long on such a flagship product like the web browser.

Also, the fact that they were copying all language packs seems ridiculous.
What is the cold start time vs a regular non-snap install?
If switching to LZO was the secret sauce to speeding up startup, suggesting decompression as the bottleneck, it would be well worth exploring using LZ4 compression instead, which would reduce compression ratio a bit, but speed up decompression and startup.
I don't think regular consumers know (or care) about packaging formats, specially for a gaming device. Also, Snaps can do things that are not possible with Flatpaks.
It does have a good reason to exist. With the way it is designed, it‚Äôs better for CLI programs, IDEs (it has a ‚Äúclassic‚Äù mode that disables sandboxing and allows access to system libraries), and can be used for lower level things (including packaging the kernel as a snap).
It‚Äôs pretty close on my system as of version 102. It takes about 1-2 seconds to launch after a reboot. And this is on Ubuntu 22.04, results would be a bit better on Fedora.
when will there be freedom of repositories for snap?
>don't care  
>  
>still wastes their time by clicking on a thread and commenting to let everyone know how little they care
Flatpak can't do everything Snap can do.
Flatpak could use some improvements but it's being worked on and it's definitely better than Snap!
snaps sucks
> will always dislike it because Canonical is making a conscious choice to go against the industry standard, which is Flatpak

Snaps were invented *before* Flatpaks, lol. Check your facts before making weak and inflammatory responses devoid of any research. Also, there is nothing that indicates that Flatpaks are an "industry standard", whatever that means.
1.  You can't create a flatpak for lxd, lxc, firejail, docker, and a multitude of other packages.  If you don't know why that's the case, you should look into it.  But that underscores the fact that flatpak and snap have different "use cases".

2.  flatpak uses unprivileged userns and seccomp (via bwrap) for its "sandboxing" and "security".  It's convenient but, for now, it's a security nightmare.  snap uses a Linux Security Module (apparmor) for its containment.  flatpak has chosen "convenience" over "security" and with "portals" is continuing down that path.  It's all fine if you understand that any security promises aren't bulletproof and are being degraded.
your negative karma explains it
That wasn't a snap thing, it was a Firefox thing. I just checked the [bugzilla report](https://bugzilla.mozilla.org/show_bug.cgi?id=1297520) and Canonical brought this issue up 6 years ago, but it seems no work was done until now.
Depends on the hardware. For me, I have a Ryzen 5600x and an NVME SSD. Native packaging and the flatpak have no noticeable startup delay for me.

Firefox snap 99 would, on average, take 7 seconds to launch for me. As of Firefox snap 102, I just did three cold launch tests and it took 1.24, 1.87, and 1.87 seconds.
It's the primary way to install Discord, Chrome, emulators, other games, etc. It's also exposed (for Chrome) in the Steam UI itself. Users probably don't care that it's Flatpak, but developers care that they need to build a Flatpak to get their app on the Steam Deck.

I'm not sure what Snaps can do that Flatpaks cannot.
So, back when I was still a newbie and doesn't care about all these formats politics, I just want to get the apps I want, I want them work well, and I want them in an easy way.

The problem is that I've seen more and more developers (whether official or re-packager) focuses on releasing Flatpak or both; though there are still a few snap exclusives like Authy, Anbox, and AdGuard Home- the latter of which I'll concede to fit better with snap.

This is likely a decision driven by Steam Deck and Fedora, given that outside of Ubuntu people either just put in both or follows those two in shipping just Flatpak. Given Steam's immutability, manual snap install would be tedious too and not something normal consumer would care for.

Other than that, permission handling overall is better with Flatpak. Flatseal, while can be confusing to really understand, is easy to explain to new users. Like "hey, if you want it to access all your files, just install Flatseal and allow home access." And even Discover is incorporating some of that functions natively so users might not even need a secondary app anymore.

And overall, for GUI apps, there are less problems on Flatpak. None of that weird Font issues that I had with Snap app. Sure, CLI apps does better on snap, but regular consumers doesn't care about that.

So, I don't disagree with the sentiment, but for now the way things are implemented on Flatpak makes more sense for the apps and things that a regular consumer want to do compared to snap. What's ideal for me is to just do it like Manjaro and just ship both. Heck, add a `~/.local` based install as well while you're at it and have both AppImage and AUR as well while you're at it. User should just have the ability to find what they want to find, and install it easily, regardless of what they use.
what value is there in  packaging the kernel as a snap specifically. I can think of reasons for the others, but not that. (not saying there isn't though)
from cold boot?
When someone puts the work in to create it:

1.  The protocol for a remote repository ("snap store") is open and is simple.  Early on someone created a very short proof-of-concept in python.  It would not be hard.

2.  Right now the snap commands and infrastructure support only one remote repository.  The tools are FOSS and, if someone wanted, they could expand the tools to deal with multiple repositories.
Look, anything locked up isn't bad. Free and open source don't promise security. They just say use at your own risk. 

Most of the user don't even know how to write a single line of code. So they don't know what they are installing. Snap store give that confidence that atleast you are installing from a trusted source. 

If you don't like snap, then you can build your own. But it will be a tedious work. So let's give them a chance. Let's unite in this fragmented world of linux.
Don't understand why I was down-voted, I said it could use some improvements not that it was bad. 

The main things are solidifying the sandbox and preventing them from taking too much storage space since they take more space than an EXE, like I said these things are being worked on by the developers and it already seems more promising than alternatives like snap.

The fact that I got down-voted for saying this just shows the elitism among linux users that scares people away from trying linux.
>Flatpak could use some improvements

yes I agree, it is not a perfect solution either, but at least it is not incredibly slow and it also does not get forced down the throat of users.

Canonical should rather make snap so good, that ppl consider using it on their own and not force their users. But I think that ship has sailed already.
>Snaps were invented before Flatpaks, lol. Check your facts before making weak and inflammatory responses devoid of any research.

Flatpaks, originally called xdg-apps, were initially discussed at GUADEC 2013, - [https://www.superlectures.com/guadec2013/sandboxed-applications-for-gnome](https://www.superlectures.com/guadec2013/sandboxed-applications-for-gnome).

Snaps were not announced until December 2014 - [https://www.markshuttleworth.com/archives/1434](https://www.markshuttleworth.com/archives/1434).
when will there be freedom of repositories for snap?
Snap being made before Flatpak doesn't mean Snap is the standard or that Flatpak can't be. It's completely irrelevant.

There were a number of car charging protocols before CCS came about, for example. Would you say the Combined Charging Standard isn't a standard?
snap sucks
Don‚Äôt care if they were invented before. They are inferior. That‚Äôs what I said, and that‚Äôs my opinion. Industry standard meaning that Flatpak is implemented in literally every other distribution besides Ubuntu. 

You are putting a TON of words in my mouth. You should stop.

Seriously what part of my statement is supposed to be taken as fact? Go back to grade school and practice your reading comprehension.
I just don‚Äôt understand why Canonical would ship FF as a snap without testing all this. Someone in QA had to have brought up how slow the cold start time was.
7 seconds on NVMe... just like good old days of HDDs
Snaps handle cli applications much better. Whether that matters to you is another story
>developers care that they need to build a Flatpak to get their app on the Steam Deck.

Many of the people packaging Flatpaks are not the actual developers themselves, so this is debatable.

>I'm not sure what Snaps can do that Flatpaks cannot.


See: https://merlijn.sebrechts.be/blog/2020-07-03-snap-vs-flatpak/
Sure.
People complain that Ubuntu's default kernel is too old and Ubuntu historically did not make it easy to run a different kernel version and still get security support.

Also, there are several different versions of the kernel that can be customized for different use cases ( realtime kernel, gaming kernel, custom OEM kernels, etc.)

Snap makes it very easy to switch between channels and tracks.

For instance, it's easy to switch the Firefox snap to use the ESR track or the Beta track.
Yes, I just did three tests. First launch was about 1.2 seconds while the other two launches were about 1.8 seconds.
that is not true, things like core or bare or snapd itself contain canonical's proprietary software.
I have no issue with snap, but I can't see a reason to use them *on the desktop* when flatpacks are already ubiquitous. Unless they offer some technical, security, or performance advantage, but I'm not aware of that being the case.

Edit: added "on the desktop"
>The fact that I got down-voted for saying this just shows the elitism among linux users that scares people away from trying linux.

Yes, but you are also on r/linux which is known to be especially bad for this kind of behaviour.
>Flatpaks, originally called xdg-apps, were initially discussed at GUADEC 2013, - https://www.superlectures.com/guadec2013/sandboxed-applications-for-gnome.

Sure, but that is just a discussion; its actual work was not started until [2014-12-17](https://github.com/flatpak/flatpak/wiki/Flatpak's-History).

Snappy beta was launched on [2014-12-09](https://ubuntu.com/blog/a-new-transactionally-updated-snappy-ubuntu-core), long before the xdg-apps first commit and long before the first release of xdg-apps on 2015-09. Their discussion does not equate to their creation, and to have a fair comparison, one would need to know when the Snappy discussion started. But based on release dates, Snaps were invented first.

**Edit:** Considering [click-packages](https://manpages.ubuntu.com/manpages/bionic/man1/click.1.html) as the predecessors of Snaps, they were indeed invented before Flatpaks were even discussed as xdg-apps in GUADEC 2013. Discussions mentioning click-packages go as far as [2013-07-13](https://askubuntu.com/questions/tagged/click-packages?tab=newest&page=4&pagesize=15).
When you pay a developer to build another snap shop. Freedom is not a gift, it's something you fight for all the time. Complain all you want, I don't give a flying duck. 

Also:
https://merlijn.sebrechts.be/blog/2020-08-02-why-one-snap-store/
https://ubuntu.com/core/docs/store-overview
>Snap being made before Flatpak doesn't mean Snap is the standard or that Flatpak can't be. It's completely irrelevant.

I didn't say that. Can you prove that Flatpak *is* the industry standard? If not, then your comment is irrelevant.
Why do people thinking about Snaps, Flatpak and AppImage like *"one cannot live while the other(s) survives"*? I tought the open source community is actually all about parallel existing alternative viable options, not about one strictly chosen solution.
>Seriously what part of my statement is supposed to be taken as fact? 

Lol
I think everyone noticed but no one cared because they had to ship the FF snap by the time of 22.04's release.
The Firefox startup time and experience (black/empty window for a bit) on a stock Ubuntu installation is downright embarrassing. I wonder how many people tried the defacto user friendly distro that is Ubuntu and wrote off Linux because of how slow the snap stuff is.
Great point! Launching a flatpack app via CLI is very cumbersome
That's fair, that's probably a Docker use-case though.
It seems to me there are interesting use cases but they're probably limited to Ubuntu. For example, they could just run CUPS as a Docker container. For the kernel, an A/B partition scheme (like what Android and ChromeOS use). I'm not sure about updating the system theme.
The first part is solvable but just offering newer or more apt packages, but channel switching is useful and not easily done with apt
>People complain that Ubuntu's default kernel is too old and Ubuntu historically did not make it easy to run a different kernel version and still get security support.

How is deploying the kernel as a snap helpful or necessary to "fix" this?
Ok, gotta say that's not bad at all! I thought it's worse.
You are wrong.  snapd is 100% FOSS (GPLv3, https://github.com/snapcore/snapd).  Same with core (see core20 here https://github.com/snapcore ).   The only thing that is not FOSS is the remote "snap store" and it doesn't run on your computer (and the protocol for the "snap store" is open).
Snap is more suited for servers. Flatpaks are also good. I prefer native packages whenever I can. As I use ubuntu based distro, .Deb is my personal choice.
snap is an attempt to centralize and turn an open source platform like linux into a monopoly, it is an abomination that we, the users, do not want.
snaps are crap
It‚Äôs not that they can‚Äôt coexist. That‚Äôs fine. But canonical needs to shift its focus from desktop back to server. Flatpak and Snap were designed with a fundamentally different philosophy. One was meant for desktop, and one was meant for server.
> Had to

They chose to go with snaps, they chose not to use deb. Linuxmint for example, uses debs provided by Firefox, there was a choice.
no, that's not what i was talking about. snap is meant to handle both cli and gui applications. flatpak is currently pretty gui focused (for now anyways)
i think there's room for a runtime setup just like flatpak, but for many cli applications. Taking wget for for example: a newer version of wget should be usable on any linux distro for the past 10 years, but it does need to make sure it has up to date openssl. a c library, and not much else.
I don't think it is limited to Ubuntu. Anyways, instead of three diffrent solutions, you get one unified tool with Snaps, and those are things that Flatpaks are unable to do.
You're so smart! You should go work for Canonical. I'm sure they have never thought of that idea.

I love how people talk to a Canonical employee and tell them how to do their job.
It definitely was pretty bad on version 99. It would take on average 7 seconds to launch, I think the worst I experienced was 11 seconds.
snap store is not open and the best proof is that there is only one snap store, snaps cannot exist outside snap store, that is neither open source or freedom.
The snap CLI experience is better true
The correct tool for servers is Docker, not fucking snap lmao
They messed up when they made the Firefox deb install the snap. I don't care if they put it by default, but the native deb should have still been there.
>that we, the users

lol
No, no. Kevandine says [here](https://discourse.ubuntu.com/t/feature-freeze-exception-seeding-the-official-firefox-snap-in-ubuntu-desktop/24210) that "...This is the result of cooperation and collaboration between the Desktop and Snap teams at Canonical and Mozilla...".

They also say, "...When Mozilla approached Canonical...". This pretty much means that Mozilla was the one that came up to Canonical with the deal to make Firefox into a Snap. Why, you may ask? Well, the Discourse post lists a couple of reasons, but I say that the biggest reason for the change is because Mozilla, or the Ubuntu Mozilla Team, simply got tired of maintaining a bunch of Firefox packages for many different versions (14.04, 16.04 (Extended Support), 18.04, 20.04, 21.10, 22.04) and wanted an easier, cross-platform solution for shipping Firefox right when a new update releases, which is once every 4 weeks. And, since snapd comes preinstalled in Ubuntu, they decided to use that, and Canonical was more than happy to oblige because, well, snapd is their package format.

The Linux Mint team, on the other hand, approached Mozilla and both parties signed a partnership that ensured that Firefox on Mint would continue to be distributed as a .deb file
Ok! I fail to see how that is *not* an example of how 
> flatpak is currently pretty gui focused (for now anyways)

but hey, maybe I missed something!
That's a pretty good use case for running those CLIs from an OCI container format via Docker or Podman. It's already somewhat common in the enterprise setting, where wide differences between production servers and development workstations (such as those workstations not even running Linux, for example) benefit from using exactly the same tooling everywhere. As long as the cost of that containerization isn't an issue, it's a well-established solution.
i literally said channel switching was neat and not easily done without snap (or flatpak, or whatever similiar tech)  and the first part didn't require snap, which is true, because other distros don't need snap to offer newer kernels.. they just release newer kernels, and different kernel packages. It's awkward, but workable.

Aare all "Ubuntu dev" flair folks knowledgeable about everything in ubuntu? why the heck would i think that?
On the Raspberry PI it was horrible. Now that they've cut by 50% the cold boot up speed, I wonder how it fares vs. regular deb.
>  snap store is not open ...

Which matches exactly with I said.  I said:  "The only thing that is not FOSS is the remote 'snap store'"

> ...  snaps cannot exist outside snap store ...

That's just not true.  The number of things you've gotten wrong on this chain shows that you don't know what you're talking about.

If someone puts a snap on their website, you can download the snap manually and install it manually without going through the snap store.  You do  need to tell snap to ignore the authentication/signing (e.g  snap install --dangerous whatever.snap), but you can do it.  There used to be a site that wasn't controlled by Canonical that hosted snaps like that.

Let's tally the things you were wrong about:

1.  You said that snapd contains Canonical's proprietary software.  I showed you that you were wrong by linking the source which is GPLv3 licensed.

2.  You said that "core" and "bare" contain Canonical's proprietary software.  I showed you that you were wrong by linking the source which is FOSS licensed.

3.  You said that snaps cannot exist outside the snap store.   That's also wrong.  Discussed above.  I know that because I created a sample snap by myself.  I never loaded it to the store.  I installed in manually on several of my computers.

You are uninformed.   You don't know what you're talking about.   Give it up.
You don't even know what docker is. Get some knowledge üôÇ.
we don't like snaps and software distribution monopolies
why can snaps only be created in snap store?
you said:
> Launching a flatpack app via CLI is very cumbersome

 I took that literally. Usually people are referring to the fact that you can't simply call the app, but rather have to use the reverse dns style to launch it when they bring that up. That's the least of the problems flatpak has for cli apps.
The storage model isn't important to me.  The common runtimes and deduplication are.
I installed snap store on a virtual machine just to confirm that it contains proprietary software

shared gnome 3.38 ubuntu stack is proprietary software  [https://imgur.com/a/FnfPNLZ](https://imgur.com/a/FnfPNLZ)

Bare is propietary software [https://imgur.com/a/wuSWPQE](https://imgur.com/a/wuSWPQE) 

core 20 is propietary software https://imgur.com/a/BtKSMjd

 a centralized store for software will never be open, that's not open source. Snaps are a complete abomination and an attack on open source and freedom.
My guy I'm a programmer I literally use docker at work
Sure, sure.
Why do you have negative karma?
> the fact that you can't simply call the app, but rather have to use the reverse dns style to launch it

is an example of how

>flatpak is currently pretty gui focused (for now anyways)

It's the reason I brought it up. snap doesn't suffer from this issue, probably because

>snap is meant to handle both cli and gui applications

after all

>snaps handle cli apps much better

a good point, not often brought up when the snap hate train rolls into station.

If I totally missed your point, which it seems I have, then what were you talking about? Could you elaborate?
That is a very important point to consider. These OCI containers have the _potential_ to offer very strict deduplication opportunities through how container image layers work (common slices across different containers can be reused and users can avoid redundant download and storage costs, but only if they are identical), but in reality there hasn't been any push across the ecosystem to significantly reduce that. With the common set of Linux distros used as the base for many images (Red Hat, Ubuntu, Debian, Alpine seem to be the most common), the ecosystem as-is won't necessarily bring those benefits, unless a single party focuses on building container images all based off of a common, stable runtime.
Do you really think that gnome-3.38 is proprietary???  LOL.

That's just a defect of the snap store.  The license on collections of software with multiple licenses is generally not set on the snap interface ... and the interface might show it as proprietary.  Here's another link showing the license is "unset" (https://snapcraft.io/install/gnome-3-38-2004-sdk/ubuntu).   That's what happens when it's a collection of software with lots of different licenses.   I already linked you to the source for core20 ... it's FOSS. ( https://github.com/snapcore/core20 )


Anything that runs on your computer as part of a default install from Ubuntu is FOSS.  

>  ... a centralized store for software will never be open ...

I agree that Ubuntu won't make their store open.  But that doesn't stop you from making your own store for and making the source open.  Someone already did a proof-of-concept long ago.
https://www.reddit.com/r/linuxmasterrace/comments/vv3va1/which\_universal\_package\_do\_you\_prefer\_and\_why/?utm\_source=share&utm\_medium=web2x&context=3
?
You picked a superficial issue is what I'm saying. The reverse dns naming is for security purposes. There are no 3rd party snap repos, so this is less of a concern there
That's what flatpakn already does though, just in a different way
If everyone can have their own snap repository mention other than snap store
>https://www.reddit.com/r/linuxmasterrace/comments/vv3va1/which_universal_package_do_you_prefer_and_why/?utm_source=share&utm_medium=web2x&context=3

ROFL
Snaps can be created anywhere lol, you can even install snaps that are not in snap store!
Thanks for the explanation of what you meant!
> If everyone can have their own snap repository mention other than snap store

What?  That's not a sentence.

The fact is that with the current snap tools (snap, snapd) one can't have *multiple* snap stores for downloading/installing snaps.  Those tools can only reference one snap store.  That doesn't stop you from pointing it to  some other snap store if it existed.  The protocol for the snap store is not very difficult.
we do not like snaps
That is fake, all snaps need to be signed in snap store and cannot be created outside snap store, snaps are centralized in snap store.
AppImage are distributed anyway, Flatpak has Flathub, Flathub beta, elementaryOS Appcenter and even Fedora has its Flatpak repositories.

Name another snap repository such as snap store
You sure don't.
You are wrong. All Snaps do not need to be signed in the Snap Store. You can make a snap by yourself and use it on your computer without needing to use the Snap Store. 

> cannot be created outside the snap store

How the hell do you think snaps are created? I've made a snap myself, for private use. I don't have it anymore, but I never had to use the Snap Store.

> snaps are centralised in snap store.

This cannot be said for all Snaps. For private use snaps, or snaps you made, you can give it to anyone, without needing for all snaps. You can even host a server and have snaps in there, and that exists, it's called "LOL" (I'm serious).

From your previous comments, it seems like you know nothing about Snap and are making false claims. Why don't you give up and accept the truth?
Did you read anything I wrote???
Accept the truth that snaps are centralized and use proprietary software. Any other linux package formats have many repositories but what a coincidence that snap has been around for years and there is only one repository.snaps are crap is canonical's attempt to turn linux into windows.

If I am liying, tell me, What other snap repository exists?
You say it is easy for anyone to make their own snap repository, mention it, say what other snap repositories exist.
Why don't you accept the facts, dumbass? Elitists like you make Linux worse.

Snaps **do not use proprietary software** with the exception of the Snap Store which is **remote**. Snap only has one repository because it's easier to find apps like that. Flatpak didn't have a centralised solution, but they had to make one as it would be hard to find software.

Snaps **are not** Canonical's attempt to turn Linux into Windows. Elitists like you think it that way because you dislike it.
There was one that existed.  The source was on github.  The author removed it.  I have a copy of the source.  It is not that difficult.
If I am liying, tell me, What other snap repository exists?
Name another snap repository such as snap store
The "LOL" Snap repository and the discontinued Click packages repository.

The LOL snap repo. Yes, it is a real thing, and the Ubuntu devs know about it too.
If I am liying, tell me, What other snap repository exists?
HAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHA
Did you read my comment?
The click packages were for ubunti phone, canocal abando them not before launching snap store, a "discontinued" repository exists of course.

If I am liying, tell me, What other snap repository exists?
The LOL Snap repository. Are you sure you read my comment? Why don't you use your brain instead of mindlessly copy pasting your comment? You are literally testing my patience.
share the link so I can download them right now if it is true.

If I am liying, tell me, What other snap repository exists?
https://repo.lolsnap.org/lol-snap/lol-server.git
what I assumed, because they don't exist, the snaps are forcibly centralized by canonial in snap store with their proprietary tools,snaps do not have freedom repositories.
They do. Even ubuntu Devs are aware of LOL's existence.
If I am liying, tell me, What other snap repository exists?
DID YOU READ MY ABOVE COMMENTS? I am convinced you can't read and understand English and you don't know anything about Snap.
If I am liying, tell me, What other snap repository exists?
https://gitlab.com/lol-snap/lol-server

https://gitlab.com/lol-snap/lol

https://repo.lolsnap.org/lol-snap/lol.git
archived and abandoned projects?
No, if you visited those links, and read at least ONE WORD, they are using a different website for development. The website is currently down though.
As one of the \~8 paid, full-time Krita developers, I can tell you all for a fact that, for better or worse, a big chunk of our development funding comes from stores like Steam and the Windows Store, without which we have very little chance of keeping up the current scale and pace of development.

So, we'll see what happens... Hopefully Microsoft will recognize the inherent flaws to this policy and go back to the drawing board...

But if anybody here values what we do for Krita and has a few extra bucks per month that they are willing to contribute to sustainable FOSS development, please consider chipping in to the [Krita Development Fund](https://fund.krita.org).

**Edit**: Good news! Someone from Microsoft has [clarified the intent and they will be adjusting the wording.](https://www.windowscentral.com/software-apps/windows-11/microsoft-responds-to-controversial-store-policy-change) (But still check out the Krita Dev Fund if you're interested in a better and more sustainable way to support our project). :)
Am I the only person who thinks this is to avoid people repackaging FOSS software and selling it on the store without compensating the actual developer? At least that seems to be the primary intent rather than somehow stopping FOSS projects from making money
I'm a maintainer of a popular open source game / engine. Someone took it and uploaded it to the Microsoft store for $5. Microsoft has done nothing despite multiple reports. It's legal to sell FOSS stuff, but they're doing it without changing the name and it's confusing users. So if this rule allows removing that listing, I'm all for it

Edit: well, ideally it would be a rule against imposters, so projects like Krita can still get funded

Edit 2: the project is [Minetest](https://www.minetest.net)
Interestingly, MS encouraged these open-source projects to publish on the Windows Store, back in 2017. Presumably they did not have a problem with their pricing back then.

> https://krita.org/en/item/krita-available-from-the-windows-store/

> Some time ago, we got in touch with a team from Microsoft that was reaching out to projects like Krita and Inkscape. They were offering to help our projects to publish in the Windows Store, doing the initial conversion and helping us get published.
Windows Store as it stands is a cesspoll, completely useless. Full of scammers and fake repacks of both proprietary and FOSS projects. Pure utter garbage.

And somehow this is kind of a tradition with MS software stores, since it was always like that since as far as it existed, and they didn't fixed in nearly a decade. Even the deceased Windows Phone store used to be a total PoS.

They really must up their screening efforts if its to be taken seriously. What they must do is to clarify how their will sort out genuine FOSS projects to allow monetization.
I can see what they're trying to do here, but there are a few things I don't quite understand.

I gather that they're trying to prevent predatory practices. Preventing people from charging for software on the store that you can get for free without the store. Basically preventing taking advantage of people's ignorance.

However what I don't understand is the examples provided such as ShotCut & Krita. If these are both free projects, why are the authors charging for getting it from the store? Since I don't understand what is going on here, I'm not for or against the practice, but at first smell, it seems fishy.
Does anyone even use the Microsoft store lol
Microsoft: all this open source will be so handy to further monetize our products  
Also Microsoft: how dare you use open source to monetize your product!?
You can really feel how much they ‚ô•Ô∏è open source
This solves one problem -- vampiric resellers who add nothing -- and creates a new one, suppressing the capacity of people to essentially conveniently donate to FOSS developers. This second problem will require a fix of its own.
Who TF uses Microsoft store?
They ban FOSS while simultaneously gobbling up Linux devs for Windows Subsystem for Linux and hiring Guido Van Rossom. They are rewriting fundamental components of Windows in Rust. They also own GitHub. The fact that they both use and discourage FOSS should be a warning sign that they‚Äôre up to some new fuckery.
I can see why they‚Äôd do that tho. Technically anyone could upload blender as it is, or with a changed splash screen or something, and trick unknowing newcomers to spend money on it, while contributing nothing back to the actual project. But to even make it illegal to ask for donations, yeah, that‚Äôs kinda shite.
Microsoft's intentions is to block misleading non-free open source listings (e.g. 7-Zip clones) and not flat out block the sale of open source software in the Microsoft Store.
I guess that mostly depends on how they enforce it. The issue is that there has always been leeches that package open source projects that they do not own and add a price tag to it, and I'm 100% fine on cracking down on those. However, this is not fine if they start taking down official versions because of that
I usually consider myself to be a fairly chill person, but articles like this turn me into some wild rabid animal filled with hate. I have to go gnaw on some sticks in the backyard with the dogs to calm down.
Stop supporting Microsoft. Discourage any Microsoft use. 

If you see someone install Edge on Linux, slap the shit out of them. 

Don't install Teams. 

Don't install or use any Microsoft software. 

Don't sign up for a Microsoft Account.

Microsoft is not here for our benefits.
This is probably their attempt to remove scammers charging gullible people for otherwise free software. I'd hope an exception is made for the actual maintainers of said software.
Microsoft üíî Linux‚Ñ¢Ô∏è
Downvote if you will but this whole EEE is thing of a past.  Linux and most big FOSS are backed/driven by cooperations like Google, Amazon etc. MS just cannot compete without embracing Linux, it‚Äôs not MS vs FOSS community anymore.
...when microsuck themselves have been illegally selling stolen software and open-source with the real authors names removed, for decades. Bunch of piece of shit hypocrites.
Why would any FOSS developer use the Microsoft store? You are helping the enemy. We've known for a long time now that they want to eventually turn PCs into a digital prison where you cannot run anything they haven't approved of (like how a game console works), and, by putting your software in their store, you are helping them along the way.

EDIT: Wow the downvotes.  Enjoy your locked down computers where you have to pay extra just to run your own software, like the way you can now enjoy purchased movies that randomly delete themselves.

https://www.flatpanelshd.com/news.php?subaction=showfull&id=1657022591

Stupid-ass consumer fucktards deserve every bit of this dystopian world they are walking into, more and more by the day.
Embrace, Enhance, Extinguish... told you it was coming and you told me I didn't understand how open source works.

"How would they do that?"

"Any and every way they can."
[deleted]
It's smart. It blocks a way of getting FOSS projects funded. This says clearly that FOSS developers aren't allowed to profit from their work on MS's platform. Only MS is raking in the moolah over the backs of artificially starved FOSS projects.
[Deleted]
Get devs to pull as many projects from Microsoft store as possible. Give them a taste of there own medicine!
The MS Store? Lol who cares. 

This is gonna go over like their phones did
I don't see a problem with this. Good for Microsoft. Sounds like a very good policy to me.

Also, let's not forget that Microsoft is the largest contributor to Open Source projects. They probably don't want other companies stealing their work.
Hmm, so Lenux it may be then?
The key word in the policy is *profit*, it doesn't say that someone can't charge money for open-source software. Covering the cost of development would not be *profit*.
They were extremely  right at one point!

I don't want GIMP compiled and sold by Vasiliy Poopkin even though he is in top 5 developers of that GIMP!

One day he goes mad  because of hangover and harms/infects   his distribution.  It would be not the first scandal of this type, but will shake the trust to opensource projects.

As far as I understand there is a recent example with Armbian where one of the developers/contributors promised to harm Amlogic devices in his releases.

So to my mind MS is right. %%OpensourceProject_NAME%% .org is represented by the same organization in their store.
Necrosmurf products were never intended for adults
[deleted]
When I still used windows I tried using the MS store a few times, every time it broke, didn't matter if it was for something that was free or paid for
Are there that many Krita developers? Holy Torvalds that‚Äôs impressive.
So I know this is me being super lazy, buuuuuuut. Are you aware of any kind of "fund" I can contribute to that would spread my donation across a bunch of different projects including Krita? It would be nice to be able to "donate" to a bunch of very useful projects even if I don't personally use them without having to spend a bunch of time researching them.
There's only 8 paid devs? What the fuck. 

I mean I'm glad I bought it off of Steam as a show of support but damn. Krita is one of the finest pieces of software ever made, hell it's the best painting software out there imho. It runs on Mac, Windows, Android, Chrome OS and Linux.

The fact that there's only eight paid devs doing this is nothing short of amazing.
The article says 

> all pricing ‚Ä¶ must ‚Ä¶ [n]ot attempt to profit from open-source or other software that is otherwise generally available for free [meaning, in price, not freedom]. 

Can't you instead make the Windows installer from your website a paid purchase as well? Effectively force people who want to use it for free to instead use a FOSS operating system.
Wow, I thought at one point they were seeking out projects to follow the same kind of model as Krita.
Why would a company that gets high off its own farts ever see the inherent flaws to it's policy?
> Hopefully Microsoft will recognize the inherent flaws

they  recognize those flaws, they are good to their bussiness.
Hey! thank you so much for your work! 

I love Krita. Sadly I could be layoff this month (also a dev), but if won't (or I get new a job) I would donate, for some reason It never came to my mind that i could donate to krita.
"Hopefully Microsoft will recognize the inherent flaws to this policy and go back to the drawing board"

Unless of course intentionally harming Linux is their end goal here.
>Steam

It never dawned on me to buy Krita off Steam. What a smart idea!
It seems people haven‚Äôt actually used MS store and commenting. 

Fedora for example is being sold by some company that isn‚Äôt related to Fedora Project or Redhat. I doubt the money you pay will be contributed to FOSS.

Banning these will ensure that the money doesn‚Äôt go to those who just leech.
Yes, that's the primary intend but doesn't change the fact that the new rules are written in a way that FOSS projects who sell their own stuff (like Krita) are collateral damage.
A Krita developer has chimed in: https://www.reddit.com/r/linux/comments/vtxr9r/comment/ifb7hgk/?utm_source=share&utm_medium=web2x&context=3

So, indeed, this policy will greatly affect their development model.
You misunderstand free software. See https://www.gnu.org/philosophy/philosophy.en.html

You can modify and sell the software as you wish
I don't think that you are the only person thinking that but regardless, that activity is perfectly legal and permitted and not an issue.

Sure it would be nice if it were against the license terms to "sell someone else's project" but it isn't. If it were you could say goodbye to so many, Redhat for one.

You couldn't say it was *your* program however. But the fact you charge a few (insert currency) for distribution doesn't do any harm as it should be easy enough for the cheaper or free version to be located and obtained instead!

You'd thus be suggesting that you are providing a service over the free version, such as bundling it up on the Microsoft store which maybe something that the original developer never bothered or intended to do.

The actual issue is preventing access to source code etc, actually stuff that goes against the license.

Plus the original developer can not charge for the software either under these rules so Microsoft is basically saying "free as in beer regardless" which is clearly to me either a result of poor understanding of how FLOSS can work or an attempt to undermine it.

Edit: If it were to be acceptable to do what Microsoft is doing, why is it not so in other similar areas. For example, why is nobody lamenting on the injustice of musicians not being compensated when their royalty free music is used in someones Youtube video? Well, the answer is, it's because it's *royalty free*.  FLOSS licenses are royalty free so maybe we are missing some FLOSS license options that permit royalties, that would address the issue.  But we have what we have and Microsoft banning commercial/profit based sales of FLOSS software is much like Youtube banning all commercial use of royalty free music.    Here, with software, many people think that such a move is a good thing as it will help the starving developer, but if Youtube were to do it, well there would be riots on the street!
But microsoft bad, upvote./s
Oh this is totally the "stated goal" the out loud part. 

The quiet part is it's Microsoft's "Embrace, Extend, Extinguish" at work. 

We're moving into Extinguish in App store land.

This is "secretly" about cutting off income streams for FOSS projects under the guise of "making it so people can't rip off things and make money"

Edit: lol it's clear as day MS is cutting off funding streams for FOSS developers.  I refuse to believe that it's simply a "unintended consequence"

That and their rapid hiring of Kernel and other low level developers to accelerate development of their requirements.  Pottering going to MS from RH is a bad thing. Esp considering his distain for everything resembling open packaging standards.

Edit 2: WSL removes the need for many people to run Linux as their desktop/laptops primary OS. It *lubricates* the use of Linux in a VM/Box enough that it's "painless" to use the preferred Linux based tools for DevOPs/ML/Sysadmin tasks.

And business will stop letting people in IT manage their own devices when they need Linux because "WSL is good enough"

So please tell me again where I'm wrong exactly?

Edit 2: 

LOL Even when the people at Krita come out and state they won't survive without the MS store sales you're still sure I'm wrong?

https://www.reddit.com/r/linuxmemes/comments/vu8jls/im_happy_to_learn_from_the_systemdgithubd_fanbois/

Good job. Drink the coolaid.
This policy does not protect the author's compensation, it destroys it. And to do what? Prevent other people from making money, which the author has explicitly allowed them to? If Microsoft really believed that this policy is good for FOSS, then they are completely stupid. But anyone who has been in the tech world for more than a decade knows this is intentional on their part.
These are the same folks who [got all upset the other day](https://sfconservancy.org/blog/2022/jun/30/give-up-github-launch/) because github started charging money for copilot when it is trained on FOSS source code... and now it sounds like they are upset that people will not be able to make money from just reselling FOSS.
You should read the linked post.
yeah, i think so too
It doesn't matter. The copy left allows it, and it is better that someone other than  Proprietary Software developers can pull money out of the Microsoft Stores.
If you have a ‚Ñ¢ they should remove it without this.
Thank you for your work with Minetest!
As long as the redistribution does not violate the license, you don't have any legal base on preventing it.  You should have licensed the game/engine under a copyleft license to ensure the freedom of users of the redistribution.
I think Microsoft hates minetest for obvious reasons
Minetest rules thanks for all the hard work
LMAO, and the trouble they had since-

https://krita.org/en/item/krita-in-the-windows-store-an-update/
Embrace, extend, extinguish.
GPL and cost are not related. GPL only mentions the requirement of redistributing source.

It is entirely within a project's right to sell builds or binaries of their project, and remain 100% GPL and open source. That's what Red Hat is.
> 
> 
> However what I don't understand is the examples provided such as ShotCut & Krita. If these are both free projects, why are the authors charging for getting it from the store?

They are using the paid store version as an easily accessible way to essentially give a donation to the project. Yes you CAN get the software directly from their site, but they also provide a link to give donations on the post-download page. Anyone is free to choose the direct download instead of the store version, and to ignore the donation links. There is no intention to mislead anyone with this.

Donations like this are important for FOSS projects. Devs need to pay their bills too.
hexchat was created at least partially because xchat windows binaries weren't free (of cost). You could of course compile it yourself though.
>However what I don't understand is the examples provided such as ShotCut & Krita. If these are both free projects, why are the authors charging for getting it from the store? Since I don't understand what is going on here, I'm not for or against the practice, but at first smell, it seems fishy.

Very few Krita users are capable of building it from source. Hell, I'm guessing that the majority don't even know that it is open source. By buying it on Steam or MS Store, they get automatic updates and prebuilt binaries, and the Krita team gets funded in return.
finaly someone asking correct question
I don't really use Windows but using the store it's definitely better than downloading '.exe' installers using your browser everytime you need to update a program.

The store it's their equivalent to Linux distro repositories.
like 12 mil people.
We're all entitled to our opinions, but lets not be disingenuous. They're not banning FOSS. They're banning selling something that you can get for free. If you want to put FOSS on the store, and not charge for it, that's still allowed by their policy.
The fact that people still believe the "Microsoft loves Linux! Microsoft Loves open-source!" crap is mind boggling. Yeah sure, they love it, as long as they are the only ones able to profit off of it. It's not a moustache twirling conspiracy, it's literally just capitalism in action, and we can see it in action. Microsoft systematically cockblocking alternatives, buying up more companies and assimilating more projects and technologies into their own. Corporation maximizing profit and trying to shut out it's competition. If you support Microsoft in all this, you don't really support the core ethos of FOSS.
> They ban FOSS while simultaneously gobbling up Linux devs for Windows Subsystem for Linux and hiring Guido Van Rossom. They are rewriting fundamental components of Windows in Rust. They also own GitHub. The fact that they both use and discourage FOSS should be a warning sign that they‚Äôre up to some new fuckery.

But I read a comment by a random person once on /r/linux saying that my mentioning of Microsoft's Halloween Documents and *Embrace, Extend, Extinguish* is an outdated take and that the *New Microsoft* loves Linux! :(

I don't want to be sent to the Anti-Microsoftphobia Workshop again by HR (recently acquired by Microsoft). Gelding with a proprietary cutting tool was painful but I was told I have to undergo the procedure to remove my outdated prejudices against *New Microsoft*. :(
>gobbling up Linux devs for Windows Subsystem for Linux

Welcome to real life, money talks. 

>Guido Van Rossom

What does that have to do with anything?
Then they should have phrased it in a better way.
So you're literally taking the freedom of users to choose what they want to use, that's the spirit!
Yeah, some kind of verification would be all they need
I was gonna say, for a company that <3's open source they sure so seem to fuck over the open source ecosystem a lot.
Have they? I‚Äôve not heard of this, can you give some examples?
>Why would any FOSS developer use the Microsoft store? You are helping the enemy.

Because some people are not fighting a holy war but just writing goods software that is useful to many different people. That includes windows users. Those are not the enemy, they're your mom and your coworker and your nurse.
>You are helping the enemy. 

How is so? It is like saying, "Why make FOSS compatible with proprietary operating systems?".
But...they would get a cut of the money. This eliminates that.
was anyone really getting funded at the MS App Store?
>All these developers putting their tools on Github for use under open source licensing (gave your IP to MSFT for free).

Thing is though, what's really the difference where you put it? The source is open and free for anyone to download and look at whether it's hosted on github, gitlab, or any other site. Open Source has always given it's IP to everyone for free, MS, google, redhat, amazon, you, me, etc etc etc. And plenty of them have used open source to profit off of. What's really different this time? I'm sure being on github made it logistically easier for microsoft to train, but I don't see what's stopping anyone else from doing the same?
The fee for copilot likely comes from the compute necessary behind it. The FOSS is partially used as a training set, but MS still has to be compensated for the compute resources used by it.
Their!
The income generated from the Store is a not-insignificant source of funding for the FOSS projects who publish on it.
Are you okay with FOSS developers not being able to *profit* from their work?
There isn't anything wrong with profit.
You should have read the linked post before commenting with misinformation.
It‚Äôs a complex product in a sense like Blender so you need people to maintain it.
Donate to what you do use. Pull up your software list on your distro of choice and look at products you've used that a developer provides you for free and donate to them.
Software in the Public interest holds for several Linux related projects

https://www.spi-inc.org/donations/
You could go to KDE rather than just Krita, similarly go to other large projects with many subcomponents like GNOME, Linux itself, FreeBSD/OpenBSD, Free Software Conservancy, FSF, OSI,  Software in the Public Interest, GNU project and so on. There's projects like OpenCollective and Liberapay but you'd have to specify what your money went to.

Point being, there isn't (should there be? I kind of don't think so but I can see the benefit) one or a small number of centralised Free software funding bodies for you to donate to who then maximally efficiently allocate your funds
The problem with "umbrella charity funds" is they eat up administration costs through bureaucracy. According to [Charity Navigator:](https://www.charitynavigator.org/index.cfm?bay=content.view&cpid=48)

*     Museums warrant higher costs up to 17.5 percent.
*     Food pantries/banks and humanitarian supply charities should have lower overhead with a cap of costs around three percent.
*     Grantmaking organizations shouldn't see costs higher than seven and a half percent.

Giving to a charity is balancing oversight with the actual funds awarded. That's why I tend to donate directly to projects, knowing the money will be better spent.
Doesn't Open Collective do something like that?
>Krita

I donate to KDE.
This sounds like an awesome idea
This makes multi-million/billion dollar corporations look even more pathetic when they come up with the excuse that they don't "have enough resources" when asked for a Linux port. Sad.
It's harder to manage a big team. 8 full time, with random outside contributions is about right. Especially true if you can't afford a full time product / project manager, testers, etc.

With a small group, everyone can be on the same page.
Thanks for the kind words and support. :)
Of course, we couldn't do it without the FOSS community behind us.

Full-time FOSS devs have an important role in making sure that things are constantly being worked on, that people are around to answer support questions, and that there are people who can guide new devs and review patches.

But we also have a bunch of regular community contributors who generously give a lot of their time to the project, as well as people who just swing by and drop some great patch on our laps out of nowhere.

I know it's pretty similar for projects like Blender and Godot, and it works out really well I think.
And tell "we give prioritary support for Linux, any other platform may or may not work at your own risk" or something along these lines.
Well, it's hard to say, because as the article is hinting at it's pretty much impossible for FOSS to restrict other people from distributing free binaries. I know that project leader, Halla, wants to make sure that Krita is widely available for free on PC platforms, and I think I generally agree with that because it really opens up digital art creation to a ton of people who may not normally be able to afford it.
That's very kind of you. 

I definitely think people should only contribute whatever time/money/energy that they can afford. That makes it very tough during all of the economic uncertainty that's come with the pandemic, the war, and all of the supply-chain related inflation.

I hope that don't get laid off (or you land on your feet).
Yeah.  The Microsoft store has a serious problem and this is a needed step to protect FOSS projects.
>It seems people haven‚Äôt actually used MS store and commenting.

We're on r/linux, and even Windows users AFAIK pretty much ignore the new MS store and most other recent developments of MS
So? Fedora WSL Remix does nothing wrong. It's a remix as outlined in Fedora's own guidelines and all required source code is being released.

It's not like Fedora upstream cares to make a WSL version, btw.
In that case, sure, but this policy, as it stands cuts off all FOSS including legitimate fundraising for organized development.

Thia is salvageable, but Microsoft's lawyers just went and flushed the baby with a bath. As out of touch lawyers tend to do.
https://mobile.twitter.com/Krita_Painting/status/1545241688599367680
> Fedora for example is being sold by some company that isn‚Äôt related to Fedora Project or Redhat. I doubt the money you pay will be contributed to FOSS.

And? As long as they provide the source, they're allowed to do that.

In fact Red Hat does the same. They sell their Red Hat distribution, which includes the Linux kernel, GNU tools and libraries and countless of other pieces of software that they haven't created themselves.
They've already mentioned that they will clarify the wording of the policy alongside what the actual intent was. Lets wait and see what the clarification says.
I actually don't. Yes the license allows that  but it  doesn't mean it's what Microsoft has to allow on their store. Currently the Microsoft store is a cesspool of trash tierd apps, fake apps and paid releases of free apps mixed alongside actual apps like Firefox, VScode, etc. Microsoft needs to get things under control if they want people to take their store seriously. There is no reason why the store can't be a safe place for new/average users to download software but right now, the store is no where near suitable for actual usage.
>	modify and sell the software as you wish

AWS, and Jeff Bezos, along with Google, and their battalion of gazillionaires, have made a f*ton of money doing just this while the original FOSS developers aren‚Äôt getting rich.  GPL v4 needs a revenue sharing clause.
Correct, but MS doesn‚Äôt have to allow it. When you go to a digital storefront and have 55 copies of the same software for different prices, it kind of damages the name of whatever the project is.. for instance, would you use Linux if 20 companies sold 50 different distributions that were actually the exact same thing, but with a different name?
Yup... it's perfectly legal. They used to sell free Linux distributions in Best Buy... they were selling the packaging and the convenience of making it available to you... not the software itself.

If you don't look into what you're buying enough to know that you're paying for free software, buyer beware.
> For example, why is nobody lamenting on the injustice of musicians not being compensated when their royalty free music is used in someones Youtube video?

When someone re-uploads someone's royalty free music as-is with zero calue added like the guy selling GIMP PRO on MS store, there's generally at least some lamenting. And if someone tries to make money off it, there's usually also outrage (though this is because record labels usually issue a takedown/take over monetitation of the original song in cases like this, making it a not very 1:1 comparison)
Based off of the tweet from microsoft themselves, they're going to try and clarify the policy further which is the right thing to do here.

https://twitter.com/gisardo/status/1544728548241448960

I fully support creators of OSS software charging fair pricing for their apps on the Microsoft store. They should and have every right to do so. I however do not support random people packaging OSS software and charging ridiculous pricing or shipping broken alternatives of popular application. That not only ruins the name of the original software but also makes the windows store a worse product for average consumers.

Yes, it might be easy for us to find alternative sources for applications but there is no reason why the Microsoft store shouldn't improve the quality of apps available on the platform.
Microsoft hasn‚Äôt followed that philosophy for over a decade. Linux users need to realize the Microsoft of today isn‚Äôt the Microsoft of the 90s and 00s IMO. WSL 1, which put linux syscalls into the Windows kernel, was the ultimate acknowledgement that the model of the past was over.
They're upset because github is charging money for copilot without following the licenses of any of the code it's trained on. Lots of github-hosted code has "no derivative works" or "non commercial only" licensing (usually due to creative commons by my understanding) and microsoft have just gone "ah well it's fair use" and ignored the problems. all of which is pretty valid imo
I did. That doesn't mean I agree with the linked post.
Registered or unregistered, that requires a lot of money and lawyers. Google Play will remove imposters, would be nice for MSStore to do the same
Also, I just realized the user you replied to is [rubenwardy](https://github.com/rubenwardy), maintainer of [minetest](https://github.com/minetest/minetest), which [does seem to be copylefted](https://github.com/minetest/minetest/blob/master/LICENSE.txt). Seems like the solution would be either trademark or a trademark-like system that only applies in the Microsoft Store, sort of like Minetest's own [Right to a name](https://content.minetest.net/policy_and_guidance/#31-right-to-a-name) concept.

I agree with what you said in your reply to my other comment, but I think it doesn't apply here.
I'm fine with people selling it, I'm not fine with them doing it under our name. The Google Play store has rules against imposters, even without a trademark

Minetest is copyleft, under the LGPLv2.1+ license
IMO a legal reason shouldn't be the only reason for removal. I don't think Microsoft should be legally obligated to take down forks of projects (assuming trademark law was followed but it sounds like it wasn't here), but Microsoft could choose to take down such projects if they do not think they are valuable (for example if they are not significantly different from the original), or if they want the profit to go to all of the developers.

In other words, people should have the freedom to redistribute free software, but Microsoft also has the freedom not to host it, and they may choose not to host particular software for good reasons (or bad ones).
You can also charge for source code if you‚Äôre doing source-only distribution.
>GPL and cost are not related. GPL only mentions the requirement of redistributing source.

Just in case your post is misinterpreted, while cost/price is explicitly mentioned in the GPL, explicitly giving you the freedom to charge nothing or whatever you want.

Section 4, paragraph 2 of GPLv3

> You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.
Why are those projects not using the donation option on the Microsoft store if that's the model they want?  That's what the Inkscape project does.

If they are not letting people know that it can be obtained through alternative sources without a required payment on the store page they are in fact misleading people even if that is not their intention.
Lol roger that. Mostly meant as a joke, I‚Äôm sure lots of people use it I just don‚Äôt think I‚Äôve ever used it once.
It's still clearly a move designed to harm FOSS projects. Banning a user for re distributing software that is freely available elsewhere for easy money would be fine, that's essentially a scam. But blanket banning all FOSS from making money inherently prevents FOSS projects from using app stores as a donation platform for people who want to voluntarily pay for their copy and support it's continued development. A not insignificant amount of money was raised for Krita this way, for example. None of the projects that had paid versions on Microsoft's store ever mislead anyone as to the license or free status elsewhere of their software. Free software can still cost money to make, devs need to make a living somehow. Since this policy is not targeted at removing accuonts impersonating free projects to cash in on them it is clearly a way for Microsoft to go "No, you may not make a living with free software on our platform, only proprietary software deserves the right to do that."
That‚Äôs the problem. There are still tiers of hosting, support, updates, and other operations that are value-add to FOSS.

While I agree with their move to ban people from charging for simply reposting FOSS apps, this language doesn‚Äôt leave room for the rest. 

>	all pricing ‚Ä¶ must ‚Ä¶ [n]ot attempt to profit from open-source or other software that is otherwise generally available for free [meaning, in price, not freedom].

So, no Red Hat. No Ubuntu. Those are supported by these value add mechanisms. 

It‚Äôs not disingenuous. That is actually the point of the article. In fact, the author states that they are waiting on a response as they raised these exact same questions.
> They're banning selling something that you can get for free

If the license permits it, there's nothing inherently wrong with selling something you can get for free.
> The fact that people still believe the "Microsoft loves Linux! Microsoft Loves open-source!" crap is mind boggling.

Is it though?

Because from where I sit, they have every reason to love software released under permissive licenses such as MIT or BSD... 

To them (and any other comercial software outfit tbh) it amounts to people volunteering to do what was previously highly technical and well payed work for absolutely free: They didn't pay for the development of any of it, yet because of the permissive license they're still free to use it to enable or add value to their proprietary software solutions.

This isn't even anything new: It's an established and well known fact Windows 2000 and XP freely incorporated parts of the BSD TCP/IP stack. Why would they pay millions to develop their own enterprise grade TCP/IP stack when they can just take what's already available and incorporate it into their own proprietary solutions?!

What MS and any other corporation has an issue with is GPL software... Precisely because they can't just just use it to add value to their proprietary solutions.
Microsoft loves open source but **not** free(dom) software
BASIC

Github Copilot

Basic Donkey Jump

DOOM

the list goes on and on...
Damn, so I guess the FOSS developer doesn't have a family with bills to pay themselves?
OTOH; it's cutting out competition? Is it not?
Open source software does not relinquish its intellectual property, though, and often has restrictions on how you can redistribute it. Github copilot has been found to spit out verbatim copies of GPL code on occasion. If the project that's using copilot is not using a compatible license, then the code has essentially been license-washed. And once you realize that copilot can output verbatim copies of copyrighted code you have to ask yourself where the line is. Should a non-verbatim copy but only with minor changes also be covered by the original license? Or is maybe any code it produces problematic since it's always on some level derived from GPL regardless of whether it's apparent to the naked eye.
Because Microsoft bad is the reason
.... And your explanation for the rest of the shenanigans?
The problem is that the people who are publishing on MS store are not the ones who own the project.

Fedora Remix is a good example.
Irrelevant to the claim in the headline and linked article. The ability to make profit or not is explicitly not the same thing as "ban all commercial activity".
It's not the fact that they have work for 8 developers that's impressive, it's that they've managed to secure funding for 8 developers that is.
https://liberapay.com/ makes it easy to track what you're donating to and donate at set intervals
Do understand the risks you take in letting a corporation manage your project on the legal side. SPI has certain rights over their patron projects that some may not approve of.

I suggest researching the relationship SPI has with its projects, and ask yourself if we need another centralized group holding power.
As I understand it, KDE does not directly fund development for projects under its umbrella like Krita.

That's not a dig at KDE or anything, just a clarification. KDE does a lot for us in terms of infrastructure (hosting our website, gitlab, bugzilla, build bot, the dev fund, etc.) and paying for development sprints (usually week long meetings where a bunch of international contributors get together). 

Though we haven't had a sprint in a few years because of all the terrible stuff that's been happening.
You aren't wrong with the "Should there be?" question. I was thinking basically the same thing when I was writing my question. I just know there are a lot of extremely useful and underfunded projects out there that I may never use directly or even be aware of, but would love to be able to contribute to.
I'm pretty sure the average Software Team is being held up by only 1 or 2 people. The 80/20 rule has gotten really perverted and now most Software Orgs seem to be 80% middle-management and talentless bootcampers and 20% actual skilled hackers.

I think part of this "shrinking tech sector" is because so many of the employees suck and can't code themselves out of a wet paper bag and someone is finally noticing "maybe these half-coders aren't worth 6-digit salaries".
I heard only 7 developers participated in the Windows 7 and Vista development.
Yeah, this is very true. That's an accurate description of the core Krita team. It does mean we have to wear a bunch of hats and jump around to make sure that all the various things that need to get done are done, but I think that's a really fun way to work.

I feel like we have a pretty good number of people for our current organization structure, but then again I'm not the person who has to do the admin stuff. :)
I literally turned down de-facto paid painting software like Clip Studio in favour of Krita. Paid alternatives severely lag behind Krita.

So yeah I was surprised it was even done by KDE devs at the price of zero, and you get the source code in its entirety if you wanna hack around. Ever since then it's been my daily driver for furry anime art, which fits Krita's Squirrel girl mascot :P
Fedora is a poor example, though. You don't get fedora on the store. You get fedora *Remix for WSL* on the store. The fedora project does not provide a build for WSL, Whitewater Foundry does. And they do it as intended by fedora/Red Hat in the context of the fedora remix program: https://fedoraproject.org/wiki/Remix

There's absolutely nothing wrong with that. You can get it for free from their github. Buying it in the store is not you paying for fedora, it's you saying "Hey, thanks for the effort of making fedora available in WSL!".

Edit: I meant to reply to the previous comment, but, meh ...
It seems to me that the proper solution would be proper Trademark policies for FLOSS. The "official" team behind the project could easily prevent other companies from selling a repackaged version of the project without rebranding it and making it a distinct product.
Banning it carte-blanche is stupid, however;

The OG Krita devs package their app for a cost on the MS store (I've bought it on steam) and they will get banned from doing this for their own project (under the existing term).
Why though? If a project has a permissive license that allows people to re-sell the software as-is, and someone does that, I don't see how that's wrong.

The project should instead have a license to prevent that if it's unwanted, right?
I think most old-school Windows users ignore the store. I remember hating the idea when it came out and, to this day, I have refused to use it. I know what software I use and I know where to find it. I'm not going to use the Store version. But, I think that most younger people that grew up with software stores and such may use the Windows Store more than you think.
I tried using the Microsoft store after using linux since it felt the most like using the AUR through pacman, but it's such shit lol.
Is it legal? Yes.
Is it wrong? Well, wouldn‚Äôt you be shitting on MS if they are the one selling Fedora Remix?
https://twitter.com/gisardo/status/1544728548241448960?s=20&t=O9ehprWDQZsN_SrMT2ZfyA
[deleted]
> Currently the Microsoft store is a cesspool of trash tierd apps, fake apps and paid releases of free apps mixed alongside actual apps

And yet Microsoft chose to single out FOSS. Paid "guides" are still fine, so are proprietary shovelware apps and Electron web views with ad banners.


One really outrageous example is "Ultimate Guide of League of Legends" https://apps.microsoft.com/store/detail/ultimate-guide-of-league-of-legends/9WZDNCRDQRN5 -- currently reduced from the suspicious price of $64.99 (close to how full-priced games are usually priced) "down" to $9.99. Oh, the "sale" only lasts for another 235 days. 4.5 out of 5 stars. Clearly not rigged at all.
"Revenue sharing" is just another name for *royalties,* which would make it *by definition* not free or open source.
My very first Linux distribution was Redhat 6.0 distributed in cd and bought at the local computer hardware store.
Didn't they move to a vm for wsl 2?
Not buying it.  Maybe if they heart Linux for as many years as they tried to destroy it, I might change my mind. That's a decade or two away.

Them using opensource kernel code in their proprietary OS is hardly any step towards it.
>	put linux syscalls into the Windows kernel,

Embrace.

Extend with NTFS and device support. 

Extinguish the need to run an *actual* Linux kernel.

Don‚Äôt be fooled. WSL1 was meant to be a master stroke cutting the heart (kernel) out of Linux development. WSL1 hasn‚Äôt worked so far because MS kernel devs can‚Äôt yet keep up with the Linux team (features & performance).  

WSL2 embraces Linux by running a $MSFT branch of the Linux kernel under HyperV; extends the kernel to use MS permissions, device drivers, etc; with the hope that they can extinguish the need to have Linux kernels boot servers. Microsoft would much prefer you to pay to use HyperV.
Lol. Oh ok. Except for this basically perfect example

Edit: For those who can't quite grasp it. Putting Linux syscalls into Windows means you don't need Linux to run linux applications....

Yeah.. they LOVE linux. Well some of it's userspace. The parts that people want.
https://mobile.twitter.com/Krita_Painting/status/1545241688599367680
> Lots of github-hosted code has "no derivative works" or "non commercial only" licensing

So like some of the software that is being resold for money in the MS store right now that these software freedom fighters are now angry that MS is stopping...
The post literally quoted one of the new policies, concretely the second point of policy 10.8.7. That removes all the conjectures on your previous comments.

>That doesn't mean I agree with the linked post.

If you read it, then it is clear you didn't understand it.

**Edit:** For everyone defending misinformation:

A Krita developer has chimed in: https://www.reddit.com/r/linux/comments/vtxr9r/comment/ifb7hgk/?utm_source=share&utm_medium=web2x&context=3

So, indeed, this policy will greatly affect their development model.
Can't you file a DMCA takedown based on your copyright of the logo and of the screenshots?
You can sell copyleft software as well, in terms of the most famous copyleft license [L]GPL, the only requirement is that you also provide the source code to anyone you sell the software to. Ofc who knows if the sellers are making any gesture to actually fulfil that.

I've just read through all of minetest's licenses and none of them forbid using / redistributing the software or assets commercially that I can see.

If you have trademark over names or logos then they need permission to use those, the same as why Redhat Linux clones can't have anything to do with the Redhat name.
Indeed, just wanted to point out the point of releasing something under a free software license is all about users' freedom, not developers' control.  Microsoft has every right not to host something, but it'd be misguided to think monopolistic monetization power as a reason is justified in free software development context.
Thank you for adding this info. I can definitely see how my original comment can be interpreted in a way I didn't intend.
> If they are not letting people know that it can be obtained through alternative sources without a required payment on the store page they are in fact misleading people even if that is not their intention.

Apparently Microsoft did not allow that:

https://krita.org/en/item/krita-in-the-windows-store-an-update/
> If they are not letting people know that it can be obtained through alternative sources without a required payment on the store page they are in fact misleading people even if that is not their intention.

Pretty much all software can be obtained through alternate sources though, it's just not usually legal. If you sell commercial software, is it misleading to also give it away for free sometimes? What if you have a promo code that lets you get 100% off, or what if you can email the developers and they'll give you a gift code? I don't think charging for money sometimes makes it wrong to sometimes give things away for free elsewhere, even if you don't tell the people you charge money to that you gave the same thing to others for free.
> Banning a user for re distributing software that is freely available elsewhere for easy money would be fine

That's exactly what this is doing though, it is not in anyway a blanket ban on FOSS from making money.

From the article:

>all pricing ‚Ä¶ must ‚Ä¶ [n]ot attempt to profit from open-source or other software that is otherwise generally available for free [meaning, in price, not freedom].
> There are still tiers of hosting, support, updates, and other operations that are value-add to FOSS.  
> ...  
> So, no Red Hat. No Ubuntu. Those are supported by the same mechanism.

Their policy doesn't have any impact on that. Their policy is only in regards to the price on the store, and also only on the software itself, not services. If you want to provide after-market or other value addons, that's not prohibited.
they love to steal, always have do.
Some FOSS developers probably do have family and all of them have bills to pay. What's the question here?
[deleted]
Absofuckin-lutely.
It aint a good example cus its under the fedora guidelines and its 100% legitimate but there are repackaged foss software that costs money on the ms store
Not irrelevant at all, but my commemt was directed at your previous comment. What is your position on profit? Do you find the ban of profittable commercial activity acceptable within this context?
They sell their product.
thanks for the consideration

Still, Debian supports SPI
It doesn't help that these boot camps exist to begin with, and academia isn't setup in a way that produces competent and adaptable programmers.

CS and SE should be integrated. You can't kick ass at one without the other.
Source?
Awesome! 

I think Krita has its strengths and weaknesses like any tool. CSP is a strong rival and they do some things really well, and other things I think Krita does better. Overall we're in a great place with digital art software these days because there are multiple solid programs competing against each other and pushing us all to make improvements.

One of the things that sets Krita apart is the community-driven FOSS angle though. While it may not be the best "business model" it's by far the best development model, and definitely our greatest strength. Some day I hope the various art communities out there will see Krita as a "public" asset that they can not only use to make whatever art they want, but also help shape and direct. I think that message can be hard to convey to people outside of the Linux/FOSS world--a pretty big chunk of our users.
Didn‚Äôt realize that Fedora actually allows using ‚ÄòFedora Remix‚Äô. 

But the point still stands - there are FOSS applications repackaged by someone completely unrelated to project which can be very misleading.
Well, I've been using Windows since 95 and adopted the store after the Windows 11 update. It's kinda of a neat place to download some well known applications and just keep them there to automatically update. I don't see a reason to ignore it when it just works. Obviously that doesn't apply to smaller projects and stuff that isn't readily available or officially supported on the store.
Does it *feel* wrong? Kinda.

But the only somewhat objective measure here is the license, and it‚Äôs within compliance. Selling FOSS software is explicitly permissible.
FOSS does not grant trademark rights. Any leech repackaging FOSS and presenting it as coming from official channels is likely violating trademarks.

Actually enforcing these in court is another story however. Trademarks are particularity tricky to nail, but there's at least a theoretical avenue for recourse.
No I wouldn't because they would be undermining their own operating system and replacing it with one I can tolerate more.
So they're making a damaging blanket policy instead of sanely policing their store. Google brain move right there.
The article that you posted:
https://twitter.com/gisardo/status/1544741955145502724

and

https://twitter.com/gisardo/status/1544728548241448960
They don't specifically single out FOSS. They mention it but alongside other apps. This is their new policy:

In cases where you determine the pricing for your product or in-app purchases, all pricing, including sales or discounting, for your digital products or services must:

- Comply with all applicable laws, regulations and regulatory guidelines, including without limitation, the Federal Trade Commission Guides Against Deceptive Pricing.

- Not attempt to profit from open-source or other software that is otherwise generally available for free, nor be priced irrationally high relative to the features and functionality provided by your product.
Here are some software/copyright freedoms to consider:
* review the source code (open source)
* compile source code & distribute binaries
* distribute modified source code
* distribute modified binaries with source code
* distribute modified binaries without source code (closed source)
* charge money to exercise any of these freedoms

The GPL enshrines one set of rights and responsibilities; alternate sets are available in other *open source* licenses (e.g. Apache License, MIT, BSD 3-clause, etc).

An open source license that included revenue sharing would help the FOSS community as even FOSS developers need money to eat, live, and code.
Yes, the native compatibility layer was too slow.
You think Microsoft is going to extinguish linux by adding support for it to Windows? You must be smoking the good stuff.
Because we all know now that WSL is implemented Linux is dying! Microsoft strikes again!
Yeah MS stopping people from reselling other people's work is good. MS stopping people from selling _their own projects_ because they're free elsewhere is bad. This policy, currently, is also doing the second thing
you know people are allowed to have not only dissenting opinions to yours, but also interpret things differently yes?
His conjecture was about the reason for pushing a new policy, how could that be removed by a quote of the policy?
it also conveniently chopped up the comment to make it look like that

> ‚Ä¢ Not attempt to profit from open-source or other software that is otherwise generally available for free, nor be priced irrationally high relative to the features and functionality provided by your product.
That'd mean they'd have to make the logo unfree. That's annoying because you'd have to remove/substitute it when packaging which is likely not what the author would like to happen.
> I've just read through all of minetest's licenses and none of them forbid using / redistributing the software or assets commercially that I can see.

Yes, if it did then it wouldn't be free software/culture.
It's not a problem with commercial use, I mentioned as much in my opening comment. It's a problem with misleading apps.

I'm glad MS is finally taking action on this. It sounds like they'll be rewording to allow Krita's etc legitimate use as well
You misunderstand. Microsoft are saying "you must package for our store for free if you package it for free on other platforms". It's exploitative.
Notice that "or" there. It is a blanket ban on all open-source, including software that doesn't even have a single official build for Windows for free. Also, they can easily argue it also means software that uses/relies on open-source components but are not open source. Like fucking Windows itself. Or Minecraft they themselves sell on the store, both rely on a lot of open source projects. I wonder if they ban Minecraft.  
And here is where the vague nature of policies comes in, they can do whatever they want as usual and just claim "yeah, we interpret it like this now"
Are you Microsoft?
Yeah I mean, that's the literal topic of this thread :P I just wouldn't have predicted they'd be so successful at that (especially given that it's also available for free) that they can fund 8 developers.
Agreed.

There is no easy to define route into programming via academia. There really needs to be degrees that focus purely on programing in the large + the required CS knowledge. 

The bootcamps are an attempt to fill the void.
Sure. I do not deny your point; I'm just pointing out, that fedora was a poor example.
You can fork whatever you want with open source and distribute it as you please as long as you make the source code available, depending on the license used.
That is a fair point. I just never really looked into it. When it was introduced I kicked back, because I don't like change. I also thought the store apps were different somehow and just never saw the point. Now I just don't feel the need to bother since the way I'm used to still works.
And MS has every rights to control what gets sold on MS store and while it‚Äôs subjective, I think it‚Äôs fairly reasonable for MS to shut these down.
Fedora has explicit trademark rules that allow the use of the Fedora name trademark if used with "Remix" to differentiate between official release by upstream and  remixed releases by 3rd parties https://fedoraproject.org/wiki/Remix
Did we read the same thing?  The tweet implied to me that the intent was not for it to be a blanket policy.
> Not attempt to profit from open-source or other software that is otherwise generally available for free, nor be priced irrationally high relative to the features and functionality provided by your product.

Depending how you read the sentence, "priced irrationally high" may just refer to repackaged free apps, not those guides. It's also not saying anything about free Electron web views with ads.

MS could have just tweaked their ranking. Prioritize submissions by the upsteams over repackaged submissions by 3rd parties, not just outright ban them.
From the Open Source Definition:

> The license shall not restrict any party from selling or giving away the software as a component of an aggregate software distribution containing programs from several different sources. **The license shall not require a royalty or other fee for such sale.**

The Free Software Definition is not explicit about it on account of it being so terse, but that's also an intended implication of it.

Any license that encodes an obligation for the licensee to pay tribute for their use of the software after it has been licensed to them is not Free or Open Source by definition.

> as even FOSS developers need money to eat, live, and code.

Of course. Which is why so many get paid to *develop* and *maintain* FOSS.

What you suggest is imposing an *obligation on licensees* to pay developers for *having developed* the software that they are *already in possession of and licensed to use.*

In other words: a kind of proprietary software license.

By all means come up with a name for this new category of license that does what you describe, but don't call it a FOSS license, because it's just not.

There are already terms to describe other kinds of *fauxpen source* licenses, like "source available."
Yes. Because you now don't need to remove windows to do Linux stuff.
It's pretty transparently a move to keep people using Windows instead of switching to Linux? Like this is textbook Embrace
That's silly logic.

WSL is lubricant.

If you NEED Linux to work, you used to either have to dual boot or run a VM, which let's be honest, is kinda janky even when the integration tools worked. 

WSL Just Works‚Ñ¢Ô∏è and you don't have to remove windows. So now you can work on all your Linux stuff natively from Windows.

So yes, it's to deliberately eat at Linux's desktop penetrative abilities. 

It's pretty obvious actually
Just because they consistently fail doesn't mean they're not trying.
A Krita developer has chimed in: https://www.reddit.com/r/linux/comments/vtxr9r/comment/ifb7hgk/?utm_source=share&utm_medium=web2x&context=3

So, indeed, this policy will greatly affect their development model. So much for the validity of certain opinions.
You can't have different interpretations of the same policies. And sure, there is always the possibility of dissent and to have other interpretations, but that doesn't mean all are equally valid or that we can't choose the best from them.
Indeed, the policy does that. His conjecture stated:

>Am I the only person who thinks this is to avoid people repackaging FOSS software and selling it on the store without compensating the actual developer?

The quoted policy states:

>      all pricing ‚Ä¶ must ‚Ä¶ [n]ot attempt to profit from open-source or other software that is otherwise generally available for free [meaning, in price, not freedom]. 

So, if you contrast his conjecture with the quote, the new policy does not prohibit duplication of apps, nor does it allow the actual developer to get any form of compensation as it states that no attempt to profit from open-source software will be permitted, and that includes the actual developers of open source projects like Krita (just as the post mentioned).

So yeah, in this case it is a conjecture formed by misunderstanding the text in the post.
This doesn't change the fact the open source software will not be profitable is this policy stays as is.
Nobody is making you package for Microsoft, so how is it exploiting?
They could simply just choose not to publish it on the store and let Microsoft Store remain the garbage it always was. If you power them, you have to play by their rules.
No, I understood that.  I just don't think it's a problem.
> I also thought the store apps were different somehow and just never saw the point.

Since Win11 there are two types of apps on the store. "Real" store apps with seamless background updates and all that nice stuff and "fake" store apps whose install button merely links to the same old setup.exe as forever. The install button has slightly different wording, IIRC it's "Get" for one and "Install" for the other type.
Well, if MS is singling out FOSS, it's discrimination. Simple is that. FOSS licenses allow selling and as long as there is no bundled malware nor unlicensed trademark use, I see no argument why the Krita developers can't be allowed to sell their app on stores. It's their app after all. They should be able to set whatever price they want.
>Fedora has explicit trademark rules that allow the use of the Fedora name trademark

i mean it can have all the rules it likes , they still have to be enforced in a court
I'm not commenting about only this one tweet. They did make a blanket policy, and when developers reacted they pedaled back. Now they're saying they'll rework the wording to make it less ambiguous. Just lawyers being idiots about stuff they don't understand, as usual.
[Cygwin](https://cygwin.com) has been around since the early 2000's and does everything wsl does but better. Its a Foss alternative to wsl.
They've chimed in that it'll greatly affect their development model *if the interpretation given in that tweet is true*. There's evidence that that interpretation isn't true.
That‚Äôs why attorneys exist. People arguing over the correct interpretation of a law or policy, which the majority opinion is usually the accepted opinion.
you tried to tell someone their opinion was wrong.

Its their opinion. just because it's not yours doesn't make it wrong.

And yes, you can have different interpretations of policy. ü§¶‚Äç‚ôÇÔ∏è

>You can't have different interpretations of the same policies.

Like there is only one christian denomination ;-)
If you already packaged it and published your tool chain under the expectation you could get paid you would feel pretty exploited by this change.
They tweeted out to say the intent is to remove misleading applications 

I‚Äôm pretty sure legit ones like Krita will stay
Why would Fedora take someone to court over that person doing what Fedora's licensing allows?
To be sure I understand, are you saying they intended to make a blanket policy and then changed their mind after the backlash, or are you saying they never intended to make a blanket policy but they mistakenly did so?
Yes and it had some severe limitations,  mostly in ease of use. But also compatibility.  With the latest additions to WSL its possible to run  unmodified Linux binaries for machine learning directly on Windows  

Don't get me wrong I've used Cygwin in the past and it's cool. But it's not on the same level as WSL and it doesn't do everything WSL does.  Not by a long shot.
Sure, and on the end only one is valid. Attorneys exist precisley because of the problems that arise from this ambiguity, and to supress it so that there is only one valid interpretation.
So there are no wrong answers... as long as they are opinions

>And yes, you can have different interpretations of policy. 

Name one example where two different  interpretations of the same policy are accepted by its policy maker.
There is no intention of rules, only the rules.

-Adrian Newey
I don't know what they intended, I don't sit on their meetings. The original policy *was* a blanket policy, regardless. The fact they had to come forward and admit they will have to reword it with outside feedback is evidence they didn't think it through and most likely didn't consult with anyone outside of Microsoft. Hence why I call it Google brain, making broad overarching decisions without regard for the final user or other developers. They're so humongous and monolithic that they can't see the effect of their own actions on anyone who is not them.
So, do you have those court results that proof that you're right then? Can I see it?
Homie it's called an opinion. You can't logic an opinion into submission
I guess this will ensure that MS Paint keeps its market share üëç
[deleted]
Do you mean like every closed case in history?
Actually, [you can](https://en.wikipedia.org/wiki/Logic).
Adrian newey is one of the biggest rulebenders in history...
Might be a good idea to look up who the person in question actually is first before calling someone an idiot.

Because then you'd get the context of the quote (F1 racing, where finding loopholes in regulations is paramount to success) and know that the guy is the most renowned race car engineer in recent history.
Well that's your opinion :) but I disagree
Well sure, if you think logic is an 'opinion'.
And you're entitled to your opinion just as much as I am :)
You can also use streamlink-twitch-gui. It will notify you every time a streamer was live.
The licensing terms in question seem to me that they are to prevent another [Rambus vs JEDEC](https://www.computerworld.com/article/2570162/ftc-opens-case-against-rambus.html) style ambush, where Rambus was a member of JEDEC, then started suing SDRAM manufacturers for patent infringement. If this is the case, this is something the EU has got wrong.
> Commission has information that AOM and its members may be imposing licensing terms (mandatory royalty-free cross licensing) on innovators that were not a part of AOM at the time of the creation of the AV1 technical, but whose patents are deemed essential to (its) technical specifications

Why is that an issue?  Or anti-competitive?  If you have one patent of a hundred patens which cover a technology and want to use that technology it‚Äôs rather natural that you‚Äôd be expected to offer everyone your patents in exchange.

While I usually applaud EU‚Äôs regulations in tech sector, this one doesn‚Äôt make sense to me.
**This sounds like racketeering to me**

1. Get all the major \[commerial\] players on-board \[to join AOM\].
2. When a non-AOM player tries to sell their product to an AOM-member, the AOM instructs them to refuse.
3. The non-AOM player is forced out of the market (can't survive) unless they join the AOM and donate their tech to AOM.

That's totally a racket (just like old-school mob-protection schemes).

I guess this means that AOM is the new mob.
Anticompetitive? I would guess that Fraunhofer complained to EU regulators. They made hundreds of millions on MP3, H.264, etc. If everyone switch to royalty-free formats, they will not be able to create a new, super expensive, patent encumbered one.
> Why is that an issue? Or anti-competitive?  
  
AFAIK, the problem with this construction isn't so much the legal construction in itself, but that the group behind it is like a de facto monopoly: google+Facebook/Meta+Amazon+Apple+Microsoft.  
  
I can be seen as an anticompetitive move when such huge players in the market form a syndicate like AOM. The AOM syndicate is effectively forcing smaller players to give up their IP for free to AOM members.  
  
If AV1 takes off and becomes a standard for all the AOM members, then it becomes an unavoidable and totally dominating standard. And that will kill off all competition. 
  
Also, there will be no way for companies to recoup development cost for advances done in media codecs, since they will have to give up their IP for free to AOM, or stay outside from what constitutes the vast majority of the techworld. 

Personally, I happen to think that the trade off is worth it, and support the AV1 standard(s), but I am not blind that while AV1 is a benefit to consumers and tech giants, it also kills off competition.
if i'm reading right the quote, AOM could force people to offer their technology royalty free even if they didn't participate in the creation of AV1. Let's imagine the situation:


You discover a slightly better way to compress a picture. You patent it. So far so good. Now, you decide to become a members of AOM. Now AOM want you to offer your patent on your picture compression royalty free even though you didn't create it for AV1.
It's also odd....    If you were proposed that licensing deal, but wanted to make as many profits as possible, you would sell the relevant patent to a troll, *then* go use AV1.

The troll can then go around suing anyone they please, and you get to both use AV1 *and* keep a pile of cash from the troll.
This likely bear more truth than most other claims.
> google+Facebook/Meta+Amazon+Apple+Microsoft.

Any non-trivial trade association in tech is likely to include those companies.  Should USB Implementers Forum be investigated as well?  Or is USB IF good because Amazon is not there?  What about Linux Foundation?  Is that fine because Apple is not a member?

> If AV1 takes off and becomes a standard for all the AOM members, then it becomes an unavoidable and totally dominating standard.

Just like USB.  Which EU mandates now.  And how is it anti-competitive to offer a format for free?  Again, should Linux Foundation be investigated for anti-competitive behaviour because they oversee a project which has became a dominating standard?

> Also, there will be no way for companies to recoup development cost for advances done in media codecs, since they will have to give up their IP for free to AOM, or stay outside from what constitutes the vast majority of the techworld.

That is just how software patents operate.  If EU wants to investigate and curtail scope of software patents I‚Äôm all for it but if they don‚Äôt they need to accept consequences.  Current model is to join pools and share patents or to sue and risk being sued.  I see nothing unusual or anti-competitive in the way AOM operates here.

It‚Äôs also not true that they don‚Äôt have a way to recoup costs.  They can choose to sue or, even better, provide some service which uses AV1.
People fetishize competition too much tbh - especially Americans
"alleged" anti-competitive.

It's great news actually. One of the reason AV1 haven't take off is that this model haven't been tested in court. EU clearing up the legal aspect is a good thing regardless of whether AV1 will succeed or not. We have been stuck with hevc/h264 for too long.

Every standard is anti-competitive. AV1/hevc/h264 are all the same. When a manufacturer pays fee to mpeg group, smaller players get nothing even if their IPs are used in h264/hevc. And we know that these small players have been suing manufacturers or creating their own pools to also collect fee, which have made codec become prohibitively expansive to use.

The whole business model is doomed. AV1 is a desperate temporary move to help mitigate the slow down of future standard adoption. But AV1 will not succeed unless the legal status becomes clear and Apple decides to support it.
> Now, you decide to become a members of AOM. Now AOM want you to offer your patent on your picture compression royalty free even though you didn't create it for AV1.

And therefore, decide you in-fact don't want to join AOM. Joining AOM is a purely voluntary affair, no-one is forcing it upon you. Doneski.
> Now AOM want you to offer your patent on your picture compression royalty free even though you didn't create it for AV1.

I see no problem with that.  If you want to participate in an organisation which maintains formats that infringe on your patents, it‚Äôs expected that you would need to offer your patents.
>Any non-trivial trade association in tech is likely to include those companies. Should USB Implementers Forum be investigated as well?   
  
I really don't think that being member of the USB IF means the member have to give up their their IP rights for free, which is one of the core issues of the EU probe. 
  
The problem is that apparently the AV1 standard included patents that the AOM founders didn't own, but allegedly the AOM syndicate used their market dominance to force those patent owners to give up their IP. 
  
Whether the above is a) true, b) an anti-trust problem even if true, is another matter, but that is the reason behind the complaint that led to the probe.

> Current model is to join pools and share patents or to sue and risk being sued.    
  
Just to iterate: that model in itself isn't a problem, but it becomes a problem if the companies behind it, makes an industry standard that contains IP rights, that none of the standard creators actually own, but since they are so dominant in the industry, can force smaller players to give up. It is the specific AOM licensing terms, combined with the market dominance that is the potential problem, not the creation of a market standard based around patent pooling.   
  
>It‚Äôs also not true that they don‚Äôt have a way to recoup costs. They can choose to sue or, even better, provide some service which uses AV1.  
  
I wasn't being clear apparently: but I was talking of *future* codec development and research, post a AV1/AOM market dominance. There just won't be any commercial market left to recoup investments from.
> People fetishize competition too much tbh - especially Americans  
 
Well, it is a proven fact that competition leads to better prices and better quality products. 
Companies Microsoft could easily lock out Linux from being used on pc's, the only thing holding them back is the fear of anti-trust regulation.  
  
And Microsoft without any competitors would be a nasty scenario. So yeah, competition is good.
I think the key concern is that if AOM controls access to AV1, then it could effectively threaten any third party with denial or reduction of service until they joined. The question is essentially if AOM ***can*** in fact exercise such control. I think the answer is no, but that assumes AV1 continues to be open and free.
> allegedly the AOM syndicate used their market dominance to force those patent owners to give up their IP.

How did they do that?  AOM couldn‚Äôt force them to join.  Neither could it force them to use AV1.  The companies are free to sue for patent infringement if they want.

This still sounds nonsensical to me.

If AOM creates a standard that everyone starts using and that standards infringes on patents someone who is not a member of AOM that should be good news for that company.  They can now sue YouTube.  On the other hand, if the company chooses to participate in AOM, they need to offer their patents to the pool.

> There just won't be any commercial market left to recoup investments from.

That again makes no sense to me.  The company is free to sue Google, Amazon, Meta etc.
> the only thing holding them back is the fear of anti-trust regulation. 

Laugh in knowing Microsoft history.

Hell, fucking Intel got more screwed by anti-trust than Microsoft did, when Microsoft did it bigger and later.
> it is a proven fact that competition leads to better prices and better quality products

It‚Äôs also a proven fact that interoperability (which standards facilitate) leads to more competition.
> That again makes no sense to me. The company is free to sue Google, Amazon, Meta etc.  
  
I am talking about new research. A company researching a new way to compress video signals would either have to donate that research to AOM for free, or being totally unable to sell it to anybody, because without support from MS/Apple/Nvidia/google and other major players (that are all AOM members), there is no commercial market left.  
There would be nothing to sue AOM members about, exactly because they would just refuse to use the new technology. So no market, and therefore no income, therefore no reason to invest in this area.  
  
Seriously, do you really think there is a market for a new x267 standard after all major players have shifted to AV1? I don't. 
  
AOM aims to become a de facto monopoly, so that big players like Apple/google/MS etc. won't have to pay smaller, more innovative companies royalties. There are also public benefits to this, and I am very much a supporter of AV1, but the drawback is that this potential monopoly, could eliminate any competition and therefore reduce innovation.
To be fair; Microsoft absolutely ***reeled*** from the threat of government anti-trust lawsuits. Their stock took a massive tumble, they stopped their massive march at vertical and horizontal integration in the OS, and it effectively began the fracturing of the company into the typical umbrella corporation.
> I am talking about new research. A company researching a new way to compress video signals would either have to donate that research to AOM for free, or being totally unable to sell it to anybody, because without support from MS/Apple/Nvidia/google and other major players (that are all AOM members), there is no commercial market left.

Should EU investigate W3C then?  W3C also requires participants to offer their patents for free.  And it includes all the big players you‚Äôve listed.  By your logic, there is no commercial market left for creating new hypertext markup language.  Or for creating a new transport protocol.

> Seriously, do you really think there is a market for a new x267 standard after all major players have shifted to AV1? I don't.

Just like there is market for AV1 now there will be market for new compression formats in the future.  If it was just a matter of paying for patents, everyone could just continue using MPEG-4 which is over 20 years old and thus not covered by any patents.  That of course doesn‚Äôt happen because there is a market for new formats just like there is market for improvements to HTML or HTTP.

> the drawback is that this potential monopoly, could eliminate any competition and therefore reduce innovation.

Just like there is no innovation in other technologies? HTTP? HTML? JavaScript?  This argument doesn‚Äôt hold any water.
> Should EU investigate W3C then? W3C also requires participants to offer their patents for free. 
  
As I said before. It isn't the creation of a market standard in it selves that caused the EU probe, but specific licensing requirements that may have been distorting the free market.  
  
>Just like there is market for AV1 now there will be market for new compression formats in the future.  
  
If Google refuse to accept uploads in that format that competes with their own AV1, and NVIDIA won't implement hardware support, and MS and Apple won't support it in either their software or hardware etc., then there de facto won't be any market for any other video codec other than AOM's own.  
  
I won't say there won't by any new development in codecs, because clearly many AOM members have commercial interest in keeping streaming sizes low, but all the development will be among AOM members, there simply won't be a commercial market left outside AOM, so no motivation to innovate either. 
  
Standards are good, but monopolies are bad for consumers and society.
> As I said before. It isn't the creation of a market standard in it
> selves that caused the EU probe, but specific licensing requirements
> that may have been distorting the free market.

And as I said before, this is how a patent pools work.  If you join
a patent pool because you need license for technology X you often need
to provide all your patents giving participants license for unrelated
technology Y.

> If Google refuse to accept uploads in that format that competes with
> their own AV1, and NVIDIA won't implement hardware support, and MS
> and Apple won't support it in either their software or hardware
> etc., then there de facto won't be any market for any other video
> codec other than AOM's own.

How is that different from HTML?  Should EU investigate W3C which
‚Äòdestroys‚Äô commercial market for hypertext markup languages?

> Standards are good, but monopolies are bad for consumers and
> society.

Any claims that AOM is a monopoly which is bad for consumers translate
directly to claims that W3C is a monopoly which is bad for consumers.
> And as I said before, this is how a patent pools work. If you join a patent pool because you need license for technology X you often need to provide all your patents giving participants license for unrelated technology Y.
  
Again, the patent pooling isn't the reason why the EU is probing, but because; a) the licensing terms are unusual. b) the AV1 standard included patents that the AOM didn't own. c) allegedly the owners of the said patents were de facto forced to give up those patents.  
 
> Should EU investigate W3C which ‚Äòdestroys‚Äô commercial market for hypertext markup languages?  
  
So what is exactly the alternative to W3C? Oh, yes, there aren't any. W3C is a de facto monopoly. 
Now, there are several of such de facto monopolies around, and their existence aren't necessarily a problem for the free market. However, such de facto monopolies should be held to other standards than normal non-monopoly standards. So if the W3C suddenly want microtransaction payments for using their validator, or for anybody using their standards, the EU would have all the rights to intervene.  
  
>Any claims that AOM is a monopoly which is bad for consumers translate directly to claims that W3C is a monopoly which is bad for consumers.  
  
But yes, there is a flipside to industry standards like the W3C, because it is *also* is a monopoly. You are not going to see any competing web standards, that eg. would make it easier to third parties to make browsers, because those chrome using companies that dominate W3C have something like 90% of the browser market, and can basically dictate how web technology is used.  
  
Yes, there are advantages and disadvantages with monopoly-like standards like W3C.
I think we have to agree to disagree because we‚Äôre going in circles.

You say it‚Äôs not like patent pool but then, as far as I can tell,
describe how patent pools work (when one joins, one contributes paste,
present and possible future patents into the pool).  I see nothing
unusual in the licensing model and as u/foamingdogfever has pointed
out, it‚Äôs likely there to prevent [trolls](https://www.computerworld.com/article/2570162/ftc-opens-case-against-rambus.html).

You say AV1 infringed existing patents, and to me that just sounds
like a situation which is already covered by patent law and thus does
not need any further investigation by EU.

You claim AOM forces companies to join and all I see is a voluntary
group which one can join or choose to sue members for patent
infringement.  If multiple *competitors* agree on a standard,
a third-party is not entitled to get their format recognised.

Lastly, there are hundreds of examples where royalty-free standards
which, according to you, are ‚Äòde facto monopolies‚Äô help competition by
improving interoperability.  As far as I‚Äôm concerned, the onus is on
the person claiming that royalty-free standard is some kind of threat
to prove that and not on alliance developing such standard to defend
it.

It‚Äôs inconceivable to me that the same entity which mandates USB
charging is now worried about a royalty-free standard which will
enable anyone to develop software and hardware working with
high-quality video encoding.
**Bender**'s back end is very shiny and metallic.
I'm 40% GPU accelerated
Byte my shiny metal texture!
What's this about an Eevee re-write?
A bit unrelated but all this new ground-breaking stuff and extra stability from Blender, KDE, Wayland, KdenLive, and etc. is going to make for such an awesome Debian 12 release.

(Unfortunately I'd have to wait a little longer than the normal Debian users would have to to get it since I'm using MX Linux. :( )
Must. Bend. Girder.
Bender Bending Rodriguez?
"yeah, well... I'm going to build my own rendering engine, with blackjack and hookers"
One typo results in a comment section with no interesting discussion. Shame.
Does it have black jack and hookers?
Typo
Hey sexy Mama, wanna kill all humans? 

<disclaimer> this is a direct quotation of Bender. I can not and will not associate with anything implying I want to kill as much as a bee, let alone a human. </disclaimer>
Dude, you came 980 years too early to the party, froze again.
It didn't before?!
Shut up baby I know it
Blender's Eevee rewrite has been happening for a while under a branch led by Clement, iirc. It's almost usable, afaik. I don't think it's planned to be included in master before 3.3 (maybe 3.4?). Its purpose is to bring it closer to Cycles.
It's about futurama
yes, Bending Unit 22
As opposed to?? This is Reddit...
This! Is! The! Question!

Edit: If not, I create my own rendering software‚Ä¶
Leave it, informative and humorous is a great combo.
KISS MY SHINY METAL ASS

\- Bender
space honey really puts things in perspective
Reddit's comment quality can be as good or bad as its users make it to be.
Thanks for the reminder. I was beginning to think that I was on a website to learn new and useful information about the Linux ecosystem.

/s
You HAVE to leave it, because Reddit is shit.
Also fair.
This is a quick bugfix release that is API and ABI compatible with previous 0.3.x releases.

**Highlights**

* Some critical bugs in the new audioconvert were fixed. The old adapter had internal buffering that was abused in some places.
* The bluetooth sources were rewritten using a ringbuffer to make them more reliable to jitter and remove old audioconvert behaviour. Many improvements to the audio converter.
* Native DSD128 and up is now supported by pw-dsdplay.

**tools**

* Support DSD128 to DSD512 as well by scaling the amount of samples to read per time slice.

**SPA**

* Format conversion is now generated with macros to remove duplication of code.
* 24bits conversions were rewritten to use the generic conversion functions.
* Temporary buffers in audioconvert are now made large enough in all cases.
* Fix draining in audioconvert. This fixes speaker-test.
* Fix the channel remapping. ([\#2502](https://gitlab.freedesktop.org/pipewire/pipewire/-/issues/2502), [\#2490](https://gitlab.freedesktop.org/pipewire/pipewire/-/issues/2490))
* Audio conversion constants were tweaked to handle the maximum ranges and provide lossless conversion between 24bits and floats.
* Vector code and C code are aligned and the unit tests are activated again. A new lossless conversion test was added.
* Fix an underrun case where the adapter would not ask for more data.
* Fix PROP\_INFO for audioconvert. ([\#2488](https://gitlab.freedesktop.org/pipewire/pipewire/-/issues/2488))
* Use the blackman window again for the resampler, the cosh window has some bugs that can cause distortion in some cases. ([\#2483](https://gitlab.freedesktop.org/pipewire/pipewire/-/issues/2483))
* Add more unit tests for audioconvert. Add end-to-end conversion tests.
* Don't leak memory in format converter.

**pulse-server**

* Card properties are now also added to sinks and sources, just like in pulseaudio.
* Increase the maxlength size to at least 4 times the fragsize to avoid xruns.
* Fix a race when setting default devices.

**Bluetooth**

* The source was rewritten to use a ringbuffer. This avoids regressions caused by audioconvert.
Hopefully this fixes my Bluetooth mic stuttering issues introduced with the previous release.
What distributions use or don't use pipewire and to what extent by default?

Edit: Thanks for the answers. Looks like I have been using it without knowing, as the main distributions (Debian/Fedora/Arch) use it by default.
What does it basically? I know it's an audio sever used by the os like the Display server for the displays. But does it have any extra benefits like better audio quality or even a sound equaliser where I can adjust the quality of the sound output??
Anyone know where to find a tutorial on how to use pipewire + wireplumber + easyeffects? Preferably with a graphical interface.
https://xkcd.com/619/  
Last night I had to remove pipewire and replace it with plain old pulse because suddenly it stopped working. No indication as to why it broke. Just unable to open devices.
Pipewire is just so damn good!
Thanks! That's great, especially excited about BT improvements.  

Unsure how to feel about DSD though, it's a pretty useless technology. Hope it doesn't add too much complexity to the code / won't be a maintenance burden.
Thank you for this detailed post. Keep up the great work! :)
It did for me :)
Fedora has used PipeWire as the default since Fedora 34 in April 2021.
Ubuntu doesn't default to it yet but is planning to switch on the next version (22.10)
Arch has been pushing pipewire over of pa for a while now, so I guess you can call it a "default".
From another thread, it seems like you can run this command to see if your distro is running it:

    ps -e | grep pipewire

Manjaro outputs this so I think it is using it:

    2745 ?        00:00:00 pipewire
    2746 ?        00:00:00 pipewire-media-
It's available / packaged in many distros  https://repology.org/project/pipewire/versions

As far as how many are using it by default I'm not sure on that. I believe Fedora 36 is by default but maybe others are as well now.
It's not in Debian stable by default at the moment.

I think there is a fairly old version available in the repo, but it's probably not worth trying because of that
Pipewire, Pulseaudio and Jackd  basically do the same thing - they sit between programs and the kernel's audio drivers and allow things like seamlessly switching audio devices (you plug in an USB headphone), network transparency, running effect plugins on the audio stream (yes, like an equalizer for example), and most importantly, mixing multiple streams from different programs together (which most audio hardware in use today can't do directly).

Pulse was the general use thing running everywhere, jackd the low latency server used mostly by musicians, and pipewire is the newer thing trying to replace them both (and doing video streams on top, a thing needed if you go from X to Wayland display servers because programs can't just grab the screen in Wayland for security reasons, and pipewire can be given special access and feed the display to programs like OBS and zoom for desktop sharing after asking you with a nice dialog window if it's okay).
This LWN article gives a good overview: https://lwn.net/Articles/847412/

In short, the main challenge it seems to solve is allowing sandboxed apps to securely pass sound around which is becoming increasingly important as we move to more container technologies at the desktop layer.
Well technically wireplumber is a media manager for PipeWire and should be bundled with the latest version of PipeWire in your distro if available.

EasyEffects (if available in your distro) is a GUI and configurator that allows you to setup effects for your inputs and outputs. It [looks like this](https://ubuntuhandbook.org/wp-content/uploads/2021/09/easyeffect-plugins.jpg).

In both these cases consult your app store, package manager, forums associated, etc. The question is if your distribution supports it or not.
What are you trying to do? That may help us guide you in the right direction.
easyeffects is pretty much self explanatory. unless you hit some bugs, then it may get complicated.
When Arch recently reverted back to wireplumber I had to delete my customised wireplumber config out of /etc to make it work again. Must have been some incompatible option in there but I didn't see anything in the logs.

Silent failure of services and lack of sensible guidance in the logs is one of the problems with a lot of modern linux services.

One of the big draws to Linux since I started using quarter of a century ago is I have always been able to troubleshoot and fix issues instead of being forced to reinstall the OS and hope for the best. We need to keep as much of that as we can. I barely understood what was going on with pulseaudio and pipewire even less so. When it is up and working it does seem to work really well though.
Did you file an issue? The pipewire devs are usually quite responsive
Similar thing happened to me last night as well!

I'm on gentoo. Recompiling with the "sound-server" USE flag for pipewire, and "-daemon" for pulseaudio did the trick.

I noticed that the line in gentoo-pipewire-launcher that starts the the pipewire-pulse interface had been commented, so I uncommented that. Don't know if it works commented, I didn't test.
Don't thank me, thank the guys that actually wrote the release notes. I just copy and paste so you don't HAVE to click on the link.
As of right now though, Ubuntu 22.10 is using pipewire-media-session rather than Wireplumber. Could change though.
It's the default selected choice via the archinstall script too
it is recommended by the wiki
Or just pipewire --version?
I have a fresh Debian stable/11/bullseye VM using gnome/wayland and there are pids for pipewire and pipewire-media-session...
**gotcha!!** ty for the replies guys.
Installing it is pretty easy, even getting the lines wiggling isn't that bad.

Getting it to output audio and doing something with it... fucking witchcraft.
I wear hearing aids that boost high frequencies. I want to set up my computer to boost the same frequencies when I am listening on headphones, but not when using speakers.
Same. The config is too powerful for when you just want some settings to flip. Some folks need it, but not most. That config should be optional
No, because I still don't know what failed and why.  
And I'm not the kind of person to file `this no work, help` so I just swapped components until I had audio working again.
They‚Äôve changed it to wireplumber on the latest builds (according to omgubuntu)

Edit: [source](https://twitter.com/collabora/status/1544426379349032966?s=21&t=q3gFNq4H14JN9UPmS2tEjA)
I have no idea what you're talking about. The Arch Wiki doesn't "recommend" any sound server over the other.
That doesn't tell whether it's actually running though
That confirms what I said.  It's not the default, but it's available.
Do you have Wireplumber? \^\^; I've personally experienced some glitches with effects, but getting it working? For me, it did technically work...

Btw, I use Fedora
You could also go to the #pipewire IRC/Matrix channel and start asking there
How recently did you read that? I checked it a few days ago and saw that pipewire-media-session was updating.

Edit: just checked the [article](https://www.omgubuntu.co.uk/2022/05/ubuntu-22-10-makes-pipewire-default/amp) and it does say that Wireplumber is in.
Are you saying one DE is more default than the other in the list of equal options offered by the installer? Or did you miss that by "fresh" I mean that I ran the installer and nothing else?
I linked the article.

I‚Äôm not sure how it handles existing installs. Maybe you have to reinstall 22.10 to see the new default.
Forgive me if I'm wrong, but what I meant is when I installed graphical -- gnome or KDE (I know there are others, but I'm assuming these two will move before others), neither drags in pipewire by default on Debian... But perhaps my memory is busted, and a fresh gnome does bring it in by default (it was a very quick install I did for someone else).

I never selected pulse audio or pipewire manually, so I'm just happily accepting what the Debian guys make appear for me.

I'm certainly not trying to get into a holy war, I was trying to offer information to the guy above asking.  I have no wish to tell you one DE is better than another (and you seem a little sensitive about it).


Edit:

> In Debian 11, PipeWire 0.3.19 is available, and can be experimentally used as a substitute for the ALSA userspace library, PulseAudio, and JACK. This is a documented but unsupported use-case.

https://wiki.debian.org/PipeWire
It is pretty old though with .3.19. January of 2021.
https://wiki.archlinux.org/title/List_of_applications

You can go through lists of applications and utilities if you feel like discovering new things.
I don't think this "problem" is exclusive to Linux, to be honest. I've been using Windows for more than a decade and just recently I discovered EarTrumpet, which is an app that allows me to independently move the audio output of applications to a different output device, which on Linux is something I just knew I could do with Pavucontrol, which comes pre-installed on virtually all distros.
Curiosity makes you more knowledgeable on whatever field you dig into.

Just a word of caution about content creators: they need to feed the beast (the audience) in order to feed themselves (a number of them are making their living out of YouTube). 

A video about installing stock Ubuntu is 20 - ish  minutes long and just one. 

On the other hand, you can do an infinite number of videos about customizing emacs and then another infinite number about setting up a number of tiling wms configured by scripts written by emacs. And in the mean time produce video on theming, on terminal emulators customization and so on.

Are you sure that the time you spend in setting up a wm is paid back by a measurable productivity increase?
>I see that things are better now, but I never stopped to notice it was bad in the first place.

Things weren't *bad*, they were average. Targeting wider audience, not you specifically. Wider audience doesn't want all the little tweaks you did. Most people would not like your setup, it's not tailored to them and it would break their workflow, that's why it's not the default. Distro maintainers can not know individual tastes of each user, and even if they'd known they could not cater to all those tastes at once.

If you find something annoying in your day to day usage - look for an alternative, they almost certainly exist. If nothing annoys you - are you sure that tweaking all things possible will bring you any benefit?
For me, one of the big methods I used to discover Linux software and capabilities was [Awesome lists](https://github.com/luong-komorebi/Awesome-Linux-Software).

I've been on Ubuntu for around 15 years now, and used what I found, and learned where I could. What really opened up the world for me however was reading through those lists and trying some of the software out. I now go out of my way to find top 10 lists of Open source software and other Awesome lists on Github. 

A very large portion of these lists are the same things, but every so often I find apps that have changed everything for me.
Watch less videos, read more (articles, books, magazines and so on). You'll cover much more ground in much less time.
The same way I keep up with everything else. RSS.

There's this subreddit, quite a few others, and in particular for finding new and interesting software on Arch, the AUR feed is invaluable. I'm finding cool new stuff on there all the time.
I don't think that this is a particularly easy to solve issue. As Linux is mostly a community project, the way to discover things is through the community.

The only way I see for someone to search for better solutions to their Linux system is to not be comfortable with the current one they have and search for a better one.
back in the day we used to just read slashdot everyday until that went terrible.
I'm not sure what the closest equivalent to that is today tha isn't youtube.
that's a massive question, a deep rabbit hole.

try making specific stuff/fixing specific problems that your or others have. that gives you a much narrower goal to aim for and then you start looking for the tools that help you get to that goal. also makes you the person that can get get stuff done and make stuff happen.

eg. goal - trying to get your RAM useage down as low as possible OR improving your desktop workflow. at first that might have taken you to other desktop environments (useful in and of itself) but eventually you would have stumbled across window managers and off you go.

other example - set up and maintain a BBS, or a mail/ftp server, or media server, or a NAS, an internet kiosk, a sign display.

the cool thing about that approach is you'll touch on a bunch of other things as well (more to discover) but keep coming back to the goal and you'll learn and discover HEAPS.

having something to show at the end of your learning seperates you out from multitute of other people who just talk about getting it done.
No problem here at all.  "Linux" is expressive, not discoverable.  You can do whatever you want with any of it (well more or less).

This is the core/best feature itself.  If you don't like something, you just go find and configure a replacement.  If you don't have any motivation to change... well you don't.

Now to harvest my downvotes: Ubuntu is the problem for new Linux users not the solution.  This situation you are in? It was created because someone boxed you up a config that "just worked" well enough that you missed the whole space.  It is fine to put on your grandma's machine or just to tinker, but staying there is a mistake IF you value learning/growth/independence.  Windows or Mac lite is a losing proposition.

P.S. for extra downvotes: this is also why spacevim, lunarvim, spacemacs, and whatever other pre-packaged editor configs are a super shitty idea.
Tbh of all the Linux channels DistroTube probably annoys me the most but I still tune for some reason lol. His take on things are at times like watching a car crash imo. He's definitely not the type of person that will help make Linux appeal to a wider range of people imo - just the ones that really love to tinker with things and that's fine, but it doesn't really grow linux.  


I have issues with Linux Experiment too, but at least he got off his elementaryOS wagon, I was getting tired of him praising that distro and calling it his daily driver. There are much better distros out there imo that don't tie your hands.. that actually can give a design language to the UI & UX that makes better sense.
I always reccomend to new linux users to be curious, watch a lot of youtube videos, maybe read some articles, if they are intrested obv

some people really don't want to tinker much and are also fine with vanilla ubuntu (ew, i know) and that's fine
I always reccomend to new linux users to be curious, watch a lot of youtube videos, maybe read some articles, if they are intrested obv

some people really don't want to tinker much and are also fine with vanilla ubuntu (ew, i know) and that's fine
I think "How am I only finding out about this cool thing NOW?" is a universal feeling that continues throughout your entire time using Linux, Windows, or any other tech ecosystem.
There are lists, videos and all that. The problem is everyone has their own experiences and preferences. So you end up in the same boat!

Asking someone more knowledgeable, you may get better answers, but you might not like the answer.

For WM the standard these days is i3-gaps. You get started right away.

For Linux, Manjaro-i3-minimal seems promising, otherwise Mint with i3-gaps is a good choice.
I don't use the gaps personally though, so a bit configurable.

There's alot of effort behind good advices. But, nobody has the final answer. Ie. maybe Void Linux is next? Nobody knows until they tried and made the efforts.
By reading forums, seeing other people's screenshots ...

Obviously wikis.
I would love a menu style launcher for console applications.
/r/unixporn, /r/unixart, any other subs that display peoples' ricing setups. These are good places to learn about new WMs and programs.
Nothing really different about this question if you were talking about the Windows platform or Apple platform or Android/google platform , etc.. 

There's no magic shortcut to finding the configurations or solutions that work for YOU (and your unique needs or expectations). 

You have to put the time in to exploring and searching and discovering the feeds or info-sources that speak to you and solve the problems you're looking to solve.
I like https://alternativeto.net

You can filter for linux applications and navigate from application to application. it's kida fun.
youtube and more youtube and reddit.
Well, as the name suggests, you could use [Discover](https://apps.kde.org/discover/) (=
Lot‚Äôs of googling, reading, reddit, news.ycombinator and other sources. But do keep in mind ‚Äúif it ain‚Äôt broke don‚Äôt fix it‚Äù. Ideally you‚Äôd only look for these tools if something is broken, which is where I personally go wrong at times. I mean looking at new tools and trying them out is fun but I‚Äôd personally be better of just getting to properly know the stuff I already have, it‚Äôd be more productive in any case.
Well, that's at least a little more proactive than waiting for news to reach me. Should've have realized the arch wiki has a list.
Similar lists:

* https://wiki.gentoo.org/wiki/Recommended_tools
* https://wiki.gentoo.org/wiki/Recommended_applications
You can't wrong with the ArchLinux wiki one of best resources for Linux users! :)
Thank you for this link kind sir!! Windows has a good ecosystem on the internet to find collections of apps for everything you may need. Linux desperately needs something similar. This is a good start.
you could do it in the windows settings btw, EarTrumpet makes it easier tho.
Same here. Been using Windows for ages before discovering this lil script for muting speakers on headphones unplug, just like on Android

https://github.com/PrateekKumarSingh/Posh-AutoMute
> Are you sure that the time you spend in setting up a wm is paid back by a measurable productivity increase?

Not everything has to be measured in concrete productivity. It has been fun tinkering with tiling WMs. And the fundamental concept makes more sense to me. That automatically makes it worth it even if I don't shave a few seconds off every hour of production.
> If nothing annoys you - are you sure that tweaking all things possible will bring you any benefit?

I'm pretty tolerant / blind to these things in the moment. That doesn't mean I can't make them better.
> Should've have realized the arch wiki has

This happens to me all the time! I spend hours trying to figure something out, and then find out the wiki has the exact info I need...
This sentence only further supports their point. It took third party software for them to get there.
Eh, not really.  
Storytime: before Win 10 20H1 or so, the normal volume menu  
was unable to edit individual volumes for Apps.  
You bought Forza Horizon 3 and it is too loud?  
Either game options or system volume has to change.  
Besides of that, Windows.  
MS APIs and/or software developer sometimes  
don't know what they do and integrated volume menu  
broke regularly the link between programs and sound  
device, if you choose a different output device.  
Don't know what kind of black magic Eartrumpet  
uses, but MS needed 4 years to catch up with it and  
even got inspired by the design of it.  
When I saw the first picture of the new menu, I thought  
that MS just bought Eartrumpet and made it default XD
That's a good perspective, and I'd honestly argue that being engaged with your environment in a way that's interesting to you personally will make you more productive anyway (though in excess it means you will lose countless hours to config/writing plugins/etc... heh)
>Not everything has to be measured in concrete productivity

If it's for fun, yes of course.

However if it's for work, time is money.

I can ride a horse for a couple of hours for fun to reach my friends in the next village, if I need to reach a  customer in the same village I want the travel time to be as short as possible or zero (conf call).
> I'm pretty tolerant / blind to these things in the moment.

There is a chance you will later want to unlearn what you're trying to learn now. Many people coming to Linux dive into bottomless rabbit hole of UI/workflow tweaking. I know I did. There was a period when I was spending more time tuning my system than using it.

After many years, you know what I realize? Ignorance is bliss.

Being productive in whatever DE is in front of me with whatever default theme it ships is what I want now. There is a small set of tools which I really use, and I care how those are configured. Login greeter, on the other hand? Session manager? Hell, I don't even know which app my distro ships for that purpose in this decade and where its settings are. And I don't care.
!arch and a search term on DDG.gg will help
I have no idea if it took a third party for this to be implemented but this functionality has been in the native windows settings since 2018 or 2017 if using preview builds.
Are you sure? I distinctly remember being able to change individual program sounds in Windows 7.
Let me gently disagree: it makes you endlessly work on the system instead of using it to get things done.
Apps, not programs.  
Programs have individual sound volume since Win 98 or so.
What's the difference?
"Apps are installed only through the Microsoft Store and have  
a special place on your systems partition, who no one has  
access to, besides of the user SYSTEM."  
"Programs are those pesky code chunks that are installed  
through exe files."
You have got to be joking. They took "app", which is short for application, which is a synonym for program, and made it a new thing?
"App" sounds more modern/fancy than "program", which I assume is why they wanna associate it with MSStore programs.
Nope, not joking and this trend had been introduced with smartphones.  
You don't have programs, software or applications on your smartphone,  
those are apps......  
Seriously, they use it that way, brainwashing and so.....
I mean, that's just a trendy way of saying "program" in smart phones though.
If it only it would be so, if it only would be so......
This text is almost 100% copied word by word from the phoronix article. Ridiculous...
The news was first reported on Phoronix: [https://www.phoronix.com/scan.php?page=news\_item&px=Systemd-Creator-Microsoft](https://www.phoronix.com/scan.php?page=news_item&px=Systemd-Creator-Microsoft)

r/linux Can you pin this comment to give credits to Phoronix. u/michaellarabel deserves it.
This is a sign that The Year Of Linux Desktop is close. Linux kernel + systemd + Windows UI.
This is literally stolen from Phoronix. I hope he's getting paid really well
I KNEW IT! everyone knew eventually that poettering was trying to uh... DO SOMETHING TO LINUX!!!!, we should have NEVER trusted systemd /s
It kind of spooks me how much hate Lennart gets in the Linux community. I'm shocked he wants to work on it at all at this point, I'm sure I would have given up years ago. I wish him luck in his new role.
Interesting that Red Hat couldn't keep him.
I saw hints of this on the fedora devel mailing list with a thread about his email and bugzilla. But I am still shocked.
I don't understand this hire. What will MS gain from having him work on systemD? He was already doing that at RedHat
Hahahahahahaha
Jesus Christ, these comments. Calm down, people.
top-5 anime betrayals lul
I thought people hated systemd ü§î But I guess some people hate its creator joining Microsoft even more

The comments in this thread are why the Linux community gets a bad rap in people‚Äôs eyes
This is like when Sasuke joined Orochimaru.
Damn, he really is Icaza 2.0...
Never have I met a sane Poettering hater
AHA!  Proof that Systemd was a secret Microsoft plot to destroy Linux.

I'm glad I'm still using Slackware and initd

/s
Sad.  Very sadüòû.
He'll fit right in with their design philosophy and coding architecture style. Great hire.
Good luck to him and he can continue contributing to his big projects, the guy has been brilliant and without a doubt, his contributions to linux in general have been precursors to changes for the better, both pulseaudio that is still fully backwards compatible with older devices and systemd that it is indispensable for modern linux systems and their integrations.
Lennart's departure to Microsoft makes this old article particularly interesting reading:

https://unixsheikh.com/articles/the-real-motivation-behind-systemd.html
It's the end!! IT'S THE EEEND!!!!
I'm sad to see him go :(
We're living in interest time guys
After years of feature creep, one of the main developers of systemd gets hired by a company famous for "embrace, extend, extinguish." Not the best news I've ever read but hopefully Poettering can resist any pressure.
The Toxic Haters in the Linux community is what scares developers and companies away from supporting Linux.  
  
The abuse against opensource developers and their projects like Firefox, systemd, Gnome, KDE, etc. has gotten totally out of hand the last decade.
  
Spewing toxic hate has become so normalized, that it is has become "accepted" behaviour even here in /r/linux. Oh, the worst may get a down vote, but that is it. 
  
That there are no real consequences  for trolling and spewing toxic hate against open source developers, has made practically all Linux developers flee to places where they can control and moderate who they are speaking with.  
  
The real enemy of Linux isn't Microsoft, but the toxic haters that are attacking Linux from the inside.
Lennart Poettering has probably been a secret agent in the employ of Microsoft for at least 15 years.
Embrace ‚òëÔ∏è
Extend ‚òëÔ∏è
Extinguish (WIP)
Great.  Can we rip that shit out now?
Money money money!
Ha!
Wow, truth *is* stranger than fiction.

Quite the interesting plan:

* Start a system-level manager
* Get hired at major distro maker, get project accepted corporately
* Push for distros to use it
* Get hired at Microsoft
* Microsoft now owns and controls early userland Linux

Great idea to consolidate system architecture onto a single project, controlled by a corporation, right?

Man, 2022 is crazy.
he always was, whenever I use systemctl I feel like using a Microsoft program
So is Windows getting a needlessly complex init system with gigantic scope creep that goes against the UNIX philosophy?
Sold his soul to the devil.
Like the sellout bitch he is.
Microsoft got tentacles everywhere!
Great news!
[deleted]
To the surprise of absolutely nobody.
Bah!!!!  That saboteur obviously STARTED at Microsoft.
And that rat bastard can stay there.
Good riddance!
I guess a Windows Registry redesign is going to happen...
To think of it, if Poettering will do something really unreasonable, now that he isn't from RedHat anymore, developers will just fork systemd and all. Why isn't it an option, anyway?
Kay Sievers next?
Not surprised at all tbh.
I see Poettering is following in the footsteps of Miguel de Icaza, another polarizing figure in the Linux community. So we can expect Poettering to fade into obscurity and leave Microsoft in about 6 years.
Distro maintainers/developers all disdained the responsibility for knowing how to setup/define the userspace of a Linux operating system, so when Lennart offered to take it from them, they mostly accepted. Now the responsibility to know how, coupled with the right to dictate how, is centralized. No diversity of operation, viewpoint nor provenance. One point of failure. What ever could go wrong?
The way I see it, this is a win-win. We don't need to cope with him anymore and he'll get to radiate his shiny personality on some Microsoft victims.

By doing that switch, Poettering has simultaneously increased the average IQ of both the Linux and Windows communities.
I'd say good riddance, but if he's still contributing to any Linux development, it's unfortunately not a win.
Terrible terrible!!!!! How will we survive without 2 months systemd  release cycle with all the new shiny features without documentation!!! üòÜü§£üòÅ
This submission has been removed due to receiving too many reports from users. The mods have been notified and will re-approve if this removal was inappropriate, or leave it removed.

This is most likely because:

* Your post belongs in r/linuxquestions or r/linux4noobs
* Your post belongs in r/linuxmemes
* Your post is considered "fluff" - things like a Tux plushie or old Linux CDs are an example and, while they may be popular vote wise, they are not considered on topic
* Your post is otherwise deemed not appropriate for the subreddit


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/linux) if you have any questions or concerns.*
[deleted]
I know, but Phoronix is not allowed on r/linux.
Thanks for letting me know, filing a DMCA takedown notice...
You're asking the MODs to pin a banned domain?
ü§£
I wouldn't mind honestly if not the fact that somewhere between win 7 and 8 they swapped ui design team for group of monkeys üòÖ
Let's hope he keeps improving Linux as much as before
Don't forget Pulseaudio... üòà
I am afraid the NT kernel is better :-)

But I would love Hyper-V-->Linux FHS-kernel--PowerShell--.Net---> /Windows.
systemd.exe/windows or as I've recently taken to calling it systemd.exe+windows coming soon? /s
I came here to say this very thing. The conspiracy theory has matured into fruition!
He'll also bring his other big project: Pulseaudio...

Just in case you thought it was easy to configure your various audio devices in Windows, Win 12 will proudly feature Pulseaudio.exe and related components... üòà
Because he is a mediocre mind who poisoned Linux with binary logs and Micro$suck like "init" system that went against everything UNIX stands for. Now the Mothership has taken him back into the fold. A job well done. Not the first time MicroShit sent their spies to destroy cometition  (see Nokia).
[deleted]
Eh, I'd say it's probably because of the tight labor market at the moment. If you're a good engineer and one who has some name recognition, you can probably write your ticket for the most part. I'm sure he got a really good pay bump to switch to Microsoft.
People go between RH and Microsoft all the time. It's not like a flood but it's also not rare.
Honestly looking at how much the community hates on Pottering, I'd be surprised if that wasn't on a list of reasons he left redhat. I bet you he got a lot of hate in internal company communications.
WSL doesn't support systemd yet :P
I hate to break it to you but Microsoft also uses Linux so
Microsoft's big money comes from developers and servers. That's the whole reason microsoft made WSL to start wtih, so many devs were jumping ship because the workflow is much better in linux, and because docker started dominating zeitgeist. 

Most likely his systemd work will be either on WSL, or something on Azure servers, maybe [Microsoft's linux distro](https://github.com/microsoft/CBL-Mariner)?
Who cares about "Linux Community".  Software first and foremost IMO.
Well now you have. 

I love the idea of a declarative service configuration in theory because after all you can still exec a shell script so you ought to have all the power of a shell script based service manager and then some and dependency management and parallel start makes tons of sense.

What I got was a system where the system would sometimes take a long time to shut down or sometimes fail entirely. The end result felt...flaky and buggy.

Better off just using runit or openrc.

I love the idea of dynamically moving audio streams between devices with pulseaudio so I could just leave my headset plugged into my desktop and toggle between it and speakers. What I got was YEARS in which the best way to have sound that didn't malfunction was to disable the whole mess and use alsa. This is with carefully chosen hardware that worked.

Nowadays I tossed the usb headset in the trash and switched to sndio and have a nice pair of bose headphones plugged into a plain 3.5mm jack and I toggle back by toggling which port is muted by hitting a button on my keyboard or a icon on my bar.

I'm not a luddite. I love shiny new things and try them all the time. I don't feel like Lennart's work is up to snuff. I think its interesting in design but mediocre in implementation because ultimately its substantially more complicated and I don't think he delivers substantial value.

I don't know him as a person but from what I've seen he seems to be egotistical and dismissive of criticism which I probably couldn't give two shits about if again he delivered.

Furthermore the thing that IS actually odious isn't the man its the fanboys. A lot of vitriol against systemd stems from the tendency of fanboys to dismiss legitimate criticism as hate fud or old farts that don't want to learn new things. 

I like new things I just like them to work and have higher standards.
Look at your flair.  You are like a priest saying they've never met a sane athiest.
There's more of us than you think, we just keep it primarily to ourselves because nothing I/we say is going to sway your opinion and vice versa.

It's like politics. Agree to disagree.
Hate is an emotion. Logic and emotions are like water and oil..
It's kill, or be killed for tech companies.
This, but with the /s moved up to the top line.

Slackware is based.
But actually, Windows NT's architecture actually is kinda neat.

It was originally designed to host multiple userlands through environment subsystems, NT originally having an OS/2 and POSIX subsystem in addition to Windows. 32 bit Windows still has NTVDM for a DOS environment. They tried a Linux subsystem the classical way with WSL1 but I/O performance sucked.

The only other system to support different environments (as in, kernel-level compatibility layers) is FreeBSD, where I think it also has/had an SVR4 compatibility layer in addition to the Linux one.

I think Linux at one point did support this, where the personality() syscall did a lot more, and also supported SVR4, but I think it was gutted in the early days as a consequence of the SCO fiasco. Don't quote me on this though.
:D
Heh.  Like a hand in a glove.
The first relavent and sane comment.
Why does it matter whether he works for IBM/RedHat or Microsoft? He will still be in charge of systemd...
Most likely he's hired to still work on SystemD and other Linux projects. You don't have to believe the Microsoft hearts Linux slogan to recognize that they are making bank using Linux in the cloud. It just makes sense for them to try to improve it.
>"embrace, extend, extinguish."

Can we stop with that? It's been over 20 years that this was alledged. I know the Linux community isn't great when it comes to "letting it go", and i'm not going to defend Windows as an OS, or the broader push to Service based Products, but people, companies and goals change.

With that said, looking into what they are actually doing, Microsoft is doing a lot for the broader open source community and hasn't done anything in recent years that would warrant suspicion.

I'm pretty sure Google or Amazon is a WAY worse company, yet i don't see a bunch of people yelling around when you order stuff from Amazon or whip out your latest Android phone. Let there be room for a grey area of "Not FOSS Libre sanctioned by the Linux gods, but also not Sell my soul for advertising chrome".
Realistically, even if the worst of the worst happens,

1. forks are still legal
2. alternative init systems / service managers / DNS caching tools / cron daemons / DHCP clients / `/dev` device populators / sysctl-on-boot tools / session managers / logging daemons / VM/container managers / periodic fsck tools / cryptsetup-on-boot tools / proxy servers / `/tmp` cleanup tools / OOM killers / backlight level restoring tools exist too
Ah, here we go. The standard redditor nonsense philosophy:  " If everyone doesn't agree with me 100% they are toxic.".

Stop acting like a child.
Hate is a human emotion like any other. Refinement of attempts to get rid of it, must inevitably come to resemble attempts to establish a [totalitarian state](https://en.wikipedia.org/wiki/Equilibrium_(film\)).
First thought. I mean, systemd I don't use, put pshhhhhaudio is almost unavoidable, unless you are a dedicated Gentoo user.
How does Microsoft now suddenly "own" anything?
When did you start using linux? I started after systemd and usr merge, and I loved the control over my system systemctl gave me, control microsoft never gave me.
what about systemctl is microsoft-y?
no this time its going to violate the Geneva convention
What about systemd goes against the UNIX philosophy and why is that a bad thing?
uNiX pHiLoSoPhY
More like systemd is a bastard child of launchd and svchost.exe.

Never mind that latest systemd tentacle he was pitching, homed, is basically a conceptual copy of Roaming Profiles. Something that has been with NT since the early days, and something i think every sysadmin that has tried to make it work has cursed to hell and back.
No, they already had it.  It was probably Poettering's 'inspiration'.
Yep, but it was long before that. Now he just switched officially.
Are you still stuck in the 1990s?
Wait, you mean people try to make money with work, rather than acting as a charity?
First of all systemd is a clone of _launchd_ from macOS, not svchost from Windows (btw, it's just like PulseAudio is a clone of macOS's CoreAudio)

It's using ini not "cuz Microsoft conspiracy" but because it's a very simple config file format that does the job. Many other things use INI, including your desktop environment (extensively), most XDG specs, etc

Second of all, you've almost certainly never fallen back on systemd's default DNS servers. 1) it's the default if the distro doesn't override it at compile time (and they all do), and 2) it's only used if you don't have DHCP or some other network settings (like Network Manager!) setting a DNS server. It's literally the last of the last resorts that's only used if the distro didn't set a different one (and again, they did)

As with NTP: similar story. It's the default only if your distro didn't do the right thing and set their own NTP server. Like all of them are supposed to do. It's so people that build systemd without reading the documentation too closely don't end up with a broken system

So no, there's no "introducing Google telemetry". There's a ~0% chance that config actually made it into your binaries, and if it did it's your distro's fault for not configuring things that must be configured
Who?
IKR?

Microsoft has been causing its users pain and anguish with their crappy, buggy, ill-thought-out software for four decades.

He'll fit right in.
What makes you think he will stop working on linux related software?
>How will we survive without 2 months systemd  release cycle

What makes you think that? He will "**continuing his work on systemd**".
You do know that systemd will be maintained just like before because not only Poettering worked on it right?  


Not to mention that he still gonna work on systemd.
Happens all the time for high-profile individuals.
Why is it blocked?
Still? I thought this was changed, in addition to YouTube links?
Wow that really was just straight up plagiarism.
I never thought I'd see the day people filed DMCA notices from a reddit community for Linux.
Great article. I never thought I'd see the day when Microsoft paid people to maintain Linux
I'm asking the mods to give credit where credit is due.
How the NT kernel is better ?
systemd-activedirectory :D
So they're finally moving from svchost.exe/windows
/s
systemd-clippy
systemd-registry
that would be great for the linux audio stack as it will actively make windows audio worse!
I'm sure it's not the hate. He probably just went for the money.
> I bet you he got a lot of hate in internal company communications.

No good company would let such a thing fly, and I doubt redhat would.
I doubt RedHat hires exclusively 14 year old redditors
The loudmouths from the community can be truly horrible. I don't know anything about the guy personally, but both PA and systemd have provided significant improvements for Linux both on desktops and servers.
Red Hat supported the systemd project and its ideas. In fact I'd argue by employing LP, they directly funded its development.

His work is 100% about gaining influence for his employer.
Hold on, does that mean systemd based distros don't work under Windows? I thought Ubuntu works under Windows
I too hate to break it to you, but I also use Linux , does that mean I should hire him?
Basically this.  

If you don't mind me extending your observation, we can make the same argument about Windows.  Before windows,  the world was populated by Unices and other operating systems that were unbelievably expensive and inaccessible.  It is before my time, but the historical read on the situation is the same as you described:  everyone wanted a nice reasonabl3 GUI based OS that was cheap and easy to access and just worked on commodity PCs.  What we got was Windows.

Sure, worse is better and the market doesn't make sane choice as per your observation.  While we can give Lennart credit for very reasonable insights there is something about his particular human nature that just makes everything worse.
What about it, every major distro uses systemd
Pretending like Lennart haters are anything but extremely vocal in every semi related thread lol.
Hear, hear!
Much of NT's architecture is due to VMS. One of the main VMS developers was poached by Microsoft (this is a trend as you may have noticed).

Plus NT 3.5 had one whole year of bug fixes whilst waiting for IBM to prep their hardware ready for the PowerPC release. This made it very solid!

I do like NT, particularly 4.0. It is basically Windows as it started getting good but with out all the bundled malware that Microsoft (criminally) adds these days.
Solaris also had a similar model with zones to my understanding
> The only other system to support different environments (as in, kernel-level compatibility layers) is FreeBSD, where I think it also has/had an SVR4 compatibility layer in addition to the Linux one.

NetBSD had that too, and OpenBSD too. About SVR4 - don't know what it was used for, have encountered traces of somebody running IE5 for Unix with that.

Also NetBSD had those for older versions of itself (I mean, I've seen such options in the kernel config file, don't know anything else), so does FreeBSD.
Sco fiasco?
Are there really benefits to this in a world where VM's and container solutions are so robust?
No, I guess it shouldn't have to matter. But MS always makes me uneasy.
They're making bank using Linux in their particular cloud environment, using not-Linux on the desktop and integrating with not-GNU-userland on mobile. It makes sense for them to try to improve Linux in ways that make their own usecases easier at the expense of other usecases.
> With that said, looking into what they are actually doing, Microsoft is doing a lot for the broader open source community and hasn't done anything in recent years that would warrant suspicion

Two days ago I read about how they're throttling some kind of Xbox cloud (whatever that is) for Linux users. And I don't see their contributions as a sign they've changed. They're mostly about letting free software interact with their proprietary products, and are arguably another attempt at "embrace." 

>I'm pretty sure Google or Amazon is a WAY worse company, yet i don't see a bunch of people yelling around when you order stuff from Amazon or whip out your latest Android phone

We are looking at different places then, as I see plenty of criticism of both Amazon and Google. Plus, I'm perfectly capable of hating all three. I don't feel the need to rank which one I dislike most.
> Can we stop with that?

No.  Corporations are *not* your friend and MS will do anything if it means killing off the competition and increasing profits.
  
\>  It's been over 20 years that this was alledged

It wasn't alleged, it was undeniably proven. Read the leaked Halloween documents.  


And just because microsoft stopped doing that one asshole tactic, doesn't mean they stopped all their other asshole tactics.  


So can we please stop with the microsoft apologetics? It's been 47 years and they've been finding new ways of being assholes and never stopped.
That wasn‚Äôt ‚Äúalleged‚Äù, that‚Äôs  from their leaked documents where it was shown that Microsoft basically wanted to try to use other software to face it out with their own stuff over time. And let‚Äôs not forget how they called Linux a cancer. Or when they gave training materials to Best Buy to have their sales personnel bash Linux. Or how they publicly accused ReactOS of stealing their work without being able to prove it.

There‚Äôs a reason people involved in open source projects have problems ‚Äúletting it go‚Äù as you put it.
>this was alledged

It wasn't alleged, it was quoted directly from Microsoft material handed over in discovery. We didn't come up with EEE, they did, they knew what they were doing.

>Can we stop with that?

We will stop when they stop.

>I'm pretty sure Google or Amazon is a WAY worse company

By what metric? Impact on FOSS? No, Microsoft is much worse.

>looking into what they are actually doing

I'll believe this isn't EEE when they don't do it.

Edit: Source:

[https://en.wikipedia.org/wiki/Embrace,\_extend,\_and\_extinguish](https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish)

Specifically reference 3: [https://www.usdoj.gov/atr/cases/f2600/v-a.pdf](https://www.usdoj.gov/atr/cases/f2600/v-a.pdf)
You realize recently they launched a product (github copilot) which IMO [basically steals](https://en.wikipedia.org/wiki/GitHub_Copilot#Licensing_controversy) FOSS code.
Bruh you have to realize that Linux diehards are like flat earthers when it comes to Microsoft. We have people who still, to this day, claim that Microsoft is going to walled garden windows. It‚Äôs been roughly a decade since the introduction of the MS store, still no walled garden. ‚ÄúGee I wonder why that is?‚Äù

Like I get it, you don‚Äôt have to like windows or MS, but most of the criticisms I‚Äôve seen of MS/windows lately have been some of the dumbest shit I‚Äôve ever heard. It‚Äôs especially bad on places like r/SteamDeck
OK shill...
My ten second worst case scenario is 

Microsoft better integrates systemd-boot with secure boot. More and more of secure-boot's features require the secure boot parts. Microsoft then uses the better integration as an excuse to stop allowing distros to use their secure boot shim keys. 

Damages Linux, forks and alternatives be damned.
I think your answer is a perfect example on how toxic behaviour has become normalized.  
  
I any case you are setting up a strawman. I am not advocating getting "rid of human emotions", but banning toxic behaviour in public forums. People can be toxic, unhinged conspiracist at home if they like, but should have no expectation of acceptance of this behaviour in the public.
Pipewire ^*

^* ^May ^or ^may ^not ^be ^dependent ^on ^systemd
They employ the head developer of the project. You think that means nothing? Money is power in the human world.
Around 99. It's not about the value of systemd (which is hard to deny) but the ergonomics and feel. The syntax, the UI.
Maybe not systemctl, but the whole systemd.
Everything and a kitchen sink in one piece of software (I still can't believe my resolv.conf references 127.0.0.*). Not quite the unix way, more like MS way (think Windows with media player, web browser etc - the stuff they were accused of monopoly).
> The Unix philosophy, originated by Ken Thompson, is a set of cultural norms and philosophical approaches to minimalist, modular software development.

https://en.wikipedia.org/wiki/Unix_philosophy

Is very easy to argue that it violates the first characteristic: minimalist

Other good quote:

> It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):[1]
>
> - Write programs that do one thing and do it well.
> - Write programs to work together.

As I see it systemd is a kitchen sink and monolith approach, not one tool that does one thing well.

Don't get me wrong, there are some cool concepts in it, but its very much a radical reinterpretation of how to manage many aspects of a Linux based system and a pretty big deviation from the traditional System V approach.

I find it needlessly over complicates many aspects of init and supervision, but have accepted it and become productive with it.

I'll even go as far as to say that I liked using it for CoreOS, but the whole philosophy about how that OS worked was substantially different than the traditional Linux distribution.
If that was one of the main reasons you started using Linux and you saw it change, would you not call it out as a reason for not liking the direction?

Arch and any current distro is standing on the back of 50+ years of development. 

Discounting concerns from people with deep experience in a technology with hand-wavey snark is just sad.
Heh.  That is a good point as well.
No, but I was pretty brutalized by Microsoft back then. Some higher up in the company gave an edict that we were replacing all our Unix machines with Windows NT 3.51. Just one problem. The engineers I supported ran FEA analysis' that took a week or more to complete. Unfortunately NT3.51 couldn't run that long without crashing, making for very angry engineers. We eventually reverted back and the guy fell out of favor and quit to take a cushy job at Microsoft. They fought open source until they lost and then embraced it, for financial reasons. I simply have seen no evidence to make me want to trust them.
Kay f\*cking Sievers

https://lkml.iu.edu/hypermail/linux/kernel/1404.0/01331.html
Lol I must have mistakenly opened r/kindergarten, otherwise I would not read such comment
>What makes you think that? 

A humble desire to make some flame!! üòÑ
I wish he would just let go of systemd and let others look after it.

After all, Pulseaudio improved from a shitshow that pegged CPU on 100% and constant crashes to actually working rather well after he lost interest in it and the community stepped in...
Because it will take some time for him to switch the working environment and fit himself into the MS structure.
[deleted]
https://www.reddit.com/r/linux/wiki/rules/banneddomains#wiki_2._spamblogs
I don't know. I thought it is, but didn't try...

Update: Just tried to submit the original Phoronix link. Still blocked. ‚òπ
It always makes zero sense how Phoronix content can be blocked from /r/linux yet legit 'spam blogs' that outright copy text/images/benchmarks is then allowed... Far from the first time. Anyhow, have filed a notice to that site.
Open source is the community that "weaponized" copyright to make licenses that force others to play nice by sharing source code. So.. it's not too weird when seen that way :)
I know, and he deserves it, just pointing out they seem to already be against him.
I'd say this guy explains the differences. But neither are bad, just different and designed with different goals in mind.

https://www.reddit.com/r/linuxquestions/comments/crck9a/what_is_something_that_the_windows_kernel_does/ex43i1s/?utm_source=reddit&utm_medium=web2x&context=3
systemd-actived
Yes joke aside it will be interesting what project exactly.
systemd-csrss.exe
Let's be honest here. Nothing can make windows audio stack worse.
Do you mean it completely ruined Linux and went against everything UNIX philosophy is?
Yes Ubuntu, Debian etc. are there to install. But they are striped down without an init system as far as i know.
They work, you just don't run systemd. WSL has it's own custom init.

Most of the stuff you'd be starting with systemd is just handled by WSL instead.
https://unixsheikh.com/articles/the-real-motivation-behind-systemd.html
Ah yes, unlike the sane pro Lennart crowd.  Get over your self.
I think they were mostly referencing the **sane** haters. They're the ones that keep quiet. Like most cases, the insane ones are a very vocal minority.
Win2k is still peak Windows in my book. XP and later started whole "call home" thing that makes you wonder who is actually in control of the computer.
Zones are closer to containers on Linux or workload partitions on AIX. AFAICT they're talking about something that provides kernel compatibility. You couldn't for example run AIX stuff just by using Solaris zones AFAIK.
[SCO v. IBM](https://en.wikipedia.org/wiki/SCO_Group,_Inc._v._International_Business_Machines_Corp.), a lawsuit about IBM having supposedly copied copyrighted UNIX code into Linux.
In addition to the adjacent reply:

At one point in the 90s, Linux was capable of running commercial Unix applications via the Intel Binary Compatibility Standard, which was a standardized ABI published in 1988 by AT&T, Intel, and SCO, so commercial Unix operating systems had a vendor neutral interface.

I think because of the lawsuits, that feature was removed. I'd imagine it was too risky (and frankly nowadays pointless).
Well, there's a benefit if you're using containers. You can throw Linux in a FreeBSD jail I think.
>Two days ago I read about how they're throttling some kind of Xbox cloud (whatever that is) for Linux users.

And the very first comment was a plausible explanation based on conflating Linux with Android and serving lower-res as a result of thinking it was a mobile client.
Yeah they‚Äôll do anything, including making windows a walled garden ecosystem with the introduction of the Microsoft store nearly a decade ago just like everyone in the Linux community (including Gabe Newell) claimed they would‚Ä¶

Oh wait
What if investing into Linux and open source is going to make them more money in the long run?
I'm out of the loop, what is all this about?
Depends on what restrictions they put on the code they trained their AI with. MIT licensed code is completely ethical IMO. GitHub has an easy way to select which license you use, and could easily filter out any project not using it when training their AI
Windows is moving towards being a walled garden with things like S mode and the Windows Store. The only thing keeping Windows from being a walled garden is legacy Win32 support. If Microsoft could get rid of it without losing a lot of users, they would have already done it. That's why they're pushing UWP for mainline applications.
> Bruh you have to realize that Linux diehards are like flat earthers when it comes to Microsoft.

This is unfortunately true.

> We have people who still, to this day, claim that Microsoft is going to walled garden windows.

But denying empirically proven market incentives is just as bad.

> It‚Äôs especially bad on places like r/SteamDeck

I haven't really looked at this sub in the last two months because it has simply become uninteresting, but I think you are victimising yourself. There have been toxic comments from both sides, but mostly I just saw beginner friendly advices from Linux users, and of course, the usual boring contents.
Banning toxic behaviour would quickly become more toxic ("we'll ban any dissent because it's toxic and everyone has agreed that banning it will make the world better", in the end it becomes a battle of orthodoxy as we see in many places now), the only sane way I (personally) see is to remind people how being constantly insulted (often gratuitously) as an open source developer is tiring and discouraging, and if people don't refrain from being too negative, let them dig open source's grave.
Tyranny is worse than needing resilience to overcome negativity.
Not really.  Your attitude is the perfect example of how modern civilization has made our men so soft that they crumble in the presence of competing ideas.

Dear lord, this is pathetic.
I don't see how that follows. It's more like launchd than anything MS
Systemd isn‚Äôt a monolith at all. But continue false proclamations. Seems to be the theme of systemd haters for the most part.

If you haven‚Äôt seen it, Benno Rice‚Äôs `The Tragedy of Systemd` talk is great.
I think the one thing that people need to keep in mind is that the unix philosophy hinges on the IO between programs being text based and thus composable via pipes.

systemd, while a bunch of programs, are tied together via binary APIs that are under the control of the systemd project. They can and will change based on the whim of the systemd maintainers.
> Is very easy to argue that it violates the first characteristic: minimalist

Except "mimimal" is a subjective term which is the problem with a lot of the "unix philosophy" stuff. The terms get defined according to whatever the speaker is trying to say.

> **As I see it** systemd is a kitchen sink and monolith approach, not one tool that does one thing well.

Which is well phrased because that's as much as can really be said. That's because "one thing" can be defined to something as restricted as "listing current mount points" to something as generic as "enable user to use computer." On a semantic level those are both tools doing "one thing"

It's not something you can use common sense on either because it's not immediately clear to me why these tools somehow objectively and categorically don't "do one thing":

* systemd to manage system services
* systemd-oom for OOM events
* systemd-nspawn for container services
* journald for logging and retrieving events.
* systemd-resolved for programmatically managing DNS configuration in ways sensitive to changes in networking.

All those feel like they could be "one thing" if the speaker just chooses to call them that. The only thing that separates systemd is that these tools are developed to be complementary to one another which is achieved by managing them all as being part of one large project.

But there is no `systemd` binary that does all the "systemd" stuff. It's already broken up.
> If that was one of the main reasons you started using Linux and you saw it change, would you not call it out as a reason for not liking the direction?

Not if the change is for the better, which in the case of systemd, it absolutely is.

> Arch and any current distro is standing on the back of 50+ years of development.

Appeal to tradition argument. Also, Arch _chose_ to switch to systemd.

> Discounting concerns from people with deep experience in a technology with hand-wavey snark is just sad

Because this conversation has been hashed, rehashed, and beaten to death a billion times now. Guess what, the predicted end times with the switch to systemd never happened.

Also, you make an interesting assumption that no one else here in favor of systemd has deep experience in technology.
This.  It is fascinating to watch humans make the same mistakes generation after generation.  This is the classic scorpion on a frogs back story which is a parable explaining risk management with respect to reputation credit.  Every single time people get in bed with Microsoft they end up regretting it.  Sure, they enjoy a bit of the journey here or there, but eventually, everyone pays the price.
What's your problem with systemd though? It's clearly not a resource hog
I'm not against systemd so I guess I have nothing against Poettering being part of it. I can confirm that PulseAudio was improved as well. I remember how bad it was when it started to replace ALSA. On my systems I even removed it and used pure ALSA. Now it's just working without issues. I switched to PipeWire. Had some minor issues but it's working fine as well.
Sure but it's not gonna affect systemd in any way.
There's nothing wrong with reporting someone has moved to take a new position elsewhere. It's very common in business and people often use LinkedIn to update people where they are and their new title.
But then

1 - moderators warn phoronix tends to redistribute old news that were shared by others first
2 - this post source is a site quoting phoronix - which is worse
/r/linux considers Phoronix a "spamblog", wow.

Mods being mods I guess. Par for the course.
Ah, so there is an exception for benchmarks. That's pretty much the only thing I use Phoronix for and they do those well for the most part.
this is so dumb, phoronix is one of the few linux portals...
Why Phoronix is even blocked?
/u/Kruug can we have this revisited? I don't know why Phoronix was banned in the first place but if CAP was responsible for it, it may not have been justified.
It's only because your site is in the automod via regex as being banned random spam blogs aren't.

And it's only blocked because CAP wanted first party sources only, which kinda kills any form of journalism being posted to /r/Linux 

The current mods might be willing to change that.

I stepped aside before CAP was removed because the rules being so strict just turned being a mod on here to a full time job.
Hopefully it's just an oversight, something left over from when CAP was still round. Phoronix is a fantastic news source and really should be allowed. I'm a subscriber and read it every morning.
systemd-explorer.exe
i enjoyed downvoting this
The Unix philosophy, as great as it is, is not the be-all and end-all of software architecture philosophies.
Correct, there's no systemd and a variety of strange things happen or fail to work out-of-the-box in WSL2.

I've been saying WSL is a recursive backronym for "**W**SL i**S**nt **L**inux". It still feels more like Cygwin than Native Linux.
[deleted]
If that's the case, why is everyone just anti-systemd, and not anti GNOME as well? It's a de-facto redhat project, with arguably just as much influence.
Not hating someone does mean you're pro them lol. I don't care or look into them, I just use their software for a smooth and stable experience, and systemd unit files are nice to deal with. I guess I'm pro- every other developer who has written a line of software on my device then?
Yes, very true. The DRM was just a big ol' downhill spiral.

Once they get you tied down to their server one way or another, they can start to do horrible things.
Agreed. Windows 2000 was the best of the Windows releases. It all went down the slippery slope after that.
I ran Win2K almost up to its EOL on a lot of machines. I still have it in a VM to run some old legacy software for files I created in the 2000s.
I don't know if there is a AIX zone, but there is for example a Linux zone on Solaris: https://docs.oracle.com/cd/E22645_01/html/817-1592/gcziw.html
User agents for Android say they're Android, that's not a great excuse. Stupid enough to "warrant suspicion" at least.
Sufficiently advanced incompetence is indistinguishable from malice.
So, if I understand correctly, you are saying that it's not in the interest of corporations to engage in monopolistic practices in order to achieve a monopoly.  
There are only a few things where virtually all schools of economic thought, from Marxists to free market fundamentalists agree on and this is just one of them.  
What really happened is that Microsoft's market research has shown that locking users into their own store would currently do more harm than good, which doesn't mean that they have no such motivations. The fact that it hasn't happened yet just means that it would be a suboptimal move under today's market conditions.
https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish
>	Windows is moving towards being a walled garden with things like S mode

S mode is optional, and takes 2(?) clicks to exit. Also (AFAIK) only ships with the Surface Go line. I‚Äôm not even sure you can download an S mode ISO, but I may be wrong.

>	the Windows Store

It‚Äôs been 10 years since it went live and we haven‚Äôt seen the walled garden yet. 

>	The only thing keeping Windows from being a walled garden is legacy Win32 support. If Microsoft could get rid of it without losing a lot of users, they would have already done it.

This is kind of the point I was trying to make. The amount of people who would jump ship, both home and enterprise users is far too much for them to consider a walled garden approach at this point, hence why they won‚Äôt do it.

>	That‚Äôs why they‚Äôre pushing UWP for mainline applications.

MS is allowing win32 apps on the store now. It seems like they‚Äôre giving up on UWP, or at least not pushing it as hard anymore.

Idk I just really don‚Äôt see it ever going walled garden. They don‚Äôt have the luxury that apple had with iOS in that people had no expectations of being able to freely install software. When I buy/install windows, I expect to be able to install whatever I want to on it.

Technically it IS possible but IMO it‚Äôs pretty unlikely to happen.
> That's why they're pushing UWP for mainline applications.

Microsoft is officially abandoning UWP and pushing developers to Windows App SDK and WinUI 3. The Windows App SDK is more decoupled from specific Windows releases and Microsoft has promoted a 3rd party that enables WinUI 3 applications to run on non-Windows platforms.
>	But denying empirically proven market incentives is just as bad.

There is no market incentive. Enterprise users will drop them faster than you can imagine if they pulled this, costing them unimaginable damages to financials. It‚Äôs a horrible idea, that‚Äôs why it hasn‚Äôt happened.

>	I haven‚Äôt really looked at this sub in the last two months because it has simply become uninteresting, but I think you are victimising yourself.

It‚Äôs just annoying when you provide people with statistical data that games run virtually the same between the two OSes and people just deny it outright. At the end of the day it‚Äôs just peoples opinions/preferences, not really anything to get really upset about.
> Banning toxic behaviour would quickly become more toxic 
  
So no moderation at all, since any moderation "becomes a battle of orthodoxy"?  
  
Toxicity really has become normalized.
> Not really. Your attitude is the perfect example of how modern civilization has made our men so soft that they crumble in the presence of competing ideas.  
  
`*triggered*`  
  
I am not talking about "competing ideas", but right out nasty attacks on open source developers without merit whatsoever.  
  
But hey, just let anti-linux people roam free here on /r/linux, sowing discontent.
Finished watching the talk, thought it was well thought out and compelling. 

It didn't change my mind about what I dislike about the project, but I felt like it did a good job justifying its growth outside of init and supervision.
I'll check the talk sometime.

The very first thing I pointed out was a contrast in it not being minimalist.

The src directory has approximately 750k lines of code with implementations of many, many previously standalone utilities and programs.

I guess we could split hairs over the meaning of a monolithic code base, but I'm inclined to say that if I have to wait for a fix to the systemd project and packaging for say, a hypothetical bug in sysctl, it makes the systemd project a monolithic replacement of previous standalone applications.

At the moment, these are all of the aspects maintained in the systemd codebase as stored in the `src` directory:

```
ac-power		debug-generator		id128			notify			rfkill			sysv-generator
activate		delta			import			nspawn			rpm			test
analyze			detect-virt		initctl			nss-myhostname		run			timedate
ask-password		dissect			integritysetup		nss-mymachines		run-generator		timesync
backlight		environment-d-generator	journal			nss-resolve		shared			tmpfiles
basic			escape			journal-remote		nss-systemd		shutdown		tty-ask-password-agent
binfmt			firstboot		kernel-install		oom			sleep			udev
boot			fsck			libsystemd		partition		socket-proxy		update-done
busctl			fstab-generator		libsystemd-network	path			stdio-bridge		update-utmp
cgls			fundamental		libudev			portable		sulogin-shell		user-sessions
cgroups-agent		fuzz			locale			pstore			sysctl			userdb
cgtop			getty-generator		login			quotacheck		sysext			vconsole
core			gpt-auto-generator	machine			random-seed		system-update-generator	veritysetup
coredump		hibernate-resume	machine-id-setup	rc-local-generator	systemctl		version
creds			home			modules-load		remount-fs		systemd			volatile-root
cryptenroll		hostname		mount			reply-password		sysupdate		xdg-autostart-generator
cryptsetup		hwdb			network			resolve			sysusers
```

Sure seems to have a lot more than say, System V init.

Am I a fan of systemd taking over the ecosystem? No. But I'd not throw me in the hater category.
It absolutely is a monolith. The claims against this are basically just "there are separate binaries for different things so its not a monolith".

If systemd wasn't a monolith distros like Gentoo wouldn't have to waste time forking logind and making it run standalone.

Try using journald, networkd, resolved, timesyncd, systemd timers, etc without the entire systemd software suite running. You can't run any of these standalone in any reasonable way.

I am not a "systemd hater", I prefer systemd in general but it's definitely a monolith. Being a monolith isn't always a bad thing but I think its lame that most of the subsystems are so coupled to the monolith.
Am enjoying the talk thus far, thanks for the recommendation.
If systemd isn't a monolith it should be possible to use only the init system as instance. But I never saw a distro like that. Elogind was extracted from systemd but needed a name change, logind to elogind.
all of his talks are really good.
So I can run any part of systems without any other part? IE it is completely modular and nothing depends on other systemd things (I can replace any piece of systemd and it will work with any module)
That is one aspect of it, but not the only aspect of it. Again from the link:

> The Unix philosophy is documented by Doug McIlroy[1] in the Bell System Technical Journal from 1978:[2]
>
> 1. Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new "features".
> 2. Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently columnar or binary input formats. Don't insist on interactive input.
> 3. Design and build software, even operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them.
> 4. Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them.

You're obviously commenting about #2, but there is more to it than application chaining on IO.
Try running any one of the components you listed on a computer that doesn't run systemd.  You cannot.  These components are not modular.  They are tightly coupled and bad from a software engineering perspective.

Some people say 'monolithic' when they mean 'not modular'.  They are not quite the same.
I think your assumptions are off here.

Edit: I'm glad your experience has been better. I've found it to be positive in some cases, a wash in most cases, and worse in few cases.
Nice appeal to popularity argument
Systemd is definitely much better when compared to PulseAudio and Avahi, especially at launch. My main issue currently with it is how there are certain features that are characteristically "Poettering": highly interesting, but just not implemented well enough to actually function properly and there's the risk that he'll just go on and add another interesting feature instead of polishing it to working state. Like systemd-homed. I really want it to actually work. Right now it's half baked at best.
To be fair, the last main moderator around here had a lot of weird automoderation rules and not all of them may have been quite cleaned up yet after he got putsched off here.
Phoronix absolutely can be spammy. I remember a lot of their posts were just mirroring newsletter posts with no meaningful commentary.

They do also have more substantial posts, but I don't blame cap in this case of not wanting to deal with the workload of removing the low effort posts.
>/r/linux considers Phoronix a "spamblog", wow.
>
>Mods being mods I guess. Par for the course.

Basically every techy subreddit considers Phoronix blogspam outside of their benchmarks.
[deleted]
Just internet janitor things
Agreed. Phoronix should one of, if not the most, respected news source on this sub!
It typically is second hand news which is worse than just going straight to the source. 

It should be the policy that's enforced rather than the website itself but that's harder to automate.
There are some people who really dislike Phoronix for their benchmark results.
CAP and I came to this decision together. When Phoronix News was blacklisted, it was very common to see the news articles copy stories from other sites and only link to other Phoronix articles that we're also copied. There was little to no original content aside from the benchmarks (which is why those are allowed).

It may be time to revisit this if Phoronix has started citing outside sources and doing original stories.
Being an r/Linux mod (although, not an active one), IIRC, those auto mod rules were in place before CAP was an active mod.
Phoronix has an issue of self referencing their own posts over and over so you click a link thinking it will get you to the source but just redirects to another phoronix post.
Great. leave them for other operating systems. Keep your dirty Microsoft inspired "software philosophies" of off my Linux! What's next Hungarian notation in the Kernel?
Your kneejerk reaction is pretty telling my dude.

Let me know how a Microsoft-controlled Linux userland goes in the next 5 years.
What a piss-weak, feable-minded strawman post.  It is just ridiculous.

There are no shortage of reasons why systemd is hated by those who hate it.  The article doesn't cover them all.

The simple, truth is that systemd is incredibly bad for software / linux freedom (as in speech).

It is a cancerous pile of shite, not unlike your post.
There is a _lot_ of anti-gnome sentiment out there, almost as much as anti-systemd.

Gnome is universally hated by everyone, except those who use it.
Like keeping your files for "ransom" (windows 11 by default deploys bitkeeper encryption on the C: drive and ties it to whatever MS cloud account you signed in with or created on first boot).
Not to mention if it's such a simple fix and they refuse to fix it, it could be seen as intentional.
>	So, if I understand correctly, you are saying that it‚Äôs not in the interest of corporations to engage in monopolistic practices in order to achieve a monopoly.
    There are only a few things where virtually all schools of economic thought, from Marxists to free market fundamentalists agree on and this is just one of them.

Then why does Google not block sideloading in Android despite 99% of users just using the play store anyway? Why does Valve allow Windows/other apps than Steam to be installed on the Deck? Why does Apple still allow installs from outside the Mac store? Are these companies just more altruistic than MS? Why are we not concerned about these companies moving towards monopolistic practices but we are with MS?

>	What really happened is that Microsoft‚Äôs market research has shown that locking users into their own store would currently do more harm than good

When is that ever going to change? When businesses go extinct?

You do realize there are far too many applications out there, from developers that might not even exist anymore that would never get a Windows store release that businesses rely on every day to function? Under what circumstance would Microsoft just yank the rug out from underneath the companies that make them as wealthy as they are?

Idk I just don‚Äôt ever see windows going walled garden. For Apple, they had the golden ticket in the sense that nobody had expectations of being able to install anything on iOS. With windows, I have the expectation I can install anything. If I can‚Äôt, I would use something else. Just like you said at the end.
So from what I understand, it's aggressive monopolization? Isn't Google doing a similar thing with Chrome?
> There is no market incentive. Enterprise users will drop them faster than you can imagine if they pulled this, costing them unimaginable damages to financials. It‚Äôs a horrible idea, that‚Äôs why it hasn‚Äôt happened.

Market incentives are independent of the current market situation. Just because the market does not give you the opportunity to do something, does not mean you don't want to do it.

> It‚Äôs just annoying when you provide people with statistical data that games run virtually the same between the two OSes and people just deny it outright.

In general, I don't think it's worth having high expectations for a general gaming community that is only bound together by their desire for a specific device. In such cases, communities inevitably go downhill.

Edit: Just saw you were downvoted, it wasn't me. In positive debates I don't downvote the other person, and I never use downvote to express disagreement.
Are you an SJW? Because "toxic" is a dog-whistle word invented by SJWs to drive their witch hunts against rational people's competing ideas.
Who are these  is anti-linux people?  I am vehemently opposed to systemd, but linux is awesome.  Systemd is not linux, at least _not yet_.
Love that you watched it! I didn't mean to imply that it would change your mind about what you dislike, just that the vast majority of the hate is ignoring the realities and complexities of things. Which makes the reality of what we have called an init system a bungled mess of shit that is _not_ purely an init system. Even if we like to lie to ourselves that it is.
I would be careful to note that while these are developed together, they are not the same tool, nor are they particularly strongly coupled in implementation. As apt as I am to mock it for this same thing, systemd is a suite of tools, not just an init system. Many of those tools are surprisingly minimal (think "I'd expect this level of functionality from a busybox command, not *you*") and self-contained.
* They are separate binaries
* built from separate source code
* and can be turned on/off at runtime or build time

That's *literally* the definition of a modular design, as opposed to monolithic design where everything is a single unchanged bloc.

Just because the components rely on specific APIs that are only provided by the base package doesn't make them a monolith. A module made for systemd is still called a module.
Strong coupling doesn't make it a monolith. That's so ridiculous that you're saying things that depend on each other make them a monolith.
Strong coupling != monolith. You can have things dependent on each other but that doesn't make them a monolith.
The only necessary parts of systemd are journald for logging and udevd for kernel events. Everything else can be stripped at build time or run time.

You never saw a distro like that because the additional components that systemd brings are actually useful..
Agreed!
Great post, thanks.  Haven't see that before.
> Try running any one of the components you listed on a computer that doesn't run systemd. 

That's not why "do one thing and do it well" is the maxim. The idea is to prevent things like `df` being modified to support ejecting your cdrom or unmounting devices. It's basically saying to stick to some specific task and just concentrate on doing that specific task as well as you can but if you notice a gap in functionality somewhere then either expand/invent some _other_ tool rather than adding functionality that's only tangentially related to the tool you're looking at.

>  These components are not modular

Composability is mentioned in the [same documents](https://en.wikipedia.org/wiki/Unix_philosophy#Origin) but it's itemized separately from "do one thing and do it well." They phrase it in terms of expecting outputs to become inputs and vice versa because I don't know if the modern idea of composability existed at the time they were writing.

But in terms of modularity, I'll point out that systemd actually routinely gets overrule when it comes to NetworkManager. I don't know of a single mainstream distribution that would use `systemd-networkd` over NetworkManager at this point.
For me, mostly it has been worse but fair point. Everyone has a different reference point.
What's half-baked about it?
This, CAP had his reasons but he made being a mod here for awful with his rules.
But that's what a news site does. Lots of reports in general news media are just writing up press releases. The point is you get it all in one place and they filter out the total nonsense.
Because 90% of their "articles" are exactly that.
r/literally
Lol, you never heard of LWN ?
Surely the same would apply to LWN then?

Or major tech blogs like ars technica, the register etc
It's bizarre because that's how news in supposed to work. They read the LKML so we don't have to.
> It typically is second hand news which is worse than just going straight to the source.

What is this nonsense? What if I'm not interested in reading hundreds of pages of mailing lists to get an overview over what's happening in Linux?

There is huge journalistic value to aggregating, summarazing and contextualizing news from other sources. Basically that's what journalism is. So what's the problem with Phoronix again?
I get it, thank you for explanation.
Wasn't it also because of the quantity of links that were being posted?
No WaYlAnD caN't Be SlOwEr *blocks phoronix*

/s
I guess that might be a valid reason.
Thank you for taking the time to reply to me, I was not expecting that. I also appreciate the explanation. 


/u/michaellarabel is it time to talk about this with the mods to see if your content meets their expectations? Might be the case that it still doesn't, but I know Phoronix has been banned for a long, long time on this subreddit and if things have changed, it would be great to see your stuff here again.
TBH the "primary sources only" rule is quite silly. What if I'm not interested in reading hundreds of pages of mailing lists to get an overview over what's happening in Linux or to get interesting things I wouldn't have noticed?

There is huge journalistic value to aggregating, summarizing and contextualizing news from other sources. Basically that's what journalism is, and that's exactly what Phoronix does.
I was actually made a mod by CAP so wouldn't know what the rules were like before him.

The 2 years I was actually active moderating here it seemed like CAP and myself were the only active ones.

Not entirely convinced his removal is a full net benefit, but he did need to ease up on his ideological approach to FOSS and Linux.
[deleted]
I guess the reason gnome haters are quieter than systemd haters is because distros let you choose your DE, but not init.

I was more meaning that the link you posted accused systemd of corporate interests and manipulation, but no one seems to accuse gnome of doing the same.

Apparently the debian devs in 2017 saw gnome and systemd as being the same thing (probably because what sparked the init debate in the first place was gnome's hard requirement on systemd):

>We believe this situation is also the result of a longer process leading to the take-over of Debian by the GNOME project agenda. Considering how far this has propagated today and the importance of Debian as a universal OS and base system in the distribution panorama, what is at stake is the future of GNU/Linux in a scenario of complete homogeneization and lock-in of all base distributions.
This is pure speculation:

`LP will implement systemd-homed, with TPM requirements, much like Windows 11; {D,H,S}aaS can be implemented because Grandma & Grandpa won't want to lose their photos, businesses won't want to lose their documents/taxes, etc.`

I don't put it past any corporation to push their agenda, e.g.:

https://www.uctoday.com/collaboration/room-kits/microsoft-teams-introduces-device-as-a-service/

>Why does Apple still allow installs from outside the Mac store? 

They don't on iOS. Not a good example.
I feel you're overreacting a bit. I started my comment by saying that one of the most basic incentives for companies is to achieve monopoly. Which makes perfect sense if you think about it, or if you look at long-term economic analysis and data. Piketty's Capital in the Twenty-First Century is an excellent empirical database, even if you don't agree with the author's Keynesian conclusions. At this point in my comment I didn't even mention Microsoft, I was talking about a structural matter.

In the rest of my comment, I was of course making an example of Microsoft, since that was the subject of the topic, but the logic and incentives of the market still applies to everyone. Including Google, Apple, Valve, even the grocery store across the street, because that is one of the driving forces of competition, and market economies are based on competition, and the purpose of competition is to "win", in other words to create monopolies. This obviously implies that companies of similar calibre will therefore try to weaken your market position, making it harder to create monopolies.

You can easily answer your specific points if you look into them a little more, but I have no desire to dwell on individual examples. Unless you are a manager of a company, the economy should be interpreted at macro level. Obviously microeconomics has its place, but it's not relevant in this case. I only address the "walled garden" Windows question, since this is where the thread started. I don't think Microsoft will be able to close Windows, I don't see them gaining such a strong position in the near future. That being said, the incentive is there. Not because it is Microsoft, but because it is a company.

Edit: typo
Yeah google pretty much is.
MS browser-related shenanigans were worse. They basically forced you into their ecosystem just by the fact that a bunch of websites didn't work at all outside of IE on Windows.

What Google is doing is very annoying and *potentially* pretty bad but the FOSS nature of Chromium makes it a much smaller threat.
I only want it to be an init system nothing else
Subsystems being separate binaries literally doesn't matter because you can't use them without systemd anyways.

The kernel can build drivers into modules that get loaded at runtime but we still consider that monolithic. Why are we changing the definition for systemd?
You can swap out modules for other systemd-specific modules?  Big woop.
What exactly is a monolith if it isn't a big blob of software of which parts can't be extracted out to run standalone?
So let's just completely decouple all of them and no more issues
A sieamese twin conjoined in the head could theoretically be separated but we just can't in real life.
Why can't you strip journald and use another logger?
I wasn't specifically talking about 'The Linux Philosophy', but more about how systemd is bad from a software design perspective when it comes to modularity and reusability due to its tight coupling and dependency tentacles.

It's not entirely unrelated though, as clearly the 'less things software does', the more reusable it is.

edit:  Also, from what I've heard, Network Manager is also a pile of poo.

But again, if a distro wanted to soley use systemd-networkd in isolation, instead of Network Manager, they cannot do so without pulling in the other systemd dependencies.  That is bad design.
Here you go: https://www.reddit.com/r/systemd/comments/ubpz5l/homed_is_still_not_ready_user_feedback/
Sure. I've been a journalist before. I'm familiar with all manner of fluff. I've even turned press releases into stories. But I wouldn't say that's "what a news site does". News is about engaging with the sources and bringing useful insight to your readers. I've had stories get dropped before because some organization was so tightly PR controlled that they refused to have real people engage with us.

You're right that journalism requires an eye for details that your readership actually cares about‚Äîwhich is not a regurgitated press release. It also takes work. Follow-up.

If you're just rewording a pull request or whatever, then you aren't adding any additional value and Reddit users are better off sharing a link to an actual mailing list archive so they can more easily follow the surrounding threads to get that additional context.

And that's ultimately what /r/linux mods have to contend with. Because their job isn't to care about Phoronix. Their job is to best serve **their** readership. If Phoronix is just a mailing list with a bad interface‚Äîeven after having provided the bare minimum service of filtering for interesting mailing list posts, your better off linking directly to the actual mailing list.
I had a fad of being featured in phoronix articles a decade ago. I was frustrated by words I carefully picked being reworded to make them more clickbaity. A journalist providing context is good, but I can definitely see both sides to the argument.
You're free to get the overview from Phoronix, but you're required to reference the original source when linking something on r/linux.

That actually seems a fair rule. The news is kept free from interpretations and r/linux doesn't become a free space for blog Ads.
Partly, yes. But personally I don't think that's grounds for banning a whole website. If certain users are posting far too many links from it then persuasion and letting people know too many would not be appreciated would be a much better way to deal with the problem.
Another issue we had way back when was that /r/Linux would turn into /r/Phoronix. The front page would be nothing but Phoronix links.

I mean, will of the community and all, but it does become an issue at times.
Yes, that is true. But the point of the rule would be the submitter submitting the link directly to the mail in question, then explaining the context in the comments.

Or making it a text post with relevant links. Essentially the article is being written on Reddit.

The reasoning is to promote the community to discuss the topic and have the OP interact with the community.
Contrary to what CAP was saying. This place hasn‚Äôt been burned to the ground by transphobic trolls and most things are naturally upvoted and downvoted.

CAP had clear powertrip issues and this sub is infinitely better with him gone.
> Not entirely convinced his removal is a full net benefit

the automod spam being nuked from orbit is enough to make his removal worth it.
Oh but it is not.  Sure, it appears to be 'just as free as ever' for you because you've already drunk the coolaid.  But this is not about you.  Take off the rose coloured glasses.

Instead of having the *freedom* to *choose* an init system that suits me (for example, during install), I cannot.  Thanks to RedHat and Poettering, I can no longer run a single major modern distro without being forced to use SystemD.

It is severely crippling to software freedom and is  an absolute indictment of the software.  What is worse is that it is by design.

If I want to avoid systemd and its tentacles, then I am shit out of luck.  I need to use an entirely different distro with no major funding and very few developers.

If you or anyone else think it's fine to limit the freedom of others because of your beliefs (or your corporation's), that says an awful lot about you.

Systemd is a cancer designed to make Linux a monoculture.
Organisations can install apps outside of the App Store. Microsoft could do the same thing, but I didn't want to go into that because my point really wasn't about a specific company.
Fair but I feel that so many web devs don't even bother about non chromium stuff, making webpages behave weirdly on Firefox
> doesn't matter because you can't use them without systemd anyways.

There's no such requirement to be considered modular. You also can't run GNU Coreutils without Glibc, does that make them a monolith?


We're not changing the definition, *you*'re comparing apples to oranges.

* A kernel can be either *monolithic*, hybrid or microkernel.

* A program can be *monolithic* or modular.

* An application can have a *monolithic* architecture or a microservices architecture.

Just because they share the same term, it doesn't mean the same definition.
Fedora replaces timesyncd with Chrony; Ubuntu uses dnsmasq instead of resolved; NetworkManager is often used on desktop instead of networkd.. other components can be disabled in favor of hardcoded values or shell scripts..

You can swap almost everything without being tied to systemd, and you most likely knew this, so you didn't have to be snarky about your comment
Can I use the modules without systemd
Systemd is literally a suite of applications that all work together. The fact that they depend on some features of others or what have you, doesn‚Äôt make it monolithic. They are strongly coupled. That‚Äôs what this methodology is. They are technically independent applications but depend on functions from others to run. Monolithic applications are a single application. Not specifically single binary though. 

It‚Äôs like vertical integration in effect. All these independent yet tightly coupled things. They aren‚Äôt easily plucked out because they‚Äôve been optimized to run with certain expectations.
Lmao keep on with false equivalency!
you probably could do that by editing the source code, but currently it's considered a core component of systemd and can't be turned off with [the available build options](https://github.com/systemd/systemd/blob/main/meson_options.txt).
(Note that it's possible to keep journald and redirect logs to a different logger.)
> I wasn't specifically talking about 'The Linux Philosophy', but more about how systemd is bad from a software design perspective when it comes to modularity and reusability due to its tight coupling and dependency tentacles.

Unfortunately for this idea they are often loosely coupled which is what makes things like using NetworkManager instead of networkd possible and why you can implement [logind](https://wiki.gentoo.org/wiki/Elogind) as a separate non-systemd daemon.

For example resolved responds to changes in networking caused by NetworkManager, running `journald` doesn't preclude running syslog, etc, etc.

>  Also, from what I've heard, Network Manager is also a pile of poo.

Nobody who had much experience with NM would have told you that.

> But again, if a distro wanted to soley use systemd-networkd in isolation, instead of Network Manager, they cannot do so without pulling in the other systemd dependencies. That is bad design.

If you want the different components to interact to a meaningful level they have to make certain assumptions about how the other components behave and how they fit into the overall system. This is going to require at least the initial components written to be complementary of one another. No complex product outside of Linux distributions _actually_ functions by the principles you think it does.
So then don't use homed? How does that affect PID 1?
There absolutely is journalistic value in providing a selection and more approachable presentation of patch notes and so on. I don't have the time to read through everything happening in the gnu world. So I have journalists I trust with making that selection for me.

Posting a link on Reddit and then providing a tldr in the comments is basically the same thing.
> That actually seems a fair rule. The news is kept free from interpretations and r/linux doesn't become 

Interpretations are a crucial part of news. Links to the original source are usually worthless for the vast majority of readers because they don't have the context for why a particular statement or fact from source matters.

I disagree that this is a fair rule, and I'm clearly not the only one. I think it it reinforces elitist and isolationist attitudes of this community and makes this subreddit a less interesting place.

> a free space for blog Ads.

Not all links to blogs are "blog ads" and a lot of blogs provide significantly more information and context than the original source.  So why not judge based on the merit of an individual blog post or link instead Instead of blocking everything based on personal prejudice?
Totally fair, and I agree that I don't want to see that either.
> The reasoning is to promote the community to discuss the topic and have the OP interact with the community.

I see the point, but a good article can promote the community to discuss the topic just as well.

> Or making it a text post with relevant links. Essentially the article is being written on Reddit.

I can see the desire, but there's a whole internet out there with other people who invested a lot of time in their own website. Would be a shame to let interesting news go unnoticed by this subreddit just because they were't created here.
It hasn't happened though.
[deleted]
Good post overall, I just wanted to add that there's another layer to this. Systemd, PulseAudio, PipeWire, and Wayland are all under the FreeDesktop banner, and if you haven't noticed, only FDO projects gain traction in the desktop world, because FDO is controlled by the major distros.

The choice is truly XDG or freedom. XDG has micro standards, env var standards, consolidates on dbus, etc. In terms of ecosystem, Linux has XDG and non-XDG.
Use Slackware or Gentoo, or Void, or Artix if you must.

Slackware is great, that's what I use. It's still bad that the pressure is being exerted on distros to adopt systemd, but to say that we're already forced to use systemd across the board isn't necessarily accurate.
This is correct.
Let's decouple them and everything would be better allow fully modularity and no need to every use any part of systemd besides the individual things you want
You're the one who posts false statements.

> Strong coupling != monolith

Systemd only exists as a monolith.  There are no examples of frankeinstein systemd variants, which linux is all about.
If it can use a different logger then journald isn't needed at all that is just a dumb requirement.
It doesn't affect pid 1? I wasn't talking about pid 1. I was talking about the systemd project.
It doesn't, they're just looking for reasons to whine
Right?  homed is brand new, of course it might be a little rough
Yeah, it's definitely a balancing act. Thanks for the thoughts on the topic, it's something we'll discuss.
[deleted]
> Nobody owes you a modern distro without systemd

You still don't get it.  A distro (or OS) is not its init system.  We *had* modern distros without systemd.  You previously could switch between inits at will.   Systemd took that freedom away after selling us the idea that systemd would be optional.  This is why it is so incredibly hated.  We both could have had what we wanted.

But, you do you.  Keep slurping from the teat of your corporate overlords, and hand-wave away the injustice of your corporate hoofs treading  on others.
Nah journald is what collects logs and redirects them to a different logger, it's still needed
You're complaining that a brand new part of systemd isn't perfect?
Yes that one is gone, but I do personally feel more things could have been changed to encourage engagement with the wider Linux community.

Phoronix and gaming on Linux both being banned is petty in my opinion.

Additionally support requests are still banned, but it isn't enforced, only removed if it hits the report number for removal.

Minimal actual manual moderation taking place from what I can see.
Why do you care so much about swapping out init systems?  Find another hobby, perhaps; since I started using Linux desktop in 2003, things have only gotten  better / more stable with systemd.  Anyone who says old bash init scripts were "easier to work with" must be looking at different things than I used to.  Back on Slackware, things were (are?) held together with "sleep" statements just to get the system to boot consistently.
If the other init systems were worth swapping to, they'd be nearly as competitive. They're just way behind in comparison. Have you wondered *why* every major distro chooses systemd? 

Work on supporting the alternatives and show with examples that they're worth a damn to use over systemd if you really want to convince anyone. As far as I've read, you dislike systemd because it's popular. 

Developers aren't forced to support half-baked inferior init systems just so you can feel good that you're using another option for the sake of it. There's no pragmatic reason.
Really well argued.  You are correct.
[deleted]
How about just use that other logger completely? Why waste effort being duplicate when you could just let another logger do the work
>complaining

You're projecting.

>brand new

2019 isn't brand new.

>isn't perfect

Losing your whole home is more than 'isn't perfect'.

___

Look, Avahi and Pulseaudio had glaring issues when they were "brand new" as well. I'm just saying these two improved a lot when Poettering left them to the community.
[deleted]
So because *you* don't care about the init system, nobody else should?

Look, run your hardware how you want, but don't go around defending choices being removed, especially since you don't even take those choices yourself and just accept what you're fed.
Let me know how a Microsoft-controlled userland looks in 5 years.
When did you honestly last try one of the other inits?  Have a look at OpenRC for starters.

edit: I just saw your flair.  Don't bother.
Switching Arch to runit wasn't that bad. You're leaving out your flair, too. Gentoo *does* support init system choice.

Your trolling's weak, dude. Get better arguments.
Another bullshit strawman.  I'm out.
Because systemd is deeply integrated with journald, where you get the last logs in systemctl output and all. You can redirect specific service logs using `StandardOutput=syslog` but not much beyond that
Which could be resolved with actual active moderating.

Content being allowed is one thing, being obviously spammed to generate web traffic is another entirely.

The rule of self promotion comes into use here which I agree with.
Noone is stopping you from changing init systems, do the work yourself if you are so passionate.  It's just that distros (who often have super limited / volunteer resources) have chosen systemd.  Your choices are to accept the work they give you (for free), or do it yourself.

It's just amazing to me that someone has come around to write a revolutionary piece of software, that was very much needed, and the toxic super minority still find reasons to complain.
Seek help
I have. I've tried practically every major distro when I was into distro hopping.
Such deep integration means that a better logger can't gain traction since it would have to also replace systemd
Why should an init system be integrated with a logger? A crappy binary logger that violates everything about UNIX tradition.
Many of the objections boil down to system design (technical) and the social habits of the project management. The way it was advocated for (i.e. marketed) and pushed to be default. The "gentle push"es to manipulate the software people depend on. That shit is creepy commercial behavior.

Another facet is the aspect of choice. You can preach that free software "isn't about choice", but systemd would not have gained its position without choice being available. Now its feature creep is creating essentially vendor lock-in, and every major distro is at the mercy of the systemd project, from a very low level in the stack. Moving away from systemd is painful to those who invested in it, and that is deliberate.

I *do* do the work myself to change init systems. Don't assume things about me based on an argument. It still stands to reason that distros should *supply* other init systems, at least as packages. They aren't necessarily beholden to *supporting* them.
Because systemd is a lot more than an init system? And you need the absolute recent state of services in a service manager?

And what's the point of crying over the UNIX philosophy when Linux itself is a big fat monolithic kernel that goes against the UNIX philosophy anyway.

Come on, the systemd hate faction should come up with new "arguments" already..
Very well said.
UNIX philosophy obviously applies to userspace programming. Most traditional UNIX kernels are monolithic.  And I don't need a "service manager", I need an init system. If I decide to run services I can write systemv scripts for them. Did you know you can use "ps" command to check the state of you "services"? Why is the feature maybe useful for web hosters was forced upon millions of desktop users who don't run any critical "services". Luckly there is still Gentoo.
>I don't need a "service manager", I need an init system. If I decide to run services I can write systemv scripts for them.

Good for you. So, why are you complaining about systemd again?

You have Gentoo, Devuan, Artix, Alpine... whatever works for you. I personally would never want to return to managing some shell scripts pretending to be an init system.

You don't like systemd, others are happy with it. It's time to accept that.
